---
title: 大数据平台常见异常处理汇总
author: 佳境
avatar: >-
  https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/custom/avatar.jpg
authorLink: shmily-qjj.top
authorAbout: 你自以为的极限，只是别人的起点
authorDesc: 你自以为的极限，只是别人的起点
categories:
  - 技术
comments: true
tags:
  - 大数据平台
  - 异常分析
keywords: 异常分析处理
description: 总结平台维护与异常处理过程
photos: >-
  https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/Bigdata-Problems-cover.png
abbrlink: BigdataExceptionsSummary
date: 2021-01-30 12:50:00
---
# 大数据平台常见异常处理汇总  
本博客记录工作中遇到的，大数据相关各个组件的异常处理过程，养成良好的问题归纳总结习惯，累积问题解决经验与思路。

## Spark相关
1. Shuffle异常导致任务失败
报错：<font size="3" color="red">org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1</font>
原因：
shuffle分为shuffle write和shuffle read两部分。
shuffle write的分区数由上一阶段的RDD分区数控制，shuffle read的分区数则是由Spark提供的一些参数控制。
shuffle write可以简单理解为类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上。
shuffle read的时候数据的分区数则是由spark提供的一些参数控制。可以想到的是，如果这个参数值设置的很小，同时shuffle read的量很大，那么将会导致一个task需要处理的数据非常大。结果导致JVM crash，从而导致取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。有时候即使不会导致JVM crash也会造成长时间的gc。
解决思路：减少shuffle的数据量和增加处理shuffle数据的分区数
①spark.sql.shuffle.partitions控制分区数，默认为200，根据shuffle的量以及计算的复杂度提高这个值 shuffle并行度 
②提高spark.executor.memory
③map side join或是broadcast join来规避shuffle的产生 
④分析数据倾斜 解决数据倾斜
⑤增加失败的重试次数和重试的时间间隔
通过spark.shuffle.io.maxRetries控制重试次数，默认是3，可适当增加，例如10。
通过spark.shuffle.io.retryWait控制重试的时间间隔，默认是5s，可适当增加，例如10s。
⑥类似RemoteShuffleService的服务，解决Shuffle单台机器IO瓶颈，记录Shuffle状态，大批量提升Shuffle效率和稳定性。

2. SparkSQL报awaitResult异常
报错：<font size="3" color="red">org.apache.spark.SparkException: Exception thrown in awaitResult</font>
原因：广播数据超时 
解决：spark.sql.broadcastTimeout=1200 默认大小300

3. HiveOnSpark不能创建SparkClient及Return Code 1异常
报错：<font size="3" color="red">FAILED：SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.</font>
<font size="3" color="red">Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask</font>
原因：以上报错证明初始化Spark失败，而以前不会失败，所以大概率是资源问题而不是代码问题，查看Yarn队列发现所提交的队列已满且已超过能申请资源的上限（虚线部分），故任务启动失败
解决：CM界面->群集->动态资源池配置->提高队列的资源权重（上限也会响应提高）->刷新动态资源池配置


## HDFS相关
1. 数据块丢失且命令无法修复
起因：多张表查询发现如下报错，提示块丢失
![alt](https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-01.png)  
分析：CM界面看HDFS丢失块，发现有2500多，大批量块丢失可能的原因：
    1.DataNode与NameNode未通信，DataNode进程未启动
    2.DataNode数据磁盘损坏，数据丢失
解决过程：
尝试修复丢失块：hdfs debug recoverLease -path <path-of-the-file> -retries <retry times>
显示修复成功，但使用hadoop fs -text <file_name> 还是报MissingBlock无法读取
使用fsck检测坏块 hdfs fsck /user/hive/warehouse
![alt](https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-02.png)  
发现绝大多数block名称都带有172.xxx.xxx.11 定位到可能是172.xxx.xxx.11节点的DataNode可能存在问题
通过CM日志和机器上进程状态判断172.xxx.xxx.11的DataNode已与NameNode保持心跳，运行正常，进而怀疑磁盘坏了（概率太小）
查看CM配置和机器磁盘，发现少配置了些硬盘路径，原因是在配置新节点磁盘路径时误修改整个配置组的磁盘路径，导致该配置组中所有DataNode缺少磁盘，进而出现块丢失且无法修复的问题。
解决：还原磁盘配置，滚动重启该配置组中的DataNode，将不同机器配置分成多个配置组，重新修改配置
重启过程中丢失块数一直在减少：
![alt](https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-03.png) 
最终恢复正常
总结：
  1.CM上修改配置一定要慎重，注意修改配置组中某台节点的配置会影响整个配置组中所有节点的配置
  2.CM显示DataNode重启成功只是进程启动成功，但日志出现“Total time to add all replicas to map”字眼才是真正完成启动
  3.https://hdfs-site/dfshealth.html#tab-overview 从HDFS WebUI获取更多信息（丢失块的信息一目了然）

2. Win开发Hadoop环境winutils
错误：<font size="3" color="red">Could not locate executable null\bin\winutils.exe in the Hadoop binaries</font>
解决：将winutils.exe放在HADOOP_HOME\bin下，然后代码里System.setProperty("hadoop.home.dir", "D:\\Programming\\Env\\Hadoop\\hadoop-2.7.2\\")或设置环境变量HADOOP_HOME和PATH后重启电脑
winutils.exe下载地址：[winutils-master](https://github.com/steveloughran/winutils)


## Yarn相关
1. 应用提交报<font size="3" color="red">Retrying connect to server 0.0.0.0</font>  
原因：应用没有认到yarn-site.xml或者yarn-site.xml配置不正确
解决：①指定HADOOP_CONF_DIR ②确认yarn-site.xml
```yarn-site.xml
 <property>
     <name>yarn.resourcemanager.address</name>
     <value>master:8032</value>
 </property>
 <property>
     <name>yarn.resourcemanager.scheduler.address</name>
     <value>master:8030</value>
 </property>
 <property>
     <name>yarn.resourcemanager.resource-tracker.address</name>
     <value>master:8031</value>
 </property>
```

2. 内存不足Container退出
报错：<font size="3" color="red">Diagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.Dump of the process-tree for container_e23_1574819880505_43157_01_000001</font>
分析："physical memory used"为物理内存占用（应用已占满9G），"virtual memory used"为虚拟内存占用，18.9GB是取决于yarn.nodemanager.vmem-pmem-ratio（yarn-site.xml中设置的虚拟内存和物理内存比例，默认2.1），报错是因为物理内存不足，是任务设置的内存少了
解决思路：
①如果资源充足，增加任务并行度分担任务负载
②增大任务可用资源（注意不要超过单台NM可分配上限yarn.scheduler.maximum-allocation-mb的值）
③适当增大yarn.nodemanager.vmem-pmem-ratio，适当调高虚拟内存比例
④[不建议]取消内存的检查：在yarn-site.xml或者程序中中设置yarn.nodemanager.vmem-check-enabled为false

## Hive相关
1. HBase外部表报<font size="3" color="red">Unexpected end-of-input</font>
起因：使用Hive创建HBase外部表时正常，但使用HBase外部表时报Unexpected end-of-input: was expecting closing
分析过程：翻阅源码部分发现异常是在解析外部表创建JSON时发生，于是对比建表语句和Hive元数据库中的TABLE_PARAMS表信息得到原因
原因：创建hbase外部表catalog太长导致schema太长，而hive元数据表mysql里的table_params字段param_value字段类型是varchar(4000)建表时由于schema太长，超过4000字符的部分被截断。而使用该表的时候会读元数据，但因为元数据不完整而报错。
解决：
①分多次建表
②改varchar(4000)为longtext然后重建表（影响无法评估，没尝试）

## HBase相关
1. 集群新增RS致另一RS异常退出
![alt](https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-01.png) 
上线的RegionServer会触发Region移动，报错前有Flush操作，因为Region移动前会先Flush Region
异常相关源码：
![alt](https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-02.png)  
Flush Memstore到HFile这个过程未发生异常，但Flush一个Memstore后跟踪Memstore总大小未发生变化，即内存清理失败 超过5次就中止这个RS
分析：版本BUG

