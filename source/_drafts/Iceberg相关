一. Hive整合Iceberg:
添加如下两个jar到$HIVE_HOME/auxlib下
iceberg-hive-runtime-0.13.2.jar
libfb303-0.9.3.jar
添加如下参数到hive-site.xml
iceberg.engine.hive.enabled=true

Hive创建Iceberg表
Hive操作Iceberg支持多种Catalog，支持Hadoop、Hive(默认)、Custom、location_based_table几种管理方式
1.HiveCatalog类型:
不设置Catalog类型时默认会使用HiveCatalog类型的Iceberg表
表元数据信息使用HiveMetaStore来管理，依赖Hive
建表示例：
-- 示例1 非分区表
CREATE TABLE iceberg_db.hive_iceberg_table (
  id BIGINT,
  name STRING
)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
LOCATION '/user/hive/warehouse/iceberg_db.db/hive_iceberg_table'
TBLPROPERTIES (
 'write.distribution-mode'='hash',
 'write.metadata.delete-after-commit.enabled'='true',   -- (每次提交后是否删除旧元数据文件) 自动清理旧元数据 metadata.json 不能清理manifest和snapshot的avro文件
 'write.metadata.previous-versions-max'='5'  -- 保留的metadata.json数量
);
-- 示例2 分区表
CREATE TABLE iceberg_db.hive_iceberg_partitioned_table (
  id BIGINT,
  name STRING,
  age int
) partitioned by (dt string)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
TBLPROPERTIES (
 'write.distribution-mode'='hash',
 'write.metadata.delete-after-commit.enabled'='true',
 'write.metadata.previous-versions-max'='5'
);

-- 示例3 指定catalog类型为HiveCatalog类型并建表:
set iceberg.catalog.<catalog_name>.type=hive;  -- 设置catalog类型
CREATE TABLE iceberg_db.hive_iceberg_partitioned_table (
  id BIGINT,
  name STRING,
  age int
) partitioned by (dt string)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
tblproperties('iceberg.catalog'='<catalog_name>')
TBLPROPERTIES (
 'write.distribution-mode'='hash',
 'write.metadata.delete-after-commit.enabled'='true',
 'write.metadata.previous-versions-max'='5'
);

2.HadoopCatalog类型
元数据信息使用底层hdfs存储来管理
示例：
set iceberg.catalog.<catalog_name>.type=hadoop;  -- 必须每次设置catalog类型
set iceberg.catalog.<catalog_name>.warehouse=hdfs://nameservice/user/iceberg/warehouse;  -- 必须每次设置warehouse存储路径
create external table iceberg_db.hadoop_iceberg_partitioned_table (
  id BIGINT,
  name STRING,
  age int
) partitioned by (dt string)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
LOCATION 'hdfs://nameservice/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table'  -- 路径必须是${iceberg.catalog.<catalog_name>.warehous}/${db_name}/${table_name}
tblproperties (
    'iceberg.catalog'='<catalog_name>',
    'write.distribution-mode'='hash',
    'write.metadata.delete-after-commit.enabled'='true',
    'write.metadata.previous-versions-max'='5'
);

3.LocationBasedTable
HDFS已经存在了Iceberg格式表的数据，可以指定tblproperties('iceberg.catalog'='location_based_table')和LOCATION，它会去LOCATION路径下加载iceberg表数据。LOCATION下已经存在Iceberg格式表数据了。
不需要加PARTITION BY，只需要加数据字段即可
比如Flink、Spark等引擎写入的数据，可以使用这种方式创建hive表
create external table iceberg_db.location_iceberg_partitioned_table (
  id BIGINT,
  name STRING,
  age INT,
  dt STRING
)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
LOCATION 'hdfs://nameservice/user/iceberg/warehouse/iceberg_db/location_iceberg_partitioned_table'
tblproperties ('iceberg.catalog'='location_based_table');
或
create table iceberg_db.location_iceberg_partitioned_table (
  id BIGINT,
  name STRING,
  age INT,
  dt STRING
)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
LOCATION 'hdfs://nameservice/user/iceberg/warehouse/iceberg_db/location_iceberg_partitioned_table'
tblproperties ('iceberg.catalog'='location_based_table');
注意：HDFS上已存在的Iceberg表必须是HadoopCatalog类型的，否则无法读取数据。表创建时创建时报错File does not exist: /user/iceberg/warehouse/iceberg_db/location_iceberg_partitioned_table/metadata/version-hint.text，表创建后，表存在，查询到的数据为空。

二. FlinkSQL整合Iceberg:
Flink 1.14则下载iceberg-flink-runtime-1.14-0.14.1.jar 放入$FLINK_HOME/lib目录下
启动flink集群：
cd $FLINK_HOME 
bin/start-cluster.sh
启动FlinkSQL Console：
bin/sql-client.sh embedded shell

1.Kafka2Iceberg Flink SQL:
```sql
set execution.checkpointing.interval=10sec; -- 必须设置checkpoint  靠checkpoint提交更新数据到Iceberg
SET execution.runtime-mode = streaming;  -- 流式写
CREATE TABLE t_kafka_source (
    id BIGINT,
    name STRING,
    age INT,
    dt STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'flink_topic1',  
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'cdh101:9092,cdh102:9092,cdh103:9092,cdh104:9092',
    'properties.group.id' = 'test',
    'format' = 'csv'
);

-- 写入HadoopCatalog类型Iceberg表
CREATE CATALOG hadoop_iceberg_catalog WITH (
  'type'='iceberg',  -- 创建HadoopCatalog类型Iceberg表在FlinkSQL中的Catalog
  'catalog-type'='hadoop',
  'warehouse'='hdfs://nameservice/user/iceberg/warehouse',
  'property-version'='1'
);
CREATE TABLE if not exists `hadoop_iceberg_catalog`.`iceberg_db`.`hadoop_iceberg_table_flink_sql` (
   id BIGINT,
   name STRING,
   age INT,
   dt STRING
) PARTITIONED BY (dt)
WITH('type'='ICEBERG',
'engine.hive.enabled'='true',  -- 支持hive查询(实测发现不加也没影响)
'read.split.target-size'='1073741824', -- 减少split数提升查询效率
'write.target-file-size-bytes'='134217728',
'write.format.default'='parquet',
'write.metadata.delete-after-commit.enabled'='true',
'write.metadata.previous-versions-max'='9',  
'write.distribution-mode'='hash');  -- 动态合并小文件
insert into hadoop_iceberg_catalog.iceberg_db.hadoop_iceberg_table_flink_sql select id,name,age,dt from t_kafka_source;

-- FlinkSQL批式查询
SET execution.runtime-mode = batch;
select id,name,age,dt from `hadoop_iceberg_catalog`.`iceberg_db`.`hadoop_iceberg_table_flink_sql`;

-- Hive创建Iceberg映射表
create external table iceberg_db.hadoop_iceberg_table_flink_sql (
  id BIGINT,
  name STRING,
  age INT,
  dt STRING
)
STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'
LOCATION 'hdfs://nameservice/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_table_flink_sql'
tblproperties ('iceberg.catalog'='location_based_table');
-- HiveSQL查询(能查到实时最新数据)
select * from iceberg_db.hadoop_iceberg_table_flink_sql; 
```
注意:不能在hive里先建表,因为Hive里建表后,Hive元数据中会保存metadata元数据信息,与Flink不兼容.Flink写入后,metadata.json更新,但Hive无感知,所以无法查询,甚至会因为Flink端删除了Hive记录的元数据文件而无法读写表.
报错org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/iceberg/warehouse/iceberg_db/hadoop_iceberg_table_flink_sql/metadata/00000-ab223d95-68e3-40db-9d24-f235543a8ee1.metadata.json
手动修改hive metastore中记录的metadata_location也是不可以的,会报错.

--写入HiveCatalog类型的Iceberg表
bin/sql-client.sh embedded -j iceberg-flink-runtime-1.13-0.14.0.jar -j  /opt/cloudera/parcels/CDH/jars/hive-metastore-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libthrift-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-common-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/hive-serde-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libfb303-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-shims-common-2.1.1-cdh6.3.1.jar shell
```sql 
set execution.checkpointing.interval=10sec; -- 必须设置checkpoint  靠checkpoint提交更新数据到Iceberg
SET execution.runtime-mode = streaming;  -- 流式写
CREATE TABLE t_kafka_source (
    id BIGINT,
    name STRING,
    age INT,
    dt STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'flink_topic1',  
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'cdh101:9092,cdh102:9092,cdh103:9092,cdh104:9092',
    'properties.group.id' = 'test',
    'format' = 'csv'
);
CREATE CATALOG hive_iceberg_catalog WITH (
  'type'='iceberg',
  'catalog-type'='hive',
  'uri'='thrift://cdh101:9083,thrift://cdh103:9083',
  'clients'='5',
  'property-version'='1',
  'warehouse'='hdfs://nameservice/user/iceberg/warehouse'
);

CREATE TABLE if not exists `hive_iceberg_catalog`.`iceberg_db`.`hive_iceberg_table_flink_sql` (
   id BIGINT,
   name STRING,
   age INT,
   dt STRING
) PARTITIONED BY (dt)
WITH('type'='ICEBERG',
'engine.hive.enabled'='true',
'read.split.target-size'='1073741824',
'write.target-file-size-bytes'='134217728',
'write.format.default'='parquet',
'write.metadata.delete-after-commit.enabled'='true',
'write.metadata.previous-versions-max'='9',  
'write.distribution-mode'='hash');

insert into hive_iceberg_catalog.iceberg_db.hive_iceberg_table_flink_sql select id,name,age,dt from t_kafka_source;
```
写入HiveCatalogIceberg表后，在Hive可以直接看到并查询表`iceberg_db`.`hive_iceberg_table_flink_sql`

三. 元数据分析
https://mvnrepository.com/artifact/org.apache.avro/avro-tools下载avro-tools.jar
java -jar /opt/software/avro-tools-1.11.1.jar tojson 7d26a4d3-ed7a-4da3-be5b-ff12c12d1151-m0.avro 




-- https://blog.csdn.net/spark_dev/article/details/122876819 实践数据湖Iceberg 14课
-- https://www.bilibili.com/video/BV1MN4y1F7zJ?p=11&spm_id_from=pageDriver&vd_source=36364c17b3db26fb7aae6a7085dfd9a1
-- https://zhuanlan.zhihu.com/p/473731690  小文件解决
-- https://blog.csdn.net/Baron_ND/article/details/120287591 Iceberg数据湖
-- https://zhuanlan.zhihu.com/p/396281054 Flink+Iceberg 













待测通 StreamX 写HiveCatalogIceberg表
bin/sql-client.sh embedded -j iceberg-flink-runtime-1.13-0.14.0.jar -j  /opt/cloudera/parcels/CDH/jars/hive-metastore-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libthrift-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-common-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/hive-serde-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libfb303-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-shims-common-2.1.1-cdh6.3.1.jar shell
<dependency>
    <groupId>org.apache.iceberg</groupId>
    <artifactId>iceberg-flink-runtime-1.13</artifactId>
    <version>0.14.1</version>
</dependency>
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-metastore</artifactId>
    <version>2.1.1</version>
</dependency>
<dependency>
    <groupId>org.apache.thrift</groupId>
    <artifactId>libthrift</artifactId>
    <version>0.9.3</version>
    <type>pom</type>
</dependency>
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-common</artifactId>
    <version>2.1.1</version>
</dependency>
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-serde</artifactId>
    <version>2.1.1</version>
</dependency>
<dependency>
    <groupId>org.apache.thrift</groupId>
    <artifactId>libfb303</artifactId>
    <version>0.9.3</version>
    <type>pom</type>
</dependency>
<dependency>
    <groupId>org.apache.hive.shims</groupId>
    <artifactId>hive-shims-common</artifactId>
    <version>2.1.1</version>
</dependency>









































三. Trino\Presto整合Iceberg
以上打通了FlinkSQL Iceberg Hive 下面打通Trino 
Trino支持HiveCatalog类型的Iceberg表,不支持HadoopCatalog类型Iceberg表,如果查询的是HadoopCatalog,location_based_table,Custome类型的Iceberg表会报错:Table is missing [metadata_location] property: iceberg_db.hadoop_iceberg_table_flink_sql





