shuffle write：分区数由上一阶段的RDD分区数控制
类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则，临时存放到各个executor所在的本地磁盘上。
shuffle read：分区数由Spark提供的参数控制
如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。
有时候即使不会导致JVM crash也会造成长时间的gc。

修改Hive/SQL表注释:
ALTER TABLE table_name SET TBLPROPERTIES('comment' = '这是表注释!');
修改Hive/SQL字段注释:
ALTER TABLE table_name CHANGE COLUMN muid muid_new STRING COMMENT '这里是列注释!'; 


PYSPARK_DRIVER_PYTHON=ipython $SPARK_HOME/bin/pyspark

内存自动清理脚本https://blog.csdn.net/wangrui1573/article/details/89838938

spark 操作 https://www.cnblogs.com/Frank99/p/8295949.html

info_dicts = spark.sql("""desc %s""" % table_name).toPandas().to_dict()   # 转换成pandas的df
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
    (info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))
df.registerTempTable('temp_table')
info_df = spark.sql("desc formatted temp_table")
col_comment_in_df = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.asDict().get('col_name'): x.asDict().get('comment')},
info_df.select('col_name', 'comment').collect()))   # 获取到df每个字段的注释
paras = []
for col_name, data_type in col_comment_in_hive.keys():
    if col_name.find("#") < 0:
        hive_comment = col_comment_in_hive.get((col_name, data_type))
        df_comment = col_comment_in_df.get(col_name)
        if hive_comment != df_comment and df_comment:  # t_comment_records中存在注释且hive表中的注释与mysql里不同
            # 需要更新注释
            paras.append((str(table_name), str(col_name), str(col_name), str(data_type), str(df_comment)))
        else:
            if hive_comment != '' and hive_comment != '无注释' and not df_comment:   # hive表有注释而mysql无对应注释
                # mysql中没有对应字段的注释而Hive中有
                logger.info("Hive表%s的%s字段有注释而dataframe中未记录" % (table_name, col_name))
                query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','in hive')""" % (table_name, col_name))
            if not hive_comment and not df_comment:
                logger.info("dataframe中与Hive中表%s的%s字段都没有注释" % (table_name, col_name))
                query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','no comment')""" % (table_name, col_name))
if paras:
    hive_cursor = create_hive_cursor()
    for para in paras:
        # hive_cursor.execute("""ALTER TABLE %s CHANGE COLUMN %s %s %s COMMENT '%s' """ % para)
        print("修改字段注释", para)

##################################################
col_comm_in_mysql = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.get('col'): x.get('comment')}, fetch))   # 得到字段-注释 的dict
df = spark.sql("""desc %s""" % table_name).toPandas()   # 转换成pandas的df
info_dicts = df.to_dict()
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
(info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))


df.dtypes
http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame
##################################################


df = df.withColumn("enddate", judge_date_udf("enddate")).withColumn().withColumn()......



Spark：任务中如何确定spark分区数、task数目、core个数、worker节点个数、excutor数量 https://blog.csdn.net/weixin_38750084/article/details/82725176

MD5 BASE64:
img_path = '/home/jiajing_qu/img.jpg'
import md5
m = md5.new()
m.update(img_path)
res = m.hexdigest()
import hashlib
import base64
with open(img_path, 'rb') as fp:
	data = fp.read()
	base64_data = base64.b64encode(data)
	file_md5= hashlib.md5(data).hexdigest()
content = {"base64": base64_data, "md5": file_md5}
{"base64": base64_data, "md5": file_md5}
with open(img_path, 'rb') as fp:
    content = fp.read()


*************************************待写博客****************************************
--------------------------------Mysql Binlog---------------------------------
mysql回滚(表引擎必须是INNODB) 可以show create table xx;查看
select * from table_name;
回滚事务步骤:
start transaction;   开始事务
savepoint a;   创建保存点a
delete from table_name where id = 1;  删除一条记录
select * from table_name;
rollback to a;  回滚到a保存点
select * from table_name;  没问题的话  执行commit;

mysql的binlog是多文件存储，定位一个LogEvent需要通过binlog filename + binlog position，进行定位
mysql的binlog数据格式，按照生成的方式，主要分为：statement-based、row-based、mixed。
mysql> show variables like 'binlog_format';
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | binlog_format | ROW   |
    +---------------+-------+
    1 row in set (0.00 sec)

-----------------------------------------------------------------------------


--------------------------------JVM GC分析--------------------------------
https://www.cnblogs.com/rainwang/p/7213918.html    GC分析   G1  不研究CMS
-------------------------------------------------------------------------
