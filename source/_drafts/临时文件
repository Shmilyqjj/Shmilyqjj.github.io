##################################################
好的学习网站
https://github.com/josonle/Coding-Now   # 开源 涉及大数据几大组件、Python机器学习和数据分析、Linux、操作系统、算法、网络等
https://www.iteblog.com/
https://www.liaoxuefeng.com
https://home.apachecn.org/docs/
https://github.com/apachecn
https://blog.csdn.net/yidan7063

##################################################
技术类书籍下载：https://github.com/SummerJoan3/books
待下载：
SparkSQL内核剖析
大数据架构师指南 (朱进云著) pdf
shell脚本实战第二版pdf

##################################################
JVM相关
OOM八种方式与解决：https://tianmingxing.com/2019/11/17/%E4%BB%8B%E7%BB%8DJVM%E4%B8%ADOOM%E7%9A%848%E7%A7%8D%E7%B1%BB%E5%9E%8B/
查看GC情况 https://www.cnblogs.com/alter888/p/10407952.html

##################################################
监控：
Graphite：https://blog.csdn.net/hffyyg/article/details/87900613
JVM-Profiler 

##################################################
大数据热点话题
八种排查处理数据倾斜 https://mp.weixin.qq.com/s/piW10KGJVgaSB_i72OVntA
联邦查询开源框架Quicksql：https://github.com/Qihoo360/Quicksql，https://www.ctolib.com/Qihoo360-XSQL.html
##################################################
Apache Ranger相关
https://blog.csdn.net/qq475781638/article/details/90247153
https://blog.csdn.net/qq_35995514/article/details/107227468?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param
https://blog.csdn.net/sudaxhh/article/details/52135184?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-8.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-8.channel_param
https://blog.csdn.net/tototuzuoquan/article/details/106505018?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param
Apache Ranger Data Masking：https://cwiki.apache.org/confluence/display/RANGER/Row-level+filtering+and+column-masking+using+Apache+Ranger+policies+in+Apache+Hive



##################################################
Cloudera Manager相关
https://cloud.tencent.com/developer/article/1544865



##################################################
Hadoop相关

hdfs dfsadmin -setQuota 10000 /tmp/qjj  设置该目录文件数配额为1w（实际9.8k都会少一点）
hdfs dfsadmin -setSpaceQuota 10G /tmp/qjj  设置该目录空间配额10G
hdfs dfsadmin -clrQuota /tmp/qjj  清空目录配额限制
hdfs dfs -setrep 2 /hbase/archive/path....  设置hdfs某个目录的副本数为2  降低冗余副本数

小文件产生情况：
1.流作业
2.过高并发度
3.源数据存在小文件
4.过多分区的数据表
应对：
1.小文件合并
2.并行度合理规划
3.数据表分区合理规划
4.对象存储
应对冷文件：
1.数据存储分层，冷数据定期转移到廉价机器
2.定期清理

hadoop checknative 查看当前支持压缩格式

NFS挂载（不如HDFS Fuse挂载性能好）
NFS挂载hdfs到本地目录 开启NFS Gateway后 mount -t nfs -o vers=3,proto=tcp,nolock,sync,rsize=1048576,wsize=1048576 nfs_ip:/ /data/hdfs_nfs
NFS卸载umount /data/hdfs_nfs
fuser -m -v /data/hdfs_nfs   对于umount.nfs: /data/hdfs_nfs: device is busy  查看哪个进程占用
rpcinfo -p  nfs_ip  查看服务是否已启动并正在运行
showmount -e nfs_ip  验证HDFS名称空间是否已导出并且可以挂载
nfsstat -m   NFS状态查看
默认rsize和wsize是1M，自己设置更大，mount之后这个参数也还是1M，因为linux内核的限制，要想提高这个阈值，要编译内核的。

HDFS元数据浏览
【oiv】offline image viwer   
1.用于查看Hadoop fsimage 
2.语法
    $> hdfs oiv -i inputfile -o outputfile -P process
3.inputfile: 要查看的fsimage文件
   outputfile: 用于保存格式化之后的文件
   process: 使用什么进程解码，XML|Web|...
【oev】
1.用于查看Hadoop 的 edit 文件 
2.语法
    $> hdfs oev -i inputfile -o outputfile -P process
3.inputfile: 要查看的edit文件
   outputfile: 用于保存格式化之后的文件
   process: 使用什么进程解码，XML|Web|...

HDFS查看文件是否存在
hadoop fs -test -e /data/;echo $?

Yarn的调度器：
--1.先进先出调度器（FIFO）
Hadoop 中默认的调度器，也是一种批处理调度器。它先按照作业的优先级高低，再按照到达时间的先后选择被执行的作业
--2.容量调度器（Capacity Scheduler)
选择占用资源小，优先级高的先执行
支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。
调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择，同时考虑用户资源量限制和内存限制
--3.公平调度器（Fair Scheduler）
同一队列中的作业公平共享队列中所有资源
公平调度是一种赋予作业（job）资源的方法，它的目的是让所有的作业随着时间的推移，都能平均的获取等同的共享资源。
所有的 job 具有相同的资源,当单独一个作业在运行时，它将使用整个集群。当有其它作业被提交上来时，系统会将任务（task）空闲资源（container）赋给这些新的作业，以使得每一个作业都大概获取到等量的CPU时间。
与Hadoop默认调度器FIFO维护一个作业队列不同，这个特性让小作业在合理的时间内完成的同时又不"饿"到消耗较长时间的大作业。
公平调度可以和作业优先权搭配使用——优先权像权重一样用作为决定每个作业所能获取的整体计算时间的比例。同计算能力调度器类似，支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。

Yarn工作流程
1.用户向Yarn提交任务
2.ResourceManager分配Container，与NodeManager通信，在Container中启动ApplicationMaster
3.ApplicationMaster向ResourceManager注册，申请资源，监控运行状态直到结束
4.应用程序执行完，Application通过RPC协议向ResourceManager注销自己。



##################################################
Python相关

常见机器学习库：mxnet、tensorflow、pytorch、pandas

pip日志自动保存：
vim ~/.pip/pip.conf 
[global]标签下添加
log=~/.pip/pip.log

pip依赖在不同机器克隆：
pip freeze > requirements.txt  # 备份
pip install -r requirements.txt  # 恢复
requirements.txt记录了依赖包信息和精确版本号


设计模式：https://www.cnblogs.com/wangjian941118/p/9289145.html

打印调用栈
import traceback
def BBQ():
    traceback.print_stack()
    a = traceback.extract_stack()
    b = traceback.format_stack()

MD5 BASE64:
img_path = '/home/jiajing_qu/img.jpg'
import md5
m = md5.new()
m.update(img_path)
res = m.hexdigest()
import hashlib
import base64
with open(img_path, 'rb') as fp:
	data = fp.read()
	base64_data = base64.b64encode(data)
	file_md5= hashlib.md5(data).hexdigest()
content = {"base64": base64_data, "md5": file_md5}
{"base64": base64_data, "md5": file_md5}
with open(img_path, 'rb') as fp:
    content = fp.read()

r = [u'varchar(255)', u'varchar(255)', u'varchar(255)', u'datetime', u'datetime']
r[0].__contains__('varchar')
r[0].__contains__('varchar(255)')
r[0].__contains__('varchar(254)')
判断类型  isinstance(comment_info, dict)    isinstance(comment_info, basestring)
enumerate(list[])

logging模块
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.info()

for i in enumerate(list):
    print(i)   # i是(0,list[0])  (1,list[1])....

pandas的dataframe转python的dict （orient指定方向）
df.to_dict(orient='records')
[df.to_dict(orient='index')]

如果用了staticmethod，那么就可以无视这个self，而将这个方法当成一个普通的函数使用。
而对于classmethod，它的第一个参数不是self，是cls，它表示这个类本身。

python import是将py文件(模块)中的元素（类，方法）加载到内存，放入sys.modules，生成同名的.pyc文件  如果没加载过就会加载到内存，如果之前已经加载过，就不再重复加载到内存，而是改变命名空间

连接数据库 插入和更新数据库后，数据库内容未变：conn的时候  autocommit = 1

正则 [\u4e00-\u9fa5] 匹配所有中文

python编译cython => .so
###################################################
Spark相关

spark_df.write.saveAsTable(hive_table_name, mode=mode)写入表报错The format of the existing table default.fdm_mx_dz_maidian_var1 is `HiveFileFormat`. It doesn't match the specified format `ParquetFileFormat`.;
解决方案1：spark_df.write.format("Hive").saveAsTable(hive_table_name, mode=mode)  解决方案2：spark_df.registerTempTable('spark_data');spark.sql('insert into hive_table_name select * from spark_data')

apache.spark.SparkException: Job aborted due to stage failure: Serialized task 32:5 was 204136673 bytes, which exceeds max allowed: spark.rpc.message.maxSize (134217728 bytes).Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.
用sc.parallelize(data,slices)时，如果data数据过大，易出现该问题
解决：增加spark.rpc.message.maxSize，该值默认大小128M
提交任务是加：–conf spark.rpc.message.maxSize=512


Atlas连接Spark  spark-atlas-connector工具
Listener可以获取Spark执行过程中各个阶段的状态信息
Spark-core中LiveListenerBus会管理注册的Listener，为一类Listener创建对应AsyncEventQueue
AsyncEventQueue广播Event到Listener
Spark批处理操作元数据跟踪：SparkAtlasEventTracker对DDL、DML相关events以及其他SQL相关的Events做监听和分析，其他Events不感兴趣会被忽略
Spark-streaming中StreamingQueryListener监听流的启动、停止、状态更新
Spark流处理操作元数据跟踪：SparkAtlasStreamingQueryEventTracker 本质也是监听Event
目前已支持收集SparkSQL执行任务的血缘信息到Atlas，后续升级CDH后与Hive元数据整合。

Listener实现：https://www.cnblogs.com/jason-dong/p/10029934.html

Spark调度模式-FIFO和FAIR：https://blog.csdn.net/dabokele/article/details/51526048
FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。
FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。

sparkSQL参数调优：https://blog.csdn.net/yuanbingze/article/details/97368552

Stage详情页Locality Level数据本地性级别分五种：
PROCESS_LOCAL: data is in the same JVM as the running code. This is the best locality possible
NODE_LOCAL: data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes
NO_PREF: data is accessed equally quickly from anywhere and has no locality preference
RACK_LOCAL: data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch
ANY: data is elsewhere on the network and not in the same rack
效率PROCESS_LOCAL > NODE_LOCAL > NO_PREF > RACK_LOCAL
如果调度任务较多，查看到任务的本地性级别不太好，可以适当调大spark.locality.wait (默认3s)
相关源码解析：https://blog.csdn.net/high2011/article/details/80705393
调优策略：尽量保证数据在同一个JVM进程，即PROCESS_LOCAL级别，如果无缓存，则一定不能是PROCESS_LOCAL，最好也只能是NODE_LOCAL

pyspark初始化
from pyspark.sql import HiveContext
from pyspark import SparkContext, SparkConf
conf = SparkConf()
# 资源
conf.set('spark.driver.memory', '3G')
conf.set('spark.executor.memory', '20G')
conf.set('spark.yarn.executor.memoryOverhead', '5G')
conf.set('spark.executor.cores', '6')  # 等价于--num-executors
conf.set('spark.executor.instances', '4')
conf.set('spark.memory.storageFraction', '0.1')
sc = SparkContext(conf=conf, appName='qjj_spark_app', master='yarn')
sc.setLogLevel("WARN")
sqlContext = HiveContext(sc)
spark = sqlContext.sparkSession

# scala sparkSession初始化
//创建Spark Context
val conf = new org.apache.spark.SparkConf().setMaster("yarn").setAppName("QjjSpark")
val sc = new org.apache.spark.SparkContext(conf)
val sqlContext=new org.apache.spark.sql.SQLContext(sc)
val spark = sqlContext

Spark临时文件设置
（spark运行时产生临时文件core.*，/tmp/spark-*,/tmp/blockmgr-*）
回答
Spark任务在运行过程中，driver会创建一个spark-开头的本地临时目录，用于存放业务jar包，配置文件等，同时在本地创建一个blockmgr-开头的本地临时目录，用于存放block data。此两个目录会在Spark应用运行结束时自动删除。此两个目录的存放路径优先通过SPARK_LOCAL_DIRS环境变量指定，若不存在该环境变量，则设置为spark.local.dir的值，若此配置还不存在，则使用java.io.tmpdir的值。客户端默认配置中spark.local.dir被设置为/tmp，因此默认使用系统/tmp目录。
但存在一些特殊情况，如driver进程未正常退出，比如被kill -9命令结束进程，或者Java虚拟机直接崩溃等场景，导致driver的退出流程未正常执行，则可能导致该部分目录无法被正常清理，残留在系统中。
当前只有yarn-client模式和local模式的driver进程会产生上述问题，在yarn-cluster模式中，已将container内进程的临时目录设置为container临时目录，当container退出时，由container自动清理该目录，因此yarn-cluster模式不存在此问题。
解决措施
可在Linux下设置/tmp临时目录自动清理，或修改客户端中spark-defaults.conf配置文件的spark.local.dir配置项的值，将临时目录指定到特定的目录，再对该目录单独设置清理机制。
默认会占用系统盘空间，修改如下参数：
spark-default.conf: spark.local.dir=/data/xxx 
spark-env.sh: export SPARK_LOCAL_DIRS=/data/xxx
spark event log会存在xxx.inporgress文件大量占用系统盘空间：spark.eventLog.dir 修改为其他非系统盘路径或HDFS上

Spark访问多个不同nameservice的HDFS集群
1.使用配置文件
val conf = new SparkConf().setAppName("Spark Word Count") 
val sc = new SparkContext()
// 先将hadoop config配置为cluster1集群
sc.hadoopConfiguration.addResource("cluster1/core-site.xml")
sc.hadoopConfiguration.addResource("cluster1/hdfs-site.xml")
// 操作数据 比如load data ....
// 再将hadoop config设为cluster2集群
sc.hadoopConfiguration.addResource("cluster2/core-site.xml") 
sc.hadoopConfiguration.addResource("cluster2/hdfs-site.xml") 
2.使用配置项
val conf = new SparkConf().setAppName("Spark Word Count")
val sc = new SparkContext()
// 先将hadoop config配置为cluster1集群
sc.hadoopConfiguration.set("fs.defaultFS", "hdfs://cluster1");
sc.hadoopConfiguration.set("dfs.nameservices", "cluster1");
sc.hadoopConfiguration.set("dfs.ha.namenodes.cluster1", "nn1,nn2");
sc.hadoopConfiguration.set("dfs.namenode.rpc-address.cluster1.nn1", "namenode001:8020");
sc.hadoopConfiguration.set("dfs.namenode.rpc-address.cluster1.nn2", "namenode002:8020");
sc.hadoopConfiguration.set("dfs.client.failover.proxy.provider.cluster1", "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");
// 操作数据 比如load data ....
// 再将hadoop config设为cluster2集群
sc.hadoopConfiguration.set("fs.defaultFS", "hdfs://cluster2");
sc.hadoopConfiguration.set("dfs.nameservices", "cluster2");
sc.hadoopConfiguration.set("dfs.ha.namenodes.cluster2", "nn3,nn4");
sc.hadoopConfiguration.set("dfs.namenode.rpc-address.cluster2.nn3", "namenode003:8020");
sc.hadoopConfiguration.set("dfs.namenode.rpc-address.cluster2.nn4", "namenode004:8020");
sc.hadoopConfiguration.set("dfs.client.failover.proxy.provider.cluster2", "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");




ANALYZE TABLE命令分析表、列信息
分析表信息->主要得到行数和表数据占用空间大小
ANALYZE TABLE xh_result_all COMPUTE STATISTICS noscan
desc formatted或DESCRIBE EXTENDED查看表详细信息
分析列信息->主要得到某个列的min、max、num_nulls、distinct_count、avg_col_len、max_col_len等信息，有助于了解数据，粗略判断数据质量
ANALYZE TABLE xh_result_all COMPUTE STATISTICS FOR COLUMNS cust_no, prob, group, dt;
DESCRIBE EXTENDED xh_result_all cust_no;


spark.sessionState.conf.getAllConfs.foreach(println) 获取SparkSession当前的配置参数

spark.sql.adaptive.enabled && spark.sql.adaptive.shuffle.targetPostShuffleInputSize=128000000; && distribute by 均匀分布的字段=该参数是用于开启spark的自适应执行，这是spark比较老版本的自适应执行，后面的targetPostShuffleInputSize是用于控制之后的shuffle阶段的平均输入数据大小，防止产生过多的task。表小文件过多导致堆外OOM或者合并小文件时使用。
spark.sql.adaptive.enabled && spark.sql.adaptive.shuffle.targetPostShuffleInputSize=128M; && distribute by 均匀分布的字段 spark可以根据数据量大小自动规划分区数，自适应分区。spark.sql.adaptive.shuffle.targetPostShuffleInputSize是控制进入shuffle阶段的数据量大小，若不适用压缩格式和压缩算法，输出文件大小会在128M左右，但如果输出snappy+parquet或采用其他压缩格式和压缩算法，则实际输出文件大小是128M*压缩比，可能几M-几十M。由于这种方式合并小文件不需要写死分区数，可以方便批量处理多张表的小文件合并。可以估算压缩比(不太好估，跟具体的数据重复度有关)将spark.sql.adaptive.shuffle.targetPostShuffleInputSize设得更大些，输出的文件，即压缩后的文件也会大一些。
所以使用动态分区进行Hive表小文件合并的完整流程：
set parquet.compression=snappy
spark.sql.adaptive.enabled;
spark.sql.adaptive.shuffle.targetPostShuffleInputSize=300M;
create table A_new stored as parquet as select * from A distribute by col_equality;
alter table A rename to A_old;
alter table A_new rename to A;
drop table A_old;


spark动态资源分配，没用到Executor时回收资源
开启动态资源调整需要（on yarn情况下）
1.将spark.dynamicAllocation.enabled设置为true。意思就是启动动态资源功能
2.将spark.shuffle.service.enabled设置为true。 在每个nodeManager上设置外部shuffle服务
　　2.1 将spark-<version>-yarn-shuffle.jar拷贝到每台nodeManager的${HADOOP_HOME}/share/hadoop/yarn/lib/下。
　　2.2 配置yarn-site.xml
　　　　<property>
　　　　　　<name>yarn.nodemanager.aux-services</name>
　　　　　　<value>mapreduce_shuffle,spark_shuffle</value>
　　　　</property>
　　　　<property>
　　　　　　<name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
　　　　　　<value>org.apache.spark.network.yarn.YarnShuffleService</value>
　　　　</property>
　　　　<property>
　　　　　　<name>spark.shuffle.service.port</name>
　　　　　　<value>7337</value>
　　　　</property>
　　　2.3 重启所有nodeManager
val sparkConf = new SparkConf()
    .set("spark.shuffle.service.enabled", "true")
    .set("spark.dynamicAllocation.enabled", "true")
    .set("spark.dynamicAllocation.minExecutors", "1") //最少占用1个Executor，下限，默认0个
        .set("spark.dynamicAllocation.maxExecutors", "6") //最多占用6个Executor，上限，不配的话默认是无限的
    .set("spark.dynamicAllocation.initialExecutors", "1") //初始executor数为1，默认初始化一个Executor，如果--num-executors更大则用--num-executors的值
    .set("spark.dynamicAllocation.executorIdleTimeout", "60") //executor闲置时间，如果某executor空闲超过60s，则remove此executor
    .set("spark.dynamicAllocation.cachedExecutorIdleTimeout", "600") //cache闲置时间 如果spark计算当中使用了rdd.cache，不加下面的配置，动态资源不会释放（默认无限制）,也就是含有cache的executor不会被remove
    .set("spark.executor.cores", "3")  //使用的executor core
    .setMaster("yarn")
    .setAppName("Spark DynamicRelease")
  val spark: SparkSession = SparkSession
    .builder
    .config(sparkConf)
    .getOrCreate()
  动态分配资源的executor申请策略：当有被挂起的任务（Pending task）时，也就表示当前的executor数量还不足够所有的task并行运行，这时候spark会申请增加资源，但是并不是出现pending task就立刻请求增加executor。由下面两个参数决定：
  1.spark.dynamicAllocation.schedulerBacklogTimeout：如果有pending task并且等待了一段时间(默认1秒)，则申请增加executor
  2.spark.dynamicAllocation.sustainedSchedulerBacklogTimeout：随后每隔N秒(默认1秒)，再检测pending task，如果仍然存在，则申请增加executor（此外每轮请求的executor数量是指数增长的，第一次1，第二次2，第三次4个，8，16，32以此类推）
动态资源分配的spark-thriftserver启动参数例子：
$SPARK_HOME/sbin/start-thriftserver.sh
--executor-memory 10g --executor-cores 5 --driver-memory 10g --driver-cores 5 \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.shuffle.service.enabled=true \
--conf spark.dynamicAllocation.initialExecutors=20 \
--conf spark.dynamicAllocation.minExecutors=20 \
--conf spark.dynamicAllocation.maxExecutors=400 \
--conf spark.dynamicAllocation.executorIdleTimeout=300s \
--conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \
--conf spark.dynamicAllocation.cachedExecutorIdleTimeout=600s \
需要考虑的问题：Shuffle问题，如果需要移除的Executor包含了Shuffle Write相关数据该怎么办？需要和Yarn集成，需要配置yarn.nodemanager.aux-services，这样executor就不用保存shuffle状态了
注意：1.executor core不要太大(最好≤3)，避免executor数下降时，等不及新申请的executor，已有executor就因为任务过重挂了 2.合理的Shuffle并行数，避免杀掉过多executors 3.对于每个Stage持续时间很短的应用，其实不适合这套机制。这样会频繁增加和杀掉Executors，造成系统颠簸。而Yarn对资源的申请处理速度并不快。
参考：[dynamic-resource-allocation](http://spark.apache.org/docs/2.3.1/job-scheduling.html#dynamic-resource-allocation)
[configuring-the-external-shuffle-service](http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service)

pyspark动态资源模板
from pyspark import SparkContext, SparkConf
import time
conf = SparkConf()
# dynamic resource
from pyspark.sql import HiveContext
from pyspark import SparkContext, SparkConf
conf = SparkConf()
conf.set('spark.driver.memory', '4G')
conf.set('spark.executor.memory', '2G')
conf.set("spark.shuffle.service.enabled", "true")
conf.set("spark.dynamicAllocation.enabled", "true")
conf.set("spark.dynamicAllocation.minExecutors", "1")
conf.set("spark.dynamicAllocation.maxExecutors", "4")
conf.set("spark.dynamicAllocation.initialExecutors", "1")
conf.set("spark.dynamicAllocation.executorIdleTimeout", "60")
conf.set("spark.dynamicAllocation.cachedExecutorIdleTimeout", "300")
sc = SparkContext(conf=conf, appName='qjj_dynamic_spark_app', master='yarn')
sc.setLogLevel("WARN")
sqlContext = HiveContext(sc)
spark = sqlContext.sparkSession



Spark-SQL合并小文件代码模板
from pyspark.sql import HiveContext
from pyspark import SparkContext, SparkConf
import time
conf = SparkConf()
# 线上Yarn环境申请资源
conf.set('spark.driver.memory', '1G')
conf.set('spark.executor.memory', '8G')
conf.set('spark.executor.memoryOverhead', '4G')
conf.set('spark.executor.cores', '5')
conf.set('spark.executor.instances', '6')
conf.set('spark.memory.storageFraction', '0.1')
sc = SparkContext(conf=conf, appName='qjj_merge_small_file_app', master='yarn')
sc.setLogLevel("WARN")
sqlContext = HiveContext(sc)
spark = sqlContext.sparkSession
OUTPUT_FILE_COUNT = 100
SQL = """
insert into target_table select * from source_table distribute by cust_no
"""
st = time.time()
spark.sql("set hive.exec.dynamic.partition.mode=nonstrict").show()
spark.sql("set hive.exec.dynamic.partition=true").show()
spark.sql("set parquet.compression=snappy").show()
spark.sql(f"set spark.sql.shuffle.partitions={OUTPUT_FILE_COUNT}").show()
spark.sql(SQL).show()
print(f"耗时:{time.time() - st}秒")

spark-sql优化：
1.join on和where条件先后顺序无所谓，可以被spark优化器优化
2.对于广播表有两种方式自动、手动 自动：spark.sql.autoBroadcastJoinThreshold=1048576设置表自动广播阈值  手动：cache table xxx;手动API：from pyspark.sql.functions import broadcast;broadcast(spark.table("b")).join(spark.table("a"), "id").show()
3.字段很多的表尽量select有用的字段而非select *，可以减少FileScan
4.报错Futures timed out after [10000 milliseconds]，可能资源紧张导致请求Yarn AppMaster超时，设置spark.yarn.am.waitTime=60s


groupByKey(11)将Shuffle并行度调整为11
计算单词的个数，第一种方式使用reduceByKey ；另外一种方式使用groupByKey
虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。
ReduceByKey优点：会先进行局部聚合再进行全局聚合，这样全局聚合时减少了网络IO
尽量少使用groupByKey 当调用 groupByKey时，所有的键值对(key-value pair) 都会被移动。在网络上传输这些数据非常没有必要。避免使用 GroupByKey。
以下函数应该优先于 groupByKey：
　　（1）combineByKey组合数据，但是组合之后的数据类型与输入时值的类型不一样。
　　（2）foldByKey 合并每一个 key 的所有值，在级联函数和“零值”中使用。

# 一些概念（参考https://juejin.im/post/5a5dc6056fb9a01c982c9505）
每个JVM只有一个SparkContext,一台服务器可以启动多个JVM
SparkSession包含了SQLContext和HiveContext
sparkContext负责与clusterManager通信，将应用程序分发给Executor。sparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给TaskScheduler，最后由TaskScheduler将Task发送给Executor运行。Task在Executor上运行，运行完释放所有资源.
Driver：
    运行main方法的Java虚拟机进程,负责监听spark application的executor进程发来的通信和连接,将工程jar发送到所有的executor进程中
    Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、分配task到executor上、计算资源的分配等调度执行作业等
    driver调度task给executor执行，所以driver最好和spark集群在一片网络内,便以通信
    driver进程通常在worker节点中,和Cluster Manager不在同一个节点上
Worker:
    启动并运行executor进程
    standalone模式下:Worker进程所在节点
    yarn模式下: yarn的nodemanager进程所在的节点
Executor:
    spark application不会共享一个executor进程
Job：
    一个spark application可能会被分为多个job，每次调用Action时，逻辑上会生成一个Job，一个Job包含了一个或多个Stage。
Stage:
    每个job都会划分为一个或多个stage（阶段），每个stage都会有对应的一批task(即一个taskset)，分配到executor上去执行
    Stage包括两类：ShuffleMapStage和ResultStage，如果用户程序中调用了需要进行Shuffle计算的Operator，如groupByKey等，就会以Shuffle为边界分成ShuffleMapStage和ResultStage。
    如果一次shuffle都没执行，那就只有一个stage
TaskSet:
    一组关联的，但相互之间没有Shuffle依赖关系的Task集合;Stage可以直接映射为TaskSet，一个TaskSet封装了一次需要运算的、具有相同处理逻辑的Task，这些Task可以并行计算，粗粒度的调度是以TaskSet为单位的。
    一个stage对应一个taskset
Task:
    driver发送到executor上执行的计算单元，每个task负责在一个阶段(stage)，处理一小片数据，计算出对应的结果
    Task是在物理节点上运行的基本单位，Task包含两类：ShuffleMapTask和ResultTask，分别对应于Stage中ShuffleMapStage和ResultStage中的一个执行基本单元。
    InputSplit-task-partition有一一对应关系,Spark会为每一个partition运行一个task来进行处理
总结SparkAPP-Action，Job，Stage，TaskSet，Task数量关系： SparkApplication-Action操作:Job:Stage:TaskSet:Task = 1:1:n:n:n*m(m个partition)
Cluster Manager:
    集群管理器，为每个spark application在集群中调度和分配资源的组件，如Spark Standalone、YARN、Mesos等
Deploy Mode:
    不论是standalone/yarn,都分为两种模式，client和cluster,区别在于driver运行的位置
    client模式下driver运行在提交spark作业的机器上,可以实时看到详细的日志信息，方便追踪和排查错误,用于测试
    cluster模式下，spark application提交到cluster manager，cluster manager(比如master)负责在集群中某个节点上，启动driver进程,用于生产环境
    通常情况下driver和worker在同一个网络中是最好的,而client很可能就是driver worker分开布置,这样网络通信很耗时,cluster没有这样的问题
DAGScheduler:
    根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler
TaskScheduler:
    将Taskset提交给Worker node集群运行并返回结果
Spark基本工作原理：
    Driver向Master申请资源；
    Master让Worker给程序分配具体的Executor
    Driver把划分好的Task传送给Executor，Task就是我们的Spark程序的业务逻辑代码
    job生成,stage划分和task分配都是发生在driver端?是
Shuffle算子:
    distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等
    容易导致部分task慢(数据倾斜)
    推算shuffle代码：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage
coalesce和repartition区别：
    coalesce: def coalesce(numPartitions: Int, shuffle: Boolean = false...
    repartition: def repartition(numPartitions: Int)
    coalesce将1000分区转换成100个分区不会shuffle反之会shuffle，但是大幅度合并分区会导致计算并行度不够，这时可以指定第二个参数shuffle为True，通过HashPartitioner分布数据，提高并行度。
    repartition也能增加和减少分区，会shuffle。如果只考虑减少分区数，建议用coalesce
    repartition只是coalesce接口中shuffle为true的简易实现
rdd的算子类型：
    transformation，rdd由一种转为另一种rdd
    action行动算子
    controller，控制算子(cache/persist) 对性能和效率的有很好的支持
RDD弹性：自动切换磁盘和内存，先存内存，内存放不下放磁盘/基于血统的容错机制/Task失败进行特定次数（4）的重试/动态调整数据分片个数
RDD是只读的
宽依赖-窄依赖：
    窄依赖：父RDD的一个分区只与一个子RDD的分区对应，子RDD的分区通常与多个RDD分区对应
    宽依赖：父RDD的一个分区要与多个子RDD的分区对应，子RDD的分区通常对应所有父RDD的分区
    对于性能优化，窄依赖更有利：
        RDD分区丢失重算时，窄依赖由于父RDD的一个分区对应子RDD的一个分区，只要重算子RDD对应的父RDD即可；而宽依赖最差情况下要计算所有父RDD
        宽依赖通常与shuffle有关
    窄依赖的函数有：map, filter, union, join(父RDD是hash-partitioned ), mapPartitions, mapValues
    宽依赖的函数有：groupByKey, join(父RDD不是hash-partitioned ), partitionBy
Stage划分原理：
    因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。（总结：遇到一个宽依赖就分一个stage）
Executor内存划分：
    Spark内存管理 源码分析https://zhuanlan.zhihu.com/p/134682872
    [堆外内存]堆外内存主要用于JVM自身，字符串，NIO Buffer等开销。堆外内存也分两块区域分别是ExecutionMemory和StorageMemory，默认各50%，UnifiedMemoryManager对堆外内存也使用了动态占用机制。堆外内存大小默认是executorMemory*0.1且最低限制384m。(所以报堆外内存不足时很可能是操作或者读写了单字段很大如Json串很长的表，Json串会被加载到堆外内存，太多大Json串作为对象放在堆外内存就会导致堆外内存不足，任务Job Aborted)
    [堆内内存]即为JVM最大堆内存(-Xmx)，Spark Executor的内存划分为四块，ExecutionMemory，StorageMemory，UserMemory，ReservedMemory(300m)。其中ExecutionMemory，StorageMemory，UserMemory是JVM使用的内存。
    - ExecutionMemory用于存放Shuffle、Join、Sort、Aggregation等计算过程中的临时数据
    - StorageMemory用于存储Spark的Cache数据，如RDD缓存
    - UserMemory用于RDD转换过程中需要的数据，如依赖信息，元数据
    - ReservedMemory预留内存，用于存储Spark内部对象
    ExecutionMemory和StorageMemory是统一内存，由spark.memory.fraction指定  默认0.6
spark数据倾斜及解决：
    因为数据倾斜是发生在shuffle过程中，首先根据Stage划分原理，定位代码中shuffle算子，结合WebUI中显示的每个task的数据量差异较大就是数据倾斜了。
    解决:1.hive端预处理，对hive数据均匀重分区，预聚合。
         2.数据倾斜的key无意义则过滤掉，有意义则单独处理这些key。
         3.提高shuffle并行度（增加shuffle read task），让原本给一个task的数据量分给多个task，从而让每个task处理比原来更少的数据
         4.rdd.reduceByKey或sparksql的group by预聚合
         5.reduce join转map join。存在小表join大表的场景。可以将小表进行广播从而避免shuffle。
         6.sample采样倾斜key单独进行join
         7.Spark-SQL distribute by col(字段尽量分布均匀无重复key，如果没有分布均匀的字段可以distribute by col,rand() distribute by 关键字控制map输出结果的分发,相同字段的map输出会发到一个reduce节点处理，如果字段是rand()一个随机数，能能保证每个分区的数量基本一致)
map、foreach、mapPartitions和foreachPartition区别：
    map：用于遍历RDD,将函数f应用于每一个元素，返回新的RDD(transformation算子)。
    foreach:用于遍历RDD,将函数f应用于每一个元素，无返回值(action算子)。
    mapPartitions:用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子）。
    foreachPartition: 用于遍历操作RDD中的每一个分区。无返回值(action算子)。
    总结：一般使用mapPartitions或者foreachPartition算子比map和foreach更加高效，推荐使用。
    详细对比：
    map: 比如一个partition中有100万条数据，10个partition，那么你的function要执行和计算1000万次。
    MapPartitions:一个partition仅仅会执行一次function，function一次接收一个partition数据，每次处理的是一个partition的数据。只要执行10次就可以了，性能比较高。
    将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接，而mapPartition为每个partition创建一个链接，则mapPartitions效率比map高的多。
    但是：
    map：function每次处理一条数据，处理完的数据在内存中过段时间就会被清掉，内存空间会被释放，一般不会导致OOM，内存溢出。
    MapPartitions： function一次接收一个partition数据，每次处理的是一个partition的数据，假如每个partition数据量很大，一次性全放入内存，容易发生OOM。
Storage内存分配：--executor-memory 18g
    WEB UI很清楚地看到 Storage Memory 的可用内存是 10.1GB，这个数是咋来的呢？根据前面的规则，我们可以得出以下的计算：
    systemMemory = spark.executor.memory
    reservedMemory = 300MB
    usableMemory = systemMemory - reservedMemory
    StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction
    如果我们把数据代进去，得出以下的结果：
    systemMemory = 18Gb = 19327352832 字节
    reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
    usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032
    StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB
    和上面的 10.1GB 对不上啊。为什么呢？这是因为 Spark UI 上面显示的 Storage Memory 可用内存其实等于 Execution 内存和 Storage 内存之和，也就是 usableMemory * spark.memory.fraction :
    StorageMemory = usableMemory * spark.memory.fraction = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB
    还是不对，这是因为我们虽然设置了 --executor-memory 18g ，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory=17179869184，然后计算的数据如下：
    systemMemory = 17179869184 字节
    reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
    usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
    StorageMemory= usableMemory * spark.memory.fraction = 16865296384 * 0.6 = 9.42421875 GB
    我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：
    systemMemory = 17179869184 字节
    reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
    usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
    StorageMemory = usableMemory * spark.memory.fraction = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB
    现在终于对上了。





Spark问题：
1.Spark Master HA主从切换会影响正在跑的任务吗？
    不会，因为之前已经申请过资源且注册过driver和executors，driver和executors之间通讯不走master。
2.Spark并行度怎么设置比较合适？
    spark作业中，各个stage的task的数量，也就代表了spark作业在各个阶段stage的并行度！并行度要与分配的资源匹配，否则会浪费资源。
    合理设置并行度，可以充分利用集群资源，减少每个task处理数据量，而增加性能加快运行速度。
    调并行度策略：一般是task数是Cpu总core数的2-3倍  本来最理想的状况是task数与application设置的core数相同，但是考虑到可能每个task跑的时间不一样，这种情况会资源浪费。task数是core的2-3倍就既不浪费还高效。
    调并行度方式：
    spark application并行度task数设置：1.spark.default.parallelism  2.数据在HDFS则增加Block数  3.repartition 4.spark.sql.shuffle.partitions
3.Spark中数据的位置是被谁管理的？
    每个数据分片都对应具体物理位置，数据的位置是被blockManager管理
4.为什么要进行序列化？
    减少存储空间和减少网络传输（缺点：序列化和反序列化消耗CPU）
5.MR和Spark的Shuffle异同？
    Hadoop是从Map(Spark的ShuffleMapTask)输出partition，不同分区数据送到不同的Reduce(Spark下一个Stage的ShuffleMapTask或者ResultTask)
    HadoopMR的Shuffle是基于归并排序，Spark的Shuffle基于Hash
    HadoopMR的Shuffle是Map端分片数据通过网络收集到reduce端，Spark的shuffle是stage间
6.reduceByKey和groupByKey区别？
    reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。（在数据移动之前聚合）
    groupByKey因为不接收函数，所以先将所有k-v都移动。开销大。
7.cache后面能不能接其他算子,它是不是action操作？
    cache之后可接算子，但是如果接了算子，如果是行动算子，会重新触发cache。cache不是action操作。通过unpersist取消cache（非lazy）
    默认的存储级别 - MEMORY_ONLY
8.Spark累加器有哪些特点？
    全局的，只增不减，记录全局集群的唯一状态
    在exe中修改它，在driver读取
    executor级别共享的，广播变量是task级别的共享
    两个application不可以共享累加器，但是同一个app不同的job可以共享
9.spark hashParitioner的弊端是什么？
    HashPartitioner确定分区的方式：partition = key.hashCode () % numPartitions 弊端：弊端是数据不均匀，容易导致数据倾斜，极端情况下某几个分区会拥有rdd的所有数据。
10.Spark中的HashShufle的有哪些不足？
    shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；
    容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息
    容易出现数据倾斜，导致OOM
11.RDD,DataFrame,DataSet区别：
    RDD无schema
    DataFrame有schema，且带有执行优化，如Filter优化，裁剪，提高效率，减少IO。
    DataSet是DataFrame的一个特例，Dataset每一个record存储的是一个强类型值而不是一个Row，可以在编译时检查类型。DataFrame和DataSet可以相互转化
12.cache,persist,checkpoint区别：
    会被重复使用的(但是)不能太大的RDD需要cache
    cache只能到memory，persist可以有多种选择，是否序列化以及磁盘还是内存
    运算时间很长或运算量太大才能得到的 RDD，computing chain 过长或依赖其他 RDD 很多的 RDD，用checkpoint，写磁盘。
    cache 机制是每计算出一个要 cache 的 partition 就直接将其 cache 到内存了。但 checkpoint 没有使用这种第一次计算得到就存储的方法，而是等到 job 结束后另外启动专门的 job 去完成 checkpoint 。 也就是说需要 checkpoint 的 RDD 会被计算两次。因此，在使用 rdd.checkpoint() 的时候，建议加上 rdd.cache()， 这样第二次运行的 job 就不用再去计算该 rdd 了，直接读取 cache 写磁盘。
    rdd.persist(StorageLevel.DISK_ONLY) 与 checkpoint 也有区别。前者虽然可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理，一旦Driver结束就被清理掉了，checkpoint的rdd不会被清理掉，可以被下一个Driver使用。
13.SparkSQL的几种Join：
    https://blog.csdn.net/wlk_328909605/article/details/82933552
    https://www.cnblogs.com/feiyumo/p/7417870.html
    https://www.cnblogs.com/suanec/p/7560399.html
14.SparkStreaming反压机制：
    Spark1.5之前通过spark.streaming.receiver.maxRate来限制每秒最大可接收条数，spark.streaming.kafka.maxRatePerPartition 参数来限制每次作业中每个 Kafka 分区最多读取的记录条数
    通过限制接收速率来适配处理能力，这种方式存才弊端：
        需要事先估计集群的处理速度，数据生产速度 - 不方便预估一个合适的值。
        需要人工干预手动调节。
        如果数据产生速度长时间大于最大接收速率，会造成数据积压不能及时处理甚至OOM。
    反压机制  version >spark1.5
        使用参数
            spark.streaming.backpressure.enabled = true
            spark.streaming.backpressure.initialRate： 启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置。
            spark.streaming.backpressure.rateEstimator：速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现。
            spark.streaming.backpressure.pid.proportional：用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。weight for response to "error" (change between last batch and this batch)
            spark.streaming.backpressure.pid.integral：错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。weight for the response to the accumulation of error. This has a dampening effect.
            spark.streaming.backpressure.pid.derived：对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.
            spark.streaming.backpressure.pid.minRate：可以估算的最低费率是多少。默认值为 100，只能设置成非负值。
        为了实现自动调节数据的传输速率，在原有的架构上新增了一个名为 RateController 的组件，这个组件继承自 StreamingListener，其监听所有作业的 onBatchCompleted 事件，并且基于 processingDelay 、schedulingDelay 、当前 Batch 处理的记录条数以及处理完成事件来估算出一个速率；这个速率主要用于更新流每秒能够处理的最大记录的条数。速率估算器（RateEstimator）可以又多种实现，不过目前的 Spark 2.2 只实现了基于 PID 的速率估算器。
        InputDStreams 内部的 RateController 里面会存下计算好的最大速率，这个速率会在处理完 onBatchCompleted 事件之后将计算好的速率推送到 ReceiverSupervisorImpl，这样接收器就知道下一步应该接收多少数据了。
        如果用户配置了 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition，那么最后到底接收多少数据取决于三者的最小值。也就是说每个接收器或者每个 Kafka 分区每秒处理的数据不会超过 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition 的值。
15.HashShuffle和SortShuffle
    HashShuffle已被弃用，原理是：每个map task的不同结果写入buffer默认32k，一个buffer对应一个小文件写磁盘，reduce拉取磁盘小文件 =>效率低 小文件过多 容易OOM(磁盘小文件多，IO增多，GC增多)
    SortShuffle分两种：普通机制(reduceByKey),Bypass机制(sortByKey,repartition) 原理：maptask数据先写内存(默认5M)，如果写入内存大小超5M就尝试申请更多内存(5.01*2-5)，如果申请成功则不溢写磁盘，否则会溢写磁盘，可能会溢写多个磁盘文件，再合并为一个大磁盘文件，且生成一个索引文件。reduce去map端拉取数据时先解析索引文件，再根据Offset获取到对应数据。
        ByPass机制触发条件：Shuffle reduce task 数量小于spark.shuffle .sort.bypassMerge Threadshold参数的值，小于200，不开启，溢写磁盘不需要排序，小于等于的时候是开启的。
        优化机制产生的磁盘小文件的个数：C*R    Hash shuffle：产生的磁盘小文件：M*R       Sort shuffle产生的磁盘小文件的个数为：2*M      Bypass机制产生的磁盘小文件的个数为:2*M
16.动态资源自适应调节 没使用executor时节约资源
    val sparkConf = new SparkConf()
    .set("spark.shuffle.service.enabled", "true")
    .set("spark.dynamicAllocation.enabled", "true")
    .set("spark.dynamicAllocation.minExecutors", "1")   //最少1个Executor
    .set("spark.dynamicAllocation.initialExecutors", "1")  //默认初始化一个Executor
    .set("spark.dynamicAllocation.maxExecutors", "6")   //最多开6个Executor
    .set("spark.dynamicAllocation.executorIdleTimeout", "60")  //executor闲置时间
    .set("spark.dynamicAllocation.cachedExecutorIdleTimeout", "60")  //cache闲置时间 如果代码用了cache，不加这个会不释放动态资源
    .set("spark.executor.cores", "3") ...






spark逻辑计划源码阅读：
http://wp.wjcodes.com/archives/205  （sparksql内核解析上）
http://wp.wjcodes.com/archives/206  （sparksql内核解析下）
https://zhuanlan.zhihu.com/p/73052739


Spark Job 默认的调度模式 - FIFO
RDD 特点 - 可分区/可序列化/可持久化
Broadcast - 任何函数调用/是只读的/存储在各个节点
Accumulator - 支持加法/支持数值类型/可并行
master 和 worker 通过 Akka 方式进行通信的


在Spark shell中用scala语言编写spark程序
sc.textFile("hdfs://master01:9000/RELEASE").flatMap(_.split(" "))
.map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://master01:9000/out")
上面代码的解释：
sc是SparkContext对象，该对象时提交spark程序的入口
textFile(hdfs://master01:9000/RELEASE)是hdfs中读取数据
flatMap(_.split(" "))先map在压平
map((_,1))将单词和1构成元组
reduceByKey(_+_)按照key进行reduce，并将value累加
saveAsTextFile("hdfs:// master01:9000/out")将结果写入到hdfs中


Spark On Yarn资源分配：
spark.driver.memory
spark.executor.memory
spark.yarn.am.memory
spark.executor.memoryOverhead：值为executorMemory * 0.07, with minimum of 384
spark.yarn.driver.memoryOverhead：值为driverMemory * 0.07, with minimum of 384
spark.yarn.am.memoryOverhead：值为AM memory * 0.07, with minimum of 384
分配Container时：
Container分配的内存大小是通过spark.executor.memory+spark.executor.memoryOverhead计算出来的且为yarn.scheduler.minimum-allocation-mb的整数倍且不大于yarn.scheduler.maximum-allocation-mb
spark.executor.memory+spark.executor.memoryOverhead <= yarn.scheduler.maximum-allocation-mb  如果大于就会申请失败  设置Container最大容器内存时并不知道机器实际剩余内存多少，可能配8G但机器只剩4G这时申请Container超过4G资源也失败
一个driver和10个executor其内存都没超过yarn.scheduler.maximum-allocation-mb，则一共分配11个container
10个NodeManager，30个SparkExecutor，则每个NM节点启动3个Executor

AM对应的Container内存由spark.yarn.am.memory加上spark.yarn.am.memoryOverhead来确定
一个Container就是一个进程=内存隔离，对应Spark的一个进程（driver或Executor）
一个SparkApplication（driver个数1，executor个数N）->Container数N，其中Driver在启动节点，Executor在各个节点以Container形式封装，N个Executor->N个Container进程，可以通过Container进程状态分析Executor状态
由这六个参数决定
storage memory在Spark WebUI上的值怎么来的？
  我记得Spark内存模型中有UserMemory，StorageMemory，ExecutionMemory和ReservedMemory，期中缓存内存和执行内存可互相借用。
  StorageMemory内存区域总大小由spark.storage.memoryFraction决定默认是0.6，而为了避免OOM这个StorageMemory只用其区域的90%（由spark.storage.safetyFraction默认0.9决定）
  所以如果Executor堆内存为x，则storage memory大致可以计算：x*0.9*0.6=0.54x
  因为互相借用，所以可以得到近似值，而不是准确值。因为内存被借用后不会立即归还，而是等其中一个区域不够才会去另一个区域拿。
每个NM能分配的Container数？
  1.yarn.nodemanager.resource.memory-mb 控制每个节点上的容器使用的最大内存总和。
  2.yarn.nodemanager.resource.cpu-vcores 控制每个节点上的容器使用的最大内核总数。
  3.yarn.sheduler.maximum-allocation-vcores
  一般就设置成4个，cloudera公司做过性能测试，如果CPU大于等于5之后，CPU的利用率反而不是很好。这个参数可以根据生成服务器决定，比如公司服务器很富裕，那就直接设置成1:1；设置成32，如果不是很富裕，可以直接设置成1:2。我们以1:2来计算。
  4.yarn.scheduler.minimum-allocation-vcores
  如果设置vcoure = 1，那么最大可以跑64/1=64个container，如果设置成这样，最小container是64/4=16个。
  5.yarn.scheduler.minmum-allocation-mb
  如果设置成2G，那么90/2=45最多可以跑45个container，如果设置成4G，那么最多可以跑24个；vcore有些浪费。
  6.yarn.scheduler.maximum-allocation-mb
  这个要根据自己公司的业务设定，如果有大任务，需要5-6G内存，那就设置为8G，那么最大可以跑11个container。
  可分配Container数由1,2,4,5决定

spark加JVM日志打印
"spark.driver.extraJavaOptions":"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps",

Spark Broadcast https://blog.csdn.net/zx8167107/article/details/79186477
spark 操作 https://www.cnblogs.com/Frank99/p/8295949.html
PYSPARK_DRIVER_PYTHON=ipython $SPARK_HOME/bin/pyspark
Structured Streaming分析https://www.jianshu.com/p/d4e2a2be9a10

df.repartition(10).rdd.getNumPartitions()  重分区并查看分区数

给读取文件的df应用schema:
pyspark.sql.types.StructType
schema = StructType()
schema.add(StructField(field_name, StringType(), True))
schema类型StructType(List(StructField(eid,StringType,true),StructField(name,StringType,true),StructField(shareholders_tree,StringType,true),StructField(shareholders_count,StringType,true),StructField(holding_tree,StringType,true),StructField(holding_count,StringType,true),StructField(group_id,StringType,true),StructField(split_tag,StringType,true),StructField(created_time,StringType,true),StructField(updated_time,StringType,true)))
df = spark.read.csv("{hdfs_file_path}".format(hdfs_file_path=hdfs_file_path),schema=schema,sep=u"\u0001")
df = spark.read.csv("{hdfs_file_path}".format(hdfs_file_path=hdfs_file_path),schema=schema,sep=u"\u0001", multiLine=True)  # 单个字段存在多行数据的情况 multiLine=True可以解决字段可能串位的问题
df = spark.read.json("/opt/test/test.json")
df.collect().asDict()  # pyspark df结果转dict
#Spark加载mongo
conf = {'spark.mongodb.input.partitionerOptions.partitionKey': '_id', 'pipeline': '{$match:{last_updated_time:{$gte:1582041600000}}}', 'spark.mongodb.input.partitionerOptions.numberOfPartitions': '2000', 'spark.mongodb.input.readPreference.name': 'secondaryPreferred', 'spark.mongodb.input.partitioner': 'MongoPaginateByCountPartitioner', 'uri': 'mongodb://user:passwd@ip:port/db.collection'}
spark.read.format("com.mongodb.spark.sql").options(**conf).load()
如果有schema schema=StructType.fromJson(schema_dict)
如果有schema schema=StructType.fromJson(schema_dict)
spark.read.format("com.mongodb.spark.sql").options(**conf).load(schema=StructType.fromJson(schema_dict))
#Spark加载mysql
conf={'user': 'spark', 'url': u'jdbc:mysql://ip:3306/db?zeroDateTimeBehavior=convertToNull&autoReconnect=true', 'driver': 'com.mysql.jdbc.Driver', 'password': '123456', 'db': u'db', 'dbtable': u"(select * from db.tb where field_name>='1582214400000' and field_name<'1582300800000' )tmp", 'tb': u'tb'} # dbtable可参考官网
df = spark.read.format("jdbc").options(**conf).load()
#spark读取avro
avro_df = spark.read.format("com.databricks.spark.avro").load(hdfs_path)
#spark读csv自动解析schema
spark.read.csv("E:\Video_Games_Sales_CSV.csv",header=True)  # 加header=True



spark sql DataFrame的差集，交集，合集
差集（与df顺序有关）：df =  df1.select("user_id","auto_id").subtract(df2.select("user_id","auto_id"))  # scala是except
交集：df =  df1.select("user_id","auto_id").intersect(df2.select("user_id","auto_id"))
并集：df =  df1.select("user_id","auto_id").union(df2.select("user_id","auto_id"))  不去重

spark sql生成df：
sentenceDataFrame = spark.createDataFrame((
      (1, "asf"),
      (2, "2143"),
      (3, "rfds")
    )).toDF("label", "sentence")
sentenceDataFrame.show()

spark.sql("""select CAST(unix_timestamp() as string)""").show(1,False)   # 查看当前时间戳，转为string
spark.sql("""select CAST(unix_timestamp() as bigint)""").show(1,False)   # 查看当前时间戳，转为bigint

df.dtypes
http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame

df = df.withColumn("enddate", judge_date_udf("enddate")).withColumn().withColumn()......

shuffle write：分区数由上一阶段的RDD分区数控制
类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则，临时存放到各个executor所在的本地磁盘上。
shuffle read：分区数由Spark提供的参数控制
如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。
有时候即使不会导致JVM crash也会造成长时间的gc。

TaskKilled (another attempt succeeded)  Task计算时间太长被干掉重试

UDF
judge_date_udf = udf(judge_date,TimestampType())
df = df.withColumn("enddate", judge_date_udf("enddate"))
-=-=-=-=-=-
from pyspark.sql.types import *
def length(ip):
    if ip:
        return len(ip)
    else:
        return 0
spark.udf.register("length", length, IntegerType())
spark.sql("select sum(length(ip)),sum(length(language)) from table").show(1, False)

info_dicts = spark.sql("""desc %s""" % table_name).toPandas().to_dict()   # 转换成pandas的df
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
    (info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))
df.registerTempTable('temp_table')
info_df = spark.sql("desc formatted temp_table")
col_comment_in_df = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.asDict().get('col_name'): x.asDict().get('comment')},
info_df.select('col_name', 'comment').collect()))   # 获取到df每个字段的注释
    paras = []
    for col_name, data_type in col_comment_in_hive.keys():
        if col_name.find("#") < 0 and col_name != 'dt_batch':   # 不对dt_batch字段做注释处理
            hive_comment = col_comment_in_hive.get((col_name, data_type))
            mysql_comment = comment_info_in_mysql.get(col_name)  # t_comment_records表中的注释
            if not mysql_comment:
                mysql_comment = ''
            if hive_comment != mysql_comment and mysql_comment:  # t_comment_records中存在注释且hive表中的注释与mysql里不同
                # 需要更新注释
                paras.append((str(table_name), str(col_name), str(col_name), str(data_type), str(mysql_comment)))
            else:
                # 不需要更新注释 可能是因为hive有注释而mysql没有，也可能是hive和mysql中都没有
                if (hive_comment != '' and hive_comment != '无注释' and hive_comment != u'None' and hive_comment) and not mysql_comment:
                    # hive有注释而mysql没有
                    logger.warn("表%s的%s字段有注释而Mysql未记录,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','in hive')""" % (table, col_name))
                if (hive_comment == '' or hive_comment == '无注释' or hive_comment == u'None' or not hive_comment) and not mysql_comment:
                    # hive和mysql中都没有
                    logger.warn("表%s的%s字段没有注释,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','no comment')""" % (table, col_name))

    if paras:
        flag = True
        hive_cursor = create_hive_cursor()
        column_comment_dict = {}  # 让spark的元数据修改的dict
        for para in paras:
            tb = para[0]
            col = para[1]
            types = para[3]
            comment = para[4]
            replace_para = []
            # 遇到'array<struct<_id:struct<_oid:string>,_code:int,name:string>>'类型自动加反引号
            for i in types.split('<'):
                logger.info(i)
                if i.startswith('_'):
                    replace_para.append(i.split(':')[0])
                if len(i.split(',')) != 1:
                    for j in i.split(','):
                        if j.startswith('_'):
                            replace_para.append(j.split(':')[0])
            replace_para = set(replace_para)
            logger.info(replace_para)
            if replace_para:
                for i in replace_para:
                    types = types.replace(i, "`" + i + "`")
            # hive的注释元数据修改
            alter_sql = """ALTER TABLE %s CHANGE COLUMN `%s` `%s` %s COMMENT '%s' """ % (tb, col, col, types, comment)
            logger.info(alter_sql)
            hive_cursor.execute(alter_sql)
            logger.info("[hive元数据] 表%s的字段注释已修改" % table_name)
            column_comment_dict[col] = comment

        # spark的注释元数据修改
        change_sql = get_spark_ddl_sql(table_name, column_comment_dict, hive_cursor)
        sql_list = change_sql.encode('utf-8').split('\n')
        for s in sql_list:
            if s:
                try:
                    hive_cursor.execute(s.rstrip(';'))
                    logger.info("[Spark SQL元数据] 表%s的SparkSQL注释元数据已修改" % table_name)
                except Exception as e:
                    logger.warn("Spark SQL元数据修改失败，错误信息：" , e)
        # if len(change_sql) > 0:
        #     filename = "/data/ddl_change.txt"
        #     if os.path.exists(filename):
        #         print(filename + "已存在,新建该文件")
        #         os.remove(filename)
        #     f = open("/data/ddl_change.txt", "w")
        #     f.write(change_sql)
        #     f.close()
        # command = "hive  -f /data/ddl_change.txt "
        # status, text = commands.getstatusoutput(command)
    else:
        logger.info("表的字段注释无需修改")


spark.driver.maxResultSize worker发送给driver的最大结果大小，超过限制会报错，如果不限制，会无限占用driver内存，可能将Driver内存耗尽

spark_ddl = df._jdf.schema().treeString()
df_schema = df.schema().json()

spark catalog=sparksql查看表元数据的API
```python
catalog = spark.catalog
catalog.listDatabases()  # 查看数据库  scala版返回df
catalog.listColumns(table_name)  # 查看表的列  scala版返回df
catalog.listTables(db_name=None)  # 查看数据库的表  scala版返回df
catalog.listFunctions()  # 查询已注册的udf  scala版返回df
catalog.setCurrentDatabase(db_name) # 设置当前catalog对象的db
catalog.currentDatabase()  # 查看当前使用的数据库
catalog.registerFunction(name, f, returnType=StringType())  #调用_jsparkSession和_judf注册UDF name->UDFname f->PythonFunction returnType->返回类型pyspark.sql.types.DataType
catalog.isCached(db_table_name) # 查看表是否被缓存
catalog.dropTempView(view_name) # 如果是Spark SQL视图，会删除事先注册好的，如果hive视图，删除元数据中hive视图
catalog.uncacheTable(table_name） # 取消缓存
catalog.cacheTable(table_name) # 缓存一张表
scala版本独有：catalog.databaseExists(db_name)  # 查看库是否存在  在python中使用这个：spark._jsparkSession.catalog().databaseExists('default')
```

比较时间大小
spark.sql("""
        SELECT t1.task_id AS ti,
            t1.table_name AS tb,
            cast(t2.execution_date AS BIGINT)-8*3600 AS ed,
            cast(unix_timestamp('%s','yyyyMMdd') AS BIGINT) AS yesterday
        FROM airflow_task_table_relation t1
        LEFT JOIN task_instance t2 ON t1.task_id = t2.task_id
        WHERE t1.table_type = 'target'
        HAVING ed >= yesterday
        AND ed <= yesterday+86400
""" % batch[0: 8]).drop('yesterday').collect()

数据抽取-根据主键获取mysql删除数据 同步更新到hive
native_all 是存量hive数据
delete_data = native_all.join(primary_key_df,primary,"left_anti")  # delete_data是hive存量数据 join sparkjdbc主键和增量字段获取的primary_key_df
all_full = all_full.join(delete_data,primary,"left_anti")  # 没join上的返回左df也就是all_full的数据 实现去除mysql删除数据

###################################################
Scala相关
获取一个变量的类型：  变量.getClass.getSimpleName
编程规范：https://blog.csdn.net/bluejoe2000/article/details/101520022
多线程Actor模型 实践：https://blog.csdn.net/gdkyxy2013/article/details/77824454?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control
多线程：https://blog.csdn.net/weixin_40873462/article/details/89680070
多线程：https://zhuanlan.zhihu.com/p/103808773
偏函数和偏应用函数https://www.jianshu.com/p/0a8a15dbb348  以及scala学习笔记

###################################################
Flink相关
Calcite解析SQL使用：https://blog.csdn.net/u011250186/article/details/106856572
基于Flink+Hive构建流批一体准实时数仓：https://mp.weixin.qq.com/s/FFlh472O6AYkNo77DCXZzQ

一站式流式处理云平台解决方案：
https://github.com/edp963/wormhole


###################################################
Kafka相关
CDH-配置 Kafka MirrorMaker 实现 Kafka 集群消息同步：https://blog.csdn.net/weixin_43215250/article/details/100521107
滴滴开源Logi-KafkaManager 一站式Kafka监控与管控平台：https://mp.weixin.qq.com/s/N2fZSnzy7yiH5p-svdpJWg
Kafka监控（kafka-manager）：https://www.cnblogs.com/zlslch/p/7252484.html


###################################################
Hive相关

sql: set -v;查看当前的server配置

Hive之序列化与反序列化（SerDe）：https://www.cnblogs.com/rrttp/p/9024153.html

Python连接Hive
1.pyspark 
2.pyhive 
3.jdbc 
from impala.dbapi import connect
conn = connect(host='host',port=10000,database='default',auth_mechanism='PLAIN',user='abcd',password='123456')
cur = conn.cursor() 
或
import jaydebeapi
conn = jaydebeapi.connect("org.apache.hive.jdbc.HiveDriver","jdbc:hive2://HOST:10000/default",["root", "123456"],"D:\Programming\Env\Java\ExternalJars\hive-jdbc-1.1.0.jar")
curs = conn.cursor()

hivevar与hiveconf等预设变量：
命名空间    使用权限    详细描述
hivevar       rw    用户自定义变量
hiveconf      rw    hive相关配置属性
system        rw    java定义的配置属性
env           r    Shell环境属性
用法: set aa = '1';  select ${hiveconf:aa};

HQL:
EXISTS在SQL中的作用是：检验查询是否返回数据。当 where 后面的条件成立，则列出数据，否则为空.
LEFT JOIN等价于LEFT OUTER JOIN.

建表指定压缩格式和存储格式
STORED AS parquet tblproperties ('parquet.compression'='snappy');
stored as orc tblproperties ('orc.compress'='snappy');
stored as parquet tblproperties ('parquet.compression'='lzo');

1.in/exists的优化方案：
  select a.id,a.name from a where a.id in (selecr b.id from b);
  select a.id,a.name from a where a.id exists (selecr b.id from b where a.id = b.id);
  优化为 select a.id,a.name from a left semi join b on a.id = b.id;
  LEFT SEMI JOIN:左半连接，会返回左边表的记录，前提是其记录对于右边表满足ON语句中的判定条件。对待右表中重复key的处理方式差异：因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join on 则会一直遍历。
left semi join 中最后 select 的结果只许出现左表，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。
2.not in优化
  原SQL：select col from t1 where a not in (select col from t2)
  替换1：select col from t1 a left join t2 b on a.col = b.col where b.col is null;
  替换2：select col from t1 where (select count(1) as num from t2 where t1.col = t2.col) = 0;
  替换3：select col from t1 where not exists (select col from t2 where t1.col=t2.col);(not in会把null当false，not exists会把null当true，所以如果字段有空值，注意判断)
  替换4：select col from t1 a left anti join t2 b on a.col = b.col;(反连接，相当于not exists)
  

数据去重的三种方法：
distinct,group by与ROW_Number()
1. Distinct用法：对select 后面所有字段去重，并不能只对一列去重。
（1）当distinct应用到多个字段的时候，distinct必须放在开头，其应用的范围是其后面的所有字段，而不只是紧挨着它的一个字段，而且distinct只能放到所有字段的前面
（2）distinct对NULL是不进行过滤的，即返回的结果中是包含NULL值的
（3）聚合函数中的DISTINCT,如 COUNT( ) 会过滤掉为NULL 的项
2.group by用法：对group by 后面所有字段去重，并不能只对一列去重。
3. ROW_Number() over(partition by xx order by xx) r  窗口函数 where  r = 1
参考https://blog.csdn.net/weixin_30938149/article/details/99863639?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase


分区分文件夹，分桶分文件
hive表和spark sql创建的hive表 元数据都存在hive中，但是不统一，spark sql的元数据基于hive，但不一致，sparksql的元数据可以desc formatted方式看到，在下方，（        spark.sql.create.version        2.2 or prior
Hive map join：https://www.jianshu.com/p/b52466e93226
                                                                                                            spark.sql.sources.schema.part.0）
查看sparksql的元数据：
元数据分区数hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.numParts')" % table_name )    part_num = hive_cursor.fetchone()[0]   part_num=int(part_num.strip())  得到0，1，2...
单独一个分区的元数据hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.part.%s')" % (table_name,num_partition))  # num_partition  0,1,2...
元数据修改：ALTER TABLE %s SET TBLPROPERTIES ('spark.sql.sources.schema.part.%s' ='%s');  。。。。 '{\"fields\": [{\"metadata\": {\"comment\": \"eid\"}, \"type\": \"string\", \"name\": \"eid\", \"nullable\": true},。。。}

修改Hive/SQL表注释: ALTER TABLE table_name SET TBLPROPERTIES('comment' = '这是表注释!');

修改Hive/SQL字段注释: ALTER TABLE table_name CHANGE COLUMN muid muid_new STRING COMMENT '这里是列注释!'; 

hive查看分区:show partitions table_name;

ALTER TABLE test_table DROP PARTITION (dt_batch='2016-08-08', hour='10');  # hive删除分区的方法
ALTER TABLE test_table DROP PARTITION (dt_batch>'20200620',dt_batch<'20201231'); hive批量删除分区


Hive合并多个分区为一个分区:
create table if not exists test_new(id int,name string,age int) partitioned by(dt string) row format delimited fields terminated by ',' stored as textfile;
alter table test_new add partition (dt="20150608") partition(dt="20150612") partition(dt="20151115") partition(dt="20160825") partition(dt="20170605") partition(dt="20200525");
create table if not exists test(id int,name string,age int) partitioned by(dt string) row format delimited fields terminated by ',' stored as textfile;
load data local inpath '/datas/root/test' into table test_new partition(dt="20150608");
load data local inpath '/datas/root/test' into table test_new partition(dt="20150612");
load data local inpath '/datas/root/test' into table test_new partition(dt="20151115");
load data local inpath '/datas/root/test' into table test_new partition(dt="20160825");
load data local inpath '/datas/root/test' into table test_new partition(dt="20170605");
load data local inpath '/datas/root/test' into table test_new partition(dt="20200525");
insert overwrite table test partition (dt="20200524")  select id,name,age from test_new where dt <= "20200524";
alter table test_new add partition (dt="20200525") ;

合并小文件的表并合并多个分区为一个分区
    1.备份原表 create table 备份表 like 原表; hadoop fs -cp 原表目录 备份表目录 msck repair table 备份表;
    2.合并原表分区 使用spark repartition或hive set hive.execution.engine=mr;set hive.merge.mapredfiles=TRUE;set hive.merge.smallfiles.avgsize=100000000;set mapred.reduce.tasks=xx;insert overwrite table 表(partition) select * from 表 where partiton_col...;  
    3.验证数据量没问题以及hdfs文件大小
    4.删除多余分区ALTER TABLE 表 DROP PARTITION (dt_batch>'20200620',dt_batch<'20201231'); ...

Hive表数据转移-表迁移：
    hadoop fs -cp 原表文件 目标表路径
    msck repair table 目标表 
    注意：如果只移动部分文件，可以msck，得到表数据就是部分文件中包含的数据，如果后面新加入文件，可以再次msck，就会有更多数据

插入文件中数据到Hive表：
    需要文件数据格式与建表格式相同
    LOAD DATA LOCAL INPATH '/data/xxx' overwrite into TABLE table_name PARTITION(dt=xxx)
    完成后去相应的文件夹下看，xxx文件被移动到那个分区目录下，并且名字没变。

##################################################
Mysql相关

查看状态和负载
查看并发线程数
show status like 'Threads%';
show status 下面有一些参数可以参考 https://www.cnblogs.com/zuxing/articles/7761262.html 和  https://blog.csdn.net/sdhymllc/article/details/88052704
select * from information_schema.processlist where user='root';与show processlist类似，能进行筛选想看的信息。
查看库的信息/机器IP
show slave status\G;
命令：show status like '%下面变量%'; 
Aborted_clients 由于客户没有正确关闭连接已经死掉，已经放弃的连接数量。 
Aborted_connects 尝试已经失败的MySQL服务器的连接的次数。 
Connections 试图连接MySQL服务器的次数。 
Created_tmp_tables 当执行语句时，已经被创造了的隐含临时表的数量。 
Delayed_insert_threads 正在使用的延迟插入处理器线程的数量。 
Delayed_writes 用INSERT DELAYED写入的行数。 
Delayed_errors 用INSERT DELAYED写入的发生某些错误(可能重复键值)的行数。 
Flush_commands 执行FLUSH命令的次数。 
Handler_delete 请求从一张表中删除行的次数。 
Handler_read_first 请求读入表中第一行的次数。 
Handler_read_key 请求数字基于键读行。 
Handler_read_next 请求读入基于一个键的一行的次数。 
Handler_read_rnd 请求读入基于一个固定位置的一行的次数。 
Handler_update 请求更新表中一行的次数。 
Handler_write 请求向表中插入一行的次数。 
Key_blocks_used 用于关键字缓存的块的数量。 
Key_read_requests 请求从缓存读入一个键值的次数。 
Key_reads 从磁盘物理读入一个键值的次数。 
Key_write_requests 请求将一个关键字块写入缓存次数。 
Key_writes 将一个键值块物理写入磁盘的次数。 
Max_used_connections 同时使用的连接的最大数目。 
Not_flushed_key_blocks 在键缓存中已经改变但是还没被清空到磁盘上的键块。 
Not_flushed_delayed_rows 在INSERT DELAY队列中等待写入的行的数量。 
Open_tables 打开表的数量。 
Open_files 打开文件的数量。 
Open_streams 打开流的数量(主要用于日志记载） 
Opened_tables 已经打开的表的数量。 
Questions 发往服务器的查询的数量。 
Slow_queries 要花超过long_query_time时间的查询数量。 
Threads_connected 当前打开的连接的数量。 
Threads_running 不在睡眠的线程数量。 
Uptime 服务器工作了多少秒。
查看什么命令使用最多：SHOW GLOBAL STATUS LIKE 'COM_%';
查看innode存储引擎状态：SHOW GLOBAL STATUS LIKE 'Innodb_%';
查看表锁状态show status like 'table%';
查看行锁状态：show global status like 'innodb_row_lock%';
查看mysql当前设置的参数show variables;
查看当前会话状态show status;
查看全局状态show global status;
查看缓存数量：show status like 'Qcache%';


MySQL出现Waiting for table metadata lock的原因以及解决方法:
出现场景https://www.cnblogs.com/digdeep/p/4892953.html
show processlist ；显示的数据里有个id字段，就是sessionid，执行 kill id就可
还可以调整超时锁的时间阈值  默认阈值1年。。。 下面改为1800s
set session lock_wait_timeout = 1800;
set global lock_wait_timeout = 1800;


三大范式：
1NF：表的每个列都有原子性(所有字段都是不可分割的原子值) - 提高数据库性能
2NF：表的每个列都与主键相关(一个表只能保存一种数据，不可以把多种数据保存在同一张表) - 减少数据冗余
3NF：表的每个列都与主键直接相关而非间接相关 - 减少数据冗余

获取一个表的主键：
select column_name from information_schema.COLUMNS where TABLE_NAME='system_state' and TABLE_SCHEMA='d_bigdata' and COLUMN_KEY='PRI';


explain分析sql性能的方法：
select_type :
    SIMPLE: 简单的查询，不包括子查询，关联查询等等
    PRIMARY: 查询中如果有复杂的部分，最外层的查询将被标记为PRIMARY
    SUBQUERY: 子查询中的第一个查询
    UNION: 关联查询，最后面的一个
type：查询语句的性能表现: 依次递增 all<index<range<index_merge<ref<eq_ref<constant/system
    All:  全表扫描，最耗性能
    index: 全索引列扫描
    range: 对单个索引列进行范围查找 ,使用 < 或者 between and 或者 in 或者 !=
    index_merge: 多个索引合并查询
    ref:  根据单个索引查找
    eq_ref: 连接时使用primary key 或者 unique类型
    constant: 常量
    system: 系统
possible_keys: 可能使用到的索引
key: 真实使用的索引
key_len: 使用到的索引长度
rows：扫描的行数
extra：包含MySQL为了解决查询的详细信息
使用PROCEDURE ANALYSE()分析优化表数据类型select * from table PROCEDURE ANALYSE();


没数据则插入 有数据则修改（根据主键或唯一索引判断的，replace into跟insert功能类似，不同点在于replace into首先尝试插入数据到表中，如果根据主键或者唯一索引判断发现表中已经有此行数据，则先删除此行数据，然后插入新的数据；否则，直接插入新数据。要注意的是：插入数据的表必须有主键或者是唯一索引！否则的话，replace into会直接插入数据，这将导致表中出现重复的数据。）
REPLACE INTO d_bigdata.system_state(name,state,info) VALUES('$2','$3','$4');
INSERT INTO dss_table_meta(table_name,create_time,update_time) VALUES ('%s','%s','%s') ON DUPLICATE KEY UPDATE create_time='%s',update_time='%s';  主键冲突则修改（table_name是主键）

查询多久时间之前的时间
select DATE_SUB(start_time,Interval 2 DAY) from table;
select DATE_SUB(now(),Interval 60 MINUTE);
select datediff(DATE_SUB(now(),Interval 2 DAY), now()) as diff;


# 无论什么的cursor（mysql、 hive），多次execute后，fetch只能fetch到最后一次的execute数据
查看hive表的注释 SHOW TBLPROPERTIES table_name ('comment');
    hive_cursor.execute("SHOW TBLPROPERTIES %s ('comment')" % source_table_name )
    table_comment = hive_cursor.fetchone()

查看mysql注释的简便方法 SELECT TABLE_COMMENT FROM INFORMATION_SCHEMA.TABLES  WHERE TABLE_NAME = 'tb' AND TABLE_SCHEMA ='db';

col_comm_in_mysql = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.get('col'): x.get('comment')}, fetch))   # 得到字段-注释 的dict
df = spark.sql("""desc %s""" % table_name).toPandas()   # 转换成pandas的df
info_dicts = df.to_dict()
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
(info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))



with xx as语法 可以实现一个或多个别名（貌似sqlserver语法，mysql不支持）
在Mysql中用临时表：  语法：create temporary table aaa select * from student; 这样就创建了aaa临时表
with as 相当于虚拟视图
with as短语，也叫做子查询部分(subquery factoring)，可以让你做很多事情，定义一个sql片断，该sql片断会被整个sql语句所用到。有的时候，是为了让sql语句的可读性更高些，也有可能是在union all的不同部分，作为提供数据的部分。
特别对于union all比较有用。因为union all的每个部分可能相同，但是如果每个部分都去执行一遍的话，则成本太高，所以可以使用with as短语，则只要执行一遍即可。如果with as短语所定义的表名被调用两次以上，则优化器会自动将with as短语所获取的数据放入一个temp表里，如果只是被调用一次，则不会。而提示materialize则是强制将with as短语里的数据放入一个全局临时表里。很多查询通过这种方法都可以提高速度。
with tmp_table as (select * from table where i>1),
with tmp_table1 as (select * from table where i>2),
with tmp_table2 as (select * from table where i>3);
select * from tmp_table;
优点
(1). SQL可读性增强。比如对于特定with子查询取个有意义的名字等。
(2)、with子查询只执行一次，将结果存储在用户临时表空间中，可以引用多次，增强性能。

小技巧：
where 1=1 and xxx 有时候场景需要根据条件拼sql，如果拼接的字符串为空，那就要把where去掉，麻烦  所以 先 where 1=1，然后所有拼接的语句有直接加 “AND xx=xx” 方便，且where 1=1 始终为True。
where 1=0同理始终为False 始终无结果
update b set address = concat('1',address);  更新一个表中address字段让它每个前面都加上1
EXISTS代替IN：
IN流程：先查询IN的子查询，然后与外表做一个笛卡尔积，然后根据条件筛选,适合子查询数据少。select * from user where id in (select id from order) a;
EXISTS流程：先遍历外表，检测是否存在于子查询结果，匹配上就放入结果集，适合子查询数据多。 select user.* from user where user.id exists (select order.id from order where user.id=order.id) a;
in 是把外表和内表作hash连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。
not in 和not exists：如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。


##################################################















##################################################
Linux相关

rsync使用： rsync -lr fileOrDir root@192.168.1.101:/root/

pwd -P . 获取当前目录的全路径
两台节点不同用户免密：只需将id_rsa.pub值复制到另一台节点另一用户的~/.ssh/authorized_keys中并确保该文件权限为600即可

shell比较浮点数大小
awk -v var=4.2 'BEGIN{print (var > 4 ? 0 : -1)}'

shell 字典
declare -A dic dic=([key1]="value1" [key2]="value2" [key3]="value3") #打印指定key的value echo ${dic["key1"]} #打印所有key值 echo ${!dic[*]} #打印所有value echo ${dic[*]} #遍历key值 for key in $(echo ${!dic[*]}) do echo "$key : ${dic[$key]}" done echo "shell定义数组" #数组 list=("value1" "value2" "value3") #打印指定下标 echo ${list[1]} #打印所有下标 echo ${!list[*]} #打印数组下标 echo ${list[*]} #数组增加一个元素 list=("${list[@]}" "value3")
遍历字典
declare -A dic path_threshold_dict=(\
[/home]="3000" \
[/tmp]="5000" \
[/root]="5000" \
[/var]="5000" \
)
for i in ${!path_threshold_dict[*]}
do
  echo "----------------------------------------"
  size_check $i ${path_threshold_dict[$i]}
done

找一个目录下前10大的目录  递归深度8层
du --max-depth=8 /home -h |sort -h -r |head -10

查看一个文件更新时间的时间戳date +%s -r file_name

```bash
# https://majing.io/posts/10000006301174
# http://c.biancheng.net/view/1114.html
# https://blog.csdn.net/zhan570556752/article/details/80399154
# https://www.cnblogs.com/jacktian-it/p/11550292.html
# https://my.oschina.net/u/3222944/blog/3066822
# https://www.runoob.com/linux/linux-shell-process-control.html
```
netstat -tunlp | grep 2181  查看端口
看端口：netstat -antp | grep 9000 查看9000端口哪个进程号在占用
lsof -i:8080 查看端口占用PID，USER等
查看目录树 yum -y install tree   tree /data/test
查进程pid  pid=$(ps -ef|grep 'keyword' |grep -v grep|awk '{print $2}')

mkdir test
chmod 1777 test
drwxrwxrwt 权限文件夹 任何人都可以在此目录拥有写权限，但是不能删除别人拥有的文件

看日志
grep -A5 -B20 "xxx" file  查看关键字xxx所在行及其前5行，后20行日志
grep -C 20 "xxx" file  查看关键字xxx所在行及其前后20行日志
grep -n "xxx" file   -n显示行号
grep -v "INFO" file   查看除INFO外的日志，-v是排除含关键字所在的行
grep "2020-07-10 08:10.*" file  查看指定时间的日志
tac xx.log | more 从尾部看日志

https://www.cnblogs.com/barfoo/p/3999882.html


cat /etc/group查看组信息

内存自动清理脚本https://blog.csdn.net/wangrui1573/article/details/89838938

局域网传文件
python -m SimpleHTTPServer 8000
wget 192.168.1.101:8000/xxx

Linux最大线程数限制，超过就创建失败（查看可生成的最大线程数cat /proc/sys/kernel/pid_max）（查看当前使用的线程数pstree -p | wc -l）

python readline一次读一行  readlines和read是读整个 写入内存  内存不够用readline
with open('filepath', 'r', encoding = 'utf-8') as f:
　　while True:
　　　　line = f.readline() # 逐行读取
　　　　if not line: # 到 EOF，返回空字符串，则终止循环
　　　　　　break
　　　　Operate(line) #对每行数据进行处理
python读大文件 分块读比较实用
def read_in_chunks(filePath, chunk_size=1024*1024):
　　file_object = open(filePath,'r',encoding='utf-8')
　　while True:
　　　　chunk_data = file_object.read(chunk_size)
　　　　if not chunk_data:
　　　　　　break
　　　　yield chunk_data
if __name__ == "__main__":
　　filePath = "C:/Users/Public/Documents/data/user_data.csv"
　　for chunk in read_in_chunks(filePath):
　　　　print(chunk)

正则判断str中特定字符后的内容 'a:"(.*?)"'

cd -	回到上一次所在目录
cat和tac都能查看文件内容，还有more 查看文件内容（space 向下翻一页、Enter 向下翻一行、Ctrl+F向下滚动一屏、Ctrl+B返回上一屏）

shell操作mysql
mysql  -hhostname -Pport -uusername -ppassword  -e  相关mysql的sql语句，不用在mysql的提示符下运行mysql，即可以在shell中操作mysql的方法。
test.sh
#!/bin/bash
HOSTNAME="192.168.111.84"                                           #数据库信息
PORT="3306"
USERNAME="root"
PASSWORD=""
DBNAME="test_db_test"                                                       #数据库名称
TABLENAME="test_table_test"                                            #数据库中表的名称
#创建数据库
create_db_sql="create database IF NOT EXISTS ${DBNAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} -e "${create_db_sql}"
#创建表
create_table_sql="create table IF NOT EXISTS ${TABLENAME} (  name varchar(20), id int(11) default 0 )"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${create_table_sql}"
#插入数据
insert_sql="insert into ${TABLENAME} values('billchen',2)"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${insert_sql}"
#查询
select_sql="select * from ${TABLENAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"
#更新数据
update_sql="update ${TABLENAME} set id=3"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${update_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"
#删除数据
delete_sql="delete from ${TABLENAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${delete_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${delete_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"

查找当前目录一个月(30天)以前大于100M的日志文件(.log)并删除
find  . -name "*.log" –mtime +30 –type f –size +100M |xargs rm –rf {} ;

/bin：是Binary的缩写，这个目录存放着系统必备执行命令

/sbin：s是Super User的意思，存放系统管理员使用的系统管理程序

/boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里

/dev：Device(设备)的缩写，该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。

/etc：所有的系统管理所需要的配置文件和子目录。

/home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。

/lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。

/media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。

/mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。

/opt：这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。

/root：该目录为系统管理员，也称作超级权限者的用户主目录。

/tmp：这个目录是用来存放一些临时文件的。

/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。

/var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。

/lost+found： 一般为空，当系统非法关机时会存放文件。

/proc：这个目录是一个虚拟目录，是系统内存的映射，可以通过访问这个目录中的文件来获取系统信息。

/selinux：SELinux是一种安全子系统，能控制程序只能访问特定文件。

/var/log/目录下记录的日志：
1. /var/log/messages — 包括整体系统信息，其中也包含系统启动期间的日志。此外，mail，cron，daemon，kern和auth等内容也记录在var/log/messages日志中。
2. /var/log/dmesg — 包含内核缓冲信息（kernel ring buffer）。在系统启动时，会在屏幕上显示许多与硬件有关的信息。可以用dmesg查看它们。
4. /var/log/boot.log — 包含系统启动时的日志。
5. /var/log/daemon.log — 包含各种系统后台守护进程日志信息。
6. /var/log/dpkg.log – 包括安装或dpkg命令清除软件包的日志。
7. /var/log/kern.log – 包含内核产生的日志，有助于在定制内核时解决问题。
8. /var/log/lastlog — 记录所有用户的最近信息。这不是一个ASCII文件，因此需要用lastlog命令查看内容。
9. /var/log/maillog /var/log/mail.log — 包含来着系统运行电子邮件服务器的日志信息。例如，sendmail日志信息就全部送到这个文件中。
10./var/log/user.log — 记录所有等级用户信息的日志。
11./var/log/Xorg.x.log — 来自X的日志信息。
12./var/log/alternatives.log – 更新替代信息都记录在这个文件中。
13./var/log/btmp – 记录所有失败登录信息。使用last命令可以查看btmp文件。例如，”last -f /var/log/btmp | more“。
14./var/log/cups — 涉及所有打印信息的日志。
15./var/log/anaconda.log — 在安装Linux时，所有安装信息都储存在这个文件中。
16./var/log/yum.log — 包含使用yum安装的软件包信息。
17./var/log/cron — 每当cron进程开始一个工作时，就会将相关信息记录在这个文件中。
18./var/log/secure — 包含验证和授权方面信息。例如，sshd会将所有信息记录（其中包括失败登录）在这里。
19./var/log/wtmp或/var/log/utmp — 包含登录信息。使用wtmp可以找出谁正在登陆进入系统，谁使用命令显示这个文件或信息等。
20./var/log/faillog – 包含用户登录失败信息。此外，错误登录命令也会记录在本文件中。
除了上述Log文件以外， /var/log还基于系统的具体应用包含以下一些子目录：
/var/log/httpd/或/var/log/apache2 — 包含服务器access_log和error_log信息。
/var/log/lighttpd/ — 包含light HTTPD的access_log和error_log。
/var/log/mail/ –  这个子目录包含邮件服务器的额外日志。
/var/log/prelink/ — 包含.so文件被prelink修改的信息。
/var/log/audit/ — 包含被 Linux audit daemon储存的信息。
/var/log/samba/ – 包含由samba存储的信息。
/var/log/sa/ — 包含每日由sysstat软件包收集的sar文件。
/var/log/sssd/ – 用于守护进程安全服务。




不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 保证数据不丢失。有些数据可能存在缓存中还未写到磁盘。

用户和组管理：
	1） useradd 添加新用户
	2） passwd 设置用户密码
	3） id 判断用户是否存在 id username
	4） su 切换用户
	5） userdel 删除用户
		 userdel  用户名	（功能描述：删除用户但保存用户主目录）
		 userdel -r 用户名	（功能描述：用户和用户主目录，都删除）
	6） who 查看登录用户信息
		 whoami	（功能描述：显示自身用户名称）
		 who am i	（功能描述：显示登录用户的用户名）
		 who		（功能描述：看当前有哪些用户登录到了本台机器上）
	7） 设置普通用户具有root权限
		修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：
				shmily  ALL=(ALL)     ALL
		修改完毕，现在可以用shmily帐号登录，然后用命令 sudo ，即可获得root权限进行操作。
	8） usermod修改用户
			usermod –g 用户组 用户名
	9） groupadd    新增组  （groupadd 组名）
	10） groupdel     删除组  （groupdel 组名）
	11） groupmod   修改组  （groupmod -n 新组名 老组名）
	12） cat  /etc/group 查看创建了哪些组
时间日期类命令：
	1）date显示当前时间
			（1）date	 			（功能描述：显示当前时间）
			（2）date +%Y			（功能描述：显示当前年份）
			（3）date +%m			（功能描述：显示当前月份）
			（4）date +%d			（功能描述：显示当前是哪一天）
			（5）date +%Y%m%d   （功能描述：显示当前年月日各种格式 ）
			（6）date “+%Y-%m-%d %H:%M:%S”		（功能描述：显示年月日时分秒）
	2） date显示非当前时间
			（1）date -d ‘1 days ago’			（功能描述：显示前一天日期）
			（2）date -d yesterday +%Y%m%d	（同上）
			（3）date -d next-day +%Y%m%d	（功能描述：显示明天日期）
			（4）date -d ‘next monday’		（功能描述：显示下周一时间）
	3） date设置系统时间
			date -s 字符串时间
	4） cal查看日历
			cal [选项]	（功能描述：不加选项，显示本月日历）
文件权限：
	1） 文件属性
	2） chmod改变权限
			chmod  [{ugoa}{+-=}{rwx}] [文件或目录] [mode=421 ]  [文件或目录]
			文件: r-查看；w-修改；x-执行文件
			目录: r-列出目录内容；w-在目录中创建和删除；x-进入目录
	3） chown改变所有者
			chown [最终用户] [文件或目录]	（功能描述：改变文件或者目录的所有者）
	4） chgrp 改变所属组
			chgrp [最终用户组] [文件或目录]	（功能描述：改变文件或者目录的所属组）
文件压缩解压：
	1） gzip/gunzip压缩
			gzip+文件		（功能描述：压缩文件，只能将文件压缩为*.gz文件）
			gunzip+文件.gz	（功能描述：解压缩文件命令）
	2）  zip/unzip压缩
			zip + 压缩名+xxx.txt + xxx.txt
			unzip + xxx.zip
			 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件）
	3）tar打包
			压缩：tar -zcvf   XXX.tar.gz + 将要打包进去的内容
			解压：tar –zxvf  xxx.tar.gz –C  xxx目录
		     （功能描述：打包目录，压缩后的文件格式.tar.gz）
			-c 产生.tar打包文件
			-v显示详细信息
			-f指定压缩后的文件名
			-z打包同时压缩
			-x解包.tar文件

Linux中重要的C运行库glibc：
    介绍：GNU发布的C运行库，glibc是linux系统中最底层的api，几乎其它任何运行库都会依赖于glibc。glibc除了封装linux操作系统所提供的系统服务外，它本身也提供了许多其它一些必要功能服务的实现。
    问题：遇到GLIBC_XX的问题一般都是缺少版本或版本不兼容
    升级：mkdir -p /lib64/new_glibc/  && cd /lib64/new_glibc/
         wget http://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz 
         tar -zxvf glibc-2.17.tar.gz
         mkdir build
         cd build
         ../configure --perfix=/usr --disable-profile --enable-add-ons --with-headers=/usr/inculde --with-binutils=/usr/bin
         make && make install 
         ll /lib64/libc.so.6  发现已经链接到libc-2.17.so
         ldd --version 
         strings /lib64/libc.so.6 | grep GLIBC_

Linux CPU负载正常 load/cpu <=1 是合理的  负载很高对计算型业务影响明显，对io\cache型应用影响小


环境变量 ~/.bash_profile
    PROMPT_COMMAND="one command"  终端任何命令执行都先执行这个设置的命令

理清shell环境变量加载顺序（6个文件）：
  echo "export aaaa=0" >> /etc/profile  全局env
  echo "export aaaa=1" >>  ~/.bash_profile  用户env
  echo "export aaaa=2" >> ~/.bashrc  shell会话env(启动单个session时运行)
  source  ...
  echo $aaaa 得到 1
  解释：/etc/profile是全局环境变量，被加载的变量全局有效，~/.bash_profile和~/.bashrc都是用户环境变量
    可以看到~/.bash_profile执行判断如果~/.bashrc文件存在则执行  但执行后才export aaaa=1 
    所以：
    加载顺序为 /etc/profile > ~/.bash_profile > ~/.bashrc
    aaaa顺序为     0  ------>       2   ------>     1
  此外还有：
    ~/.bash_history 用户shell环境  按时间戳记录history命令（盲猜history就是读取它）
    ~/.bash_logout  用户shell环境  退出shell时执行，可以自定义一些清理工作
    /etc/bashrc     全局shell环境  主要用于指定umask权限，指定全局alias，指定[root@hadoop01 ~]#格式（叫PS1）

rz命令上传文件 sz命令下载文件





*************************************待写博客****************************************
--------------------------------Mysql Binlog---------------------------------
mysql回滚(表引擎必须是INNODB) 可以show create table xx;查看
select * from table_name;
回滚事务步骤:
start transaction;   开始事务
savepoint a;   创建保存点a
delete from table_name where id = 1;  删除一条记录
select * from table_name;
rollback to a;  回滚到a保存点
select * from table_name;  没问题的话  执行commit;

mysql的binlog是多文件存储，定位一个LogEvent需要通过binlog filename + binlog position，进行定位
mysql的binlog数据格式，按照生成的方式，主要分为：statement-based、row-based、mixed。
mysql> show variables like 'binlog_format';
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | binlog_format | ROW   |
    +---------------+-------+
    1 row in set (0.00 sec)

mysql表占用空间大小
SELECT (`DATA_LENGTH`+ `INDEX_LENGTH`)/1024/1024  as `table_data_size`  from information_schema.TABLES WHERE TABLE_NAME ='tb' and TABLE_SCHEMA='db';

获取字段和类型 列表：
                select
                    column_name,data_type
                from
                    information_schema.columns
                where
                    table_name='tb' and table_schema='db'



-----------------------------------------------------------------------------
算法数据结构相关
数据结构可视化：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html
Leetcode刷题套路汇总 https://mp.weixin.qq.com/s/WrrmkB2eJFDQuWOF9Mp1Yg




-----------------------------------------------------------------------------
数仓相关：

OLAP OLTP区别：
OLAP是联机分析处理 OLTP是也叫联机事务处理
OLAP主要是对应数据仓库，OLTP对应数据库
OLAP面向决策层 OLTP面向业务层
OLAP 系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。      OLTP 系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作。
OLAP数据量大，DML少，数仓  OLTP数据量少，DML频繁，并行事务处理多。
具体：https://www.cnblogs.com/hhandbibi/p/7118740.html


-----------------------------------------------------------------------------
HBase相关

为何行存储比列存储效率更高速度更快？适用场景？
- 对于行存储，这个查询需要两次IO将全部数据放入内存后，进行页间数据的跳读（类随机读取)
- 对于列存储，只需要一次IO将page2放入内存后进行连续读取，如果字段2还有多页的话，也都是进行的物理连续读取
行存储适合OLTP，列存储适合OLAP

压测工具：YCSB

HBase宕机处理：
HMaster宕机：HMaster没有单点问题，可以起多个HMaster来保证总有一个活动的HMaster。
HRegionServer宕机：HMaster会将这个宕机的RS管理的Region分配到其他机器，而这个宕机的RS中的MemStore中数据还未持久化到HDFS，可以通过重播WAL的方式，保证数据不丢失。

HBase性能优化方法总结
    6.1 高可用
    在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。
    6.2 预分区
    每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。
    6.3 RowKey设计
    一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。
    6.4 内存优化
    HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。
    6.5 基础优化
    1．允许在HDFS的文件中追加内容
    hdfs-site.xml、hbase-site.xml
    属性：dfs.support.append
    解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。
    2．优化DataNode允许的最大文件打开数
    hdfs-site.xml
    属性：dfs.datanode.max.transfer.threads
    解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096
    3．优化延迟高的数据操作的等待时间
    hdfs-site.xml
    属性：dfs.image.transfer.timeout
    解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。
    4．优化数据的写入效率
    mapred-site.xml
    属性：
    mapreduce.map.output.compress
    mapreduce.map.output.compress.codec
    解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。
    5．设置RPC监听数量
    hbase-site.xml
    属性：hbase.regionserver.handler.count
    解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。
    6．优化HStore文件大小
    hbase-site.xml
    属性：hbase.hregion.max.filesize
    解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。
    7．优化hbase客户端缓存
    hbase-site.xml
    属性：hbase.client.write.buffer
    解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。
    8．指定scan.next扫描HBase所获取的行数
    hbase-site.xml
    属性：hbase.client.scanner.caching
    解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。
    9．flush、compact、split机制
    当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。
    涉及属性：
    即：128M就是Memstore的默认阈值
    hbase.hregion.memstore.flush.size：134217728
    即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。
    hbase.regionserver.global.memstore.upperLimit：0.4
    hbase.regionserver.global.memstore.lowerLimit：0.38
    即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit

    AutoFlush：通过调用HTable.setAutoFlushTo(false)方法可以将HTable写客户端自动flush关闭，这样可以批量写入数据到HBase，而不是有一条put就执行一次更新，只有当put填满客户端写缓存的时候，才会向HBase服务端发起写请求。默认情况下auto flush是开启的。

    WAL Flag：在HBase中，客户端向集群中的RegionServer提交数据时（Put/Delete操作），首先会写到WAL（Write Ahead Log）日志，即HLog，一个RegionServer上的所有Region共享一个HLog，只有当WAL日志写成功后，再接着写MemStore， 然后客户端被通知提交数据成功，如果写WAL日志失败，客户端被告知提交失败，这样做的好处是可以做到RegionServer宕机后的数据恢复。

    对于不太重要的数据，可以在Put/Delete操作时，通过调用Put.setWriteToWAL(false)或Delete.setWriteToWAL(false)函数，放弃写WAL日志，以提高数据写入的性能。

    压缩
    数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。
    HColumnDescriptor hcd = new HColumnDescriptor(familyName);
    hcd.setCompressionType(Algorithm.SNAPPY);

    批量写
    通过调用HTable.put(Put)方法可以将一个指定的row key记录写入HBase，同样HBase提供了另一个方法：通过调用HTable.put(List<Put>)方法可以将指定的row key列表，批量写入多行记录，这样做的好处是批量执行，只需要一次网络I/O开销，这对于对数据实时性要求高，网络传输RTT高的情景下可能带来明显的性能提升。

    多线程并发
    写在客户端开启多个 HTable 写线程，每个写线程负责一个 HTable 对象的 flush 操作，这样结合定时 flush 和写 buffer（writeBufferSize），可以既保证在数据量小的时候，数据可以在较短时间内被 flush（如1秒内），同时又保证在数据量大的时候，写 buffer 一满就及时进行 flush。

    批量读
    通过调用HTable.get(Get) 方法可以根据一个指定的 row key 获取一行记录，同样 HBase 提供了另一个方法：通过调用HTable.get(List) 方法可以根据一个指定的 row key 列表，批量获取多行记录，这样做的好处是批量执行，只需要一次网络 I/O 开销，这对于对数据实时性要求高而且网络传输 RTT 高的情景下可能带来明显的性能提升。
    缓存查询结果
    对于频繁查询 HBase 的应用场景，可以考虑在应用程序中做缓存，当有新的查询请求时，首先在缓存中查找，如果存在则直接返回，不再查询 HBase；否则对 HBase 发起读请求查询，然后在应用程序中将查询结果缓存起来。至于缓存的替换策略，可以考虑 LRU 等常用的策略。

    HBase数据表优化预分区
    默认情况下，在创建HBase表的时候会自动创建一个Region分区，当导入数据的时候，所有的HBase客户端都向Region写数据，知道这个 Region足够大才进行切分，一种可以加快批量写入速度的方法是通过预先创建一些空的Regions，这样当数据写入HBase的时候，会按照 Region分区情况，在进群内做数据的负载均衡。

    Rowkey优化
    rowkey是按照字典存储，因此设置rowkey时，要充分利用排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放到一块。

    rowkey若是递增生成的，建议不要使用正序直接写入，可以使用字符串反转方式写入，使得rowkey大致均衡分布，这样设计的好处是能将 RegionServer的负载均衡，否则容易产生所有新数据都在集中在一个RegionServer上堆积的现象，这一点还可以结合table的与分区 设计。
    减少Column Family数量

    不要在一张表中定义太多的column family。目前HBase并不能很好的处理超过2-3个column family的表，因为某个column family在flush的时候，它临近的column family也会因关联效应被触发flush，最终导致系统产生更过的I/O;

    设置最大版本数
    创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions) 设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置 setMaxVersions(1)。
    缓存策略（setCaching）

    创建表的时候，可以通过HColumnDEscriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中。

    设置存储生命期
    创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命周期，过期数据将自动被删除。
    磁盘配置

    每台RegionServer管理10-1000个Regions。每个Region在1-2G，则每台server最少要10G，最大要 1000*2G=2TB，考虑3备份，需要6TB。方案1是3块2TB磁盘，2是12块500G磁盘，带宽足够时，后者能提供更大的吞吐率，更细力度的冗 余备份，更快速的单盘故障恢复。

    分配何时的内存给RegionServer
    在不影响其他服务的情况下，越大越好。在HBase的conf目录下的hbase-env.sh的最后添加export HBASE_REGIONSERVER_OPTS="- Xmx16000m $HBASE_REGIONSERVER_OPTS"
    其中16000m为分配给REgionServer的内存大小。

    写数据的备份数
    备份数与读性能是成正比，与写性能成反比，且备份数影响高可用性。有两种配置方式，一种是将hdfs-site.xml拷贝到hbase的conf目录下，然后在其中添加或修改配置项dfs.replication的值为要设置的备份数，这种修改所有的HBase用户都生效。另一种方式是改写HBase代码，让HBase支持针对列族设置备份数，在创建表时，设置列族备份数，默认为3，此种备份数支队设置的列族生效。

    Region大小设置
    配置项hbase.hregion.max.filesize，所属配置文件为hbase-site.xml，默认大小是256m。
    在当前RegionServer上单个Region的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的 Region。小Region对split和compaction友好，因为拆分Region或compact小Region里的StoreFile速度 非常快，内存占用低。缺点是split和compaction会很频繁，特别是数量较多的小Region不同的split，compaction，会导致 集群响应时间波动很大，Region数量太多不仅给管理上带来麻烦，设置会引起一些HBase个bug。一般 512M 以下的都算小 Region。大 Region 则不太适合经常 split 和 compaction，因为做一次 compact 和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。
    此外，大 Region 意味着较大的 StoreFile，compaction 时对内存也是一个挑战。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。compaction 是无法避免的，split 可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split(RegionServer 不会对未到达 100G 的 Region 做 split)。
    再配合 RegionSplitter 这个工具，在需要 split 时，手动 split。手动 split 在灵活性和稳定性上比起自动 split 要高很多，而且管理成本增加不多，比较推荐 online 实时系统使用。内存方面，小 Region 在设置 memstore 的大小值上比较灵活，大 Region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 StoreFile 过多影响读性能。

    RegionServer的请求处理IO线程数
    较少的IO线程适用于处理单次请求内存消耗较高的Big Put场景（大容量单词Put或设置了较大cache的scan，均数据Big Put）或RegionServer的内存比较紧张的场景。

    较多的IO线程，适用于单次请求内存消耗低，TPS要求（每次事务处理量）非常高的场景。这只该值的时候，以监控内存为主要参考
    在hbase-site.xml配置文件中配置项hbase.regionserver.handle.count

    当scan一张表的时候,只要返回rowkey（不需要CF，qualifier，value，timestaps）,时可以设置scan加一个filterList,并且设置MUST_PASS_ALL操作,filterList中add FirstKeyOnlyFilter或者KeyOnlyFilter.这样可以减少网络通信量.

    Bloom Filter通过空间换时间,提高读操作性能。Hbase利用Bloomfilter 来提高随机读（Get）的性能,对于顺序读（Scan）而言,设置BloomFilter是没有作用的.

常用HBase集群修复命令
1. hbase hbck 检查输出所以ERROR信息，每个ERROR都会说明错误信息。
2. hbase hbck -fixTableOrphans 先修复tableinfo缺失问题，根据内存cache或者hdfs table 目录结构，重新生成tableinfo文件。
3. hbase hbck -fixHdfsOrphans 修复regioninfo缺失问题，根据region目录下的hfile重新生成regioninfo文件。
4. hbase hbck -fixHdfsOverlaps 修复region重叠问题，merge重叠的region为一个region目录，并从新生成一个regioninfo。
5. hbase hbck -fixHdfsHoles 修复region缺失，利用缺失的rowkey范围边界，生成新的region目录以及regioninfo填补这个空洞。
6. hbase hbck -fixMeta 修复meta表信息，利用regioninfo信息，重新生成对应meta row填写到meta表中，并为其填写默认的分配regionserver。
7. hbase hbck -fixAssignments 把这些offline的region触发上线，当region开始重新open 上线的时候，会被重新分配到真实的RegionServer上 , 并更新meta表上对应的行信息。

-----------------------------------------------------------------------------
Java相关

java并发中锁的原理之对象头：https://www.jianshu.com/p/a32de78b7d04

@PostConstruct：被@PostConstruct修饰的方法会在服务器加载Servlet的时候运行，并且只会被服务器执行一次，可以使用@PostConstruct注解一个方法来完成初始化，@PostConstruct注解的方法将会在依赖注入完成后被自动调用
  用法：
  @PostConstruct
  private void init() {
      logger.info("@PostConstruct运行一次，也是唯一一次运行");
  }

thread.start()可以单开个线程后台执行一些定时任务

java进程堆转储Dump  jmap -dump:format=b,file=heap.hprof [pid]  生成.hprof快照文件
JVM参数设置发生OOM自动生成Dump转储文件 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${目录}
生成.hprof文件后使用JProfiler或VisualVM工具分析，也可以使用Java自带JVisualVM分析
VisualVM在IDEA可以结合VisualVM Launcher插件使用

查看Java运行情况：
  jmap -heap pid 查看内存使用情况
  jstat -gcutil pid 查看GC情况 
  jstat -gc pid 查看GC情况
  jstack -l pid 查看JVM进程中线程运行情况

Maven打jar插件依赖打入jar
    <!-- 打jar插件 -->
    <build>
        <plugins>
<!--            <plugin>-->
<!--                <artifactId>maven-assembly-plugin</artifactId>-->
<!--                <configuration>-->
<!--                    <archive>-->
<!--                        <manifest>-->
<!--                            <mainClass>com.dss.governance.Application</mainClass>-->
<!--                        </manifest>-->
<!--                    </archive>-->
<!--                    <descriptorRefs>-->
<!--                        <descriptorRef>jar-with-dependencies</descriptorRef>-->
<!--                    </descriptorRefs>-->
<!--                </configuration>-->
<!--            </plugin>-->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>2.3</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <transformer
                                        implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.dss.governance.Application</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

-----------------------------------------------------------------------------
Git相关
【本地代码库回滚】：
git reset --hard commit-id :回滚到commit-id，讲commit-id之后提交的commit都去除
git reset --hard HEAD~3：将最近3次的提交回滚
【远程代码库回滚】：
git reset --hard HEAD~1 本地代码回滚到上一版本（或者指定版本）
git push -f origin master  加入-f参数，强制提交，远程端将强制更新到reset版本
【从master拉新分支开发】：
1. 切换到被copy的分支（master），并且从远端拉取最新版本
$ git checkout master
$ git pull
2.从当前分支拉copy开发分支
$ git checkout -b dev
3.把新建的分支push到远端
$ git push origin dev
4.拉取远端分支
由于当前的分支并没有和本地分支关联，根据提示进行关联
$ git pull
5.关联
$ git branch --set-upstream-to=origin/dev
6. 再次拉取
$ git pull

编辑Git配置文件
git config -e [--global]
设置提交代码时的用户信息
git config [--global] user.name "[name]"
git config [--global] user.email "[email address]"
重置git链接方式ssh与http切换
git remote -v
git remote rm origin
git remote add origin git仓库的SSH地址
git remote -v
拉取
git pull origin master
存入暂存区
git add [file1] [file2] ... # 多个文件
git add [dir]   # 目录含其下子目录
git add .  # 当前目录所有文件
提交
git commit -m [message]
git commit [file1] [file2] ... -m [message]
上传
git push origin master
删除
git rm [file1] [file2] ... # 删除工作区文件，并且将这次删除放入暂存区
分支
git branch  列出本地所有分支
git branch -r  列出所有远程分支
git checkout [branch-name]  切换分支
git branch -d [branch-name]  删除本地分支
git push origin --delete [branch-name]  删除远程分支
状态
git status 暂存区和远程差异
git diff  暂存区和工作区差异

上传一个项目
cd $PROJECT_ROOT
git init
git remote add origin 项目地址
git add .
git commit -m "程序源代码"
git push -u origin master


-----------------------------------------------------------------------------
其他

虚拟机网络连接模式：
桥接模式：虚拟主机和真实主机外网在同一IP段，能访问外网。例子：真实主机：192.168.1.100  虚拟主机192.168.1.101
NAT模式：虚拟主机和真实主机网段不同，能访问外网。例子：真实主机：192.168.2.100  虚拟主机192.168.1.101
仅主机模式：虚拟主机和真实主机IP段不同且不能访问外网。例子：真实主机：192.168.2.100  虚拟主机192.168.1.100


分布式 CAP 的一致性

DataX学习

com.google.common.cache.Cache缓存

--------------------------------JVM GC分析--------------------------------
https://www.cnblogs.com/rainwang/p/7213918.html    GC分析   G1  不研究CMS
-------------------------------------------------------------------------

可以尝试做的项目：
1.ClusterProfiler
    类似于JVM Profiler的分布式集群资源信息收集和JVM进程跟踪（目标：应用级指标监控，不侵入用户代码）
    现有的同类开源工具, 比如 Etsy 的 statsd-jvm-profiler , 可以在单个应用程序级别收集度量, 但是不提供动态代码注入收集度量的能力。在这些工具的启发下, 我们的探查器提供了新功能, 如任意 Java 方法/参数分析。
2.海量数据统计排序
    基于问题： 1.如果给你尽可能多的资源，磁盘IO,网络，CPU都不受限，如何读取一个大文件中的结果并统计和？（读取怎么读？多线程读，用什么数据结构存储读到的数据：队列 多个线程一个队列还是多个队列，哪个好 多个队列 多个队列怎么累加求和效率高  如果计算中断了需要从0开始计算很耗资源怎么办？怎么缓存？设置ID，类似程序计数器，保存中间结果和ID。 如果多线程处理时对数据做了重复累加怎么办？怎么避免重复累加？BitMap。。。。）
              2.内存放不下求哪些数据不重复，哪些数据重复
              3.内存放不下求TopK
              4.内存放不下的10个文件每个1G，求搜索频率最高的关键字
              5.两个大文件，内存放不下，统计共有的元素（求交集）(给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？)
              6.内存放不下，求中位数
              7.内存放不下，对文件内容排序
              8.海量IP数据提取访问最多的IP
              9.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
              10.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？(按位计算，先比较最高位再比较次高位依次比较相当于折半)
    海量数据问题的最优解决方案
    技术栈：Hash+BitMap+BloomFilter+堆排序/快速排序/归并排序+Trie树+HashMap+HashSet+TreeMap+按位计算+MapReduce+Spark

QCON笔记：
Ray，批流无缝，框架
RDMA
HDFS Cache 在HDFS3.3.0功能类似alluxio，利用dn的空余内存
基于HDFS CACHE会争抢ram资源的缺点，傲腾DC持久化内存可以解决，帮助cache更多数据来加速大数据应用。
两种模式
可以当做内存，也可以当做快速持久化存储
持久化内存不能脱离DRAM
可以节省DRAM给workload，提高内存使用效率
bandwidth
hadoop streaming
hadoop task为进程模型，spark为线程模型
ad hoc查询
OAP加速组件
spark秉持计算与存储分离
python pandas 单机版数据分析
spark出现koalas，pip install koalas
delta，高性能的spark存储格式。format(parquet)
直接改成format(delta)
Apache Arrow-Based Spark Native SQL Engine
apache arrow是一个标准数据格式，多种使用dataframe的计算框架都通用




