##################################################
好的学习网站
https://www.iteblog.com/
https://www.liaoxuefeng.com
https://home.apachecn.org/docs/
https://github.com/apachecn

##################################################
JVM相关
OOM八种方式与解决：https://tianmingxing.com/2019/11/17/%E4%BB%8B%E7%BB%8DJVM%E4%B8%ADOOM%E7%9A%848%E7%A7%8D%E7%B1%BB%E5%9E%8B/
查看GC情况 https://www.cnblogs.com/alter888/p/10407952.html

##################################################


##################################################
大数据热点话题
八种排查处理数据倾斜 https://mp.weixin.qq.com/s/piW10KGJVgaSB_i72OVntA

##################################################


##################################################
Cloudera Manager相关
https://cloud.tencent.com/developer/article/1544865



##################################################
Hadoop相关

NFS挂载（不如HDFS Fuse挂载性能好）
NFS挂载hdfs到本地目录 开启NFS Gateway后 mount -t nfs -o vers=3,proto=tcp,nolock,sync,rsize=1048576,wsize=1048576 nfs_ip:/ /data/hdfs_nfs
NFS卸载umount /data/hdfs_nfs
fuser -m -v /data/hdfs_nfs   对于umount.nfs: /data/hdfs_nfs: device is busy  查看哪个进程占用
rpcinfo -p  nfs_ip  查看服务是否已启动并正在运行
showmount -e nfs_ip  验证HDFS名称空间是否已导出并且可以挂载
nfsstat -m   NFS状态查看
默认rsize和wsize是1M，自己设置更大，mount之后这个参数也还是1M，因为linux内核的限制，要想提高这个阈值，要编译内核的。

HDFS元数据浏览
【oiv】offline image viwer   
1.用于查看Hadoop fsimage 
2.语法
    $> hdfs oiv -i inputfile -o outputfile -P process
3.inputfile: 要查看的fsimage文件
   outputfile: 用于保存格式化之后的文件
   process: 使用什么进程解码，XML|Web|...
【oev】
1.用于查看Hadoop 的 edit 文件 
2.语法
    $> hdfs oev -i inputfile -o outputfile -P process
3.inputfile: 要查看的edit文件
   outputfile: 用于保存格式化之后的文件
   process: 使用什么进程解码，XML|Web|...

HDFS查看文件是否存在
hadoop fs -test -e /data/;echo $?

Yarn的调度器：
--1.先进先出调度器（FIFO）
Hadoop 中默认的调度器，也是一种批处理调度器。它先按照作业的优先级高低，再按照到达时间的先后选择被执行的作业
--2.容量调度器（Capacity Scheduler)
支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。
调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择，同时考虑用户资源量限制和内存限制
--3.公平调度器（Fair Scheduler）
所有的 job 具有相同的资源
公平调度是一种赋予作业（job）资源的方法，它的目的是让所有的作业随着时间的推移，都能平均的获取等同的共享资源。
所有的 job 具有相同的资源,当单独一个作业在运行时，它将使用整个集群。当有其它作业被提交上来时，系统会将任务（task）空闲资源（container）赋给这些新的作业，以使得每一个作业都大概获取到等量的CPU时间。
与Hadoop默认调度器FIFO维护一个作业队列不同，这个特性让小作业在合理的时间内完成的同时又不"饿"到消耗较长时间的大作业。
公平调度可以和作业优先权搭配使用——优先权像权重一样用作为决定每个作业所能获取的整体计算时间的比例。同计算能力调度器类似，支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。

Yarn工作流程
1.用户向Yarn提交任务
2.ResourceManager分配Container，与NodeManager通信，在Container中启动ApplicationMaster
3.ApplicationMaster向ResourceManager注册，申请资源，监控运行状态直到结束
4.应用程序执行完，Application通过RPC协议向ResourceManager注销自己。

##################################################
Python相关
打印调用栈
import traceback
def BBQ():
    traceback.print_stack()
    a = traceback.extract_stack()
    b = traceback.format_stack()

MD5 BASE64:
img_path = '/home/jiajing_qu/img.jpg'
import md5
m = md5.new()
m.update(img_path)
res = m.hexdigest()
import hashlib
import base64
with open(img_path, 'rb') as fp:
	data = fp.read()
	base64_data = base64.b64encode(data)
	file_md5= hashlib.md5(data).hexdigest()
content = {"base64": base64_data, "md5": file_md5}
{"base64": base64_data, "md5": file_md5}
with open(img_path, 'rb') as fp:
    content = fp.read()

r = [u'varchar(255)', u'varchar(255)', u'varchar(255)', u'datetime', u'datetime']
r[0].__contains__('varchar')
r[0].__contains__('varchar(255)')
r[0].__contains__('varchar(254)')
判断类型  isinstance(comment_info, dict)    isinstance(comment_info, basestring)
enumerate(list[])

logging模块
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.info()

for i in enumerate(list):
    print(i)   # i是(0,list[0])  (1,list[1])....

pandas的dataframe转python的dict （orient指定方向）
df.to_dict(orient='records')
[df.to_dict(orient='index')]

如果用了staticmethod，那么就可以无视这个self，而将这个方法当成一个普通的函数使用。
而对于classmethod，它的第一个参数不是self，是cls，它表示这个类本身。

python import是将py文件(模块)中的元素（类，方法）加载到内存，放入sys.modules，生成同名的.pyc文件  如果没加载过就会加载到内存，如果之前已经加载过，就不再重复加载到内存，而是改变命名空间

连接数据库 插入和更新数据库后，数据库内容未变：conn的时候  autocommit = 1

正则 [\u4e00-\u9fa5] 匹配所有中文
###################################################
Spark相关

groupByKey(11)将Shuffle并行度调整为11
计算单词的个数，第一种方式使用reduceByKey ；另外一种方式使用groupByKey
虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。
ReduceByKey优点：会先进行局部聚合再进行全局聚合，这样全局聚合时减少了网络IO
尽量少使用groupByKey 当调用 groupByKey时，所有的键值对(key-value pair) 都会被移动。在网络上传输这些数据非常没有必要。避免使用 GroupByKey。
以下函数应该优先于 groupByKey ：
　　（1）combineByKey组合数据，但是组合之后的数据类型与输入时值的类型不一样。
　　（2）foldByKey 合并每一个 key 的所有值，在级联函数和“零值”中使用。

# 一些概念（参考https://juejin.im/post/5a5dc6056fb9a01c982c9505）
每个JVM只有一个SparkContext,一台服务器可以启动多个JVM
SparkSession包含了SQLContext和HiveContext
Driver：
    运行main方法的Java虚拟机进程,负责监听spark application的executor进程发来的通信和连接,将工程jar发送到所有的executor进程中
    Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、分配task到executor上、计算资源的分配等调度执行作业等
    driver调度task给executor执行，所以driver最好和spark集群在一片网络内,便以通信
    driver进程通常在worker节点中,和Cluster Manager不在同一个节点上
Worker:
    启动并运行executor进程
    standalone模式下:Worker进程所在节点
    yarn模式下: yarn的nodemanager进程所在的节点
Executor:
    spark application不会共享一个executor进程
Job：
    一个spark application可能会被分为多个job，每次调用Action时，逻辑上会生成一个Job，一个Job包含了一个或多个Stage。
Stage:
    每个job都会划分为一个或多个stage（阶段），每个stage都会有对应的一批task(即一个taskset)，分配到executor上去执行
    Stage包括两类：ShuffleMapStage和ResultStage，如果用户程序中调用了需要进行Shuffle计算的Operator，如groupByKey等，就会以Shuffle为边界分成ShuffleMapStage和ResultStage。
    如果一次shuffle都没执行，那就只有一个stage
TaskSet:
    一组关联的，但相互之间没有Shuffle依赖关系的Task集合;Stage可以直接映射为TaskSet，一个TaskSet封装了一次需要运算的、具有相同处理逻辑的Task，这些Task可以并行计算，粗粒度的调度是以TaskSet为单位的。
    一个stage对应一个taskset
Task:
    driver发送到executor上执行的计算单元，每个task负责在一个阶段(stage)，处理一小片数据，计算出对应的结果
    Task是在物理节点上运行的基本单位，Task包含两类：ShuffleMapTask和ResultTask，分别对应于Stage中ShuffleMapStage和ResultStage中的一个执行基本单元。
    InputSplit-task-partition有一一对应关系,Spark会为每一个partition运行一个task来进行处理
总结SparkAPP-Action，Job，Stage，TaskSet，Task数量关系： SparkApplication-Action操作:Job:Stage:TaskSet:Task = 1:1:n:n:n*m(m个partition)
Cluster Manager:
    集群管理器，为每个spark application在集群中调度和分配资源的组件，如Spark Standalone、YARN、Mesos等
Deploy Mode:
    不论是standalone/yarn,都分为两种模式，client和cluster,区别在于driver运行的位置
    client模式下driver运行在提交spark作业的机器上,可以实时看到详细的日志信息，方便追踪和排查错误,用于测试
    cluster模式下，spark application提交到cluster manager，cluster manager(比如master)负责在集群中某个节点上，启动driver进程,用于生产环境
    通常情况下driver和worker在同一个网络中是最好的,而client很可能就是driver worker分开布置,这样网络通信很耗时,cluster没有这样的问题
DAGScheduler:
    根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler
TaskScheduler:
    将Taskset提交给Worker node集群运行并返回结果
Spark基本工作原理：
    Driver向Master申请资源；
    Master让Worker给程序分配具体的Executor
    Driver把划分好的Task传送给Executor，Task就是我们的Spark程序的业务逻辑代码
    job生成,stage划分和task分配都是发生在driver端?是
Shuffle算子:
    distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等
    容易导致部分task慢(数据倾斜)
    推算shuffle代码：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage
coalesce和repartition区别：
    coalesce: def coalesce(numPartitions: Int, shuffle: Boolean = false...
    repartition: def repartition(numPartitions: Int)
    coalesce将1000分区转换成100个分区不会shuffle反之会shuffle，但是大幅度合并分区会导致计算并行度不够，这时可以指定第二个参数shuffle为True，通过HashPartitioner分布数据，提高并行度。
    repartition也能增加和减少分区，会shuffle。如果只考虑减少分区数，建议用coalesce
    repartition只是coalesce接口中shuffle为true的简易实现
rdd的算子类型：
    transformation，rdd由一种转为另一种rdd
    action行动算子
    controller，控制算子(cache/persist) 对性能和效率的有很好的支持
宽依赖-窄依赖：
    窄依赖：父RDD的一个分区只与一个子RDD的分区对应，子RDD的分区通常与多个RDD分区对应
    宽依赖：父RDD的一个分区要与多个子RDD的分区对应，子RDD的分区通常对应所有父RDD的分区
    对于性能优化，窄依赖更有利：
        RDD分区丢失重算时，窄依赖由于父RDD的一个分区对应子RDD的一个分区，只要重算子RDD对应的父RDD即可；而宽依赖最差情况下要计算所有父RDD
        宽依赖通常与shuffle有关
    窄依赖的函数有：map, filter, union, join(父RDD是hash-partitioned ), mapPartitions, mapValues
    宽依赖的函数有：groupByKey, join(父RDD不是hash-partitioned ), partitionBy
Stage划分原理：
    因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。（总结：遇到一个宽依赖就分一个stage）
Executor内存划分：
    Spark Executor的内存划分为四块，ExecutionMemory，StorageMemory，UserMemory，ReservedMemory(300m)。其中ExecutionMemory，StorageMemory，UserMemory是JVM使用的内存。
    - ExecutionMemory用于存放Shuffle、Join、Sort、Aggregation等计算过程中的临时数据
    - StorageMemory用于存储Spark的Cache数据，如RDD缓存
    - UserMemory用于RDD转换过程中需要的数据，如依赖信息，元数据
    - ReservedMemory预留内存，用于存储Spark内部对象
    ExecutionMemory和StorageMemory是统一内存，由spark.memory.fraction指定  默认0.6






Spark问题：
1.Spark Master HA主从切换会影响正在跑的任务吗？
    不会，因为之前已经申请过资源且注册过driver和executors，driver和executors之间通讯不走master。
2.Spark并行度怎么设置比较合适？
    spark作业中，各个stage的task的数量，也就代表了spark作业在各个阶段stage的并行度！并行度要与分配的资源匹配，否则会浪费资源。
    合理设置并行度，可以充分利用集群资源，减少每个task处理数据量，而增加性能加快运行速度。
    调并行度策略：一般是task数是Cpu总core数的2-3倍  本来最理想的状况是task数与application设置的core数相同，但是考虑到可能每个task跑的时间不一样，这种情况会资源浪费。task数是core的2-3倍就既不浪费还高效。
    调并行度方式：
    spark application并行度task数设置：1.spark.default.parallelism  2.数据在HDFS则增加Block数  3.repartition 4.spark.sql.shuffle.partitions
3.Spark中数据的位置是被谁管理的？
    每个数据分片都对应具体物理位置，数据的位置是被blockManager管理
4.为什么要进行序列化？
    减少存储空间和减少网络传输（缺点：序列化和反序列化消耗CPU）
5.MR和Spark的Shuffle异同？
    Hadoop是从Map(Spark的ShuffleMapTask)输出partition，不同分区数据送到不同的Reduce(Spark下一个Stage的ShuffleMapTask或者ResultTask)
    HadoopMR的Shuffle是基于归并排序，Spark的Shuffle基于Hash
    HadoopMR的Shuffle是Map端分片数据通过网络收集到reduce端，Spark的shuffle是stage间
6.reduceByKey和groupByKey区别？
    reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。（在数据移动之前聚合）
    groupByKey因为不接收函数，所以先将所有k-v都移动。开销大。
7.cache后面能不能接其他算子,它是不是action操作？
    cache之后可接算子，但是如果接了算子，如果是行动算子，会重新触发cache。cache不是action操作。通过unpersist取消cache（非lazy）
    默认的存储级别 - MEMORY_ONLY
8.Spark累加器有哪些特点？
    全局的，只增不减，记录全局集群的唯一状态
    在exe中修改它，在driver读取
    executor级别共享的，广播变量是task级别的共享
    两个application不可以共享累加器，但是同一个app不同的job可以共享
9.spark hashParitioner的弊端是什么？
    HashPartitioner确定分区的方式：partition = key.hashCode () % numPartitions 弊端：弊端是数据不均匀，容易导致数据倾斜，极端情况下某几个分区会拥有rdd的所有数据。
10.Spark中的HashShufle的有哪些不足？
    shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；
    容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息
    容易出现数据倾斜，导致OOM


Spark Job 默认的调度模式 - FIFO
RDD 特点 - 可分区/可序列化/可持久化
Broadcast - 任何函数调用/是只读的/存储在各个节点
Accumulator - 支持加法/支持数值类型/可并行
master 和 worker 通过 Akka 方式进行通信的






spark加JVM日志打印
"spark.driver.extraJavaOptions":"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps",

Spark Broadcast https://blog.csdn.net/zx8167107/article/details/79186477
spark 操作 https://www.cnblogs.com/Frank99/p/8295949.html
PYSPARK_DRIVER_PYTHON=ipython $SPARK_HOME/bin/pyspark
Structured Streaming分析https://www.jianshu.com/p/d4e2a2be9a10

df.repartition(10).rdd.getNumPartitions()  重分区并查看分区数

给读取文件的df应用schema:
pyspark.sql.types.StructType
schema = StructType()
schema.add(StructField(field_name, StringType(), True))
schema类型StructType(List(StructField(eid,StringType,true),StructField(name,StringType,true),StructField(shareholders_tree,StringType,true),StructField(shareholders_count,StringType,true),StructField(holding_tree,StringType,true),StructField(holding_count,StringType,true),StructField(group_id,StringType,true),StructField(split_tag,StringType,true),StructField(created_time,StringType,true),StructField(updated_time,StringType,true)))
df = spark.read.csv("{hdfs_file_path}".format(hdfs_file_path=hdfs_file_path),schema=schema,sep=u"\u0001")
df = spark.read.csv("{hdfs_file_path}".format(hdfs_file_path=hdfs_file_path),schema=schema,sep=u"\u0001", multiLine=True)  # 单个字段存在多行数据的情况 multiLine=True可以解决字段可能串位的问题
df = spark.read.json("/opt/test/test.json")
df.collect().asDict()  # pyspark df结果转dict
#Spark加载mongo
conf = {'spark.mongodb.input.partitionerOptions.partitionKey': '_id', 'pipeline': '{$match:{last_updated_time:{$gte:1582041600000}}}', 'spark.mongodb.input.partitionerOptions.numberOfPartitions': '2000', 'spark.mongodb.input.readPreference.name': 'secondaryPreferred', 'spark.mongodb.input.partitioner': 'MongoPaginateByCountPartitioner', 'uri': 'mongodb://user:passwd@ip:port/db.collection'}
spark.read.format("com.mongodb.spark.sql").options(**conf).load()
如果有schema schema=StructType.fromJson(schema_dict)
如果有schema schema=StructType.fromJson(schema_dict)
spark.read.format("com.mongodb.spark.sql").options(**conf).load(schema=StructType.fromJson(schema_dict))
#Spark加载mysql
conf={'user': 'spark', 'url': u'jdbc:mysql://ip:3306/db?zeroDateTimeBehavior=convertToNull&autoReconnect=true', 'driver': 'com.mysql.jdbc.Driver', 'password': '123456', 'db': u'db', 'dbtable': u"(select * from db.tb where field_name>='1582214400000' and field_name<'1582300800000' )tmp", 'tb': u'tb'} # dbtable可参考官网
df = spark.read.format("jdbc").options(**conf).load()
#spark读取avro
avro_df = spark.read.format("com.databricks.spark.avro").load(hdfs_path)
#spark读csv自动解析schema
spark.read.csv("E:\Video_Games_Sales_CSV.csv",header=True)  # 加header=True



spark sql DataFrame的差集，交集，合集
差集（与df顺序有关）：df =  df1.select("user_id","auto_id").subtract(df2.select("user_id","auto_id"))  # scala是except
交集：df =  df1.select("user_id","auto_id").intersect(df2.select("user_id","auto_id"))
并集：df =  df1.select("user_id","auto_id").union(df2.select("user_id","auto_id"))  不去重

spark sql生成df：
sentenceDataFrame = spark.createDataFrame((
      (1, "asf"),
      (2, "2143"),
      (3, "rfds")
    )).toDF("label", "sentence")
sentenceDataFrame.show()

spark.sql("""select CAST(unix_timestamp() as string)""").show(1,False)   # 查看当前时间戳，转为string
spark.sql("""select CAST(unix_timestamp() as bigint)""").show(1,False)   # 查看当前时间戳，转为bigint

df.dtypes
http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame

df = df.withColumn("enddate", judge_date_udf("enddate")).withColumn().withColumn()......

shuffle write：分区数由上一阶段的RDD分区数控制
类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则，临时存放到各个executor所在的本地磁盘上。
shuffle read：分区数由Spark提供的参数控制
如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。
有时候即使不会导致JVM crash也会造成长时间的gc。

TaskKilled (another attempt succeeded)  Task计算时间太长被干掉重试

UDF
judge_date_udf = udf(judge_date,TimestampType())
df = df.withColumn("enddate", judge_date_udf("enddate"))
-=-=-=-=-=-
from pyspark.sql.types import *
def length(ip):
    if ip:
        return len(ip)
    else:
        return 0
spark.udf.register("length", length, IntegerType())
spark.sql("select sum(length(ip)),sum(length(language)) from table").show(1, False)

info_dicts = spark.sql("""desc %s""" % table_name).toPandas().to_dict()   # 转换成pandas的df
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
    (info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))
df.registerTempTable('temp_table')
info_df = spark.sql("desc formatted temp_table")
col_comment_in_df = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.asDict().get('col_name'): x.asDict().get('comment')},
info_df.select('col_name', 'comment').collect()))   # 获取到df每个字段的注释
    paras = []
    for col_name, data_type in col_comment_in_hive.keys():
        if col_name.find("#") < 0 and col_name != 'dt_batch':   # 不对dt_batch字段做注释处理
            hive_comment = col_comment_in_hive.get((col_name, data_type))
            mysql_comment = comment_info_in_mysql.get(col_name)  # t_comment_records表中的注释
            if not mysql_comment:
                mysql_comment = ''
            if hive_comment != mysql_comment and mysql_comment:  # t_comment_records中存在注释且hive表中的注释与mysql里不同
                # 需要更新注释
                paras.append((str(table_name), str(col_name), str(col_name), str(data_type), str(mysql_comment)))
            else:
                # 不需要更新注释 可能是因为hive有注释而mysql没有，也可能是hive和mysql中都没有
                if (hive_comment != '' and hive_comment != '无注释' and hive_comment != u'None' and hive_comment) and not mysql_comment:
                    # hive有注释而mysql没有
                    logger.warn("表%s的%s字段有注释而Mysql未记录,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','in hive')""" % (table, col_name))
                if (hive_comment == '' or hive_comment == '无注释' or hive_comment == u'None' or not hive_comment) and not mysql_comment:
                    # hive和mysql中都没有
                    logger.warn("表%s的%s字段没有注释,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','no comment')""" % (table, col_name))

    if paras:
        flag = True
        hive_cursor = create_hive_cursor()
        column_comment_dict = {}  # 让spark的元数据修改的dict
        for para in paras:
            tb = para[0]
            col = para[1]
            types = para[3]
            comment = para[4]
            replace_para = []
            # 遇到'array<struct<_id:struct<_oid:string>,_code:int,name:string>>'类型自动加反引号
            for i in types.split('<'):
                logger.info(i)
                if i.startswith('_'):
                    replace_para.append(i.split(':')[0])
                if len(i.split(',')) != 1:
                    for j in i.split(','):
                        if j.startswith('_'):
                            replace_para.append(j.split(':')[0])
            replace_para = set(replace_para)
            logger.info(replace_para)
            if replace_para:
                for i in replace_para:
                    types = types.replace(i, "`" + i + "`")
            # hive的注释元数据修改
            alter_sql = """ALTER TABLE %s CHANGE COLUMN `%s` `%s` %s COMMENT '%s' """ % (tb, col, col, types, comment)
            logger.info(alter_sql)
            hive_cursor.execute(alter_sql)
            logger.info("[hive元数据] 表%s的字段注释已修改" % table_name)
            column_comment_dict[col] = comment

        # spark的注释元数据修改
        change_sql = get_spark_ddl_sql(table_name, column_comment_dict, hive_cursor)
        sql_list = change_sql.encode('utf-8').split('\n')
        for s in sql_list:
            if s:
                try:
                    hive_cursor.execute(s.rstrip(';'))
                    logger.info("[Spark SQL元数据] 表%s的SparkSQL注释元数据已修改" % table_name)
                except Exception as e:
                    logger.warn("Spark SQL元数据修改失败，错误信息：" , e)
        # if len(change_sql) > 0:
        #     filename = "/data/ddl_change.txt"
        #     if os.path.exists(filename):
        #         print(filename + "已存在,新建该文件")
        #         os.remove(filename)
        #     f = open("/data/ddl_change.txt", "w")
        #     f.write(change_sql)
        #     f.close()
        # command = "hive  -f /data/ddl_change.txt "
        # status, text = commands.getstatusoutput(command)
    else:
        logger.info("表的字段注释无需修改")

可能Driver内存不足：
Diagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.
Dump of the process-tree for container_e23_1574819880505_43157_01_000001

spark.driver.maxResultSize worker发送给driver的最大结果大小，超过限制会报错，如果不限制，会无限占用driver内存，可能将Driver内存耗尽

spark_ddl = df._jdf.schema().treeString()
df_schema = df.schema().json()

spark catalog=sparksql查看表元数据的API
```python
catalog = spark.catalog
catalog.listDatabases()  # 查看数据库  scala版返回df
catalog.listColumns(table_name)  # 查看表的列  scala版返回df
catalog.listTables(db_name=None)  # 查看数据库的表  scala版返回df
catalog.listFunctions()  # 查询已注册的udf  scala版返回df
catalog.setCurrentDatabase(db_name) # 设置当前catalog对象的db
catalog.currentDatabase()  # 查看当前使用的数据库
catalog.registerFunction(name, f, returnType=StringType())  #调用_jsparkSession和_judf注册UDF name->UDFname f->PythonFunction returnType->返回类型pyspark.sql.types.DataType
catalog.isCached(db_table_name) # 查看表是否被缓存
catalog.dropTempView(view_name) # 如果是Spark SQL视图，会删除事先注册好的，如果hive视图，删除元数据中hive视图
catalog.uncacheTable(table_name） # 取消缓存
catalog.cacheTable(table_name) # 缓存一张表
scala版本独有：catalog.databaseExists(db_name)  # 查看库是否存在  在python中使用这个：spark._jsparkSession.catalog().databaseExists('default')
```

比较时间大小
spark.sql("""
        SELECT t1.task_id AS ti,
            t1.table_name AS tb,
            cast(t2.execution_date AS BIGINT)-8*3600 AS ed,
            cast(unix_timestamp('%s','yyyyMMdd') AS BIGINT) AS yesterday
        FROM airflow_task_table_relation t1
        LEFT JOIN task_instance t2 ON t1.task_id = t2.task_id
        WHERE t1.table_type = 'target'
        HAVING ed >= yesterday
        AND ed <= yesterday+86400
""" % batch[0: 8]).drop('yesterday').collect()

数据抽取-根据主键获取mysql删除数据 同步更新到hive
native_all 是存量hive数据
delete_data = native_all.join(primary_key_df,primary,"left_anti")  # delete_data是hive存量数据 join sparkjdbc主键和增量字段获取的primary_key_df
all_full = all_full.join(delete_data,primary,"left_anti")  # 没join上的返回左df也就是all_full的数据 实现去除mysql删除数据

###################################################
Hive相关
分区分文件夹，分桶分文件
hive表和spark sql创建的hive表 元数据都存在hive中，但是不统一，spark sql的元数据基于hive，但不一致，sparksql的元数据可以desc formatted方式看到，在下方，（        spark.sql.create.version        2.2 or prior
Hive map join：https://www.jianshu.com/p/b52466e93226
                                                                                                            spark.sql.sources.schema.part.0）
查看sparksql的元数据：
元数据分区数hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.numParts')" % table_name )    part_num = hive_cursor.fetchone()[0]   part_num=int(part_num.strip())  得到0，1，2...
单独一个分区的元数据hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.part.%s')" % (table_name,num_partition))  # num_partition  0,1,2...
元数据修改：ALTER TABLE %s SET TBLPROPERTIES ('spark.sql.sources.schema.part.%s' ='%s');  。。。。 '{\"fields\": [{\"metadata\": {\"comment\": \"eid\"}, \"type\": \"string\", \"name\": \"eid\", \"nullable\": true},。。。}

修改Hive/SQL表注释: ALTER TABLE table_name SET TBLPROPERTIES('comment' = '这是表注释!');

修改Hive/SQL字段注释: ALTER TABLE table_name CHANGE COLUMN muid muid_new STRING COMMENT '这里是列注释!'; 

hive查看分区:show partitions table_name;

ALTER TABLE test_table DROP PARTITION (dt_batch='2016-08-08', hour='10');  # hive删除分区的方法
##################################################
Mysql相关


获取一个表的主键：
select column_name from information_schema.COLUMNS where TABLE_NAME='system_state' and TABLE_SCHEMA='d_bigdata' and COLUMN_KEY='PRI';


explain分析sql性能的方法：
select_type :
    SIMPLE: 简单的查询，不包括子查询，关联查询等等
    PRIMARY: 查询中如果有复杂的部分，最外层的查询将被标记为PRIMARY
    SUBQUERY: 子查询中的第一个查询
    UNION: 关联查询，最后面的一个
type：查询语句的性能表现: 依次递增 all<index<range<index_merge<ref<eq_ref<constant/system
    All:  全表扫描，最耗性能
    index: 全索引列扫描
    range: 对单个索引列进行范围查找 ,使用 < 或者 between and 或者 in 或者 !=
    index_merge: 多个索引合并查询
    ref:  根据单个索引查找
    eq_ref: 连接时使用primary key 或者 unique类型
    constant: 常量
    system: 系统
possible_keys: 可能使用到的索引
key: 真实使用的索引
key_len: 使用到的索引长度
rows：扫描的行数
extra：包含MySQL为了解决查询的详细信息

没数据则插入 有数据则修改
REPLACE INTO d_bigdata.system_state(name,state,info) VALUES('$2','$3','$4');


查询多久时间之前的时间
select DATE_SUB(start_time,Interval 2 DAY) from table;
select DATE_SUB(now(),Interval 60 MINUTE);
select datediff(DATE_SUB(now(),Interval 2 DAY), now()) as diff;

查看并发线程数
show status like 'Threads%';
show status 下面有一些参数可以参考 https://www.cnblogs.com/zuxing/articles/7761262.html 和  https://blog.csdn.net/sdhymllc/article/details/88052704

select * from information_schema.processlist where user='root';与show processlist类似，能进行筛选想看的信息。

# 无论什么的cursor（mysql、 hive），多次execute后，fetch只能fetch到最后一次的execute数据
查看hive表的注释 SHOW TBLPROPERTIES table_name ('comment');
    hive_cursor.execute("SHOW TBLPROPERTIES %s ('comment')" % source_table_name )
    table_comment = hive_cursor.fetchone()

查看mysql注释的简便方法 SELECT TABLE_COMMENT FROM INFORMATION_SCHEMA.TABLES  WHERE TABLE_NAME = 'tb' AND TABLE_SCHEMA ='db';

col_comm_in_mysql = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.get('col'): x.get('comment')}, fetch))   # 得到字段-注释 的dict
df = spark.sql("""desc %s""" % table_name).toPandas()   # 转换成pandas的df
info_dicts = df.to_dict()
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
(info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))

查看库的信息/机器IP
show slave status\G;

MySQL出现Waiting for table metadata lock的原因以及解决方法:
出现场景https://www.cnblogs.com/digdeep/p/4892953.html
show processlist ；显示的数据里有个id字段，就是sessionid，执行 kill id就可
还可以调整超时锁的时间阈值  默认阈值1年。。。 下面改为1800s
set session lock_wait_timeout = 1800;
set global lock_wait_timeout = 1800;

with xx as语法 可以实现一个或多个别名（貌似sqlserver语法，mysql不支持）
在Mysql中用临时表：  语法：create temporary table aaa select * from student; 这样就创建了aaa临时表
with as 相当于虚拟视图
with as短语，也叫做子查询部分(subquery factoring)，可以让你做很多事情，定义一个sql片断，该sql片断会被整个sql语句所用到。有的时候，是为了让sql语句的可读性更高些，也有可能是在union all的不同部分，作为提供数据的部分。
特别对于union all比较有用。因为union all的每个部分可能相同，但是如果每个部分都去执行一遍的话，则成本太高，所以可以使用with as短语，则只要执行一遍即可。如果with as短语所定义的表名被调用两次以上，则优化器会自动将with as短语所获取的数据放入一个temp表里，如果只是被调用一次，则不会。而提示materialize则是强制将with as短语里的数据放入一个全局临时表里。很多查询通过这种方法都可以提高速度。
with tmp_table as (select * from table where i>1),
with tmp_table1 as (select * from table where i>2),
with tmp_table2 as (select * from table where i>3);
select * from tmp_table;
优点
(1). SQL可读性增强。比如对于特定with子查询取个有意义的名字等。
(2)、with子查询只执行一次，将结果存储在用户临时表空间中，可以引用多次，增强性能。

小技巧：
where 1=1 and xxx 有时候场景需要根据条件拼sql，如果拼接的字符串为空，那就要把where去掉，麻烦  所以 先 where 1=1，然后所有拼接的语句有直接加 “AND xx=xx” 方便，且where 1=1 始终为True。
where 1=0同理始终为False 始终无结果

##################################################















##################################################
Linux相关
```bash
# https://majing.io/posts/10000006301174
# http://c.biancheng.net/view/1114.html
# https://blog.csdn.net/zhan570556752/article/details/80399154
# https://www.cnblogs.com/jacktian-it/p/11550292.html
# https://my.oschina.net/u/3222944/blog/3066822
# https://www.runoob.com/linux/linux-shell-process-control.html
```
netstat -tunlp | grep 2181  查看端口
查看目录树 yum -y install tree   tree /data/test

mkdir test
chmod 1777 test
drwxrwxrwt 权限文件夹 任何人都可以在此目录拥有写权限，但是不能删除别人拥有的文件

cat /etc/group查看组信息

内存自动清理脚本https://blog.csdn.net/wangrui1573/article/details/89838938

局域网传文件
python -m SimpleHTTPServer 8000
wget 192.168.1.101:8000/xxx

Linux最大线程数限制，超过就创建失败（查看可生成的最大线程数cat /proc/sys/kernel/pid_max）（查看当前使用的线程数pstree -p | wc -l）

python readline一次读一行  readlines和read是读整个 写入内存  内存不够用readline
with open('filepath', 'r', encoding = 'utf-8') as f:
　　while True:
　　　　line = f.readline() # 逐行读取
　　　　if not line: # 到 EOF，返回空字符串，则终止循环
　　　　　　break
　　　　Operate(line) #对每行数据进行处理
python读大文件 分块读比较实用
def read_in_chunks(filePath, chunk_size=1024*1024):
　　file_object = open(filePath,'r',encoding='utf-8')
　　while True:
　　　　chunk_data = file_object.read(chunk_size)
　　　　if not chunk_data:
　　　　　　break
　　　　yield chunk_data
if __name__ == "__main__":
　　filePath = "C:/Users/Public/Documents/data/user_data.csv"
　　for chunk in read_in_chunks(filePath):
　　　　print(chunk)

正则判断str中特定字符后的内容 'a:"(.*?)"'

shell操作mysql
mysql  -hhostname -Pport -uusername -ppassword  -e  相关mysql的sql语句，不用在mysql的提示符下运行mysql，即可以在shell中操作mysql的方法。
test.sh
#!/bin/bash
HOSTNAME="192.168.111.84"                                           #数据库信息
PORT="3306"
USERNAME="root"
PASSWORD=""
DBNAME="test_db_test"                                                       #数据库名称
TABLENAME="test_table_test"                                            #数据库中表的名称
#创建数据库
create_db_sql="create database IF NOT EXISTS ${DBNAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} -e "${create_db_sql}"
#创建表
create_table_sql="create table IF NOT EXISTS ${TABLENAME} (  name varchar(20), id int(11) default 0 )"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${create_table_sql}"
#插入数据
insert_sql="insert into ${TABLENAME} values('billchen',2)"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${insert_sql}"
#查询
select_sql="select * from ${TABLENAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"
#更新数据
update_sql="update ${TABLENAME} set id=3"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${update_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"
#删除数据
delete_sql="delete from ${TABLENAME}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${delete_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${delete_sql}"
mysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e "${select_sql}"

查找当前目录一个月(30天)以前大于100M的日志文件(.log)并删除
find  . -name "*.log" –mtime +30 –type f –size +100M |xargs rm –rf {} ;
*************************************待写博客****************************************
--------------------------------Mysql Binlog---------------------------------
mysql回滚(表引擎必须是INNODB) 可以show create table xx;查看
select * from table_name;
回滚事务步骤:
start transaction;   开始事务
savepoint a;   创建保存点a
delete from table_name where id = 1;  删除一条记录
select * from table_name;
rollback to a;  回滚到a保存点
select * from table_name;  没问题的话  执行commit;

mysql的binlog是多文件存储，定位一个LogEvent需要通过binlog filename + binlog position，进行定位
mysql的binlog数据格式，按照生成的方式，主要分为：statement-based、row-based、mixed。
mysql> show variables like 'binlog_format';
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | binlog_format | ROW   |
    +---------------+-------+
    1 row in set (0.00 sec)

mysql表占用空间大小
SELECT (`DATA_LENGTH`+ `INDEX_LENGTH`)/1024/1024  as `table_data_size`  from information_schema.TABLES WHERE TABLE_NAME ='tb' and TABLE_SCHEMA='db';

获取字段和类型 列表：
                select
                    column_name,data_type
                from
                    information_schema.columns
                where
                    table_name='tb' and table_schema='db'



-----------------------------------------------------------------------------
算法数据结构相关
数据结构可视化：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html





-----------------------------------------------------------------------------
数仓相关：

OLAP OLTP区别：
OLAP是联机分析处理 OLTP是也叫联机事务处理
OLAP主要是对应数据仓库，OLTP对应数据库
OLAP面向决策层 OLTP面向业务层
OLAP 系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。      OLTP 系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作。
OLAP数据量大，DML少，数仓  OLTP数据量少，DML频繁，并行事务处理多。
具体：https://www.cnblogs.com/hhandbibi/p/7118740.html
















-----------------------------------------------------------------------------


--------------------------------JVM GC分析--------------------------------
https://www.cnblogs.com/rainwang/p/7213918.html    GC分析   G1  不研究CMS
-------------------------------------------------------------------------
