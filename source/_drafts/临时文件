##################################################
好的学习网站
https://www.iteblog.com/
https://www.liaoxuefeng.com
https://home.apachecn.org/docs/
https://github.com/apachecn

##################################################
JVM相关
OOM八种方式与解决：https://tianmingxing.com/2019/11/17/%E4%BB%8B%E7%BB%8DJVM%E4%B8%ADOOM%E7%9A%848%E7%A7%8D%E7%B1%BB%E5%9E%8B/

##################################################


##################################################
大数据热点话题
八种排查处理数据倾斜 https://mp.weixin.qq.com/s/piW10KGJVgaSB_i72OVntA

##################################################


##################################################
Cloudera Manager相关
https://cloud.tencent.com/developer/article/1544865



##################################################
Hadoop相关
NFS挂载hdfs到本地目录 开启NFS Gateway后 mount -t nfs -o vers=3,proto=tcp,nolock,sync,rsize=1048576,wsize=1048576 nfs_ip:/ /data/hdfs_nfs
NFS卸载umount /data/hdfs_nfs



##################################################
Python相关
打印调用栈
import traceback
def BBQ():
    traceback.print_stack()
    a = traceback.extract_stack()
    b = traceback.format_stack()

MD5 BASE64:
img_path = '/home/jiajing_qu/img.jpg'
import md5
m = md5.new()
m.update(img_path)
res = m.hexdigest()
import hashlib
import base64
with open(img_path, 'rb') as fp:
	data = fp.read()
	base64_data = base64.b64encode(data)
	file_md5= hashlib.md5(data).hexdigest()
content = {"base64": base64_data, "md5": file_md5}
{"base64": base64_data, "md5": file_md5}
with open(img_path, 'rb') as fp:
    content = fp.read()

r = [u'varchar(255)', u'varchar(255)', u'varchar(255)', u'datetime', u'datetime']
r[0].__contains__('varchar')
r[0].__contains__('varchar(255)')
r[0].__contains__('varchar(254)')
判断类型  isinstance(comment_info, dict)    isinstance(comment_info, basestring)
enumerate(list[])

logging模块
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.info()

for i in enumerate(list):
    print(i)   # i是(0,list[0])  (1,list[1])....

对数组和

###################################################
Spark相关
Spark Broadcast https://blog.csdn.net/zx8167107/article/details/79186477
spark 操作 https://www.cnblogs.com/Frank99/p/8295949.html
PYSPARK_DRIVER_PYTHON=ipython $SPARK_HOME/bin/pyspark
Structured Streaming分析https://www.jianshu.com/p/d4e2a2be9a10

df.repartition(10).rdd.getNumPartitions()  重分区并查看分区数

df.dtypes
http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame

df = df.withColumn("enddate", judge_date_udf("enddate")).withColumn().withColumn()......

shuffle write：分区数由上一阶段的RDD分区数控制
类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则，临时存放到各个executor所在的本地磁盘上。
shuffle read：分区数由Spark提供的参数控制
如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。
有时候即使不会导致JVM crash也会造成长时间的gc。

TaskKilled (another attempt succeeded)  Task计算时间太长被干掉重试

UDF
judge_date_udf = udf(judge_date,TimestampType())
df = df.withColumn("enddate", judge_date_udf("enddate"))
-=-=-=-=-=-
from pyspark.sql.types import *
def length(ip):
    if ip:
        return len(ip)
    else:
        return 0
spark.udf.register("length", length, IntegerType())
spark.sql("select sum(length(ip)),sum(length(language)) from table").show(1, False)

info_dicts = spark.sql("""desc %s""" % table_name).toPandas().to_dict()   # 转换成pandas的df
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
    (info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))
df.registerTempTable('temp_table')
info_df = spark.sql("desc formatted temp_table")
col_comment_in_df = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.asDict().get('col_name'): x.asDict().get('comment')},
info_df.select('col_name', 'comment').collect()))   # 获取到df每个字段的注释
    paras = []
    for col_name, data_type in col_comment_in_hive.keys():
        if col_name.find("#") < 0 and col_name != 'dt_batch':   # 不对dt_batch字段做注释处理
            hive_comment = col_comment_in_hive.get((col_name, data_type))
            mysql_comment = comment_info_in_mysql.get(col_name)  # t_comment_records表中的注释
            if not mysql_comment:
                mysql_comment = ''
            if hive_comment != mysql_comment and mysql_comment:  # t_comment_records中存在注释且hive表中的注释与mysql里不同
                # 需要更新注释
                paras.append((str(table_name), str(col_name), str(col_name), str(data_type), str(mysql_comment)))
            else:
                # 不需要更新注释 可能是因为hive有注释而mysql没有，也可能是hive和mysql中都没有
                if (hive_comment != '' and hive_comment != '无注释' and hive_comment != u'None' and hive_comment) and not mysql_comment:
                    # hive有注释而mysql没有
                    logger.warn("表%s的%s字段有注释而Mysql未记录,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','in hive')""" % (table, col_name))
                if (hive_comment == '' or hive_comment == '无注释' or hive_comment == u'None' or not hive_comment) and not mysql_comment:
                    # hive和mysql中都没有
                    logger.warn("表%s的%s字段没有注释,存入t_no_comment_records表" % (table, col_name))
                    query.query("""INSERT IGNORE INTO t_no_comment_records(table_name,col,state) VALUES ('%s','%s','no comment')""" % (table, col_name))

    if paras:
        flag = True
        hive_cursor = create_hive_cursor()
        column_comment_dict = {}  # 让spark的元数据修改的dict
        for para in paras:
            tb = para[0]
            col = para[1]
            types = para[3]
            comment = para[4]
            replace_para = []
            # 遇到'array<struct<_id:struct<_oid:string>,_code:int,name:string>>'类型自动加反引号
            for i in types.split('<'):
                logger.info(i)
                if i.startswith('_'):
                    replace_para.append(i.split(':')[0])
                if len(i.split(',')) != 1:
                    for j in i.split(','):
                        if j.startswith('_'):
                            replace_para.append(j.split(':')[0])
            replace_para = set(replace_para)
            logger.info(replace_para)
            if replace_para:
                for i in replace_para:
                    types = types.replace(i, "`" + i + "`")
            # hive的注释元数据修改
            alter_sql = """ALTER TABLE %s CHANGE COLUMN `%s` `%s` %s COMMENT '%s' """ % (tb, col, col, types, comment)
            logger.info(alter_sql)
            hive_cursor.execute(alter_sql)
            logger.info("[hive元数据] 表%s的字段注释已修改" % table_name)
            column_comment_dict[col] = comment

        # spark的注释元数据修改
        change_sql = get_spark_ddl_sql(table_name, column_comment_dict, hive_cursor)
        sql_list = change_sql.encode('utf-8').split('\n')
        for s in sql_list:
            if s:
                try:
                    hive_cursor.execute(s.rstrip(';'))
                    logger.info("[Spark SQL元数据] 表%s的SparkSQL注释元数据已修改" % table_name)
                except Exception as e:
                    logger.warn("Spark SQL元数据修改失败，错误信息：" , e)
        # if len(change_sql) > 0:
        #     filename = "/data/ddl_change.txt"
        #     if os.path.exists(filename):
        #         print(filename + "已存在,新建该文件")
        #         os.remove(filename)
        #     f = open("/data/ddl_change.txt", "w")
        #     f.write(change_sql)
        #     f.close()
        # command = "hive  -f /data/ddl_change.txt "
        # status, text = commands.getstatusoutput(command)
    else:
        logger.info("表的字段注释无需修改")

可能Driver内存不足：
Diagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.
Dump of the process-tree for container_e23_1574819880505_43157_01_000001

spark_ddl = df._jdf.schema().treeString()
df_schema = df.schema().json()

spark catalog=sparksql查看表元数据的API
```python
catalog = spark.catalog
catalog.listDatabases()  # 查看数据库  scala版返回df
catalog.listColumns(table_name)  # 查看表的列  scala版返回df
catalog.listTables(db_name=None)  # 查看数据库的表  scala版返回df
catalog.listFunctions()  # 查询已注册的udf  scala版返回df
catalog.setCurrentDatabase(db_name) # 设置当前catalog对象的db
catalog.currentDatabase()  # 查看当前使用的数据库
catalog.registerFunction(name, f, returnType=StringType())  #调用_jsparkSession和_judf注册UDF name->UDFname f->PythonFunction returnType->返回类型pyspark.sql.types.DataType
catalog.isCached(db_table_name) # 查看表是否被缓存
catalog.dropTempView(view_name) # 如果是Spark SQL视图，会删除事先注册好的，如果hive视图，删除元数据中hive视图
catalog.uncacheTable(table_name） # 取消缓存
catalog.cacheTable(table_name) # 缓存一张表
scala版本独有：catalog.databaseExists(db_name)  # 查看库是否存在
```

###################################################
Hive相关
hive表和spark sql创建的hive表 元数据都存在hive中，但是不统一，spark sql的元数据基于hive，但不一致，sparksql的元数据可以desc formatted方式看到，在下方，（        spark.sql.create.version        2.2 or prior
Hive map join：https://www.jianshu.com/p/b52466e93226
                                                                                                            spark.sql.sources.schema.part.0）
查看sparksql的元数据：
元数据分区数hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.numParts')" % table_name )    part_num = hive_cursor.fetchone()[0]   part_num=int(part_num.strip())  得到0，1，2...
单独一个分区的元数据hive_cursor.execute("SHOW TBLPROPERTIES %s ('spark.sql.sources.schema.part.%s')" % (table_name,num_partition))  # num_partition  0,1,2...
元数据修改：ALTER TABLE %s SET TBLPROPERTIES ('spark.sql.sources.schema.part.%s' ='%s');  。。。。 '{\"fields\": [{\"metadata\": {\"comment\": \"eid\"}, \"type\": \"string\", \"name\": \"eid\", \"nullable\": true},。。。}

修改Hive/SQL表注释: ALTER TABLE table_name SET TBLPROPERTIES('comment' = '这是表注释!');

修改Hive/SQL字段注释: ALTER TABLE table_name CHANGE COLUMN muid muid_new STRING COMMENT '这里是列注释!'; 

hive查看分区:show partitions table_name;

ALTER TABLE test_table DROP PARTITION (dt_batch='2016-08-08', hour='10');  # hive删除分区的方法
##################################################
Mysql相关

show status like 'Threads%'; 查看并发线程数
show status 下面有一些参数可以参考 https://www.cnblogs.com/zuxing/articles/7761262.html 和  https://blog.csdn.net/sdhymllc/article/details/88052704

select * from information_schema.processlist where user='root';与show processlist类似，能进行筛选想看的信息。

# 无论什么的cursor（mysql、 hive），多次execute后，fetch只能fetch到最后一次的execute数据
查看hive表的注释 SHOW TBLPROPERTIES table_name ('comment');
    hive_cursor.execute("SHOW TBLPROPERTIES %s ('comment')" % source_table_name )
    table_comment = hive_cursor.fetchone()

查看mysql注释的简便方法 SELECT TABLE_COMMENT FROM INFORMATION_SCHEMA.TABLES  WHERE TABLE_NAME = 'tb' AND TABLE_SCHEMA ='db';

col_comm_in_mysql = reduce(lambda x, y: dict(x, **y), map(lambda x: {x.get('col'): x.get('comment')}, fetch))   # 得到字段-注释 的dict
df = spark.sql("""desc %s""" % table_name).toPandas()   # 转换成pandas的df
info_dicts = df.to_dict()
col_comment_in_hive = reduce(lambda x, y: dict(x, **y), map(lambda x: {(x[0], x[2]): x[1]}, zip \
(info_dicts['col_name'].values(), info_dicts['comment'].values(), info_dicts['data_type'].values())))

查看库的信息/机器IP
show slave status\G;

##################################################















##################################################
Linux相关
```bash
# https://majing.io/posts/10000006301174
# http://c.biancheng.net/view/1114.html
# https://blog.csdn.net/zhan570556752/article/details/80399154
# https://www.cnblogs.com/jacktian-it/p/11550292.html
# https://my.oschina.net/u/3222944/blog/3066822
# https://www.runoob.com/linux/linux-shell-process-control.html
```
netstat -tunlp | grep 2181  查看端口
查看目录树 yum -y install tree   tree /data/test

mkdir test
chmod 1777 test
drwxrwxrwt 权限文件夹 任何人都可以在此目录拥有写权限，但是不能删除别人拥有的文件

cat /etc/group查看组信息

内存自动清理脚本https://blog.csdn.net/wangrui1573/article/details/89838938

局域网传文件
python -m SimpleHTTPServer 8000
wget 192.168.1.101:8000/xxx

Linux最大线程数限制，超过就创建失败（查看可生成的最大线程数cat /proc/sys/kernel/pid_max）（查看当前使用的线程数pstree -p | wc -l）





*************************************待写博客****************************************
--------------------------------Mysql Binlog---------------------------------
mysql回滚(表引擎必须是INNODB) 可以show create table xx;查看
select * from table_name;
回滚事务步骤:
start transaction;   开始事务
savepoint a;   创建保存点a
delete from table_name where id = 1;  删除一条记录
select * from table_name;
rollback to a;  回滚到a保存点
select * from table_name;  没问题的话  执行commit;

mysql的binlog是多文件存储，定位一个LogEvent需要通过binlog filename + binlog position，进行定位
mysql的binlog数据格式，按照生成的方式，主要分为：statement-based、row-based、mixed。
mysql> show variables like 'binlog_format';
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | binlog_format | ROW   |
    +---------------+-------+
    1 row in set (0.00 sec)

mysql表占用空间大小
SELECT (`DATA_LENGTH`+ `INDEX_LENGTH`)/1024/1024  as `table_data_size`  from information_schema.TABLES WHERE TABLE_NAME ='tb' and TABLE_SCHEMA='db';

获取字段和类型 列表：
                select
                    column_name,data_type
                from
                    information_schema.columns
                where
                    table_name='tb' and table_schema='db'



-----------------------------------------------------------------------------


--------------------------------JVM GC分析--------------------------------
https://www.cnblogs.com/rainwang/p/7213918.html    GC分析   G1  不研究CMS
-------------------------------------------------------------------------
