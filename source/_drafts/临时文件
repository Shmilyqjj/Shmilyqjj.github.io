首先在**Master**节点上设置:
```console
 hadoop fs -mkdir /alluxio/journal
 vim conf/alluxio-site.properties
 # Common properties
 alluxio.master.hostname=hadoop101  # 另一台master hadoop102    # 该项为本机外部可见地址(对Alluxio集群中其他节点可见的接口地址而非localhost等) 每个主节点上的第一个属性都必须是其自己的外部可见主机名
 alluxio.underfs.hdfs.configuration=/opt/module/cdh-hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/cdh-hadoop-2.7.2/etc/hadoop/hdfs-site.xml
 # Worker properties
 alluxio.worker.memory.size=512MB
 alluxio.worker.tieredstore.levels=1
 alluxio.worker.tieredstore.level0.alias=MEM
 alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk
 # HA properties
 alluxio.zookeeper.enabled=true
 alluxio.zookeeper.address=hadoop101:2181,hadoop102:2181,hadoop103:2181
 alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal

 vim masters   # 务必在masters中列出所有master的地址
 hadoop101
 hadoop102
```
在**Worker**节点上设置:
```console
 # HA properties
 alluxio.zookeeper.enabled=true
 alluxio.zookeeper.address=hadoop101:2181,hadoop102:2181,hadoop103:2181

 # Worker properties
 alluxio.worker.memory.size=512MB
 alluxio.worker.tieredstore.levels=1
 alluxio.worker.tieredstore.level0.alias=MEM
 alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk

 # Client节点只需设置alluxio.zookeeper.enabled和alluxio.zookeeper.address即可

 # 测试部署是否成功
 bin/alluxio-start.sh all
 alluxio fsadmin report
 alluxio runTests    # 如果出现Passed the test则说明部署成功

 # 测试高可用模式的自动故障处理: (假设此时hadoop101位primary master)
 ssh hadoop101
 jps | grep AlluxioMaster
 kill -9 <AlluxioMaster PID>
 alluxio fs leader  # 显示新的primary Master(可能需要等待一小段时间选举)
```



https://blog.csdn.net/qq_32447301/article/details/90321610
-------------------------------------------------------------------

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileSystem;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

public class readTest {
    public static void main(String[] args) throws IOException {
        String url = args[0];
        HDFSIO hi = new HDFSIO(url);
        long hdfsStartTime=System.currentTimeMillis();
        hi.readFile("/user/hive/warehouse/test.db/");
        long hdfsEndTime=System.currentTimeMillis();
        System.out.println("读取运行时间:"+(hdfsEndTime-hdfsStartTime)+" ms");
    }
}

class HDFSIO{
    private Configuration conf = new Configuration();
    public HDFSIO(String HDFSURL){
        conf.set("fs.defaultFS",HDFSURL);
        System.setProperty("HADOOP_USER_NAME","hdfs");
    }
    public void readFile(String path) throws IOException {
        FileSystem fs = FileSystem.get(conf);
        FSDataInputStream in = fs.open(new Path(path));
        BufferedReader br = null;
        try {
            br = new BufferedReader(new InputStreamReader(in));
        } finally {
            br.close();
        }
    }
}


shuffle write：分区数由上一阶段的RDD分区数控制
类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则，临时存放到各个executor所在的本地磁盘上。
shuffle read：分区数由Spark提供的参数控制
如果这个参数值设置的很小，同时shuffle read量很大，那么单个task处理的数据量也会很大，这可能导致JVM crash，从而获取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。
有时候即使不会导致JVM crash也会造成长时间的gc。





mysql回滚(表引擎必须是INNODB) 可以show create table xx;查看
select * from table_name;
回滚事务步骤:
start transaction;   开始事务
savepoint a;   创建保存点a
delete from table_name where id = 1;  删除一条记录
select * from table_name;
rollback to a;  回滚到a保存点
select * from table_name;  没问题的话  执行commit;


