{"meta":{"title":"佳境的小本本","subtitle":"佳境Shmily的个人网站","description":"佳境Shmily的个人网站","author":"佳境Shmily","url":"https://shmily-qjj.top"},"pages":[{"title":"album","date":"2019-09-21T13:47:59.000Z","updated":"2020-04-12T14:15:48.006Z","comments":true,"path":"album/index.html","permalink":"https://shmily-qjj.top/album/index.html","excerpt":"","text":"该模块正在开发balabala","keywords":"相册模块正在开发"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-04-12T14:15:48.006Z","comments":false,"path":"about/index.html","permalink":"https://shmily-qjj.top/about/index.html","excerpt":"","text":"[佳境Shmily] 与&nbsp; 佳境&nbsp; （ Shmily ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-04-12T14:15:48.008Z","comments":false,"path":"client/index.html","permalink":"https://shmily-qjj.top/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"看剧","date":"2019-09-21T13:32:48.000Z","updated":"2020-07-25T03:02:05.166Z","comments":true,"path":"bangumi/index.html","permalink":"https://shmily-qjj.top/bangumi/index.html","excerpt":"","text":"","keywords":"好剧安利"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-04-12T14:15:48.009Z","comments":false,"path":"donate/index.html","permalink":"https://shmily-qjj.top/donate/index.html","excerpt":"","text":"","keywords":"感谢大佬的打赏，我会努力更博滴！"},{"title":"lab","date":"2019-09-21T13:47:59.000Z","updated":"2020-04-12T14:15:48.010Z","comments":false,"path":"lab/index.html","permalink":"https://shmily-qjj.top/lab/index.html","excerpt":"","text":"该模块正在开发balabala","keywords":"Lab实验室模块正在开发"},{"title":"links","date":"2019-11-04T06:11:06.000Z","updated":"2020-07-10T15:34:17.924Z","comments":true,"path":"links/index.html","permalink":"https://shmily-qjj.top/links/index.html","excerpt":"","text":"","keywords":"博友圈"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-04-12T14:15:48.008Z","comments":true,"path":"comment/index.html","permalink":"https://shmily-qjj.top/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-07-25T03:43:31.921Z","comments":false,"path":"music/index.html","permalink":"https://shmily-qjj.top/music/index.html","excerpt":"","text":"欢迎光顾佳境的音乐小窝！这里有我的原创及翻唱作品，欢迎收听呦！戳这里：Shmily_佳境猜你想听：吹梦到西洲 cover恋恋故人难神秘园之歌 指弹版风之诗 cover押尾桑Cries In The Drizzle我心永恒 指弹版拥抱 cover徐秉龙 所有作品：","keywords":"佳境音乐小窝"},{"title":"rss","date":"2019-09-21T15:59:59.000Z","updated":"2020-04-12T14:15:48.013Z","comments":true,"path":"rss/index.html","permalink":"https://shmily-qjj.top/rss/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-04-12T14:15:48.016Z","comments":false,"path":"video/index.html","permalink":"https://shmily-qjj.top/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/bangumi/linglong.jpg', title: 'aaa', status: '已追完', progress: 100, jp: '废土风格的国产动漫巅峰之作', time: '放送时间: 2019-07-13', desc: ' 不久的未来，人类的世界早已拥挤不堪，迈向星河、寻找新家园的行动迫在眉捷。正当一切有条不紊的推进之时，月相异动，脚下的大地爆发了长达数十年、剧烈的地质变化，人类在这场浩劫中所剩无几。当天地逐渐恢复平静，人们从废墟和深渊中重新踏上了这片熟悉而又陌生的大地。习惯了主宰一切的我们是否还是这个世界的主人？' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 影视安利 分享好电影和好剧 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"b占"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-04-12T14:15:48.014Z","comments":true,"path":"tags/index.html","permalink":"https://shmily-qjj.top/tags/index.html","excerpt":"","text":""},{"title":"技术分享","date":"2019-09-21T14:53:25.000Z","updated":"2020-04-12T14:15:48.015Z","comments":false,"path":"tech/index.html","permalink":"https://shmily-qjj.top/tech/index.html","excerpt":"","text":"开发中","keywords":"技术分享模块正在设计和开发"}],"posts":[{"title":"使用PySpark优化Pandas","slug":"使用PySpark优化Pandas","date":"2020-08-10T03:26:08.000Z","updated":"2020-08-04T13:51:45.331Z","comments":true,"path":"de99f40d/","link":"","permalink":"https://shmily-qjj.top/de99f40d/","excerpt":"","text":"前言&emsp;&emsp;Pandas一直是非常受欢迎的数据分析利器，它基于Numpy，专为解决数据分析任务。因其基于Python，只能单节点单核心运行，所以在大数据分析场景下，瓶颈很明显。PySpark是基于Spark JavaClient的上层接口，可以结合Python语言以及Spark分布式运行的特点，来解决Pandas在大数据下的瓶颈。本篇文章主要对比Pandas API与PySparkAPI，总结一些Pandas应用场景下使用PySpark提高效率的方案。 对比 特点 Pandas PySpark 运行方式 单机单核 分布式 并行机制 不支持 支持 数据位置 单机内存 多节点内存和磁盘 大数据支持 差 优 数据处理方式 无懒加载 懒加载+优化无用操作 DataFrame 可变 不可变 基本原则 需要对大量数据进行分析的场景下，在大数据处理的源头必须使用PySpark 数据经过一系列操作、聚合后数据量减少，且迫不得已用Pandas的情况下再使用Pandas(用Pandas处理的数据尽量更少) 如果可以，尽量全程使用PySpark进行分析操作 数据创建文中所有Spark Dataframe对象简称df,Pandas的Dataframe对象简称pd_df。 Pandas pd_df = pd.read_csv(&#39;/datas/root/csv_data/csv_file.csv&#39;) # 1.读csv数据源 pd_df = spark.sql(&quot;select col1,col2 from table&quot;).to_pandas # 2.读Hive数据源 pd_df = spark.sql(&quot;select * from table&quot;).to_pandas # 3.读Hive整个表 # 4.读MySQL表数据 pd_df = pd.read_sql(&#39;select * from table&#39;, con=pymysql.connect(host=&quot;localhost&quot;,user=username,passwd=password,db=database_name,charset=&quot;utf8&quot;)) # 5.从list，set，dict创建dataftame pd.DataFrame({&quot;id&quot;:[1,2,3,4],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;]}) PySpark df = spark.read.option(&#39;inferSchema&#39;,&quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&#39;/data/data_test/csv_file.csv&#39;) # 1.读csv数据源 df = spark.sql(&quot;select col1,col2 from table&quot;) # 2.读Hive数据源 df = spark.table(&#39;table&#39;) # 3.读Hive整个表 # 4.读MySQL表数据 conf = { &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh101:3306/&quot;, &quot;dbtable&quot;: &#39;test.a&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, } df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() # 5.从list，set，dict创建dataftame df = spark.createDataFrame(pd.DataFrame({&quot;id&quot;:[1,2,3,4],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;]})) 或 df = spark.createDataFrame([(1,&#39;qjj&#39;),(2,&#39;zxw&#39;),(3,&#39;zzz&#39;),(4,&#39;abc&#39;)], [&#39;id&#39;, &#39;name&#39;]) 数据结构 Pandasindex索引：自动创建行结构：Series结构，属于Pandas DataFrame列结构：Series结构，属于Pandas DataFrame pd_df[&#39;col&#39;] = 0 # 列添加 pd_df[&#39;col&#39;] = 1 # 列修改 pd_df.dtypes # 查看字段和类型 PySparkindex索引：无行结构：Row对象，属于Spark DataFrame列结构：Column对象，属于Spark DataFrame from pyspark.sql.functions import lit df = df.withColumn(&quot;col&quot;, lit(0)) # 列添加 df = df.withColumn(&quot;col&quot;, lit(1)) # 列修改 df.dtypes # 查看字段和类型 数据显示 Pandas pd_df或print(pd_df) PySpark df.show() # 打印前20行且每个字段打印不超过20字符 df.show(30) # 打印前30行且每个字段打印不超过20字符 df.show(100,False) # 打印前100行且每个字段打印字符数不限 数据排序 Pandas pd_df.sort_index(by=&#39;score&#39;, ascending=False) # 按轴（字段score）进行倒序排序 pd_df.sort_values(by=&#39;score&#39;) # 在列中按值进行排序 PySpark df.sort(&#39;score&#39;, ascending=False) # 按列（score字段）倒序排序 df.orderBy(&#39;score&#39;) # 按列（score字段）顺序排序 数据选择或切片 Pandas # 1.取一列 pd_df.col_name # 2.取多列 pd_df[[&#39;id&#39;,&#39;score&#39;]] # 3.取第一行 pd_df.ix[0] # 4.取前两行 pd_df.head(2) PySpark # 1.取一列 df.select(&#39;score&#39;).show() # 2.取多列 df.select(&#39;id&#39;,&#39;score&#39;).show() df.select(df[&#39;id&#39;],df[&#39;score&#39;]).show() # 2.取多列 每个值加20 df.select(df[&#39;id&#39;] + 20,df[&#39;score&#39;]).show() # 3.取第一行 df.first() # 4.取前两行 df.head(2) 或 df.take(2) 数据过滤 Pandas pd_df[pd_df[&#39;score&#39;]&gt;=60] pd_df[pd_df[&#39;score&#39;]&gt;=60][pd_df[&#39;id&#39;]&gt;=5] PySpark df.filter(&#39;score&gt;=60&#39;) 或 df.where(&#39;score&gt;=60&#39;) df.filter(&#39;score&gt;=60 and id&gt;=5&#39;) 或 df.where(&#39;score&gt;=60 and id&gt;=5&#39;) 数据去重 Pandas pd_df.drop_duplicates(&#39;col&#39;) PySpark df.drop_duplicates() # data中一行元素全部相同时才去除 df.drop_duplicates([&#39;a&#39;,&#39;b&#39;]) # data根据’a&#39;,&#39;b&#39;组合列删除重复项，默认保留第一个出现的值组合。传入参数keep=&#39;last&#39;则保留最后一个 取唯一值 Pandas pd_df[&#39;col&#39;].unique() PySpark df.select(&#39;col&#39;).distinct().count() 或df.drop_duplicates([&#39;col&#39;]).count() 分组聚合 Pandas pd_df.groupby(&#39;col&#39;).mean() PySpark df.groupBy(&#39;col&#39;).mean().show() df.groupBy(&#39;col&#39;).avg(&#39;score&#39;).show() from pyspark.sql import functions df.groupBy(&#39;col&#39;).agg(functions.avg(&#39;score&#39;), functions.min(&#39;score&#39;), functions.max(&#39;score&#39;)).show() 数据统计 Pandas pd_df.count() # 输出每一列的非空行数 pd_df.describe() # 描述某些列的count, mean, std, min, 25%, 50%, 75%, max PySpark df.count() # 输出总行数 df.describe().show() # 描述某些列的count, mean, stddev, min, max 数据合并TODO:待完善测试 Pandas Pandas下有concat方法，支持轴向合并 Pandas下有merge方法，支持多列合并 同名列自动添加后缀，对应键仅保留一份副本 pd_df.join() 支持多列合并 pd_df.append() 支持多行合并 PySpark 可以使用sql实现concat、merge功能 Spark下有join方法即df.join() 同名列不自动添加后缀，只有键值完全匹配才保留一份副本 数据应用对应pd.apply(f)方法 即给df的每一列应用函数f Pandas pd_df.apply(f) # 将df的每一列应用函数f PySpark df.foreach(f) 或者 df.rdd.foreach(f) # 将df的每一列应用函数f df.foreachPartition(f) 或者 df.rdd.foreachPartition(f) # 将df的每一分区数据应用函数f 空值处理 Pandas # 对缺失数据自动添加NaNs pd_df.fillna(1) # fillna函数 将NaN的地方替换为1.0 pd_df.dropna() # dropna函数 将含有NaN的行删除 PySpark 不自动添加NaNs，且不抛出错误 df.na.fill(1).show() # fillna函数 将null的地方替换为1.0 df.na.drop().show() # dropna函数 将含有null的行删除 SQL支持 Pandas import pymysql con = pymysql.connect(host=&quot;localhost&quot;, user=&quot;root&quot;, password=&quot;123456&quot;, database=&quot;test&quot;, charset=&#39;utf8&#39;, use_unicode=True) sql_cmd = &quot;SELECT * FROM a&quot; # a是test库下的表名 pd_df = pd.read_sql(sql_cmd, con) PySpark # sql操作 df.registerTempTable(&#39;score_table&#39;) # 将已有数据注册成临时表（关闭SparkSession这个表就会消失） spark.sql(&quot;desc score_table&quot;).show() spark.sql(&quot;&quot;&quot;select count(1) as count from score_table&quot;&quot;&quot;).show() # UDF高级功能函数注册操作 from pyspark.sql.types import StringType # 引入返回值类型 spark.udf.register(&quot;get_length&quot;, lambda x: len(x), StringType()) # 注册UDF函数 spark.sql(&quot;select get_length(&#39;name&#39;) from score_table&quot;).show() # 使用UDF函数 互相转换 Pandas df = spark.createDataFrame(pandas_df) # Pandas转Spark df PySpark pandas_df = spark_df.toPandas() # Spark转Pandas df 注：Spark转Pandas df会将Spark df全部数据拉到Driver端单机单节点运行，性能差且网络IO占用高，尽量避免将大量数据转成Pandas DataFrame。 diff操作 Pandas pd_df.diff() # diff函数是用来将数据进行某种移动之后与原数据进行比较得出的差异数据 PySpark 没有diff操作（Spark的上下行是相互独立，分布式存储的） 高级用法（优化） PySpark连续编写转换函数 spark.table(&#39;ods_test.test&#39;).filter(&#39;age=22&#39;).where(&#39;dt=&quot;20200524&quot;&#39;).groupBy(&#39;id&#39;).avg(&#39;age&#39;).registerTempTable(&#39;tmp&#39;) for i in spark.sql(&quot;select id,&#39;avg(age)&#39; as avg_age from tmp&quot;).collect(): print(i[0], i[1]) 读取MySQL大表优化partitionColumn：分区字段，需要是数值类的（partitionColumn must be a numeric column from the table in question.），经测试，除整型外，float、double、decimal都是可以的lowerBound：下界，必须为整数，不能大于upperBound否则报错upperBound：上界，必须为整数，与lowerBound一起确定分区数据量步长，lowerBound和upperBound并不会过滤数据。numPartitions：最大分区数量，必须为整数，当为0或负整数时，实际的分区数为1；并不一定是最终的分区数量，例如“upperBound - lowerBound&lt; numPartitions”时，实际的分区数量是“upperBound - lowerBound”；以上四个参数必须同时制定否则报错。在分区结果中，分区是连续的，虽然查看每条记录的分区，不是顺序的，但是将rdd保存为文件后，可以看出是顺序的。 conf = { &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh102:3306/&quot;, &quot;dbtable&quot;: &#39;db_users.tb_user_records&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, &quot;partitionColumn&quot;: &quot;duration&quot;, # 这个字段为int类型 &quot;lowerBound&quot;: &quot;0&quot;, &quot;upperBound&quot;: &quot;10000&quot;, &quot;numPartitions&quot;: &quot;5&quot; } df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() df1.rdd.getNumPartitions() # 会得到5个分区 该操作的目的是增加并行JDBC连接数，增加读取速度以及增加DataFrame的分区数从而增加计算的并发度。并发度即为Spark的Task数，这个数量一般根据总core数（executor_coresnum_executors）来计算：Task数≈总core数（2~3倍）如果数据量较少，则不需要以这种方式读取，否则可能降低效率伪代码，帮助理解原理： # 情况一： if partitionColumn || lowerBound || upperBound || numPartitions 有任意选项未指定，报错 # 情况二： if numPartitions == 1 忽略这些选项，直接读取，返回一个分区 # 情况三： if numPartitions &gt; 1 &amp;&amp; lowerBound &gt; upperBound 报错 # 情况四： numPartitions = min(upperBound - lowerBound, numPartitions) if numPartitions == 1 同情况二 else 返回numPartitions个分区 delta = (upperBound - lowerBound) / numPartitions 分区1数据条件：partitionColumn &lt;= lowerBound + delta || partitionColumn is null 分区2数据条件：partitionColumn &gt; lowerBound + delta &amp;&amp; partitionColumn &lt;= lowerBound + 2 * delta ... 最后分区数据条件：partitionColumn &gt; lowerBound + n*delta 也就是说，需要合理设置numPartitions和upperBound和upperBound的值，避免某个分区数据量过大。尽量使用范围基本确定且分区字段值分布相对均匀的Int类型字段做分区字段。 其他Python三方库：SparklingPandasSparklingPandas 参考pandas与pyspark对比Spark：使用partitionColumn选项读取数据库原理","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://shmily-qjj.top/tags/pandas/"},{"name":"pyspark","slug":"pyspark","permalink":"https://shmily-qjj.top/tags/pyspark/"},{"name":"优化","slug":"优化","permalink":"https://shmily-qjj.top/tags/优化/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"高效运行Python方案","slug":"高效运行Python方案","date":"2020-07-26T03:16:00.000Z","updated":"2020-07-31T16:39:36.935Z","comments":true,"path":"2ed52290/","link":"","permalink":"https://shmily-qjj.top/2ed52290/","excerpt":"","text":"高效运行Python&emsp;&emsp;Python以其简洁的语法，丰富的三方库，强大的功能而受到越来越多人的欢迎，但没有十全十美的编程语言，Python的运行效率一直被人们诟病。在一些场景下，我们希望Python也能够高效率运行，充分利用系统资源，所以这篇文章记录一些加快Python程序运行效率的方法，让我们的Python更高效！ 加速已有代码&emsp;&emsp;这部分介绍的方案主要针对已有Python代码在不想做太大改动的情况下的优化方案。 使用numba加速numba官方网站 优点： 无学习成本，只加一行代码（高级用法和调优除外） 动态编译，直接翻译机器码，不走Python虚拟机，性能达到C语言水平 支持GPU加速 兼容常用的科学计算库 局限： 我测试时有些场景会报WARN，需要调一下参数，也可能环境原因 对部分第三方库有兼容性问题 测试： pip install numba 扩展： from numba import jit @jit 在方法前加装饰器-常用做法，object模式：默认nopython模式，但如果遇到不兼容的第三方库会退化成python模式，保证能运行但不能提速。 @jit(nopython=True,fastmath=True) 牺牲一点数学精度来提高速度（默认精度高） @jit(nopython=True,parallel=True) 自动进行并行计算 原理：numba加速Python代码的原理是使用jit即时编译直接将Python代码翻译成机器码（上图左侧流程），避免了编译成Python字节码pyc再走Python虚拟机（上图右侧流程），直接提高了运行效率。 结论：从上面的测试结果可以看到有将近300倍的效率提升，能大幅加速Python脚本的执行效率，对大量数据友好，对循环友好。我测试即使在低端处理器环境运行，也能有100+倍的性能提升。这个方案提速效果相当明显，而且对原有代码和环境改动很小，推荐哦！ 使用modin加速pandasmodin官方网站pandas是很常用的数据分析库，功能强大，但它有个缺点就是对大数据的支持并不好，不适合大规模数据。优点： 无学习成本，只改一行代码 可以分布式跑，基于ray 支持GPU加速 局限性： 目前支持93%的Pandas API 分布式运行功能为为实验性功能 随着运行核心数增加，会占用更多内存 安装时可能会更改原有pandas版本，需留意 需要安装ray或dask依赖包，还有一些其他依赖包 测试： def pandas_test(): import pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x: x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x: 1 if x.a%2==0 else 0, axis=1) print(&#39;pandas_df.apply Time: {:5.2f}s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg({&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]}) print(&#39;pandas_df.groupby Time: {:5.2f}s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;pandas_df.read_csv Time: {:5.2f}s&#39;.format(time() - start)) def modin_pandas_test(): import modin.pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x:x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x:1 if x.a%2==0 else 0, axis=1) print(&#39;modin_pandas_df.apply Time: {:5.2f}s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg({&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]}) print(&#39;modin_pandas_df.groupby Time: {:5.2f}s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;modin_pandas_df.read_csv Time: {:5.2f}s&#39;.format(time() - start)) if __name__ == &#39;__main__&#39;: pandas_test() modin_pandas_test() 单机跑Apply API速度大概快了3.5倍多。分布式还没测试。 结论：使用modin模块的pandas代替普通的pandas，本质是将单机单核跑的任务负载分散到多核心甚至多机器来加速运算。基本可以满足使用pandas的业务需求场景，而且核心数越多，机器数越多，运行效率提升越高，但相应需要更大的内存。适合对大量数据操作的场景。此外，pandas官网给出了一些优化效率的建议，参考：Enhancing performance 使用pandarallel加速pandaspandarallel官方网站优点： 无学习成本，只添加1-2行代码 充分利用CPU 局限性： 理论上只提速物理核心数倍的效率。 有使用成本（实现新进程，通过共享内存发送数据等等），因此只有计算量足够高时，才更有效。 使用：pandarallel-example 结论：对于非常少量的数据，不值得使用。对大量数据，可以尝试该方案，不会像modin一样依赖pandas版本，可以在原有pandas版本上操作。 编写高效代码&emsp;&emsp;除了上面已经提到的方案，在我们平时编码时也要注意编码效率，这部分主要介绍编写Python代码时一些提高运行效率的方法、技巧和工具。 使用PySpark优点： 使用Pyspark的dataframe进行数据操作数据分析简单高效，有较低的学习成本。 只需要一行代码即可实现pyspark dataframe和pandas dataframe互相转换。 Pyspark dataframe可以直接registerTempTable，然后可以很容易地使用pyspark.sql对这个表做sql分析。 分布式运行，分析效率效率高，对大量数据很友好。 功能强大，支持udf。 局限： 写代码要注意，避免小文件，减少driverResultSet（注意尽量避免让driver单点运算全部数据） 需要更多内存做计算 使用： # 例如以前的pandas分析作业，可以移植到pyspark # ①pandas dataframe转pyspark dataframe： df = spark.createDataFrame(pandas_dataframe) # ②pyspark dataframe转pandas dataframe: pandas_dataframe = spark_dataframe.toPandas() # ③代码中将spark dataframe注册成临时表（随sparkSession销毁，不占空间） df.registerTempTable(‘tmp’) # ④对数据做SQL分析 df = spark.sql(“””select * from tmp limit 10”””) 结果为新的dataframe # ⑤结果输出 df.show() / df.writeInsertInto(table_name) / df.write.option(‘header’,True),csv(file) # …… 很多种输出方式，也可以继续转回pandas dataframe做后续操作 PySpark使用文档 结论：在数据量特别大的情况下，分布式计算是首选，所以对于大规模数据分析，目前PySpark是比较推荐的方式。 使用DaskDask官方网站优点： 高效处理大量数据 支持分布式局限： 只有来自pandas的某些功能才能移植到Dask上执行 仅在不适合主存储器的数据集上，才建议使用Dask 示例： # 低速： import numpy as np import pandas as pd df = pd.Dataframe(np.random.randint(0, 6, size=(100000000, 5)), columns = list(&#39;abcde&#39;) df.groupby(&#39;a&#39;).mean() # 高速： import dask.dataframe as dd df_dask = dd.from_pandas(df, npartitions=50) df_dask.groupby(&#39;a&#39;).mean().compute() 详细了解Dask 使用多线程优点：能提高IO密集型Python程序效率。因为在一个线程因IO阻塞等待时，CPU切换到其他线程，CPU利用率高。局限：由于GIL(Global Interpreter Lock)机制限制Python解释器任何时刻都只能执行一个线程，在计算密集型Python程序并不能提高执行效率，反而可能因线程切换降低效率。使用： # 用法1 import threading import time class myThread(threading.Thread): def __init__(self,threadID,name,counter): threading.Thread.__init__(self) self.threadId = threadID self.name = name self.counter = counter def run(self): # 线程创建执行run函数 while self.counter &lt; 8: time.sleep(2) self.counter += 1 print(self.threadId,self.name,self.counter,time.ctime(time.time())) print(&quot;Thread Stop&quot;) thread1 = myThread(1, &quot;Thread-1&quot;, 1) thread2 = myThread(2, &quot;Thread-2&quot;, 2) thread1.start() thread2.start() # 用法2 import threading from queue import Queue import time def testThread(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(5): t = threading.Thread(target=testThread, arg=(i, )) t.start() GIL：GIL是CPython解释器引入的锁，GIL在解释器层面阻止了真正的并行运行。解释器在执行任何线程之前，必须等待当前正在运行的线程释放GIL，事实上，解释器会强迫想要运行的线程必须拿到GIL才能访问解释器的任何资源，例如栈或Python对象等，这也正是GIL的目的，为了阻止不同的线程并发访问Python对象。这样GIL可以保护解释器的内存，让垃圾回收工作正常，不会出现运行死锁。但事实上，这却造成了程序员无法通过并行执行多线程来提高程序的性能。如果我们去掉GIL，就可以实现真正的并行。GIL并没有影响多处理器并行的线程，只是限制了一个解释器只能有一个线程在运行。结论：IO包括磁盘IO和网络IO，所以可以在磁盘IO密集型Python任务或网络延迟是瓶颈的Python任务中使用Python多线程。 使用多进程优点：可以提高计算密集型Python程序执行效率。会用到多个CPU核心。绕过GIL机制，充分利用CPU。核心原理是以子进程的形式，平行的运行多个python解释器，从而令python程序可以利用多核CPU来提升执行速度。由于子进程与主解释器相分离，所以他们的全局解释器锁也是相互独立的。每个子进程都能够完整使用一个CPU内核。 局限： 进程间进行数据的交互会产生额外的I/O开销。 整个内存空间被复制到每个子进程中，这样对于比较复杂的程序造成的额外开销也很大。 使用： # 用法1 import multiprocessing def method(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() # 用法2 from multiprocessing.pool import ThreadPool # 可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。 # 如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 def my_print(item): print(item[0]+item[1]) pool_size = 10 # 进程池大小 items = [(1,2),(2,3),(3,4),(4,5)] pool = ThreadPool(pool_size) # 创建一个进程池 pool.map(my_print, items) # 往进程池中填进程 pool.close() # 关闭进程池，不再接受进程 pool.join() # 等待子进程结束以后再继续往下运行，通常用于进程间的同步 等待进程池中进程全部执行完 # 共享内存-共享变量 import multiprocessing from ctypes import c_char_p import time int_val = multiprocessing.Value(&#39;i&#39;, 0) # int类型共享变量 s = (c_char_p, &#39;str&#39;) # str类型共享变量 def method(num): for i in range(10): time.sleep(0.1) with int_val.get_lock(): # 仍然需要使用 get_lock 方法来获取锁对象 int_val.value += num print(int_val.value) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() 结论：如果Python程序瓶颈在CPU数量或是CPU密集型，都可采用多进程。 使用Cython优点： Python代码可通过一定工具转Cython代码 性能达到C语言水平 局限： 需要修改转换工具 高级用法学习成本高 使用：学习Cython：cython-book pip install cython 使用concurrent.futures介绍：对threading和multiprocessing进一步封装的包，方便实现线程池和进程池。使用： # 线程池 import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ThreadPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) # 进程池 import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ProcessPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) 扩展： 在两个CPU核心的机器上运行多进程程序，比其他两个版本都快。 这是因为，ProcessPoolExecutor类会利用multiprocessing模块所提供的底层机制，完成下列操作： 1. 把numbers列表中的每一项输入数据都传给map。 2. 用pickle模块对数据进行序列化，将其变成二进制形式。 3. 通过本地套接字，将序列化之后的数据从煮解释器所在的进程，发送到子解释器所在的进程。 4. 在子进程中，用pickle对二进制数据进行反序列化，将其还原成python对象。 5. 引入包含gcd函数的python模块。 6. 各个子进程并行的对各自的输入数据进行计算。 7. 对运行的结果进行序列化操作，将其转变成字节。 8. 将这些字节通过socket复制到主进程之中。 9. 主进程对这些字节执行反序列化操作，将其还原成python对象 10.最后，把每个子进程所求出的计算结果合并到一份列表之中，并返回给调用者。 multiprocessing开销比较大，原因就在于：主进程和子进程之间通信，必须进行序列化和反序列化的操作。 详细参考：python concurrent.futures 常见代码优化 在set中查找比在list查找快 list_data = list(data) set_data = set(data) # 低速： 789 in list_data # 高速： 789 in set_data 用dict而非两个list进行匹配查找 # 已知list_a,list_b # 低速： list_b[list_a.index(123)] # 高速： dict(zip(list_a,list_b)).get(123,None) 优先用for循环，比while略快 在循环体中避免重复计算 用循环机制代替递归函数 # 低速： def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # 高速： def fib(n): if n in (1,2): return 1 a, b = 1, 1 for i in range(2,n): a,b = b, a+b return b 使用缓存机制加速递归函数 # 低速： def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # 高速： from functools import lru_cache @lru_cache(100) def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) 使用collections.Counter加速计数 import time data = [x**2 % 1989 for x in range(2000000)] # 低速 st = time.time() values_count = {} for i in data: i_cnt = values_count.get(i, 0) values_count[i] = i_cnt + 1 print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) # 高速 st = time.time() from collections import Counter values_count = Counter(data) print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) 使用collections.ChainMap加速字典合并 # 低速 dict_a = {i: i + 1 for i in range(1, 1000000, 2)} dict_b = {i: i * 2 + 1 for i in range(1, 1000000, 3)} dict_c = {i: i * 3 + 1 for i in range(1, 1000000, 5)} dict_d = {i: i * 4 + 1 for i in range(1, 1000000, 7)} result = dict_a.copy() result.update(dict_b) result.update(dict_c) result.update(dict_d) print(result.get(9999)) # 高速 from collections import ChainMap chain = ChainMap(dict_a, dict_b, dict_c, dict_d) print(chain.get(9999)) 使用map代替推导式进行加速 a = [x**2 for x in range(1, 1000000, 3)] # 低速 a = map(lambda x: x**2, range(1, 1000000, 3)) # 高速 使用filter代替推导式进行加速 a = [x for x in range(1, 1000000, 3) if x % 7 == 0] # 低速 a = filter(lambda x: x % 7 == 0, range(1, 1000000, 3)) # 高速 numpy向量化加速-使用np.array代替list集合 a = range(1, 1000000, 3) b = range(1, 1000000, -3) c = [3 * a[i] - 2 * b[i] for i in range(0, len(a)] # 低速 import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.arange(1, 1000000, -3) array_c = 3 * array_a - 2 * array_b # 高速 使用np.ufunc代替math.func # 低速 import math a = range(1, 1000000, 3) b = [math.log(x) for x in a] # 高速 import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.log(array_a) pandas df.to_excel效率低于df.to_csv 查看Python性能日志使用profilerpython中的profiler可以帮助我们测量程序执行过程中详细的时间和空间复杂度。使用时通过-o参数传入可选输出文件以保留性能日志。 python -m cProfile [-o output_file] my_python_file.py 使用profile导入profile监控python程序整体执行耗时。 import profile profile.run(&#39;main()&#39;) 使用line_profiler监控方法耗时。 # pip install line_profiler def a(): pass def main(): a() from line_profiler import LineProfiler lp = LineProfiler(a,main) lp.run(&#39;main()&#39;) lp.print_stats() 在ipython中获取代码耗时%time code 获取执行code这一行代码的耗时 %%time 获取耗时 %%timeit -n 10 获取执行10次的平均耗时 %prun method() 获取执行method方法的耗时详情，输出与profiler一样","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://shmily-qjj.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"Apache Kudu总结","slug":"Apache Kudu总结","date":"2020-07-05T04:26:08.000Z","updated":"2020-07-31T16:39:36.933Z","comments":true,"path":"5f26355/","link":"","permalink":"https://shmily-qjj.top/5f26355/","excerpt":"","text":"Apache Kudu前言&emsp;&emsp;在Kudu出现前，由于传统存储系统的局限性，对于数据的快速输入和分析还没有一个完美的解决方案，要么以缓慢的数据输入为代价实现快速分析，要么以缓慢的分析为代价实现数据快速输入。随着快速输入和分析场景越来越多，传统存储层的局限性越来越明显，Kudu应运而生，它的定位介于HDFS和HBase之间，将低延迟随机访问，逐行插入、更新和快速分析扫描融合到一个存储层中，是一个既支持随机读写又支持OLAP分析的存储引擎。本篇文章研究一下Kudu，对其应用场景，架构原理及基本使用做一个总结。 Kudu介绍 在Kudu出现前，无法对实时变化的数据做快速分析： 以上设计方案的缺陷： 1.数据存储多份造成冗余，存储资源浪费。 2.架构复杂，运维成本高，排查问题困难。 而Kudu就融合了动态数据与静态数据的处理，同时支持随机读写和OLAP分析。 Kudu与HDFS,HBase的对比： 适用场景 既有随机读写随机访问，又有批量扫描分析的场景(OLAP) HTAP（Hybrid Transactional Analytical Processing）混合事务分析处理场景 要求分析结果实时性高（如实时决策，实时更新）的场景 实时数仓 支持数据逐行插入、更新操作 同时高效运行顺序读写和随机读写任务的场景 Kudu作为持久层与Impala紧密集成的场景 解决HBase(Phoenix)大批量数据SQL分析性能不佳的场景 跨大量历史数据的查询分析场景（Time-series场景） 特点及缺点 特点 基于列式存储 快速顺序读写 使用 LSM树 以支持高效随机读写 查询性能和耗时较稳定 不依赖Zookeeper 有表结构，需要定义Schema，需要定义唯一键，支持SQL分析（依赖Impala，Spark等引擎） 支持增删列,单行级ACID（不支持多行事务-不满足原子性） 查询时先查询内存再查询磁盘 数据存储在Linux文件系统，不依赖HDFS存储 缺点 暂不支持除PK外的二级索引和唯一性限制 不支持多行事务 不支持BloomFilter优化join 不支持数据回滚 不能修改PK，不支持AUTO INCREMENT PK 每表最多不能有300列，每个TServer数据压缩后不超8TB 数据类型少，不支持Map，ARRAY，Struct等复杂类型 与相似类型存储引擎对比&emsp;&emsp;本文重点说Kudu，但我们也需要了解其他类似组件，了解它们各自擅长的地方，才能更好地做技术选型。这里简单对比一下Kudu，Hudi和DeltaLake这三种存储方案，因为它们都具有相似的特性，能解决类似的问题。 特性 Kudu Hudi Delta Lake 行级别更新 支持 支持 支持 schema修改 支持 支持 支持 批流共享 支持 支持 支持 可用索引 是 是 否 多并发写 支持 不支持 支持 版本回滚 不支持 支持 支持 实时性 高 近实时 差 使用HDFS 不支持 支持 支持 空值处理 默认null error 默认null 并发读写 支持 不支持并发写 支持 云存储 不支持 支持 支持 兼容性 Spark，Impala，Presto Spark，Presto，Hive，MR 较好 依赖Spark，有限支持Hive，Presto 选择建议：考虑实时数仓方案以及SQL支持方面可选Kudu，数据湖方案及可回滚可选DeltaLake和Hudi，考虑兼容性高且应对读多写少读少写多都有很好的方案选Hudi，考虑并发写能力读多写少且与Spark紧密结合选DeltaLake。 Kudu架构原理Raft算法介绍&emsp;&emsp;为了更好地理解Kudu，需要简单了解一下Raft算法。Raft是一个一致性算法，在分布式系统中一致性算法就是让多个节点在网络不稳定甚至部分节点宕机的情况下能对某个事件达成一致。而Raft是一个用于管理日志一致性的协议，它将分布式一致性分解为多个子问题：LeaderElection，LogReplication，Safety，LogCompaction等。 &emsp;&emsp;Raft将系统中的角色分为Leader，Follower和Candidate。正常运行时只有Leader和Follower，选举时才会有Candidate。&emsp;&emsp;Leader:接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。&emsp;&emsp;Follower:接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。响应Candidate的邀请投票请求。把客户端请求重定向到Leader。&emsp;&emsp;Candidate:Leader选举过程中的临时角色，由于Follower转变而来。 Leader选举发生在Follower接收不到Leader的HeartBeat导致ElectionTimeout超时的情况下。①每个Follower都有一个时钟ElectionTimeout，是个随机值，表示Follower等待成为Leader的时间，谁的时钟先跑完则先发起Leader选举。（收到Leader心跳时会清零ElectionTimeout）②Follower将其任期(Term)加1然后转为Candidate状态，并且给自己投票，然后携Term_id和日志index给其他节点发起选举（RequestVote RPC）。有三种情况：①赢得半数以上选票，成为Leader②收到Leader消息，Leader被抢了，成为Follower③选举超时时，没有节点赢得多数选票，选举失败，Term_id自增1，进行下一轮选举 ③Raft协议所有日志都只能从Leader写入Follower，Leader节点日志只会增加（index+1），不会删除和覆盖。所以Leader必须包含全部日志，能被选举为Leader的节点一定包含了所有已经提交的日志。 每个节点最多只能给一个候选人投票，先到先服务的原则。选举胜出规则：节点Term_id越大越新则可能胜出，但可能有Term_id相同的情况，Term_id相同，比较日志Index越大越新则胜出。这一点很像Zookeeper选举的规则。详细过程：首先会有一个Candidate选自己然后发起投票-&gt;Follower收到邀票，如果这个Follower还没给其他节点投票(一个节点只能一票),且对比Term_id比自己大，就把票投给这个Candidate，如果Term_id比自己小且自己还没投票，就拒绝请求，给自己投票。 概括：增加任期编号-&gt;给自己投票-&gt;重置ElectionTimeout-&gt;发送投票RPC给其他节点 日志复制过程：Leader接收来自客户端的请求并将其以日志的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致。 Raft同步日志由编号index、term_id和命令组成。=&gt;有助于选举和根据term持久化日志永远只有一个流向Leader-&gt;Follower 1.日志复制的保证： 1.如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的（原因：leader 最多在一个任期里的一个日志索引位置创建一条日志条目，日志条目在日志的位置从来不会改变）。 2.如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的（原因：每次 RPC 发送附加日志时，leader 会把这条日志条目的前面的日志的下标和任期号一起发送给 follower，如果 follower 发现和自己的日志不匹配，那么就拒绝接受这条日志，这个称之为一致性检查）。 2.网络故障或Leader崩溃时保证一致性： 网络崩溃讲集群分为两拨，没有Leader存在的另一波会重新选主，这时网络恢复，会出现两个Leadr的情况，这是会产生冲突的，这时会根据任期Term_id将任期低的Leader自动降级为Follower，Leader和Follower日志有冲突的时候，Leader将校验Follower最后一条日志是否和Leader匹配，如果不匹配，将递减查询，直到匹配，匹配后，删除冲突的日志。这样就实现了主从日志的一致性。 递减查询，直到匹配，强制覆盖 =&gt;Leader会强制Follower复制它的日志，Leader会从最后的LogIndex从后往前试，直到找到日志一致的index，然后开始复制，覆盖该index之后的日志条目。 场景：发生了网络分区或者网络通信故障，使得Leader不能访问大多数Follwer了，那么Leader只能正常更新它能访问的那些Follower，而大多数的Follower因为没有了Leader，他们重新选出一个Leader，然后这个 Leader来接受客户端的请求，如果客户端要求其添加新的日志，这个新的Leader会通知大多数Follower。如果这时网络故障修复 了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新（递减查询匹配日志）。 日志压缩日志不能无限增长，否则会导致重播日志时耗时很长。所以对日志进行压缩，定量Snapshot。 &emsp;&emsp;Raft参考：Raft算法详解&emsp;&emsp;Raft算法在Kudu中的应用：多个TMaster之间通过Raft协议实现数据同步和高可用–Raft负责在多个Tablet副本中选出Leader和Follower，Leader Tablet负责发送写入数据给Follower Tablet，大多数副本都完成了写操作则会向客户端确认。 Kudu的一致性模型相关资料不多，可以一起讨论 Kudu为用户提供了两种一致性模型(snapshot consistency和external consistency)。默认的一致性模型是snapshot consistency。这种一致性模型保证用户每次读取出来的都是一个可用的快照，但这种一致性模型只能保证单个client可以看到最新的数据，但不能保证多个client每次取出的都是最新的数据。另一种一致性模型external consistency(后开始的事务一定可以看到先提交的事务的修改。所有事务的读写都加锁可以解决这个问题，缺点是性能较差。)可以在多个client之间保证每次取到的都是最新数据，但是Kudu没有提供默认的实现，需要用户做一些额外工作。 为了实现external consistency，Kudu提供了两种方式： 1.在client之间传播timestamp token。在一个client完成一次写入后，会得到一个timestamp token，然后这个client把这个token传播到其他client，这样其他client就可以通过token取到最新数据了。不过这个方式的复杂度很高，基于HybridTime方案，这也就是为什么Kudu高度依赖NTP。 2.通过commit-wait方式，这有些类似于Google的Spanner。但是目前基于NTP的commit-wait方式延迟实在有点高。不过Kudu相信，随着[分布式事务实现-Spanner](https://blog.csdn.net/weixin_30650039/article/details/94998723)的出现，未来几年内基于real-time clock的技术将会逐渐成熟。 LSM树LSM树(Log-Structured Merge Tree)&emsp;&emsp;Kudu与HBase在写的过程中都采用了LSM树的结构，LSM树的主要思想就是随机写转换为顺序写来提高写性能，随机读写需要磁盘的机械臂不断寻道，延迟较高，而转换为顺序写后机械臂不会频繁寻址，性能较好。&emsp;&emsp;LSM树原理是把一棵大的树拆分成N棵小树，小树存在于内存中，随着更新和写入操作，小树存放数据达到一定大小后会写入磁盘，小树到了磁盘中，定期与磁盘中的大树做合并。&emsp;&emsp;大家都知道HBase的MemStore，Kudu在写入方面的设计与之类似，Kudu先将对数据的修改保留在内存中，达到一定大小后将这些修改操作批量写入磁盘。但读取的时候稍微麻烦些，需要读取历史数据和内存中最近修改操作。所以写入性能大大提升，而读取时要先去内存读取，如果没命中，则会去磁盘读多个文件。 压缩和编码&emsp;&emsp;我们都知道列式存储的压缩效果很好，那么为什么列式存储比行存储压缩效果好呢？&emsp;&emsp;比如一个列存的国家名，那只能包含“美国”，“日本”，“韩国”，“加拿大”等值，而这些值会存储在一起，而不是分散到包含很多不相关的其他列值之间。这样列式存储也就不需要将每个值都完完整整保存起来，所以压缩效果显著。&emsp;&emsp;编码对于列式存储的优化更加明显，编码和压缩作用相同，比如上面的例子，编码会将数据的值转换为一种更小的表现形式，比如，“美国”编码为1，“日本”编码为2，“韩国”编码为3，“加拿大”编码为4…则Kudu只存储1，2，3，4…而不存储长字符串，占用空间大大减少。 Kudu一些概念 Table：具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。Tablet：Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。TabletServer：简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。Catalog Table：目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。Master：负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。WAL：一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。逻辑复制：Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。 存储与读写Kudu的存储结构：&emsp;&emsp;如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。&emsp;&emsp;MemRowSet：插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。&emsp;&emsp;DiskRowSet：DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。&emsp;&emsp;BaseData：DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。&emsp;&emsp;BloomFile：根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。&emsp;&emsp;AdhocIndex：存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。&emsp;&emsp;DeltaMemStore：每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。&emsp;&emsp;DeltaFile：DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。&emsp;&emsp;RedoFile：重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。&emsp;&emsp;UndoFile：撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。 &emsp;&emsp;DeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式： Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。 Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。 Kudu读流程： Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。 Client找到所需Tablet所在TServer，TServer接受读请求。 如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。 如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。 Kudu写流程： Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。 因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。 如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。 如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。 写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。 MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。 Kudu更新流程： Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。 Kudu检查是否符合表结构。 如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。 如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。 分区方式&emsp;&emsp;Kudu的分区即为Tablet，如果主键设计不好以及分区不合理都会造成数据发生单点读写问题，也就是热点问题。Kudu分区设计方案需要根据场景和读取写入的方式来制定。最好是将读写操作都能分散到大部分节点。&emsp;&emsp;在Kudu中只有主键才能被用来分区。分区模式有三种： 基于Hash分区(Hash Partitioning):&emsp;&emsp;哈希分区通过哈希值将行分配到许多Buckets(存储桶)之一,一个Bucket对应一个Tablet。&emsp;&emsp;优点：按ID哈希分区可以将数据均匀分布，写操作会分布在多个节点，减轻热点和Tablet大小不均匀问题。也就是基于ID哈希分区写效率高&emsp;&emsp;缺点：按ID查询数据会读取单个Tablet(Bucket)，单点读取效率低。 基于Range分区(Range Partitioning):&emsp;&emsp;由PK范围划分组成，一个区间对应一个Tablet。将数据按给定的主键范围的存储到各个TS节点上。&emsp;&emsp;优点：如果按日期范围分区，单个ID的读取会跨多个节点并行执行，效率高。也就是基于时间范围分区查询效率高。&emsp;&emsp;缺点：如果按日期范围分区会有写热点问题，而且一旦数据量超出最后一个Range，接下来的数据将全部写入最后一个Range分区，发生倾斜。 多级分区(Multilevel Partitioning):可以在单表上组合分区类型。&emsp;&emsp;优点：结合以上两种分区方式，保留两种分区类型的优点–既可以数据分布均匀，又可以在每个分片中保留指定的数据。也就是基于ID哈希分区且基于时间范围分区组合方式读写效率都会提高&emsp;&emsp;缺点：优点太多… 复制策略&emsp;&emsp;如果一个TServer出现故障，副本的数量由3减到2个，Kudu会尽快恢复副本数。两种复制策略：3-4-3：如果一个副本丢失，先添加替换的副本，再删除失败的副本，Kudu默认使用这种复制策略。3-2-3：如果一个副本丢失，先删除失败的副本，再添加替换的副本。 一些细节 为什么Kudu要比HBase、Cassandra扫描速度更快？&emsp;&emsp;HBase、Cassandra都有列簇(CF)，并不是纯正的列存储，那么一个列簇中有几个列，但这几个列不能一起编码，压缩效果相对不好，而且在扫描其中一个列的数据时，必然会扫描同一列簇中的其他列。Kudu没有列簇的概念，它的不同列数据都在相邻的数据区域，可以在一起压缩，也可以对不同列使用不同压缩算法，压缩效果很好；而且需要哪列读哪列不会读其他列，读取时不需要进行Merge操作，根据BaseData和Delta数据得到最终数据。Kudu扫描性能可媲美Parquet。还有，Kudu的读取方式避免了很多字段的比较操作，CPU利用率高。 Kudu一个Tablet中存很多很多DiskRowSet，怎么才能快速判断Key在哪个DiskRowSet？&emsp;&emsp;首先肯定不能遍历，O(n)的复杂度是很难受的。它使用二叉查找树，每个节点维护多个DiskRowSet的最大Key和最小Key，这样就可在O(logn)时间内定位Key所在DiskRowSet。 Kudu不同的列类型不同，使用的编码和压缩方式？&emsp;&emsp;Kudu每列都有类型，编码方式和压缩方式，编码方式根据数据类型不同有合适的默认值，压缩方式默认不压缩。 Kudu的部署&emsp;&emsp;Kudu有两种进程Master和TServer，Kudu服务是可以单独部署在集群的，但大多数情况可能是与Hadoop集群共置，不同的环境需要不同的部署方案，本节用数据化运营的思想来说Kudu服务的部署。 Master部署&emsp;&emsp;Master高可用，一般配置3或5个Master来保证HA，同一时刻只有一个Master工作，半数以上Master存活，服务都可正常运行。Master之间需要达成共识，大多数Master“投票”得到Leader，其他的为Follower。如果该Master出现问题，也是通过Raft一致性算法来做选举，既容错又高效。&emsp;&emsp;一般配置3或5个Master，7个就有点多了没必要。Master数目必须为奇数个。给定一组需要写N个副本（一般为3或5）的Tablet，可以接受(N-1)/2个写入错误。&emsp;&emsp;由于Mater中只保存元数据，数据量会一直比较小，即使被频繁请求，被全量加载到内存，也不需要占用大量系统资源。 TServer部署&emsp;&emsp;根据业务量，了解大概要存储多少数据。因为Kudu列式存储与Parquet相似，可以根据相同数据量Parquet占用磁盘大小来粗略估计需要多少存储空间。&emsp;&emsp;TServer数量和配置大致给多少呢？ 假设: Parquet格式存储的数据集大小60TB 每个TServer数据磁盘最大8T 给数据磁盘预留25%的磁盘空间 Tablet冗余副本3 TServer数量 = (Parquet格式存储的数据集大小 * 冗余数) / (TS磁盘容量 * ( 1 - 磁盘预留)) TServer数量 = (60 * 3) / (8 * (1 - 0.25)) = 30 存储介质&emsp;&emsp;在CM部署Master和TServer时，我们可以看到如下配置：&emsp;&emsp;Kudu设计时就对数据和WAL分开存储的，为什么呢？&emsp;&emsp;之前说过WAL仅支持追加写，单个操作乍一看会顺序写WAL，但同时执行多个任务时，更像是随机写WAL，这就很考验WAL底层存储的IOPS(IO Per Second)了。传统机械盘IOPS也就几百，而NVMe-SSD的IOPS能达到万级甚至百万级，所以Kudu WAL尽量存储在SSD中。&emsp;&emsp;那WAL的SSD盘大概要选多大呢？ Kudu的WAL日志是可以控制大小，日志段数量的。 默认日志段大小8M，数量1-80个，按默认的8M，80个算，如果共2000个Tablet，需要WAL的SSD大小： 8 * 80 * 2000 = 1280000MB 约1.3TB &emsp;&emsp;Tablet的数据可以用机械盘HDD来存储(SSD更好)，可以与DataNode处于同一块磁盘，这样更方便管理，因为这样可以充分利用磁盘负载和磁盘空间，不至于一个盘爆满另一个盘空余很多。 Kudu使用环境：四台机器CDH6.3.1集群，6核心12线程，内存分别为：20GB，14GB，14GB和10GB。OS:CentOS7;Impala:3.2.0-cdh6.3.1;Kudu:1.10.0-cdh6.3.1(3Master+3TServer);Hive:2.1.1-cdh6.3.1;依次启动HDFS、hive、Kudu、Impala。 Kudu + Impala&emsp;&emsp;Impala定位是一款实时查询引擎(低延时SQL交互查询)，快的原因：基于内存计算，无需MR，C++编写，兼容HiveQL和支持数据本地化。这与Kudu场景相吻合，Kudu官网也说Impala和Kudu可以无缝整合。&emsp;&emsp;进入Impala配置，Kudu服务处勾选Kudu即可。&emsp;&emsp;注意：=, &lt;=, ‘\\&lt;’, ‘>‘, &gt;=, BETWEEN, IN等操作会从Impala谓词下推到Kudu，性能高。而!=, like和其他Impala关键字会让Kudu返回所有结果再让Impala过滤，效率低下。因为Kudu没二级索引，所以没有主键的谓词也会造成全表扫描。 1.使用Impala创建Hash分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE DATABASE IF NOT EXISTS impala_kudu; -- 建内部表，Impala发生drop操作会删除Kudu上对应数据 CREATE TABLE impala_kudu.first_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 8 -- 使用Hash分区 STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- 查看Kudu中已能看到刚创建的表 kudu table list cdh102:7051,cdh103:7051,cdh104:7051 -- 查看表以及tablets kudu table list cdh102:7051,cdh103:7051,cdh104:7051 --list_tablets 在WebUI上可以看到该表对应8个Tablet以及每个Tablet信息。 2.使用Impala创建RANGE分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE TABLE impala_kudu.second_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY RANGE (id) ( -- 使用Range分区 只有主键可以做RANGE分区的字段 PARTITION 0 &lt;= values &lt;= 3, -- 如果id范围取多个值，则为values，如果分区字段按是单个值，则为value PARTITION 4 &lt;= values &lt;= 7, PARTITION 8 &lt;= values &lt;= 11, PARTITION 11 &lt; values ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- ------------------------------------------------------------------------------------ CREATE TABLE impala_kudu.third_kudu_table( state STRING, name String, PRIMARY KEY(state,name) ) PARTITION BY RANGE (state) ( -- 联合主键时，RANGE分区可以使用其中一个字段 PARTITION value = &#39;succeed&#39;, -- 如果分区字段按是单个值，则为value PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 3.使用Impala创建混合分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE TABLE impala_kudu.fourth_kudu_table( id INT, state String, name String, PRIMARY KEY(id,state) ) PARTITION BY HASH (id) PARTITIONS 4, -- 混合分区 HASH+RANGE RANGE (state) ( PARTITION value = &#39;succeed&#39;, PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; -- 最终Tablet数为HASH分区数乘以RANGE分区数 ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 4.在Impala映射已经存在的Kudu表 [kudu@cdh102 /]# kudu table list cdh102:7051,cdh103:7051,cdh104:7051 impala::impala_kudu.first_kudu_table impala::impala_kudu.test impala::impala_kudu.second_kudu_table impala::impala_kudu.fourth_kudu_table impala::impala_kudu.third_kudu_table CREATE EXTERNAL TABLE impala_kudu.fifth_kudu_table STORED AS KUDU -- Kudu表映射到Impala中，不能指定字段，主键和分区方式，由Kudu决定 TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39;, &#39;kudu.table_name&#39; = &#39;impala::impala_kudu.first_kudu_table&#39; ); drop table impala_kudu.fifth_kudu_table; -- 删除不会对Kudu中表有影响 5.转储一张Hive表到Kudu show create table default.test; CREATE TABLE impala_kudu.test( id INT, name STRING, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 2 -- 数据量少，分2个桶 STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); INSERT INTO impala_kudu.test SELECT * FROM default.test; SELECT * FROM impala_kudu.test; -- 这时会发现数据顺序发生变化了，因为Hash分区的原因 在官网了解更多：Using Apache Kudu with Apache Impala Kudu + Sparkhttps://blog.csdn.net/nazeniwaresakini/article/details/104220206/ Kudu + Hive与Hive MetaStore集成官网写的很详细。在官网了解更多：Using the Hive Metastore with Kudu Kudu APIsKudu常用Command LinesKudu客户端命令Kudu有很多命令，大致分几类： su - kudu kudu cluster 集群管理，包括健康状态检查，移动tablet，rebalance等操作 kudu diagnose 集群诊断工具 kudu fs 在本地Kudu文件系统做操作，检查一致性，列出元顺据，数据集更新，数据转储 kudu hms 操作HiveMetaStore，包括检查与Kudu元数据一致性，自动修复元数据，列出元数据 kudu local_replica 操作本地副本，包括从远程copy副本过来，获取空间占用情况，删除Tablet，获取副本列表，转储本地副本等 kudu master 操作KuduMaster，可以运行master，获取master状态，时间戳，flag等信息， kudu pbc protobuf容器文件操作 kudu perf 集群性能测试，运行负载，显示本地Tablet行数等 kudu remote_replica 操作远程TServer上的副本，远程复制，删除，转储，列出Tablet kudu table 操作Kudu表，包括添加范围分区，设置blockSize，设置列的压缩类型，编码类型，默认值，注释，复制表数据到另一表，建表，删除列，删表，描述表，删除范围的分区，获取和更改表其他配置，列出表，找到Row所在Tablet，列重命名，表重命名，scan，获取表的统计信息 kudu tablet 操作Kudu的Tablet 包括更换Tablet的Leader，Raft配置 kudu test 测试 kudu tserver 操作TabletServer包括运行，设置Flag，获取状态，时间戳，列出TServers等 kudu wal 操作Kudu WAL，转储WAL日志文件 Kudu常用Java API： Maven Dep： &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;/dependency&gt; //KuduDDL.java Kudu数据定义API包括：建表，删表，增加字段和删除字段 package top.qjj.shmily.operations; import org.apache.kudu.ColumnSchema; import org.apache.kudu.Schema; import org.apache.kudu.Type; import org.apache.kudu.client.*; import org.apache.kudu.shaded.com.google.common.collect.ImmutableList; import java.util.LinkedList; public class KuduDDL{ public static void main(String[] args) { String kuduMasterAddrs = &quot;cdh102,cdh103,cdh104&quot;; KuduDDLOperations kuduDDLOperations = KuduDDLOperations.getInstance(kuduMasterAddrs); //创建Kudu表 String tableName = &quot;kudu_table_with_hash&quot;; //1.Schema指定 LinkedList&lt;ColumnSchema&gt; schemaList = new LinkedList&lt;&gt;(); schemaList.add(kuduDDLOperations.newColumn(&quot;id&quot;, Type.INT32, true)); schemaList.add(kuduDDLOperations.newColumn(&quot;name&quot;, Type.STRING, false)); Schema schema = new Schema(schemaList); //2.设置建表参数-哈希分区 // CreateTableOptions options = new CreateTableOptions(); // options.setNumReplicas(1); //设置存储副本数-必须为奇数否则会抛异常 // List&lt;String&gt; hashKey = new LinkedList&lt;String&gt;(); // hashKey.add(&quot;id&quot;); // options.addHashPartitions(hashKey,2); //哈希分区 设置哈希键和桶数 //2.设置建表参数-Range分区 CreateTableOptions options = new CreateTableOptions(); options.setRangePartitionColumns(ImmutableList.of(&quot;id&quot;)); //设置id为Range key int temp = 0; for(int i = 0; i &lt; 10; i++){ //id 每10一个区间直到100 PartialRow lowLevel = schema.newPartialRow(); //定义用来分区的列 lowLevel.addInt(&quot;id&quot;, temp); //与字段类型对应 INT32则addInt INT64则addLong PartialRow highLevel = schema.newPartialRow(); temp += 10; highLevel.addInt(&quot;id&quot;, temp); options.addRangePartition(lowLevel, highLevel); } //3.开始建表 boolean result = kuduDDLOperations.createTable(tableName, schema, options); System.out.println(result); //添加字段 kuduDDLOperations.addColumn(tableName, &quot;test&quot;, Type.INT8); //删除字段 kuduDDLOperations.deleteColumn(tableName, &quot;test&quot;); //删除Kudu表 boolean delResult = kuduDDLOperations.dropTable(tableName); System.out.println(delResult); //关闭连接 kuduDDLOperations.closeConnection(); } } class KuduDDLOperations { private static volatile KuduDDLOperations instance; private KuduClient kuduClient = null; private KuduDDLOperations(String masterAddr){ kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); } public static KuduDDLOperations getInstance(String masterAddr){ if(instance == null){ synchronized (KuduDDLOperations.class){ if(instance == null){ instance = new KuduDDLOperations(masterAddr); } } } return instance; } public void closeConnection(){ try { kuduClient.close(); } catch (Exception e) { e.printStackTrace(); } } public ColumnSchema newColumn(String name, Type type, boolean isKey){ ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(isKey); return column.build(); } /** * 创建表 * 注意：Impala DDL对表字段名大小写不敏感，但Kudu层已经转为小写，且Kudu API中字段名必须小写； * 注意：Impala DDL建表表名大小写敏感且到Kudu层表名不会被转成小写，且Kudu API对表名大小写敏感。 * @param tableName 表名 * @param schema Schema信息 * @param tableOptions 建表参数 TableOptions对象 * @return boolean */ public boolean createTable(String tableName, Schema schema, CreateTableOptions tableOptions){ try { kuduClient.createTable(tableName, schema, tableOptions); System.out.println(&quot;Create table successfully!&quot;); return true; } catch (KuduException e) { e.printStackTrace(); } return false; } /** * 删库跑路 * @param tableName 要删的表名 * @return boolean 是否需要跑路 */ public boolean dropTable(String tableName){ try { kuduClient.deleteTable(tableName); System.out.println(&quot;Drop table successfully!&quot;); return true; } catch (KuduException e) { e.printStackTrace(); } return false; } /** * 给Kudu表添加字段 * @param tableName 表名 * @param column 字段名 * @param type 类型 * @return */ public boolean addColumn(String tableName, String column, Type type) { AlterTableOptions alterTableOptions = new AlterTableOptions(); alterTableOptions.addColumn(new ColumnSchema.ColumnSchemaBuilder(column, type).nullable(true).build()); try { kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;成功添加字段&quot; + column + &quot;到表&quot; + tableName); return true; } catch (KuduException e) { e.printStackTrace(); } return false; } /** * 删除Kudu表指定字段 * @param tableName 表名 * @param column 字段名 * @return */ public boolean deleteColumn(String tableName, String column){ AlterTableOptions alterTableOptions = new AlterTableOptions().dropColumn(column); try { kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;成功删除表&quot; + tableName + &quot;的字段&quot; + column); return true; } catch (KuduException e) { e.printStackTrace(); } return false; } } // ------------------------------------------------------------------------------------------------------------- //KuduDML.java Kudu数据操作API包括：CRUD package top.qjj.shmily.operations; import org.apache.kudu.client.SessionConfiguration.FlushMode; import org.apache.kudu.client.*; public class KuduDML { public static void main(String[] args) { String masterAddr = &quot;cdh102,cdh103,cdh104&quot;; KuduDMLOperations kuduDMLOperations = KuduDMLOperations.getInstance(masterAddr); //插入数据 kuduDMLOperations.insertRows(); //更新一条数据 kuduDMLOperations.updateRow(); //删除一条数据 kuduDMLOperations.deleteRow(); //查询数据 kuduDMLOperations.selectRows(); //关闭Client连接 kuduDMLOperations.closeConnection(); } } class KuduDMLOperations { private static volatile KuduDMLOperations instance; private KuduClient kuduClient = null; private KuduDMLOperations(String masterAddr){ kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); } public static KuduDMLOperations getInstance(String masterAddr){ if(instance == null){ synchronized (KuduDMLOperations.class){ if(instance == null){ instance = new KuduDMLOperations(masterAddr); } } } return instance; } public void closeConnection() { try { kuduClient.close(); } catch (KuduException e) { e.printStackTrace(); } } /** * 以kudu_table_with_hash表(id INT,name STRING)为例 插入数据 * 注意：写数据时数据不支持为null，需要对进来的数据判空 */ public void insertRows(){ try { KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); //打开表 KuduSession kuduSession = kuduClient.newSession(); //创建会话Session kuduSession.setFlushMode(FlushMode.MANUAL_FLUSH); //设置数据提交方式 /** * 1.AUTO_FLUSH_SYNC（默认） 目前比较慢 * 2.AUTO_FLUSH_BACKGROUND 目前有BUG * 3.MANUAL_FLUSH 目前效率最高 远远高于其他 * 关于这三个参数测试调优可看这篇：https://www.cnblogs.com/harrychinese/p/kudu_java_api.html */ int numOps = 3000; kuduSession.setMutationBufferSpace(numOps); //设置MANUAL_FLUSH需要设置缓冲区操作次数限制 如果超限会抛异常 int nowOps = 0; //记录当前操作数 for(int i = 0; i &lt;= 100; i++){ Insert insert = table.newInsert(); //字段数据 insert.getRow().addInt(&quot;id&quot;, i); insert.getRow().addString(&quot;name&quot;, &quot;小&quot;+i); nowOps += 1; if(nowOps == numOps / 2){ //所以缓冲区操作次数达到一半时进行flush提交数据，避免抛异常 kuduSession.flush(); //提交数据 nowOps = 0; //计数器归零 } kuduSession.apply(insert); } kuduSession.flush(); //保证最后都提交上去了 kuduSession.close(); System.out.println(&quot;数据成功写入Kudu表&quot;); } catch (KuduException e) { e.printStackTrace(); System.out.println(&quot;数据写入失败，原因：&quot; + e.getMessage()); } } /** * 以kudu_table_with_hash表(id INT,name STRING)为例 查询数据 */ public void selectRows(){ try { KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); // 打开表 KuduScanner scanner = kuduClient.newScannerBuilder(table).build(); //创建Scanner while (scanner.hasMoreRows()){ for (RowResult r: scanner.nextRows()) { System.out.println(r.getInt(&quot;id&quot;) + &quot; - &quot; + r.getString(1)); } } scanner.close(); } catch (KuduException e) { e.printStackTrace(); System.out.println(&quot;查询失败，原因：&quot; + e.getMessage()); } } /** * 以kudu_table_with_hash表(id INT,name STRING)为例 更新一条数据 */ public void updateRow(){ try { KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); session.setFlushMode(FlushMode.AUTO_FLUSH_SYNC); Update update = table.newUpdate(); PartialRow row = update.getRow(); //定义用来分区的列 row.addInt(&quot;id&quot;, 66); row.addString(&quot;name&quot;, &quot;qjj&quot;); session.apply(update); session.close(); } catch (KuduException e) { e.printStackTrace(); System.out.println(&quot;数据更新失败，原因：&quot; + e.getMessage()); } } /** * 以kudu_table_with_hash表(id INT,name STRING)为例 删除一条数据 */ public void deleteRow(){ try { KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); Delete delete = table.newDelete(); delete.getRow().addInt(&quot;id&quot;,18); //根据主键唯一删除一条记录 session.flush(); session.apply(delete); session.close(); } catch (KuduException e) { e.printStackTrace(); System.out.println(&quot;数据删除失败，原因：&quot; + e.getMessage()); } } } Kudu常用Python API： import kudu from kudu.client import Partitioning from datetime import datetime # Connect to Kudu master server client = kudu.connect(host=&#39;cdh102,cdh103,cdh104&#39;, port=7051) # Define a schema for a new table builder = kudu.schema_builder() builder.add_column(&#39;key&#39;).type(kudu.int64).nullable(False).primary_key() builder.add_column(&#39;ts_val&#39;, type_=kudu.unixtime_micros, nullable=False, compression=&#39;lz4&#39;) schema = builder.build() # Define partitioning schema partitioning = Partitioning().add_hash_partitions(column_names=[&#39;key&#39;], num_buckets=3) # Create new table client.create_table(&#39;python-example&#39;, schema, partitioning) # Open a table table = client.table(&#39;python_example&#39;) # Create a new session so that we can apply write operations session = client.new_session() # Insert a row op = table.new_insert({&#39;key&#39;: 1, &#39;ts_val&#39;: datetime.utcnow()}) session.apply(op) # Upsert a row op = table.new_upsert({&#39;key&#39;: 2, &#39;ts_val&#39;: &quot;2020-01-01T00:00:00.000000&quot;}) session.apply(op) # Updating a row op = table.new_update({&#39;key&#39;: 1, &#39;ts_val&#39;: (&quot;2020-07-12&quot;, &quot;%Y-%m-%d&quot;)}) session.apply(op) # Delete a row op = table.new_delete({&#39;key&#39;: 2}) session.apply(op) # Flush write operations, if failures occur, capture print them. try: session.flush() except kudu.KuduBadStatus as e: print(session.get_pending_errors()) # Create a scanner and add a predicate scanner = table.scanner() scanner.add_predicate(table[&#39;ts_val&#39;] == datetime(2020, 7, 12)) # Open Scanner and read all tuples # Note: This doesn&#39;t scale for large scans result = scanner.open().read_all_tuples() Kudu优化1.使用SSD会显著提高Kudu性能。（因为如果取多个字段，列式存储在传统磁盘上会多次寻址，而使用SSD不会有寻址问题）2.kudu性能调优3.memory_limit_hard_bytes 该参数是单个TServer能够使用的最大内存量。如果写入量很大而内存太小，会造成写入性能下降。如果集群资源充裕，可以将它设得比较大，比如设置为单台服务器内存总量的一半。官方也提供了一个近似估计的方法，即：每1TB实际存储的数据约占用1.5GB内存，每个副本的MemRowSet和DeltaMemStore约占用128MB内存，（对多读少写的表而言）每列每CPU核心约占用256KB内存，另外再加上块缓存，最后在这些基础上留出约25%的余量。4.block_cache_capacity_mb Kudu中也设计了BlockCache，不管名称还是作用都与HBase中的对应角色相同。默认值512MB，经验值是设置1~4GB之间，我们设了4GB。5.memory.soft_limit_in_bytes/memory.limit_in_bytes这是Kudu进程组（即Linux cgroup）的内存软限制和硬限制。当系统内存不足时，会优先回收超过软限制的进程占用的内存，使之尽量低于阈值。当进程占用的内存超过了硬限制，会直接触发OOM导致Kudu进程被杀掉。我们设为-1，即不限制。6.maintenance_manager_num_threads单个TServer用于在后台执行Flush、Compaction等后台操作的线程数，默认是1。如果是采用普通硬盘作为存储的话，该值应与所采用的硬盘数相同。7.max_create_tablets_per_ts创建表时能够指定的最大分区数目（hash partition * range partition），默认为60。如果不能满足需求，可以调大。8.follower_unavailable_considered_failed_sec当Follower与Leader失去联系后，Leader将Follower判定为失败的窗口时间，默认值300s。9.max_clock_sync_error_usec NTP时间同步的最大允许误差，单位为微秒，默认值10s。如果Kudu频繁报时间不同步的错误，可以适当调大，比如15s。 Kudu异常处理Apache Kudu Troubleshooting HTAP混合事务分析处理HTAP，即Hybrid Transactional Analytical Processing，我们知道OLAP、OLTP，而HTAP就是结合两者场景，既需要联机事务处理有需要联机分析处理，这也是Kudu的场景。HTAP的场景举例： 管理层希望看到实时的数据汇总报表 客服人员希望能够尽快访问某设备的最新数据以便尽快排除故障 乘车线路拥堵立刻感知并立刻规划线路 车联网，物联网 总结&emsp;&emsp;Kudu–Fast Analytics on Fast Data.一个Kudu实现了整个大数据技术栈中诸多组件的功能，有分布式文件系统（好比HDFS），有一致性算法（好比Zookeeper），有Table（好比Hive表），有Tablet（好比Hive分区），有列式存储（如Parquet），有顺序和随机读取（如HBase），所以看起来kudu像一个轻量级的，结合了HDFS+Zookeeper+Hive+Parquet+HBase等组件功能并在性能上进行平衡的组件。它轻松地解决了随机读写+快速分析的业务场景，解决了实时数仓的诸多难点，同时降低了存储成本和运维成本。&emsp;&emsp;学Kudu时让我想到曾经看过的终结者系列电影，万物互联，主角不小心被街边一个不起眼的监控探头拍到，就会立刻引来终结者的追杀，任何有网络的地方留下任何痕迹都会立刻被终结者感知…很明显，这就是在快速变化的数据上进行快速分析，如果没有Kudu，大量的物联网数据就只能批处理了，就没了时效性，主角就可以随便浪了。没准”天网”系统里就部署了Kudu节点呢？！哈哈哈！&emsp;&emsp;在实时数仓、实时计算和物联网蓬勃发展的今天，你确定不学一下Kudu吗？ 参考资料1.《Kudu:构建高性能实时数据分析存储系统》2.Apache Kudu - Fast Analytics on Fast Data3.Kudu专注于大规模数据快速读写，同时进行快速分析的利器4.Kudu基础入门5.Kudu、Hudi和Delta Lake的比较6.迟到的Kudu设计要点面面观7.迟到的Kudu设计要点面面观-前篇8.kudu-列式存储管理器-第四篇（原理篇）","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"Kudu","slug":"Kudu","permalink":"https://shmily-qjj.top/tags/Kudu/"},{"name":"实时","slug":"实时","permalink":"https://shmily-qjj.top/tags/实时/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"基础数据结构与算法","slug":"基础算法学习","date":"2020-04-27T07:45:00.000Z","updated":"2020-06-01T10:32:27.836Z","comments":true,"path":"6a894937/","link":"","permalink":"https://shmily-qjj.top/6a894937/","excerpt":"","text":"前言算法与数据结构从现在开始重视还来得及！为何要重视算法与数据结构？ 算法能力代表了基本功水平，能代表一个程序员功底是否扎实。 算法和数据结构的思想以及时间复杂度空间复杂度的概念是提高工作效率，学习能力和成长潜力的重要途径。 算法能力是设计一个系统(造轮子)的重要基础。 下层基础决定上层建筑！算法与数据结构是我的薄弱项，需要多做总结！尽量对每个数据结构的经典题目做总结，尽量使用LeetCode原题来帮助理解。LeetCode主页先贴这里:Shmilyqjj的力扣主页，大家互相监督学习呦！ 基本概念学习算法有帮助的一些基本概念。 顺序存储与链式存储物理结构：是指数据的逻辑结构在计算机中的存储形式。逻辑结构：是指数据对象中数据元素之间的互相关系。 基础数据结构数据结构是算法的基石。要做到对算法的融会贯通和举一反三，熟悉各种数据结构是必备的。优秀的算法取决于采用哪种数据结构。重点：熟悉每种数据结构优缺点，应用场景，熟练掌握其思想并灵活运用。 数组和字符串特点：逻辑结构与物理结构一致优点：构建简单，索引某个元素的复杂度O(1)缺点：必须连续分配一段内存，查询过程遍历时间复杂度O(n)，删除过程时间复杂度O(n)场景：元素个数确定，经常按索引查询，不经常插入和删除 经典题目：翻转字符串字母异位词 链表分为单向链表和双向链表优点：能灵活分配内存空间，插入和删除元素效率高O(1)缺点：不能通过下标读取，读取第n个位置元素复杂度O(n)场景：元素个数不确定，经常插入和删除，不经常查询 经典题目：快慢指针-&gt;判断是否有环快慢指针-&gt;翻转链表快慢指针-&gt;寻找倒数第K元素快慢指针-&gt;找链表中间位置构建虚假链表头 栈后进先出(LIFO),可以采用单链表实现复杂度O(1)，虽然也可以用数组实现但时间复杂度可能较大场景：解决问题时只关心最近一次操作，解决完后要找之前的操作。 经典题目：匹配括号，判断是否有效 队列先进先出(FIFO),可以采用双链表实现。场景：按一定顺序处理过来的数据。 经典题目：广度优先搜索 双端队列和普通队列的区别是：可以在队头队尾都可以查看，添加和删除数据，复杂度都为O(1)，可以采用双链表实现。场景：实现长度动态变化的窗口或连续区间，动态窗口。 经典题目：返回滑动窗口最大值 树充分应用递归，树的问题基本都与递归有关面试常考：普通二叉树，平衡二叉树，完全二叉树，二叉搜索树，四叉树，多叉树需要了解：红黑树 ，AVL自平衡二叉搜索树 遍历方法：前序遍历：根-&gt;左子树-&gt;右子树中序遍历：左子树-&gt;根-&gt;右子树后序遍历：左子树-&gt;右子树-&gt;根深度优先遍历DFS：包含以上三种广度优先遍历BFS：即层次遍历，每一层从左向右输出 经典题目：二叉搜索树的第K个最小元素 如何学习数据结构与算法好的学习方法往往能起到事半功倍的效果，这里根据算法大佬们的学习经历总结一下算法学习方法，警示自己督促自己养成正确学习方法吧。 参考链接我是如何学习数据结构与算法的 斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://shmily-qjj.top/tags/算法/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shmily-qjj.top/tags/数据结构/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"MySQL索引原理深入","slug":"MySQL索引原理深入","date":"2020-03-24T02:16:00.000Z","updated":"2020-05-04T12:53:02.526Z","comments":true,"path":"7c15e85/","link":"","permalink":"https://shmily-qjj.top/7c15e85/","excerpt":"","text":"MySQL索引原理深入索引可以大大提高Mysql检索速度，为什么能提高，怎么做到的？这些细节必须深入学习和分析，才能对技术运用了如指掌。今天来学习一下Mysql的索引原理与底层存储选型，为了能够对Mysql有更深入的了解。 索引定义索引是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息。索引相当于我们看书的目录。 优点1.快速检索数据2.保证数据记录唯一性3.实现表与表之间的参照完整性4.使用ORDER BY/GROUP BY子句进行数据检索时，利用索引可以减少排序和分组的时间 缺点1.索引需要占磁盘物理空间2.增删改操作时索引也要动态地维护，有性能开销3.创建索引耗时，数据量越大耗时也越大 分类1.普通索引：无唯一性限制2.唯一索引：UNIQUE，有唯一性限制3.主键索引：唯一索引的特殊类型，在主键上创建索引4.候选索引：唯一性，切决定记录的处理顺序5.聚集索引：Clustered Index聚簇索引，索引列的键值的物理顺序与逻辑顺序相同6.非聚集索引：Non-Clustered Index非聚簇索引，索引列的键值的物理顺序与逻辑顺序无关7.全文索引：主要针对文本的内容进行分词，加快查询速度8.联合索引：多列组成的索引，查询效率提升高于多个单列索引合并的效率 应用场景1.在经常搜索的列上创建索引2.在主键上创建索引3.在用来JOIN的列上创建索引4.在经常通过WHERE根据范围检索的列上创建索引5.在经常GROUP BY/ORDER BY的列上创建索引6.在经常DISTINCT的列上创建索引 索引字段要求1.列值的唯一性太小不适合建索引2.列值太长不适合建索引3.更新频繁的列不适合建索引 索引失效1.LIKE的使用（LIKE XXX%可以用索引，但LIKE %xxx不能）2.部分操作符（&lt;,&lt;=,=,&gt;,&gt;=,BETWEEN,IN可以用索引，但&lt;&gt;,not in,!=不能）3.判空操作（is null或is not null）4.int类型字段（如手机号没用varchar存，查186开头的，不能）5.联合索引（设置了col1和col2两个字段联合索引，WHERE col1=’xxx’或WHERE col1=’xxx’ AND col2=’xxx’或WHERE col2=’xxx’ AND col1=’xxx’都可用索引，但WHERE col2=’xxx’不能）6.对索引列操作（计算、函数、自动类型转换、手动类型转换都会使索引失效）7.SELECT *（尽量使用覆盖索引，尽量取用到的字段值而非使用星号,这样WHERE的时候覆盖索引效率高）8.字符串不加单引号引起索引失效9.尽量避免索引列有null值 Mysql索引数据结构选型该部分会详细说明：索引数据结构的选型过程以及各自的优缺点B树与B+树的区别为什么以B+树作为Mysql的索引数据结构Innodb引擎和MyISAM引擎的区别以及索引实现和存储区别聚簇索引与非聚簇索引区别 数据结构选型过程过程：哈希表-&gt;二叉查找树-&gt;红黑树-&gt;二叉平衡树AVL-&gt;B树-&gt;B+树 哈希表哈希算法把任意的key变换成固定长度的key地址。性能不错，但是有哈希冲突问题，一般用链地址法来解决（类似HashMap）。这样查数据的时候先计算Hash，然后遍历链表，直到拿到key。时间复杂度O(1),看来很理想。但是为什么没用哈希？如果用哈希表做索引的数据结构，select * from tb where id &gt; 1这样的范围查找场景，就要把索引数据全部加载到内存再筛选，太慢。虽然Hash做索引的数据结构可以快速定位key，但没法做到高效的范围查找。ps:当然，如果业务是经常使用where条件单条查询数据，Hash索引效率更高复杂度O(1)。使用Hash索引，InnoDB和MyISAM不支持，可用MEMORY，NDB引擎。 二叉查找树BinarySearchTree（BST）是支持数据快速查找的数据结构，复杂度O(log2n)~O(n)之间,也可以高速检索数据能不能解决范围查找呢？能！比如我找id &gt; 3，我只要找到比它大的根节点和右子树即可。那为啥二叉查找树不能做索引的数据结构？因为如果二叉排序树是平衡的，则n个节点的二叉排序树的高度为log2(n+1),其查找效率为O(log2n)，近似于折半查找。如果二叉排序树完全不平衡，则其深度可达到n，查找效率为O(n)，退化为顺序查找。而数据库中经常有以自增id为主键索引的场景，必然会线性查找，性能太低。 红黑树通过自动调整树形态让二叉树保持基本平衡，复杂度O(logn)，因为基本平衡，查询效率不会明显降低，不存在O(n)的情况。吃瓜群众：那就用这个做索引数据结构吧！万万不可用红黑树做索引的数据结构！还是自增id为主键索引的情况，如果红黑树按顺序插入数据，整个红黑树会明显右倾，查询效率会明显降低。像我这种数据结构渣渣，送自己一个宝贝：红黑树算法图形模拟 AVL树AVL树，也是通过调整形态保持二叉树平衡，它虽然在调整形态时会有更多性能开销，但它绝对平衡。它能根本解决红黑树的右倾问题。复杂度O(logn)。那AVL树这么好，为啥还是不能用于索引数据结构？AVL树每个节点只能存一个数据，每次比较只能加载一个数据到内存，查询较深的AVL树节点，就要有多次的磁盘IO开销，磁盘IO是数据库瓶颈，这样肯定不合理的呀！对磁盘IO的优化方案就是一次尽可能多读或者多写数据，读1b和读1kb速度基本一样的，希望磁盘能一次加载更多数据进内存，这就是B树，B+树原理了。 B树与B+树B树：又名多路查找树，特点：①节点数据递增，遵循左小右大②M阶B树，每个节点最多可以有M个子节点③根节点至少有两个儿子④除根结点之外的所有非叶子结点至少有ceil(M/2)个子节点(ceil(2.1) = 3)⑤所有叶子节点都在同一层次B树代替AVL树：让每个节点存的key数目适当增加，即增加M（B树的阶数），磁盘读取次数大大降低，尽可能减少磁盘IO，加快检索速度，还能支持范围查找。B+树：与B树区别：一是B树每个节点（包括非叶子节点）都存数据，B+树非叶子节点有索引作用，而数据存在叶子节点 –&gt; B树的每个节点存不了太多数据，B+树每个叶子节点能存很多索引（地址）。所以B+树高度更低，减少了磁盘IO。二是B树节点之间没索引，B+相邻叶子节点之间有索引指针 –&gt; WHERE范围查询性能很好。三是B+树的查询效率更稳定，因为数据都存在叶子节点，查数据的操作次数相同。 综上，Mysql基于B+树实现的索引。 InnoDB与MyISAM的区别 InnoDB MyISAM 默认支持ACID 不支持ACID 支持外键 不支持外键 性能好 性能好于Innodb 必须有主键 可没有主键 数据与索引存放在一起 数据与索引分开存放 聚集索引方式 非聚集索引方式 支持表级、行级(默认)锁 支持表级锁 崩溃易恢复 崩溃难恢复 聚集索引与非聚集索引(InnoDB与MyISAM实现索引的区别)mysql&gt; show global variables like &quot;%datadir%&quot;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------+ mysql&gt; create table innodb_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.02 sec) mysql&gt; create table myisam_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=myisam DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.00 sec) ll /var/lib/mysql/db_name/ 总用量 128 -rw-rw----. 1 mysql mysql 65 3月 24 04:30 db.opt -rw-rw----. 1 mysql mysql 8586 3月 24 04:31 innodb_table.frm -rw-rw----. 1 mysql mysql 98304 3月 24 04:31 innodb_table.ibd -rw-rw----. 1 mysql mysql 8586 3月 24 04:33 myisam_table.frm -rw-rw----. 1 mysql mysql 0 3月 24 04:33 myisam_table.MYD -rw-rw----. 1 mysql mysql 4096 3月 24 04:33 myisam_table.MYI 从建表后生成的文件可看出，InnoDB生成frm(建表语句)和ibd(数据+索引)，而MyISAM生成frm(建表语句),MYD(数据文件)和MYI(索引文件)。 MyISAM引擎把数据和索引分开成数据文件和索引文件两个文件，这叫做非聚集索引方式。Innodb 引擎把数据和索引放在同一个文件里了，这叫做聚集索引方式。 更详细一点的解释：聚集索引物理顺序与逻辑顺序一致，非聚集索引的物理顺序与逻辑顺序不一致。非聚集索引的叶子节点存key和对应地址，不存数据；聚集索引的叶子节点存key和对应的value，value内存地址是连续的 为什么MyISAM比InnoDB快？上面我们已经提到InnoDB使用聚集索引，MyISAM使用非聚集索引。两种引擎的数据组织方式不同。如图是两种引擎组织数据的方式查询时InnoDB需要通过主键索引树先拿到主键值后再去辅助索引树拿到整条记录（建表的时候InnoDB就会自动建立好主键索引树），而MyISAM拿到数据的索引即可直接以Offset形式直接在数据文件中定位数据。而且InnoDB因为支持ACID，还要检查MVCC多版本并发控制，而MyISAM不支持事务，也是其快的原因。还有MyISAM维护了一个保存整表行数的变量，count(*)很快。 其他数据库索引数据结构该部分会详细说明为什么Mysql用B+树索引而MongoDB用B树。1.MongoDB本身很少有范围搜索操作，做单一查询比较多2.Mysql关系型数据库，做范围检索的操作很多，如join，where x&gt;1等，B+树叶子节点有指针，遍历效率高 参考资料下面列出参考资料，我认为写得好的已加粗深入理解 Mysql 索引底层原理为什么Mongodb索引用B树，而Mysql用B+树?Mysql—索引失效磁盘IO概念及优化入门知识MVCC多版本并发控制Mysql索引Mysql索引必须了解的几个重要问题 其他细节1.ORDER BY与索引失效：当order by的字段出现在where条件中时，才会利用索引而不排序，更准确的说，order by中的字段在执行计划中利用了索引时，不用排序操作。这个结论不仅对order by有效，对其他需要排序的操作也有效。比如group by 、union 、distinct等。（出现在Order by 后的索引列都是用于排序的，不会用于查找，所以索引无效）2.innodb引擎的4大特性：插入缓冲（insert buffer),二次写(double write),自适应哈希索引(ahi),预读(read ahead)3.主键和唯一索引的区别：①唯一索引列允许空值，主键不允许空值 ②主键可以被其他表引用为外键，唯一索引不能 ③一个表只能一个主键但可有多个唯一索引","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"系统学习JVM","slug":"系统学习JVM","date":"2020-03-21T04:19:00.000Z","updated":"2020-06-01T10:32:27.837Z","comments":true,"path":"508b5c7/","link":"","permalink":"https://shmily-qjj.top/508b5c7/","excerpt":"","text":"系统学习JVMJava跨平台，一次编译到处运行，垃圾回收等特性离不开JVM，学习JVM的原理可以让我们在工作中更快速定位问题。写这篇的目的就是避免零零散散地学习JVM，那样效率很低，也方便以后回顾和复习。 字节码学习之前先要学会简单分析字节码。用户Java代码与JVM交互沟通的桥梁。代码编译为.class字节码给JVM运行。 $ javac Hello.java $ javap -c Hello.class # javap可查看字节码的操作数 $ javap -p -v Hello # -p打印私有字段和方法 -v尽量多打印一些信息 当在java代码中添加一些注释信息后，.class的MD5不一样了。因为javac可以指定输出一些额外内容到.class javac -g:lines 强制生成LineNumberTable | javac -g:vars 强制生成LocalVariableTable | javac -g 生成所有debug信息 当然如果使用IDEA，可以使用jclasslib Bytecode viewerb插件（插件商店搜索即可） JVM的程序运行是在栈上完成的，运行main方法自动分配一个栈帧，退出方法体时候再弹出相应栈帧。从javap得到的结果看，大多数字节码指令是不断操作栈帧。整个过程：Java 文件-&gt;编译器-&gt;字节码-&gt;JVM-&gt;机器码整个过程：Hello.java -&gt; Hello.class -&gt; Java类加载器(JVM中) -&gt; 执行引擎(JVM中) -&gt; 通过操作系统接口解释执行+JIT 如下有两段代码：我们可以通过字节码文件判断它们的执行结果 public class A{ # 第一段 static int a = 0; static { a = 1; b = 1; } static int b = 0; public static void main(String[] args) { System.out.println(a); System.out.println(b); } } //执行结果：1 0 //字节码如下： 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: iconst_0 13: putstatic #5 // Field b:I 16: return -------------------------------------------------------------------------------------------------- public class A{ # 第二段 static int a = 0; static { a = 1; b = 1; } static int b; public static void main(String[] args) { System.out.println(a); System.out.println(b); } } //执行结果：1 1 //字节码如下： 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: return 其他信息：stack=1, locals=0, args_size=0中stack表示该方法最大操作数栈深度为4，JVM根据这个分配栈帧中操作栈深度，locals变量存储了局部变量的存储空间，单位是Slot(槽)，args_size指方法参数个数其他字节码指令表可参照：https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html JVM定义JVM（JAVA虚拟机）是一个规范，定义了.class文件的结构，加载机制，数据存储，运行时栈等内容。JDK8以后Java是编译与解释混合执行模式。JDK8以后JVM的技术实现是HotSpot(包含一个解释器和两个编译器)。两个编译器：可以动态编译，含server模式和client模式。 client模式是一种轻量级编译器，也叫C1编译器，占用内存小，启动快，但是执行效率没有server模式高，默认状态下不进行动态编译，适用于桌面应用程序。 server模式是一种重量级编译器，也叫C2编译器，启动慢，占用内存大，执行效率高，默认是开启动态编译的，适合服务器应用。 -XX:RewriteFrequentPairs 用于开启动态编译。 -Xint:禁用JIT编译，UYZNGSUYZNGS即禁用两个编译器，纯解释执行。 -Xcomp:纯编译执行，如果方法无法编译，则回退到解释执行模式解释无法编译的代码。 内存管理 JVM内存区域如何划分？Java内存布局一直在调整，Java8开始彻底移除了持久代，使用MetaSpace(元空间)来代替。 =&gt; -XX:PermSize和-XX:MaxPermSize失效 Java的运行时数据区可以分成堆、元空间(含方法区)、虚拟机栈、本地方法栈和程序计数器 堆：存放绝大多数Java对象，是JVM中最大的一块内存，随着频繁创建对象，堆空间占用越来越大，需要不定期的GC。（JVM主要GC区域：堆和元空间）。是线程共享的。 对象是否被分配在堆中取决于对象的基本类型和Java类中存在的位置： 基本数据类型（byte,short,int,long,float,double,char）如果在方法体内声明则在栈上(栈帧的局部变量表)直接分配，其他情况在堆上分配。 int[]这样的数组类型不属于基本数据类型，在堆上分配。 栈：分虚拟机栈和本地方法栈。 虚拟机栈：Java中每个方法被调用时都会创建一个栈帧，执行完后再出栈，所有栈帧都出栈后线程结束。每一个方法对应一个栈帧，每一个线程对应一个栈。栈帧中包括：局部变量表，操作数，动态链接，返回地址，这些不是线程共享的。 本地方法栈：与虚拟机栈相似，但它主要包含Native对象。本地方法栈有一个叫returnAddress的数据类型。 元空间：存放类名与字段(类的元数据)，运行时常量池，JIT优化。 先对比一下JDK8和以前版本的方法区 Perm区(永久代)在JDK8废除，用元空间来取代。好处：元空间的出现解决了类和类加载器元数据过多导致的OOM问题，它是非堆区，使用操作系统内存，不会出现方法区内存溢出，省去了GC扫描压缩的开销，每个加载器有专门的存储空间；坏处：无限制使用操作系统内存会导致操作系统崩溃，所以一般要加-XX:MaxMetaspaceSize参数来控制大小。元空间不支持压缩，有内存碎片问题。 方法区：包含在元空间中。方法区存储：类信息、静态（static）变量，常量（final），编译后的代码等数据。是线程共享的。 元空间内存管理由元空间虚拟机完成。 程序计数器：[JVM中唯一不会OOM的区域]在多线程切换的情况下，Java通过程序计数器来记录字节码执行到什么地方，这样能保证切换回来时能够从原来的地方继续执行。（相当于字节码的行号指示器）。程序计数器实现了异常处理，跳转，循环分支的功能。因为每个线程都有其独立的程序计数器，所以是线程私有的。 JVM类加载机制类加载过程：加载-&gt;验证-&gt;准备-&gt;解析-&gt;初始化 大多数情况按这个流程加载。加载：将类的同名.class文件加载到方法区验证：检查.class是否合规。如果.class不合规，抛异常。如果任何.class都能加载就不安全了。准备：为一些类变量分配内存，并初始化为默认值。此时，实例对象还没有分配内存，所以这些动作是在方法区上进行的。 类加载的准备阶段会给类变量分配内存和初始化默认值。所以下面这段，我们不手动给a赋值也能编译通过。 public class test_java { static int a; public static void main(String[] args) { System.out.println(a); // output:0 } } 类变量有两个阶段可以被赋值，一是类加载准备阶段，二是初始化阶段。而局部变量只有一次初始化，如果没赋初值，不能使用，下面代码编译不通过。 public class test_java { public static void main(String[] args) { int a; System.out.println(a); } } 解析：保证引用的完整性。做了：类或接口解析，类方法解析，接口方法解析，字段解析。 这个阶段相关的报错信息： java.lang.NoSuchFieldError 根据继承关系从上往下没找到相关字段时报错 java.lang.IllegalAccessError 不具备访问权限时报错 java.lang.NoSuchMethodError 找不到相关方法时报错 初始化：初始化成员变量，这一步才开始执行字节码。 public class A { static{ System.out.println(1); } public A(){ System.out.println(&quot;A&quot;); } public static void main(String[] args) { A ab = new B(); ab = new B(); } } class B extends A{ static { System.out.println(&quot;2&quot;); } public B(){ System.out.println(&quot;B&quot;); } //执行结果: 1 2 A B A B 原因:初始化子类先调用父类无参构造，static在类加载的准备阶段执行一次，不重复执行。 //static只会执行一次，对应cint方法 //对象初始化调用构造方法，每次新建对象都会执行，对应init方法 如果你自己写一个java.lang包，改写了String类，编译后发现没起作用。JRE不能被轻易篡改，否则可能会有安全问题。这就是类加载机制在起作用。类加载机制流程： 双亲委派机制：当某个类加载器需要加载某个.class文件时，它首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，才会去真正加载这个类。比如Object类，毫无疑问会交给最上层的类加载器加载，保证只有一个被加载的Object类。如果没有双亲委派机制，会有多个Object类，很混乱。类加载器运行有先后顺序的，下面是类加载器的种类： BootstrapClassLoader（启动类加载器）：c++编写，加载java核心库 java.*,构造ExtClassLoader和AppClassLoader。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作 ExtentionClassLoader （标准扩展类加载器）：java编写，加载扩展库，如classpath中的jre(lib/ext下jar包和.class)，javax.*和java.ext.dirs指定位置中的类，开发者可以直接使用标准扩展类加载器。 AppClassLoader（系统类加载器）：java编写，加载程序所在的目录，classpath位置下其他所有jar和.class。我们写的代码最先尝试使用这个进行加载，再通过双亲委派机制递归委托上级类加载器。 CustomClassLoader（用户自定义类加载器）：java编写,用户自定义的类加载器,可加载指定路径的class文件。支持自定义扩展功能。 双亲委派机制作用：1、防止一个.class被重复加载，一个一个去上面问，加载过了就不加载了。保证数据安全。2、保证核心.class不被篡改，即使篡改也不会加载，即使加载也不会是同一个.class对象。（不同的类加载器加载同一个.class得到的是不同的对象）。保证.class执行没问题。 可以覆盖HashMap类的实现吗？可以，用到Java的endorsed技术，我们可以把自己的HashMap类打成jar放在-Djava.endorsed.dirs指定的目录，类名和包名应该与jdk原生的一致。这个目录下的jar会被优先加载，比rt.jar优先级更高。 哪些地方打破了Java的类加载机制?举例子：1.tomcat使用war包发布应用，由WebAppClassLoader类加载器优先加载，它加载自己目录的.class但不传递给父类加载器，但它可以通过SharedClassLoader实现共享和分离。2.Java的SPI机制，例子：Mysql的JDBC。使用JDBC Driver前使用Class.forName(“com.mysql.jdbc.driver)，但如果删除这行代码也能正确加载到驱动类，因为使用ServiceLoader来动态装载。 如何加载远程.class文件，怎么加密.class文件？通过实现一个新的自定义类加载器。 JVM的GCGC Roots：可达性分析法，是GC实现的一种方法(另一种是引用计数法)，GC Roots是一组活跃的引用，程序在接下来的运行中能直接或间接引用或能被引用的对象。从GC Roots不断向下追溯遍历，会产生Reference Chain引用链。GC Roots遍历过程是找出所有活对象，并把其余空间认定为无用，而不是找到死对象。如果一个对象连续两次遍历过程中跟GC Roots没有任何直接或间接引用，则会被GC掉。GC Roots包括： 活动线程相关的各种引用 类的静态变量的引用 JNI引用 GC Roots是引用不是对象引用级别（引用链的表现）： 强引用：[有用且必须]内存不足直到抛OOM，这种强引用的对象也不会被回收。 - 容易造成内存泄露(比如一个User类没有字段info，用HashMap&lt;User,String&gt;存，用完User但因为被HashMap使用而未能回收，就造成内存泄露) 软引用：[有用非必须]维护一些可有可无的对象，内存足够的时候不会被回收，内存不足会回收。如果回收了软引用对象后内存还不够则抛出OOM。 弱引用：[可能有用非必须]引用的对象相比软引用，要更加无用一些，生命周期更短。GC时无论内存是否充足都会回收弱引用关联的对象。 虚引用：[无用]形同虚设的引用，任何时候都可被回收。 //强引用 Shmily shmily = new Shmily(); //软引用 SoftReference&lt;Shmily&gt; softReference = new SoftReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = softReference.get(); //弱引用 WeakReference&lt;Shmily&gt; weakReference = new WeakReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = weakReference.get(); //虚引用 虚引用的使用必须和引用队列（Reference Queue）联合使用 ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(new Shmily(), referenceQueue); Shmily shmily = phantomReference.get(); //所有以上对象出了强引用之外，一旦被回收，get方法返回null。 //以上创建软引用，弱引用的对象softReference和weakReference还都属于强引用，用完也需要回收避免内存溢出，方法如下： ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(softReference, referenceQueue); 可能发生OOM的内存区域：除了程序计数器，都有可能。但主要是发生在堆上。OOM发生原因： 内存不足需要扩容。 错误的引用方式，没有及时切断GC Roots的引用，导致内存泄漏。(典型) 没有进行数据范围检查，比如全量查询了某个数据库。 无限制无节制使用MemoryOverHead JVM的垃圾回收算法GC的标记过程：从GC Roots遍历所有可达的活跃对象并标记。GC触发条件：1.老年代不足 2.调用了System.gc() 3.通过MinorGC进入老年代的对象大小总和大于老年代的大小（担保失败） 4.Eden区不够存放新创建的对象GC算法： 标记清除算法：标记-标记已用对象，清除-清除未被标记的对象。 缺点：产生内存碎片 场景：适合在收集频率低的老年代使用 复制算法：内存空间分等大两块，一块满了，未被标记的对象复制到另一块。 优点：解决了内存碎片问题，效率最高 缺点：会有一半的内存空间浪费 场景：适合收集频率高且追求收集效率的年轻代使用 标记整理算法：移动所有存活的对象，且按内存地址顺序依次排列，然后将末端内存全部回收。 优点：解决了内存碎片问题，同时解决标记复制算法的内存空间浪费问题 缺点：效率低于复制算法和标记清除算法 场景：适合在收集频率低的老年代使用 JVM采用分代收集算法，对不同的区域采用不用的收集算法。 GC种类： MinorGC 发生在年轻代的GC触发条件：Eden区不够存放新创建的对象 MajorGC 发生在老年代的GC 与FullGC区别是只清理老年代而不清理年轻代触发条件：① FullGC 全堆垃圾回收（如元空间引起的年轻代和老年代回收）触发条件：①调用System.gc ②老年代空间不足(可能无足够连续空间) ③担保机制失败(Eden大对象无法存入老年代，因为检测到老年代无足够连续内存空间) ④Minor GC后进入老年代的平均大小大于老年代可用内存 Mixed GC[G1收集器特有] 收集整个YoungGeneration和部分OldGeneration Java的大部分对象生命周期都不长，它们位于年轻代(Young Generation)，而生命周期较长的位于老年代(Old Generation)。 年轻代的GC：年轻代使用复制算法，因为年轻代大部分对象生命周期短，如果发生GC只会有少量对象存活，复制这部分对象是高效的。年轻代分为Eden:From Survivor:To Survivor = 8:1:1三个空间。对象首先在Eden区，如果Eden区满了就会触发MinorGC。单数次MinorGC:在MinorGC后，存活的对象进入Form Survivor区。双数次MinorGC，Eden和From Survivor区一起清理，存活对象被复制到To区，并清空From区。从上面可以得知每次GC都有一个Survivor区空闲，由于Eden:From Survivor:To Survivor = 8:1:1，年轻代GC复制算法只浪费了10%的内存空间，同时做到了高效，无碎片和节约空间。扩展：TLAB(Thread Local Allocation Buffer)，是JVM给每个线程单独开辟的区域，用来加速对象分配。在Eden区分多个TLAB，TLAB通常比较小，对象优先分配在TLAB上，对象较大才会在Eden区分配。TLAB是一种优化，类似于逃逸分析的对象在栈上分配的优化。老年代的GC：老年代一般使用标记整理和标记清除算法。因为老年代很多对象存活率高，占用较大，不方便复制。对象怎么进入老年代：1. 达到一定年龄每次发生MinorGC，对象年龄加1，达到阈值(最大值是15可通过‐XX:+MaxTenuringThreshold调)，进入老年代。2. 分配担保机制因为Survivor区只占年轻代10%的空间，发生MinorGC时无法保证每次Eden+其中一个Survivor存活的对象大小都小于另一个Survivor区空间，通过分配担保机制，另一个Survivor区放不下的对象直接进入老年代。JVM每次MinorGC前会检查老年代最大可用连续内存空间是否大于新生代对象的总空间，如果是的话确保MinorGC是安全的。3. 大对象直接进入老年代超过一定大小的对象直接进入老年代。(通过-XX:PretenureSizeThreshold设置，默认0表示都要先走年轻代)4. 动态年龄判定为了使内存分配更灵活，JVM不一定要求对象年龄达到MaxTenuringThreshold(15)才晋升为老年代，若Survivor区相同年龄对象总大小大于Survivor区空间的一半，则大于等于这个年龄的对象将会在MinorGC时移到老年代。JVM常见垃圾回收器：如果垃圾收集算法是JVM垃圾回收的方法论，那垃圾回收器就是上述算法的实现。 年轻代垃圾回收器 1. Serial垃圾回收器 单线程的垃圾回收器，垃圾回收时暂停一切用户线程，使用复制算法。 优点：简单轻量级，使用资源少。 场景：用于客户端应用，因为客户端应用不会频繁创建对象。 2. ParNew垃圾回收器 Serial回收器的多线程版本，多条GC线程并行回收，垃圾回收时仍暂停一切用户线程 优点：多CPU环境下收集效率高些，GC停顿时间缩短。 场景：多CPU场景下使用，ParNew适合交互多计算少的场景。 3. Parallel Scavenge垃圾回收器 多线程回收器 场景：多CPU下使用，追求CPU吞吐量，适用于交互少计算多的场景。 老年代垃圾回收器 1. Serial Old垃圾回收器 与年轻代的Serial垃圾回收器对应，也是单线程，使用标记-整理算法。 优点：简单轻量级，使用资源少。 场景：也适用于客户端应用 2. Parallel Old垃圾回收器 Parallel Scavenge垃圾回收器的老年代版本。 场景：多CPU下使用，追求CPU吞吐量，适用于交互少计算多的场景。 3.CMS垃圾回收器 以最短GC时间为目标，用户线程与GC线程可并发执行，垃圾回收过程用户不会感到明显卡顿。 长期来看G1、ZGC等更高级的垃圾回收器是趋势。 CMS垃圾回收器 全称：Mostly Concurrent Mark and Sweep Garbage Collector（主要并发­标记­清除­垃圾收集器） CMS在年轻代使用复制算法，在老年代使用标记-清除算法。它把耗时的GC操作通过多线程并发执行的。 优点：避免老年代GC出现长时间卡顿 缺点：对老年代的回收没整理阶段，产生内存碎片随时间推移增多时必须要FullGC才能清理。可能会导致大对象创建失败。 场景：不希望GC停顿时间长且CPU资源较充足 回收过程：1.初始标记阶段：只标记GC Roots直接关联的对象和年轻代中的引用，不向下追溯，缩短了标记时GC暂停时间。2.并发标记阶段，并发地追溯可达对象，持续时间较长但跟用户线程并行执行。3.并发预清理，这个过程会清理dirty状态的老年代对象。4.可选的预清理。5.最终标记，会GC暂停。6.并发清理，用户线程重新激活，删除不可达对象。 关于CMS的碎片整理问题：两个参数 UseCMSCompactAtFullCollection（默认开启）：FullGC时压缩，整理内存碎片，会造成较长时间停顿。 CMSFullGCsBeforeCompaction：每隔几次FullGC后执行一次带压缩的FullGC。 总结CMS中有哪些会造成STW(GC停顿)的操作： STW = stop the world. 初始标记阶段-较短停顿 最终标记阶段-较短停顿 老年代的回收-较长停顿 Full GC阶段-较长停顿 G1垃圾回收器 全称：Garbage First（尽可能多地收集垃圾以减少FullGC） 目前比较好的收集器，关注低延迟，用于替代CMS的功能更强大的新型收集器。 引入了分区概念，弱化了分带概念 优点：避免老年代GC出现长时间卡顿，同时与CMS相比解决了CMS产生碎片的缺陷。 缺点： 场景：不希望GC停顿时间长且CPU资源较充足 回收过程： 关于CMS的碎片整理问题：两个参数 总结G1中有哪些会造成STW(GC停顿)的操作： STW = stop the world. GC小技巧 GC日志查看 加-XX:+PrintGCDetails参数 查看GC日志，有关GC日志的解析后续我会单写一个博客。 使用Sun公司的gchisto，gcviewer离线分析工具 使用JDK自带的JConsole 使用jstat -gcutil pid命令 使用JvisualVM工具 查看当前Java版本垃圾回收信息 $ java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=266248768 -XX:MaxHeapSize=4259980288 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC java version &quot;1.8.0_191&quot; Java(TM) SE Runtime Environment (build 1.8.0_191-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) 设置应用的垃圾回收器： -XX:+UseSerialGC 年轻代和老年代都用串行收集器 -XX:+UseParNewGC 年轻代使用 ParNew，老年代使用 Serial Old [JDK9被抛弃] -XX:+UseParallelGC 年轻代使用 ParallerGC，老年代使用 Serial Old -XX:+UseParallelOldGC 新生代和老年代都使用并行收集器 -XX:+UseConcMarkSweepGC，表示年轻代使用 ParNew，老年代的用 CMS -XX:+UseG1GC 使用 G1垃圾回收器 -XX:+UseZGC 使用 ZGC 垃圾回收器 常量池分静态常量池和运行时常量池，静态常量池在 .class 中，运行时常量池在方法区中。字符串池在JDK 1.7 之后被分离到堆区。String str = new String(“Hello world”) 创建了 2 个对象，一个驻留在字符串池，一个分配在 Java 堆，str 指向堆上的实例。String.intern() 能在运行时向字符串池添加常量。为什么String为final：1.为了实现字符串池：创建字符串常量时，JVM会检测字符串常量池，如果已存在，直接返回常量池中的实例的引用，如果不存在就实例化并放入字符串常量池。因为String为Final类型，我们可以十分肯定字符串常量池不存在两个相同的字符串。2.为了线程安全：因为它不可变，本身就是线程安全的3.节约内存4.HashMap的key往往用String是因为String不可变，在被创建时HashCode就被缓存了不需要重新计算。 GC是怎么判断对象是被标记的？通过枚举根节点的方式，通过jvm提供的一种oopMap的数据结构，简单来说就是不要再通过去遍历内存里的东西，而是通过OOPMap的数据结构去记录该记录的信息,比如说它可以不用去遍历整个栈，而是扫描栈上面引用的信息并记录下来。总结:通过OOPMap把栈上代表引用的位置全部记录下来，避免全栈扫描，加快枚举根节点的速度，除此之外还有一个极为重要的作用，可以帮HotSpot实现准确式GC【这边的准确关键就是类型，可以根据给定位置的某块数据知道它的准确类型，HotSpot是通过oopMap外部记录下这些信息，存成映射表一样的东西】。 CMS收集器是否会扫描年轻代？会，在初始标记的时候会扫描新生代。虽然cms是老年代收集器，但是我们知道年轻代的对象是可以晋升为老年代的，为了空间分配担保，还是有必要去扫描年轻代。 小标题1小标题2原理（中标题） 字体斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本 字颜色大小 This is some text! This is some text! This is some text! 一些常用Java命令参考资料Java双亲委派机制及其作用拉勾网MetaSpace整体介绍深入理解JMM和GC","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://shmily-qjj.top/tags/Java/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"线程进程与锁","slug":"线程进程与锁","date":"2020-02-11T03:20:08.000Z","updated":"2020-05-04T12:53:02.530Z","comments":true,"path":"6f97dc89/","link":"","permalink":"https://shmily-qjj.top/6f97dc89/","excerpt":"","text":"线程进程与锁线程，进程与锁是一定要掌握的基础知识点，希望能通过写博客的方式加深印象，并且在以后能够随时补充和回看。 进程概念1.什么是进程进程是可并发执行的程序在某个数据集合上的一次计算活动，也是操作系统进行资源分配和调度的基本单位。 2.进程的三种基本状态运行态：当进程得到处理机，其执行程序正在处理机上运行时的状态称为运行状态。就绪态：当一个进程已经准备就绪，一旦得到CPU，就可立即运行，这时进程所处的状态称为就绪状态。阻塞态：若一个进程正等待着某一事件发生(如等待输入输出操作的完成)而暂时停止执行的状态称为等待状态。处于等待状态的进程不具备运行的条件，即使给它CPU，也无法执行。系统中有几个等待进程队列（按等待的事件组成相应的等待队列）。 进程的五种状态：新建-就绪-运行-阻塞-死亡就绪-&gt;运行:等待cpu调度运行-&gt;阻塞:放弃cpu时间片/IO请求运行-&gt;死亡:run运行完或抛异常 3.进程状态切换过程运行到等待：等待某事件的发生（如等待I/O完成）等待到就绪：事件已经发生（如I/O完成）运行到就绪：时间片到（例如，两节课时间到，下课）或出现更高优先级进程，当前进程被迫让出处理器。就绪到运行：当处理机空闭时，由调度（分派）程序从就绪进程队列中选择一个进程占用CPU。 4.并发与并行区别？并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。 线程概念1.什么是线程线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 2.线程与进程的关系 进程是系统资源分配的基本单位-线程是任务调度执行的基本单位 进程切换开销大-线程间切换开销小 进程间资源和地址空间独立-同一进程下线程共享资源和地址空间 进程崩溃不影响其他进程-线程崩溃整个进程都崩溃（多进程健壮性强） 3.线程间共享的资源 线程间共享堆和方法区 虚拟机栈、本地方法栈、程序计数器不共享 为什么程序计数器资源不共享：程序计数器不共享是为了线程切换后能恢复到正确的执行位置 为什么虚拟机栈和本地方法栈私有：栈帧用于存储局部变量表、操作数栈、常量池引用等信息，而本地方法栈则为虚拟机使用到的Native方法服务，两者相似。为了保证局部变量不被其他线程访问，所以不共享。 为什么堆和方法区共享：新创建的对象存放在堆中，类信息、常量、静态变量、即时编译器编译后的代码存放在方法区。 多线程不一定提高效率，只是让CPU利用率更高，如果频繁切换可能效率反而更低(见第7条解释)。 4.线程间通信方式 全局变量 线程上下文 共享内存 套接字Socket IPC通信如何实现线程间通讯：1.通过类变量直接将数据放到主存中 2.通过并发的数据结构来存储数据 3.使用volatile变量或者锁 4.调用atomic类 5.进程间通信方式 管道 有名管道 信号量 共享内存 消息队列 信号 套接字 6.守护线程？一种后台特殊进程，所有用户的线程都退出了，没有被守护的对象了，也不需要守护线程了，这时守护线程关闭。（JVM退出了它会关闭）比如GC线程就是守护线程。用户线程可以转为守护线程：thread.setDaemon(true); 7.线程切换开销中断处理，多任务处理，用户态切换等原因导致CPU从一个线程切换到另一个线程。线程上下文切换代价是高昂的，上下文切换的延迟由很多因素决定，平均要50-100ns，而CPU每核心每ns执行十几条指令，切换的过程就花费几百至几千条指令的执行时间。如果跨核上下文切换，代价更加高昂。 7.线程的几种状态？ 新建(NEW):线程被创建出来但还未启动就绪(RUNNABLE):调用start()后，线程准备就绪，等到CPU分配资源就可以运行阻塞(BLOCKED):线程处于活跃状态，等待Monitor监视器锁，等待线程同步锁等待(WAITING):等待另一个线程执行，如调用了wait(),就要等另一个线程notify()或notifyAll()才唤醒计时等待(TIMED_WAITING):调用了wait(timeout)或join(timeout)指定了超时时间终止(TERMINATED):线程执行完毕 终止并释放资源 总结： 线程池 为什么要用线程池？ 频繁创建和销毁线程费时低效而且浪费内存(线程死亡，相关对象变成垃圾)。线程池尽可能减少了对象创建和销毁的次数，让线程运行完不立即销毁，而是重复使用，从而提高效率。 ThreadPoolExecutor是线程池的核心类，构造参数的意义： corePoolSize：常驻核心线程数，如果为0且池中无线程执行时自动销毁线程池。如果大于0，线程执行完毕后不会销毁线程，而是进入缓存队列等待再次被运行。这个参数设置过小会频繁创建销毁线程，过大会浪费系统资源。 maximunPoolSize：线程池能创建最大的线程数量。如果常驻核心线程数和缓存队列都已经满了，新的任务进来就会创建新的线程来执行。但是数量不能超过maximunPoolSize，否侧会采取拒绝接受任务策略。 keepAliveTime：线程存活时间，未执行的线程空闲超过该时间则终止。多余线程会销毁直到线程数量达到corePoolSize。如果corePoolSize等于MaximumPoolSize，超过空闲时间也不会销毁任何线程。 unit：存活时间单位，和keepAliveTime配合使用。 workQueue：线程池执行的任务队列（缓存队）列，用来存放等待被执行的任务。 threadFactory：线程工厂，用来创建线程，一般有三种选择策略。（一般用默认即可） ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; handler：拒绝处理策略，线程数量大于最大线程数就会采用拒绝处理策略，四种策略为 ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：忽略最新任务 ThreadPoolExecutor.DiscardOldestPolicy：忽略最早的任务（最先加入队列的任务） ThreadPoolExecutor.CallerRunsPolicy：把任务交给当前线程执行。 自定义拒绝策略：新建RejectedExecutionHandler对象然后重写rejectedExecution方法 常见线程池newCachedThreadPool：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 适用大量耗时较少的线程任务 newFixedThreadPool：创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。 线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor：单线程串行执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。 单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newScheduleThreadPool：创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。 该线程池多用于执行延迟任务或者固定周期的任务。 常见线程池线程池执行流程： 协程协程是一种用户态的轻量级线程，协程的调度完全由用户控制，协程间切换只需要保存任务的上下文，没有内核开销。 锁分类1.公平锁和非公平锁：公平锁指多个线程按照申请锁的顺序排队来依次获取锁。非公平锁是没有顺序获取锁。(因为公平锁挂起和恢复存在一定开销，所以非公平锁性能好些)(非公平锁获取方式是随机抢占。公平锁和非公平锁就差在 !hasQueuedPredecessors() ，也就是前边没有排队者的话，我就可以获取锁了。tryAcquire方法。)2.可重入锁：又名递归锁，是指同一个线程在外层的方法获取到了锁，在进入内层方法会自动获取到锁。3.共享锁S和排它锁X：多个线程可以同时获取一个共享锁，一个共享锁可被多个线程拥有。排它锁也叫独占锁，同一时刻只能被统一线程占用，其他线程需要等待。4.互斥锁和读写锁：一次只能有一个线程拥有互斥锁，读写锁多个读者可同时读，写必须互斥。写优先于读，如果有写，读必须等待。5.乐观锁和悲观锁：乐观锁认为读取数据时其他线程不会对数据做修改，不加锁，更新数据时采用尝试更新不断重试的方式。悲观锁认为读取数据时其他线程会对数据做修改，会出问题，所以默认加锁。6.分段锁：提升并发程序性能的手段之一，粒度更小。将数据分成一段一段的存储（如ConcurrentHashMap的Segment），然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问，能够实现真正的并发访问。所以说，ConcurrentHashMap在并发情况下，不仅保证了线程安全，而且提高了性能。7.锁的状态：无锁、偏向锁、轻量级锁、重量级锁：偏向锁是减少无竞争且只有一个线程使用锁的情况下，使用轻量级锁产生的性能消耗。轻量级锁每次申请、释放锁都至少需要一次CAS，但偏向锁只有初始化时需要一次CAS。重量级锁，其他线程试图获取锁时，都会被阻塞，只有持有锁的线程释放锁之后才会唤醒这些线程，进行竞争。 无锁状态-&gt;偏向锁状态-&gt;轻量级锁状态-&gt;重量级锁状态 随竞争情况逐渐升级 不可逆 后续会在概念部分详细说 偏向锁：仅有一个线程进入临界区 轻量级锁：多个线程交替进入临界区 重量级锁：多个线程同时进入临界区 8.自旋锁：线程没获得锁时不会被挂起而是空循环，可以减少线程阻塞造成的线程切换的概率。首先，阻塞或唤醒Java线程需要操作系统切换CPU状态来完成，状态切换耗费CPU，可能切换CPU的时间比同步代码块中代码的执行时间都要长，为了很短的同步锁定时间而花费很长的线程切换时间是不值得的，如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。8.自适应自旋锁：自适应是值虚拟机会记录一个锁对象自旋时间和状态，如果之前自旋等待成功获得锁，这次自旋也很大概率成功，会允许持续更长时间的自选等待，后面的自旋会大概率拿到锁。如果一个对象的锁自旋等待很少能成功获取到锁，后续减少自旋次数甚至忽略自旋过程，直接阻塞，避免浪费CPU资源。（简而言之，自旋时间根据之前锁和线程状态动态变化，来减少线程阻塞时间）11.读写锁：对资源读取和写入的时候拆分为2部分处理，读的时候可以多线程一起读，写的时候必须同步地写。(如果已占用读锁，其他线程想写需要等读锁释放，其他线程想读可以读；如果已占用写锁，其他线程无论想读想写，都要等写锁释放) 概念1.从底层角度看，本质上只有一个关键字和两个接口：Synchronized和Lock接口以及ReadWriteLock接口（读写锁） 2.synchronized与reentrantLock简述 synchronized是一个隐式的重入锁，比较笨重，实现方式是锁主存和缓存一致性。现在的JDK版本已经做了很多优化synchronized的措施：自适应的自旋锁、锁粗化、锁消除、轻量级锁等 reentrantLock是一个显式的重入锁，比较灵活，可以扩展为分段锁，实现方式是AQS+双向链表（默认是NonFairSync非公平锁实现，而NonFairSync继承Sync，Sync继承AbstractQueuedSynchronizer，AQS维护volatile变量state作为同步状态）。一个线程可多次获取锁，每次获取都会计数count++，解锁时count--直到变0. ReentrantLock采用的是独占锁。Semaphore，CountDownLatch等采用的是共享锁，即有多个线程可以同时获取锁。 3.synchronized和Lock区别： 1、synchronize是java内置关键字，而Lock是一个类。（通过javap看字节码，发现有monitorenter和monitorexit命令，分别对应进入monitor加锁和释放） 2、synchronize可以作用于变量、方法、代码块，而Lock是显式地指定开始和结束位置。 3、synchronize不需要手动解锁，当线程抛出异常的时候，会自动释放锁；而Lock则需要手动释放，所以lock.unlock()需要放在finally中去执行。 4、性能方面，如果竞争不激烈的时候，synchronize和Lock的性能差不多，如果竞争激烈的时候，Lock的效率会比synchronize高。 5、Lock可以知道是否已经获得锁，synchronize不能知道。Lock扩展了一些其他功能如让等待的锁中断、知道是否获得锁等功能，Lock 可以提高效率。 6、synchronize是悲观锁的实现，而Lock则是乐观锁的实现，采用的CAS的尝试机制。 4.synchronized和ReenTrantLock区别： 1、ReenTrantLock可以中断锁的等待，提供了一些高级功能。 2、ReenTrantLock默认非公平锁，可以设为公平锁，synchronized不行 3、ReenTrantLock可以绑定多个锁条件。 4、synchronized是JVM关键字，ReentrantLock是Java API 5、ReentrantLock只能修饰代码块，synchronized既可以修饰方法也可以修饰代码块 6、ReentrantLock需要手动加锁和释放锁，synchronized自动释放 7、ReentrantLock可以知道是否成功获得了锁，synchronized不能 5.锁膨胀：无锁、偏向锁、轻量级锁、重量级锁及升级过程无锁：每个对象都有一把看不见的锁（内部锁 也叫 Monitor锁） 特点：不断尝试修改资源，失败的线程重试直到成功。偏向锁：特点：一段同步代码一直被同一个线程访问，锁总是被同一线程多次获得，降低锁的获取代价。只有一个线程执行同步代码块时能提高性能。 当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID（锁标志位）。 在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。 引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。 撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 轻量级锁：当前对象持有偏向锁时被另外的线程访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。重量级锁：当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。 特点：等待锁的线程都会进入阻塞状态。 6.锁消除： 先说“逃逸分析技术”，该技术在编译期使用，分析对象的动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中，称为方法逃逸。 该技术将确定不会发生逃逸的对象放入栈内存而非堆(故不是所有对象都在堆中)。 为了减少锁的请求和释放操作，“逃逸分析技术”在编译期分析出那些本来不存在竞争却加了锁的代码，让他们的锁失效，从而达到减少锁的请求和释放的目的。 而锁消除就是，发现不存在多线程并发抢占问题的时候，编译后去掉该锁。 7.锁偏向： 先说一下Java对象头，它包括Mark Words、Klass Words两部分，可能还包括数组的长度（如果对象是个数组）。 Klass Word里面存的是一个地址，占32位或64位，是一个指向当前对象所属于的类的地址，可以通过这个地址获取到它的元数据信息。 Mark Word！重点，这里面主要包含对象的Hashcode、年龄分代、锁标志位等，大小为32位或64位。锁状态不同时MarkWord里内容会不同。 锁偏向：当第一个线程请求时，会判断锁的对象头里的ThreadId字段的值，如果为空，则让该线程持有偏向锁，并将ThreadId的值置为当前线程ID。当前线程再次进入时，如果线程ID与ThreadId的值相等，则该线程就不会再重复获取锁了。因为锁的请求与释放是要消耗系统资源的。 如果有其他线程也来请求该锁，则偏向锁就会撤销，然后升级为轻量级锁。如果锁的竞争十分激烈，则轻量级锁又会升级为重量级锁。 对象头： 8.锁粗化：在编译期间将相邻的同步代码块合并成一个大同步块。这样做可以减少反复申请和释放同一个锁对象导致的系统开销。当一个循环中存在加锁操作时，可以将加锁操作提到循环外面执行，一次加锁代替多次加锁，提升性能。 9.volatile：是Java关键字，功能是保证被修饰的元素： 任何进程读取时都会清空本进程持有的共享变量值，而强制从主存获取(每次访问变量都刷新，每次都能得到最新变量) 任何进程写完毕时都强制将共享变量写回主存 防止指令重排(指令重排作用是JVM单线程程序在不影响运行结果的条件下的优化。多线程指令重排会出问题。) 保证内存可见性(一个线程对共享变量改动，要让其他线程立刻知道) 并不保证操作原子性它是怎么避免指令重排的呢？通过内存屏障内存屏障是通过JVM生成内存屏障指令，在读写操作前后添加内存屏障(解决L1,L2,L3高速缓存与主存速度不一致导致的缓存一致性问题)volatile读操作的后面插入一个LoadLoad屏障。volatile写操作的后面插入一个StoreLoad屏障。它的内存读写过程：LoadStoreLoadLoadStoreStoreStoreLoad注：StoreLoad就是触发后续指令中的线程缓存回写到内存; 而LoadLoad会触发线程重新从主存里面读数据进行处理。 应用：volatile应用于单例模式，防止因指令重排导致单例模式返回一个初始化到一半的对象 public class Singleton { /** * 双重校验 保证线程安全，提高执行效率，节约内存空间 */ private static volatile Singleton instance; //防止JVM指令重排 private Singleton(){} public static Singleton getInstance(){ /** * 如果instance不加volatile，JVM指令重排会先分配地址再初始化（此时这个地址存在但没值）， * 所以这里判断不为null为true时有可能对象还未完成初始化，单例可能返回了一个未初始化完的对象。 */ if(instance == null){ synchronized (Singleton.class){ if(instance == null){ instance = new Singleton(); } } } return instance; } } 10.CAS：全称Compare-And-Swap，它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子的。CAS是CPU的原子指令，底层是汇编语言，Unsafe类中的compareAndSwapInt，是一个本地方法，该方法的实现位于unsafe.cpp。CAS缺点 循环时间长CPU开销大 只能保证一个共享变量的原子操作 会引发ABA问题(当变量从A修改为B再修改回A时，变量值等于期望值A，但是无法判断是否修改，CAS操作在ABA修改后依然成功。比如说一个线程one从内存位置V中取出A，这个时候另一个线程two也从内存位置V中取出A，并且线程two进行了一些操作将值变成了B，然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作时发现内存中仍然是A，然后线程one操作成功。这个线程one的操作可能出问题。) 11.AQS：AbstractQueuedSynchronizer，维护一个volatile int state（代表共享资源状态）和一个FIFO线程等待队列。 死锁1.什么是死锁两个或多个线程互相持有对方需要的资源，导致一直等待状态，互相等待对方释放资源，如果没有主动释放资源，就会死锁。 2.死锁产生条件 1、存在循环等待 2、存在资源竞争 3、已经获得的资源不会被剥夺 4、请求与保持，一个线程因请求资源被阻塞时，拥有资源的线程的状态不会改变。 3.避免死锁 产生死锁条件中任意一条不满足，就不会产生死锁 4.死锁定位修复 执行程序时，程序没停止，也不继续运行，则是死锁 通过jps获取端口号，再jstack工具可以看到 其他关于AQS和CAS详细：https://www.jianshu.com/p/2a48778871a9锁状态升级详解：http://www.jetchen.cn/synchronized-status/Java并发-volatile内存可见性和指令重排：https://blog.csdn.net/jiyiqinlovexx/article/details/50989328","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"线程、进程","slug":"线程、进程","permalink":"https://shmily-qjj.top/tags/线程、进程/"},{"name":"概念","slug":"概念","permalink":"https://shmily-qjj.top/tags/概念/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"Apache Zeppelin初探","slug":"Apache Zeppelin初探","date":"2020-02-11T02:16:00.000Z","updated":"2020-04-12T14:15:47.997Z","comments":true,"path":"174820fd/","link":"","permalink":"https://shmily-qjj.top/174820fd/","excerpt":"","text":"Apache Zeppelin什么是ZeppelinApache Zeppelin是一个高性能，高可用，高可靠的分布式Key-Value存储与可视化平台，它是集数据摄取，数据分析，数据可视化与协作于一身的notebook形式的基于Web的工具，支持多种解释器(Interpreter),能广泛支持多种大数据查询引擎和计算引擎(如Spark，Flink，Presto，Kylin…)，多种存储系统(如JDBC数据源，HBase，Elasticsearch，Hive，Neo4j，Alluxio，Ignite…),以及多种脚本语言(如python,scala,R,shell…)和markdown。 Apache Zeppelin支持的部分组件： Zeppelin优势 为数据分析与可视化提供便利：在Zeppelin中以笔记本（notebook）的形式组织和管理交互式数据探索任务，一个笔记本（note）可以包括多个段（paragraph）。段是进行数据分析的最小单位，即在段中可以完成数据分析代码的编写以及结果的可视化查看 为多人协作提供便利：可以共享你的notebook，使他人也能看到你的数据分析笔记和结果 提供权限管理：可以管理notebook的权限以及执行者是否对已有数据有修改权限 支持多种查询计算引擎：兼容多种主流大数据查询，计算引擎，使得数据分析更加方便，数据分析人员可以对底层无感知 为临时获取某些数据提供便利：有需要临时获取一些数据的需求，通过配置Interpreter即可 配置与部署简单：已完全支持的组件只需简单填写解释器参数即可使用，支持安装第三方解释器 支持简单任务调度：Linux Crontab调度器功能 Zeppelin适用场景 多个部门需要在大数据平台取数据做分析的场景 需要多种查询引擎做数据分析的场景 需要对多种数据源进行数据可视化的场景 需要多人协作的场景 数据平台与数据分析分离，对数据分析人员无感知的场景 Zeppelin详细解释器Interpreters（重要）Zeppelin Interpreter是一个插件，允许将支持的语言/数据处理后端插入Zeppelin。通过简单的配置即可将语言/数据处理查询后端接入Zeppelin。Zeppelin解释器Zeppelin-Spark解释器 NotebookZeppelin的工作簿(Notebook)支持分为多个段，每段支持绑定多个不同的解释器，支持做单独处理和执行不同的操作，结果会一直被保留，可以选择让其他人浏览或者修改。Notebook提供给数据分析人员的前端工作环境，方便数据分析和数据可视化。 Interpreter Group解释器组：默认情况下，每个解释器属于一个解释器组，一个解释器组可能包含多个解释器同一InterpreterGroup中的Interpreter可以相互引用例如Spark解释器组包括Spark支持，PSpark，SparkSql和其他依赖项同一解释器组中的Zeppelin程序在同一JVM运行解释器组是开启、停止解释器运行的基本单位。(同时开启，停止) Interpreter binding mode解释器绑定模式：可选’shared’, ‘scoped’, ‘isolated’ 其一shared：共享模式，绑定解释器的每个Notebook共享单个解释器实例(方便不同Notebook间共享变量，但资源利用率低)scoped：作用域模式，在相同解释器程序中创建新的解释器实例(每个Notebook拥有自己的回话，资源利用率略高，不能直接共享变量)isolated：隔离模式，每个Notebook创建新的解释器程序(笔记本之间互不影响，不能直接共享变量) 比如shared模式下，每个Notebook都可以使用SparkInterpreter但是只有一个SparkContext 如果isolated模式，每个Notebook都可以使用SparkInterpreter但每个Notebook有单独的SparkContext 解释器绑定模式-官方详细介绍 Interpreter生命周期Zeppelin 0.8.0以后支持LifecycleManager来控制解释器生命周期(之前是关闭UI界面后生命周期结束)NullLifecycleManager不做操作，要像以前一样自行控制生命周期TimeoutLifecycleManager(默认生命周期管理)默认超过1小时关闭解释器，可以更改 Generic ConfInterpreterZeppelin解释器配置由所有用户和Notebook共享，如果想使用其他的设置，需要创建新的解释器，能实现但不方便，ConfInterpreter可以提供对解释器设置的更细粒度的控制和更大的灵活性。ConfInterpreter是可以被任何解释器使用的通用解释器，输入格式应为属性文件格式。它用于为任何解释器进行自定义设置。用户需要将ConfInterpreter放在Notebook的第一段如上图%spark.conf独立设置了该Notebook中的Spark解释器 Interpreter进程恢复0.8.0版本前，关闭Zeppelin会同时关闭所有正在运行的解释器程序，但是我们可能只是想维护Zeppelin服务器而不想关闭解释器程序，Interpreter进程恢复就派上用场了。0.8.0版本后，设置zeppelin.recovery.storage.class属性的值默认org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage不开启进程恢复设置为org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorage开启进程恢复，关闭Zeppelin不会关闭解释器程序如果开启了进程恢复，关闭了Zeppelin，又想再关闭解释器程序，则执行bin/stop-interpreter.sh 官方文档官方Docs 常见问题及错误排除 Interpreter * is not found**：检查是否已经配置了该解释器，如果配置了，检查该解释器是否已被点亮(右上角设置图标点为蓝色并保存) 详细深入了解: Apache Zeppelin官网","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"Zeppelin","slug":"Zeppelin","permalink":"https://shmily-qjj.top/tags/Zeppelin/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"分享我的技术调研流程","slug":"分享我的技术调研流程","date":"2020-01-03T13:10:00.000Z","updated":"2020-04-12T14:15:48.000Z","comments":true,"path":"4b21953d/","link":"","permalink":"https://shmily-qjj.top/4b21953d/","excerpt":"","text":"技术调研流程分享为何制定流程？ 明确调研需求，提高调研文档质量，规范调研流程，保证调研产出。 我是如何制定调研流程的？ 先总结一版自己的调研流程，再查阅资料以及查看阿里等大厂的调研报告，结合部门目前的实际情况来明确部门的调研流程。 技术调研流程整个调研流程分四个阶段 第一阶段：需求分析 分析目前/未来可能出现的瓶颈点 明确调研目标和方向（为了实现新需求？为了优化瓶颈点？） 引入新工具后的结果衡量（效率提升、成本降低等，如何衡量） 结构化思考新工具引入的目标和衡量标准：场景（适用场景、知识要求）、效率（性能、效果预测）、成本（容量、硬件资源、维护成本）、稳定性（故障分析工具、监控完善度）等 第二阶段：准备阶段 理解需求 结合现状评估可行性和收益 第三阶段：调研阶段 简单调研 短时间内粗略了解所调研技术的应用场景和部署环境，进一步评估和权衡可行性和收益 经过权衡后发现值得调研，发送邮件至直接上级并抄送部门Leader (标题：申请调研xxx 内容：简述xxx值得详细调研的理由) 协商决定是否批准，若批准，则开始进入详细调研阶段 详细调研 包括但不限于： 先设计调研方法与调研过程 预估调研时间，并在北森设定Deadline，根据调研报告的要求按时完成调研报告 了解相关技术在其他公司的应用及收益 原理及核心技术调研 总结适合我们的场景及解决方案 调研过程遇到的问题与解决方案 未解决的问题/收集需求可随时讨论，有必要的话可以开讨论会 设计落地方案 第四阶段：反馈与落地 调研反馈 必须产出一份调研报告 选择反馈形式：分享会、文档、邮件、群通知（如果是分享会，则要有完善的PPT，会前共享出来） 技术落地 根据自己设计的落地方案得出详细的部署文档和使用文档 配合运维部署 后续阶段：落地后如何跟进 出现问题及时跟进解决，并把问题与解决方案更新到使用文档中，如果影响较大，要在更新完使用文档后发群通知。 相关的新人文档/Wiki更新 文档要求所有调研文档统一保存在gitlab部门文档中的技术调研文档目录 部署文档要求 这部分为了方便让运维人员傻瓜式部署，并可以把简单的运维工作交给运维。 尽量打包好主从节点的分发包（提前编译好） 例： xxx-master.zip xxx-worker.zip 或整理conf配置文件包 xxx-master-conf.zip xxx-worker-conf.zip 尽量采用傻瓜式命令 例:sudo -uhdfs tar -zxvf /opt/alluxio-2.1.0-bin.tar.gz -C /opt/ sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh master sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh worker chmod 777 /opt/alluxio-2.1.0/logs/user ... 尽量写出部署过程可能的报错及解决方案 常用维护方案总结 使用文档要求 这部分目的是方便大家使用新技术新组件。 格式包括但不限于：场景1：示例代码/操作 场景2：示例代码/操作 场景3：示例代码/操作 ... 常见错误及解决 遇到问题请联系：调研人 调研报告要求 这部分的目的是调研时可能有遗漏的点，可以从这个列表里做参考。 开头写清 标题 + 调研人 + 调研时间 可以参考但不限于这些点： xx是什么 xx的优缺点 xx的应用场景 xx的功能/特性 xx的原理与架构简述 相似技术横向对比 初步评估带来的收益 遇到的问题Q&amp;A xx的兼容性（支持什么不支持什么） xx技术的核心点 xx的性能与扩展性（测试结果） xx的部署难度 如何部署与简单实践 应用该技术带来的工作量和学习成本 总结 注意事项 关注缺点的优先级高于关注优点的优先级（优点再多，也可能因为一个缺点而不能被应用） 明确场景，及时沟通需求，明确需求细节 多搜集信息，不急于出结果（搜集足够的信息才能做出比较准确的判断） 要从可行性，稳定性，可维护性，工作量和学习成本等几个重要方面考虑 合理安排时间，自己规定了Deadline，就要及时交付反馈","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"技术调研","slug":"技术调研","permalink":"https://shmily-qjj.top/tags/技术调研/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"Alluxio-基于内存的虚拟分布式存储系统","slug":"Alluxio-基于内存的虚拟分布式存储系统","date":"2020-01-01T14:16:00.000Z","updated":"2020-06-25T03:00:56.536Z","comments":true,"path":"44511/","link":"","permalink":"https://shmily-qjj.top/44511/","excerpt":"","text":"什么是AlluxioAlluxio 是世界上第一个虚拟的分布式存储系统，它为计算框架和存储系统构建了桥梁，使计算框架能够通过一个公共接口连接到多个独立的存储系统,使计算与存储隔离。 Alluxio 是内存为中心的架构，以内存速度统一了数据访问速度，使得数据的访问速度能比现有方案快几个数量级,为大数据软件栈带来了显著的性能提升在大数据生态系统中，Alluxio 位于数据驱动框架或应用（如 Apache Spark、Presto、Tensorflow、Apache HBase、Apache Hive 或 Apache Flink）和各种持久化存储系统（如 Amazon S3、Google Cloud Storage、OpenStack Swift、GlusterFS、HDFS、IBM Cleversafe、EMC ECS、Ceph、NFS 和 Alibaba OSS）之间,Alluxio 统一了存储在这些不同存储系统中的数据,为其上层数据框架提供统一的客户端API和全局命名空间 Alluxio最新动态:为了方便大家可以持续跟进Alluxio发展动态，这里给出两条跟进Alluxio最新发展和动态的途径:Alluxio官方文档Alluxio知乎专栏 Alluxio优势 内存速度 I/O:Alluxio 能够用于分布式共享缓存服务，这样与 Alluxio 通信的计算应用程序可以透明地缓存频繁访问的数据（尤其是从远程位置）,以提供近似于内存级 I/O 吞吐率，同时提升稳定性。 简化云存储和对象存储接入:与传统文件系统相比,云存储系统和对象存储系统使用不同的语义,这些语义对性能的影响也不同于传统文件系统。常见的文件系统操作（如列出目录和重命名）通常会导致显著的性能开销。当访问云存储中的数据时，应用程序没有节点级数据本地性或跨应用程序缓存。将 Alluxio 与云存储或对象存储一起部署可以缓解这些问题,因为这样将从Alluxio中检索读取数据,而不是从底层云存储或对象存储中检索读取。 简化数据管理:Alluxio 提供对多数据源的单点访问,便捷地管理远程的存储系统,并向上层提供统一的命名空间。除了连接不同类型的数据源之外,Alluxio 还允许用户同时连接到不同版本的同一存储系统,如多个版本的HDFS,并且无需复杂的系统配置和管理，提高了数据访问灵活性。 应用程序部署简易:Alluxio 管理应用程序和文件或对象存储之间的通信，将应用程序的数据访问请求转换为底层存储接口的请求。Alluxio 与 Hadoop 兼容,现有的数据分析应用程序,如Spark和MapReduce程序,无需更改任何代码就能在Alluxio上运行。 分层存储特性:综合使用了内存、SSD和磁盘多种存储资源。通过Alluxio提供的LRU、LFU等缓存策略可以保证热数据一直保留在内存中，冷数据则被持久化到level 2甚至level 3的存储设备上 方便迁移可插拔:Alluxio提供多种易用的API方便将整个系统迁移到Alluxio Alluxio的特征:对Alluxio的优势和特征进行了概括 点击可进入官网介绍超大规模工作负载:支持超大规模工作负载并具有HA高可用性灵活的API :计算框架可使用HDFS、S3、Java、RESTful或POSIX为基础的API来访问Alluxio智能数据缓存和分层 : 使用包括内存在内的本地存储，来充当分布式缓存,很大程度上改善I/O性能，且缓存对用户透明存储系统接口 : 通过一系列接口集成HDFS，S3，Azure Blob Store，Google Cloud Store等存储系统统一全局命名空间 : 多个存储系统安装到一个统一的名称空间中，不需要创建永久数据副本，方便管理多数据源安全性 : 通过内置审核、基于角色的访问控制、LDAP、活动目录和加密通信，提供数据保护监控和管理 : 提供了用户友好的Web界面和命令行工具，允许用户监控和管理集群分层次的本地性 : 将更多的读写安排在本地,实现成本和性能的优化 Alluxio的应用场景Alluxio 的落地非常依赖场景，否则优化效果并不明显（无法发挥内存读取的优势）1.计算应用需要反复访问远程云端或机房的数据（存储计算分离）2.混合云,计算与存储分离,异构的数据存储带来的系统耦合（Alluxio提供统一命名空间，统一访问接口）3.多个独立的大数据应用（比如不同的Spark Job）需要高速有效的共享数据（数据并发访问）4.计算框架所在机器内存占用较高,GC频繁,或者任务失败率较高,Alluxio通过数据的OffHeap来减少GC开销5.有明显热表/热数据，相同数据被单应用多次访问6.需要加速人工智能云上分析（如TensorFlow本地训练，可通过FUSE挂载Alluxio FS到本地） 我也做了很多Allxuio的性能测试工作,效果都不是很理想,有幸与Alluxio PMC范斌和李浩源交流了测试结果不如人意的原因,大佬是这么说的:”如果HDFS本身已经和Spark和Hive共置了，那么这个场景并不算Alluxio的目标场景。计算和存储分离的情况下才会有明显效果，否则通常是HDFS已经成为瓶颈时才会有帮助。“还有,如果HDFS部署在计算框架本地,作业的输入数据可能会存在于系统的高速缓存区,则Alluxio对数据加速也并不明显。所以:应用场景很关键,新技术产生时,一定要了解其应用场景和原理并经过考虑之后再做一些性能测试之类的后续工作!官方介绍的Alluxio应用场景 Alluxio原理如图，一个完整的Alluxio集群部署在逻辑上包括master、worker、client及底层存储(UFS)。master和worker进程通常由集群管理员维护和管理，它们通过RPC通信相互协作，从而构成了Alluxio服务端。而应用程序则通过Alluxio Client来和Alluxio服务交互，读写数据或操作文件、目录。 Alluxio核心组件Alluxio使用了单Master和多Worker的架构,Master和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件,Client通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。Alluxio用户一般只与Alluxio的Client组件进行交互。 Master: 负责管理整个集群的全局元数据并响应Client对文件系统的请求。在Alluxio文件系统内部，每一个文件被划分为一个或多个数据块(block)，并以数据块为单位存储在Worker中。Master节点负责管理文件系统的元数据(如文件系统的inode树、文件到数据块的映射)、数据块的元数据(如block到Worker的位置映射)，以及Worker元数据(如集群当中每个Worker的状态)。所有Worker定期向Master发送心跳消息汇报自己状态，以维持参与服务的资格。Master通常不主动与其他组件通信，只通过RPC服务被动响应请求，同时Master还负责实时记录文件系统的日志(Journal)，以保证集群重启之后可以准确恢复文件系统的状态。Master分为Primary Master和Secondary Master，Secondary Master需要将文件系统日志写入持久化存储，从而实现在多Master（HA模式下）间共享日志，实现Master主从切换时可以恢复Master的状态信息。Alluxio集群中可以有多个Secondary Master，每个Secondary Master定期压缩文件系统日志并生成Checkpoint以便快速恢复，并在切换成Primary Master时读取之前Primary Master写入的日志。Secondary Master不处理任何Alluxio组件的任何请求。 Worker: Alluxio Master只负责响应Client对文件系统元数据的操作，而具体文件数据传输的任务由Worker负责，如图，每个Worker负责管理分配给Alluxio的本地存储资源(如RAM,SSD,HDD),记录所有被管理的数据块的元数据，并根据Client对数据块的读写请求做出响应。Worker会把新的数据存储在本地存储，并响应未来的Client读请求，Client未命中本地资源时也可能从底层持久化存储系统中读数据并缓存至Worker本地。Worker代替Client在持久化存储上操作数据有两个好处:1.底层读取的数据可直接存储在Worker中，可立即供其他Client使用 2.Alluxio Worker的存在让Client不依赖底层存储的连接器，更加轻量化。Alluxio采取可配置的缓存策略，Worker空间满了的时候添加新数据块需要替换已有数据块，缓存策略来决定保留哪些数据块。 Client: 允许分析和AI/ML应用程序与Alluxio连接和交互，它发起与Master的通信，执行元数据操作，并从Worker读取和写入存储在Alluxio中的数据。它提供了Java的本机文件系统API，支持多种客户端语言包括REST，Go，Python等，而且还兼容HDFS和Amazon S3的API。可以把Client理解为一个库，它实现了文件系统的接口，根据用户请求调用Alluxio服务，客户端被编译为alluxio-2.0.1-client.jar文件，它应当位于JVM类路径上，才能正常运行。当Client和Worker在同一节点时，客户端对本地缓存数据的读写请求可以绕过RPC接口，使本地文件系统可以直接访问Worker所管理的数据，这种情况被称为短路写，速度比较快，如果该节点没有Worker在运行，则Client的读写需要通过网络访问其他节点上的Worker，速度受网络宽带的限制。 Alluxio读写场景与参数 Alluxio读场景与性能分析: 命中本地Worker Client向Master检索存储该数据的Worker位置 如果本地存有该数据，则”短路读”,避免网络传输 短路读提供内存级别访问速度，是Alluxio最高读性能的方式 命中远程Worker Client请求的数据不在本地Worker则Client将从远程Worker读取数据 远程Worker将数据返回本地Worker并写一个本地副本，请求频繁的数据会有更多副本，从而实现热度优化计算的本地性，也可选NO_CACHE读取方式禁用本地副本写入 远程缓存命中，读取速度受网络速度限制 未命中Worker Alluxio任何一个Worker没有缓存所需数据，则Client把请求委托给本地Worker从底层存储系统(UFS)读取，缓存未命中的情况下延迟较高 Alluxio 1.7前Worker从底层读取完整数据块缓存下来并返回给Client，1.7版本后支持异步缓存，Client读取，Worker缓存，不需要等待缓存完成即可返回结果 指定NO_CACHE读取方式则禁用本地缓存 Alluxio写场景与性能分析: 仅写缓存 写入类型通过alluxio.user.file.writetype.default来设置，MUST_CACHE仅写本地缓存而不写入UFS 如果”短路写”可用，则直接写本地Worker避免网络传输，性能最高3.如果无本地Worker，即”短路写”不可用，数据写入远端Worker，写速度受限于网络IO 数据没有持久化，机器崩溃或需要释放数据用于较新的写入时，数据可能丢失 同步写缓存和持久化存储 alluxio.user.file.writetype.default=CACHE_THROUGH，同步写入Worker和UFS 速度比仅写缓存的方式慢很多，需要数据持久化时使用 仅写持久化存储 alluxio.user.file.writetype.default=THROUGH，只将数据写入UFS，不会创建Alluxio缓存中的副本 输入数据重要但不立刻使用的情况下使用该方式 异步写持久化存储(目前2.0.1为实验性) alluxio.user.file.writetype.default=ASYNC_THROUGH 可以以内存的速度写入Alluxio Worker，并异步完成持久化 实验性功能-如果异步持久化到底层存储前机器崩溃，数据丢失，异步写机制要求文件所有块都在同一个Worker中 Alluxio读写参数总结 写参数: alluxio.user.file.writetype.default CACHE_THROUGH:数据被同步写入AlluxioWorker和底层存储 MUST_CACHE:数据被同步写入AlluxioWorker,不写底层存储 THROUGH:数据只写底层存储,不写入AlluxioWorker ASYNC_THROUGH:数据同步写入AlluxioWorker并异步写底层存储(速度快) 读参数: alluxio.user.file.readtype.default CACHE_PROMOTE:数据在Worker上,则被移动到Worker的最高层,否则创建副本到本地Worker CACHE:数据不在本地Worker中时直接创建副本到本地Worker NO_CACHE:仅读数据,不写副本到Worker 是否缓存全部数据块: alluxio.user.file.cache.partially.read.block (v1.7以前,V1.7以后采取异步缓存策略) false读多少缓存多少,一个数据块只有完全被读取时，才能被缓存 true读部分缓存全部,没有完全读取的数据块也会被全部存到Alluxio内 Worker写文件数据块的数据分布策略: alluxio.user.block.write.location.policy.class LocalFirstPolicy (alluxio.client.block.policy.LocalFirstPolicy) 默认值,首先返回本地主机，如果本地worker没有足够的块容量，它从活动worker列表中随机选择一名worker。 MostAvailableFirstPolicy (alluxio.client.block.policy.MostAvailableFirstPolicy) 返回具有最多可用字节的worker。 RoundRobinPolicy (alluxio.client.block.policy.RoundRobinPolicy) 以循环方式选择下一个worker，跳过没有足够容量的worker。 SpecificHostPolicy (alluxio.client.block.policy.SpecificHostPolicy) 返回具有指定主机名的worker。此策略不能设置为默认策略。 目前有六种策略,详见配置项列表Alluxio的分层存储概念: Alluxio workers节点使用包括内存在内的本地存储来充当分布式缓冲缓存区,可以很大程度上改善I/O性能。每个Alluxio节点管理的存储数量和类型由用户配置,Alluxio还支持层次化存储,让数据存储获得类似于L1/L2 cpu缓存的优化。单层存储设置(推荐): 默认使用两个参数alluxio.worker.memory.size=16GB + alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk来设置Alluxio Worker的缓存大小 也可以单层多个存储介质并指定每个介质可用空间大小alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd + alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB alluxio.worker.memory.size和alluxio.worker.tieredstore.level0.dirs.quota的区别-&gt;ramdisk的大小默认由前者决定,后者可以决定除内存外的其他介质如ssd和hdd的大小 多层存储设置: 多层存储的配置-使用两层存储MEM和HDD alluxio.worker.tieredstore.levels=2 # 最大存储级数 在Alluxio中配置了两级存储 alluxio.worker.tieredstore.level0.alias=MEM # alluxio.worker.tieredstore.level0.alias=MEM 配置了首层(顶层)是内存存储层 alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # 设置了ramdisk的配额是100GB alluxio.worker.tieredstore.level0.dirs.quota=100GB alluxio.worker.tieredstore.level0.watermark.high.ratio=0.9 # 回收策略的高水位 alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 alluxio.worker.tieredstore.level1.alias=HDD # 配置了第二层是硬盘层 alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3 # 定义了第二层3个文件路径各自的配额 alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB alluxio.worker.tieredstore.level1.watermark.high.ratio=0.9 alluxio.worker.tieredstore.level1.watermark.low.ratio=0.7 写数据默认写入顶层存储,也可以指定写数据的默认层级 alluxio.user.file.write.tier.default 默认0最顶层,1表示第二层,-1倒数第一层 Alluxio收到写请求,直接把数据写入有足够缓存的层,如果缓存全满,则置换掉底层的一个Block. Alluxio缓存回收策略缓存回收: Alluxio中的数据是动态变化的,存储空间不足时会为新数据腾出空间 异步缓存回收与同步缓存回收 alluxio.worker.tieredstore.reserver.enabled=true (默认异步回收) 在读写缓存工作负载较高的情况下异步回收可以提升性能 alluxio.worker.tieredstore.reserver.enabled=false (同步回收) 请求所用空间比Worker上请求空间更多时,同步回收可以最大化Alluxio空间利用率,同步回收建议使用小数据块配置(64-128MB)来降低回收延迟 缓存回收中空间预留器的水位(阈值) Worker存储利用率达到高水位时,基于回收策略回收Worker缓存直到达到配置的低水位 高水位: alluxio.worker.tieredstore.level0.watermark.high.ratio=0.95 (默认95%) 低水位: alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 (默认70%) 比如配置了32GB(MEM)+100GB(SSD)=132GB的Worker内存,当内存达到高水位132x0.95=125.4GB时开始回收缓存,直到到达低水位132x0.7=92.4GB时停止回收缓存 自定义回收策略 alluxio.worker.allocator.class=alluxio.worker.block.allocator.MaxFreeAllocator (Alluxio中新数据块分配策略的类名) alluxio.worker.evictor.class=alluxio.worker.block.evictor.LRUEvictor (当存储层空间用尽时块回收策略的类名) 贪心回收策略: 回收任意数据块直到释放出所需空间 LRU回收策略: 回收最近最少使用数据块直到释放出所需空间 部分LRU回收策略: 在最大剩余空间的目录回收最近最少使用数据块 LRFU回收策略: 基于权重分配的最近最少使用和最不经常使用策略回收数据块,如果权重完全偏向最近最少使用,则LRFU变为LRU Alluxio异步缓存策略 Alluxio v1.7以后支持异步缓存 异步缓存是将Alluxio的缓存开销由客户端转移到Worker上,第一次读数据时,在不设置读属性为NO_CACHE的情况下Client只负责从底层存储读数据,然后缓存任务由Worker来执行,对Client读性能没有影响,也不需要像V1.7版本前那样设置alluxio.user.file.cache.partially.read.block来决定缓存部分或全部数据,而且Worker内部也在Client读取底层存储系统的数据方面做了优化,设置读属性为CACHE的情况下: Client顺序读完整数据块时Worker顺便缓存完整数据块 Client只读部分数据或非顺序读数据时Worker不会读取时顺便缓存,等客户端读取完以后再向Worker系欸点发送异步缓存命令,Worker节点再从底层存储中获取完整的块 异步缓存使得第一次从Alluxio读取和直接从底层存储读取花费相同时间,且数据异步缓存到Alluxio中,提高集群整体性能 异步缓存参数调整 Worker在异步缓存的同时也响应Client读取请求,可通过设置Worker端的线程池大小来加快异步缓存的速度 alluxio.worker.network.netty.async.cache.manager.threads.max 指定Worker线程池大小,该属性默认为8,表示最多同时用八核从其他Worker或底层存储读数据并缓存,提高此值可以加快后台异步缓存的速度,但会增加CPU使用率 Alluxio元数据 Alluxio元数据的存储在Alluxio新的2.x版本中，对元数据存储做了优化，使其能应对数以亿级的元数据存储。首先，文件系统是INode-Tree组成的，即文件目录树，Alluxio Master管理多个底层存储系统的元数据，每个文件目录都是INode-Tree的节点，在Java对象中，可能一个目录信息本身占用空间不大，但映射在JavaHeap内存中，算上附加信息，每个文件大概要有1KB左右的元数据，如果有十亿个文件和路径，则要有约1TB的堆内存来存储元数据，完全是不现实的。所以，为了方便管理元数据，减小因为元数据过多对Master性能造成的影响，Alluxio的元数据通过RocksDB键值数据库来管理元数据，Master会Cache常用数据的元数据，而大部分元数据则存在RocksDB中，这样大大减小了Master Heap的压力，降低OOM可能性，使Alluxio可以同时管理多个存储系统的元数据。通过RocksDB的行锁，也可以方便高并发的操作Alluxio元数据。高可用过程中，INode-Tree是进程中的资源，不共享，如果ActiveMaster挂掉，StandByMaster节点可以从Journal持久日志（位于持久化存储中如HDFS）恢复状态。这样会依赖持久存储（如HDFS）的健康状况，如果持久存储服务宕机，Journal日志也不能写，Alluxio高可用服务就会受到影响。所以，Alluxio通过Raft算法保证元数据的完整性，即使宕机，也不会丢失已经提交的元数据。 Alluxio元数据一致性 Alluxio读取磁层存储系统的元数据,包括文件名,文件大小,创建者,组别,目录结构等 如果绕过Alluxio修改底层存储系统的目录结构,Alluxio会同步更新alluxio.user.file.metadata.sync.interval=-1 Alluxio不主动同步底层存储元数据alluxio.user.file.metadata.sync.interval=正整数 正整数指定了时间窗口,该时间窗口内不触发元数据同步alluxio.user.file.metadata.sync.interval=0 时间窗口为0,每次读取都触发元数据同步时间窗口越大,同步元数据频率越低,Alluxio Master性能受影响越小 Alluxio不加载具体数据,只加载元数据,若要加载文件数据,可以通过load命令或FileStream API 在Alluxio中创建文件或文件夹时可以指定是否持久化alluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH mkdir /xxxalluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH touch /xxx/xx Alluxio RPCAlluxio 1.x中 Master RPC using Thrift（元数据操作） Workers RPC using Netty（数据操作）而新的Alluxio 2.x中 使用gRPC保证高吞吐，方便代码维护 Alluxio的Metrics度量指标信息可以让用户深入了解集群上运行的任务,是监控和调试的宝贵资源。Alluxio的度量指标信息被分配到各种相关Alluxio组件的实例中。每个实例中，用户可以配置一组度量指标槽，来决定报告哪些度量指标信息。现支持Master进程,Worker进程和Client进程的度量指标 。 度量指标的sink参数为alluxio.metrics.sink.xxxConsoleSink: 输出控制台的度量值。CsvSink: 每隔一段时间将度量指标信息导出到CSV文件中。JmxSink: 查看JMX控制台中注册的度量信息。GraphiteSink: 给Graphite服务器发送度量信息。MetricsServlet: 添加Web UI中的servlet，作为JSON数据来为度量指标数据服务。 可选度量的配置 Master的Metrics 配置方法 master.* 例如:master.CapacityTotal常规信息CapacityTotal: 文件系统总容量（以字节为单位）。CapacityUsed: 文件系统中已使用的容量（以字节为单位）。CapacityFree: 文件系统中未使用的容量（以字节为单位）。PathsTotal: 文件系统中文件和目录的数目。UnderFsCapacityTotal: 底层文件系统总容量（以字节为单位）。UnderFsCapacityUsed: 底层文件系统中已使用的容量（以字节为单位）。UnderFsCapacityFree: 底层文件系统中未使用的容量（以字节为单位）。Workers: Worker的数目。逻辑操作DirectoriesCreated: 创建的目录数目。FileBlockInfosGot: 被检索的文件块数目。FileInfosGot: 被检索的文件数目。FilesCompleted: 完成的文件数目。FilesCreated: 创建的文件数目。FilesFreed: 释放掉的文件数目。FilesPersisted: 持久化的文件数目。FilesPinned: 被固定的文件数目。NewBlocksGot: 获得的新数据块数目。PathsDeleted: 删除的文件和目录数目。PathsMounted: 挂载的路径数目。PathsRenamed: 重命名的文件和目录数目。PathsUnmounted: 未被挂载的路径数目。RPC调用CompleteFileOps: CompleteFile操作的数目。CreateDirectoryOps: CreateDirectory操作的数目。CreateFileOps: CreateFile操作的数目。DeletePathOps: DeletePath操作的数目。FreeFileOps: FreeFile操作的数目。GetFileBlockInfoOps: GetFileBlockInfo操作的数目。GetFileInfoOps: GetFileInfo操作的数目。GetNewBlockOps: GetNewBlock操作的数目。MountOps: Mount操作的数目。RenamePathOps: RenamePath操作的数目。SetStateOps: SetState操作的数目。UnmountOps: Unmount操作的数目。 Worker的Metrics 配置方法 192_168_1_1.* 例如:192_168_1_1.CapacityTotal常规信息CapacityTotal: 该Worker的总容量（以字节为单位）。CapacityUsed: 该Worker已使用的容量（以字节为单位）。CapacityFree: 该Worker未使用的容量（以字节为单位）。逻辑操作BlocksAccessed: 访问的数据块数目。BlocksCached: 被缓存的数据块数目。BlocksCanceled: 被取消的数据块数目。BlocksDeleted: 被删除的数据块数目。BlocksEvicted: 被替换的数据块数目。BlocksPromoted: 被提升到内存的数据块数目。BytesReadAlluxio: 通过该worker从Alluxio存储读取的数据量，单位为byte。其中不包括UFS读。BytesWrittenAlluxio: 通过该worker写到Alluxio存储的数据量，单位为byte。其中不包括UTF写。BytesReadUfs-UFS:${UFS}: 通过该worker从指定UFS读取的数据量，单位为byte。BytesWrittenUfs-UFS:${UFS}: 通过该worker写到指定UFS的数据量，单位为byte。 Client的Metrics 配置方法 client.* 例如:clien.BytesReadRemote常规信息NettyConnectionOpen: 当前Netty网络连接的数目。逻辑操作BytesReadRemote: 远程读取的字节数目。BytesWrittenRemote: 远程写入的字节数目。BytesReadUfs: 从ufs中读取的字节数目。BytesWrittenUfs: 写入ufs的字节数目。 配置示例 vim metrics.properties # List of available sinks and their properties. alluxio.metrics.sink.ConsoleSink alluxio.metrics.sink.CsvSink alluxio.metrics.sink.JmxSink alluxio.metrics.sink.MetricsServlet alluxio.metrics.sink.PrometheusMetricsServlet alluxio.metrics.sink.GraphiteSink master.GetFileBlockInfoOps master.GetNewBlockOps master.FreeFileOps 192_168_1_101.BytesReadAlluxio 192_168_1_101.BytesWrittenAlluxio 192_168_1_101.BlocksAccessed 192_168_1_101.BlocksCached 192_168_1_101.BlocksCanceled 192_168_1_101.BlocksDeleted 192_168_1_101.BlocksEvicted 192_168_1_101.BlocksPromoted 192_168_1_102.BytesReadAlluxio 192_168_1_102.BytesWrittenAlluxio 192_168_1_102.BlocksAccessed 192_168_1_102.BlocksCached 192_168_1_102.BlocksCanceled 192_168_1_102.BlocksDeleted 192_168_1_102.BlocksEvicted 192_168_1_102.BlocksPromoted 192_168_1_103.BytesReadAlluxio 192_168_1_103.BytesWrittenAlluxio 192_168_1_103.BlocksAccessed 192_168_1_103.BlocksCached 192_168_1_103.BlocksCanceled 192_168_1_103.BlocksDeleted 192_168_1_103.BlocksEvicted 192_168_1_103.BlocksPromoted 然后访问 http://192.168.1.101:19999/metrics/json/ 可得到监控信息喜欢看源码的小伙伴可以戳这里哟-&gt;Alluxio源码入口 Alluixo审计日志Alluxio提供审计日志来方便管理员可以追踪用户对元数据的访问操作。开启审计日志： 讲JVM参数alluxio.master.audit.logging.enabled设为true审计日志包含如下条目： key desc succeeded 如果命令成功运行，值为true。在命令成功运行前，该命令必须是被允许的。 allowed 如果命令是被允许的，值为true。即使一条命令是被允许的它也可能运行失败。 ugi 用户组信息，包括用户名，主要组，认证类型。 ip 客户端IP地址。 cmd 用户运行的命令。 src 源文件或目录地址。 dst 目标文件或目录的地址。如果不适用，值为空。 perm user:group:mask，如果不适用值为空。 Alluxio安装和部署准备工作1.下载Alluxio压缩包并上传到NN所在集群2.解压并进入安装目录 tar -zxvf alluxio-2.0.1-bin.tar.gz -C /opt/module/ mv /opt/module/alluxio-2.0.1 /opt/module/alluxio cd /opt/module/alluxio cp conf/alluxio-site.properties.template conf/alluxio-site.properties cp conf/alluxio-env.sh.template conf/alluxio-env.sh 常规集群参数配置常规非高可用集群配置，针对1.x和2.x版本通用conf/alluxio-env.sh vim conf/alluxio-env.sh ALLUXIO_HOME=/opt/module/alluxio-2.0.1 ALLUXIO_LOGS_DIR=/opt/module/alluxio-2.1.0/logs ALLUXIO_MASTER_HOSTNAME=hadoop101 ALLUXIO_RAM_FOLDER=/mnt/ramdisk ALLUXIO_UNDERFS_ADDRESS=hdfs://hadoop101:9000/alluxio ALLUXIO_WORKER_MEMORY_SIZE=512MB JAVA_HOME=/opt/module/jdk1.8.0_161 # 设置ALLUXIO_MASTER_JAVA_OPTS作用于master JVM # 设置ALLUXIO_WORKER_JAVA_OPTS作用于worker JVM # 以及ALLUXIO_JAVA_OPTS同时作用于master以及worker JVM # 增加worker JVM GC事件的logging, 输出写至worker节点的logs/worker.out文件中 ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # 设置master JVM的的heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; conf/alluxio-site.properties vim conf/alluxio-site.properties # Common properties alluxio.master.hostname=hadoop101 alluxio.master.mount.table.root.ufs=hdfs://192.168.1.101:9000/alluxio alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk vim conf/masters hadoop101 vim conf/workers hadoop102 hadoop103 scp -r /opt/module/alluxio/ root@hadoop102:/opt/module/ scp -r /opt/module/alluxio/ root@hadoop103:/opt/module/ # 打开Alluxio服务 alluxio format alluxio-start.sh master alluxio-start.sh workers NoMount 或直接 alluxio-start.sh all 访问Master节点的WEB UI: hadoop101:19999 访问Worker节点的WEB UI: hadoop102:30000 #测试部署是否成功 bin/alluxio runTests # 如果出现Passed the test则说明部署成功 bin/alluxio-stop.sh all # 关闭集群 出现类似以下界面即为部署成功此时可以通过命令alluxio fsdamin report来查看集群状态 高可用集群参数配置高可用(HA)通过支持同时运行多个master来保证服务的高可用性，多个master中有一个master被选为primary master作为所有worker和client的通信首选，其余master为备选状态(StandBy)，它们通过和primary master共享日志来维护同样的文件系统元数据，并在primary master失效时迅速接替其工作(master主从切换过程中，客户端可能会出现短暂的延迟或瞬态错误)搭建高可用集群前的准备:①确保Zookeeper服务已经运行②一个单独安装的可靠的共享日志存储系统(可用HDFS或S3等系统)③这个配置针对Alluxio 2.x版本，不适用于1.x版本④需要事先创建好ramdisk挂载目录 注意去掉中文注释 否则会报错 在所有机器上配置env.sh vim alluxio-env.sh ALLUXIO_HOME=/opt/alluxio ALLUXIO_LOGS_DIR=/opt/alluxio/logs ALLUXIO_RAM_FOLDER=/mnt/ramdisk JAVA_HOME=/opt/module/jdk1.8.0_161 # 设置ALLUXIO_MASTER_JAVA_OPTS作用于master JVM # 设置ALLUXIO_WORKER_JAVA_OPTS作用于worker JVM # 以及ALLUXIO_JAVA_OPTS同时作用于master以及worker JVM # 增加worker JVM GC事件的logging, 输出写至worker节点的logs/worker.out文件中 ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # 设置master JVM的的heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; 在101机器上配置Master和Worker vim alluxio-site.properties # 192.168.1.101 Master Worker # Common properties alluxio.master.hostname=192.168.1.101 # 要写其他机器能识别的地址而非localhost等 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # 如果底层HDFS存储为高可用，则要写hdfs配置文件地址 alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # 指向高可用或非高可用的HDFS地址（可以是根目录，也可以是某个文件夹） # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 # Zookeeper地址中间逗号隔开 alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal # 回滚日志的地址，写入可靠的分布式HDFS alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* # 可以模拟很多用户来实现权限控制 alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在102机器上配置Master和Worker vim alluxio-site.properties # 192.168.1.102 Master Worker # Common properties alluxio.master.hostname=192.168.1.102 # 要写其他机器能识别的地址而非localhost等 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在103机器上配置Worker vim alluxio-site.properties # 192.168.1.103 Worker # Common properties # Worker不需要写alluxio.master.hostname参数和alluxio.master.journal.folder参数 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在所有机器上指定Master和Worker节点 vim masters 192.168.1.101 192.168.1.102 vim workers 192.168.1.101 192.168.1.102 192.168.1.103 # 测试部署是否成功 alluxio format alluxio-start.sh all SudoMount alluxio fsadmin report alluxio runTests # 如果出现Passed the test则说明部署成功 # 测试高可用模式的自动故障处理: (假设此时hadoop101位primary master) ssh hadoop101 jps | grep AlluxioMaster kill -9 &lt;AlluxioMaster PID&gt; alluxio fs leader # 显示新的primary Master(可能需要等待一小段时间选举) 部署说明 Alluxio可以像CM一样，部署在同一网络中的节点上且不需要机器间免密登陆。免密登陆只是为了方便使用start-all.sh脚本一键启动。非免密登陆的集群可以使用Ansible自动化运维工具对每个节点执行启动和挂载等操作(在每个Master上使用部署Alluxio的用户分别执行alluxio-start.sh master,然后如果使用非root用户启动Alluxio服务，则要在每个worker的root用户执行alluxio-mount.sh Mount local ,然后用部署Alluxio的用户执行alluxio-start.sh worker,并在所有节点alluxio-start.sh job_master,alluxio-start.sh job_worker即可)，作用等同于start-all.sh脚本，不会对Alluxio服务的运行造成影响。 Mount和SudoMount需要在root权限下执行，因为只有root用户有权限创建和访问RamFS，启动Alluxio的用户要有这个RamFS的读写执行权限，Alluxio的RAM FLODER（ramdisk）可以理解为是在普通HDD磁盘目录上挂载的一个RamFS文件系统，RamFS是把系统的RAM作为存储，且RamFS不会使用swap交换内存分区，Linux会把RamFS视为一个磁盘文件目录。 查看RamFS的方法： mount | grep -E “(tmpfs|ramfs)” 这里的tmpFS也是基于内存的存储系统，但它会使用到Swap分区，使读写效率降低，Alluxio也可以使用tmpFS作为缓存。 了解更多:ramfs和tmpfs的区别 Alluxio的”/“目录权限由启动Mater和Worker的用户决定，并与UFS中对应的文件夹权限一致，可以修改Alluxio根目录权限，Alluxio创建文件和文件夹的用户和组与Linux用户合组一致，并且与持久化到HDFS的文件的用户和组一致。 Mount|SudoMount|Umount|SudoUmount说一下这四个参数，Mount和SudoMount是挂载RamFS，后者带sudo权限，Umount和SudoUmount是卸载RamFS，后者带sudo权限。Mount和SudoMount会格式化已存在的RamFS。5.关于用户模拟的一些理解和使用很重要参考这篇文章：User Impersonation相关配置问题分析与解决 Alluxio部署前，要决定用哪个用户启动Alluxio，如果底层存储是HDFS，建议使用启动NameNode进程的用户来启动Alluxio Master和Workers,保证HDFS权限映射：Alluxio On HDFS Mount参数一般只在Worker节点使用 可以在HDFS建立一个777权限的文件路径作为Alluxio的底层存储 job_master和job_worker官网没做介绍，但在当前版本这两个组件必须启动，否则会影响persist功能以及一些其他功能(我目前只知道persist会Time Out) 配置这块踩了好多坑，终于，Alluxio基本服务部署完毕,一些关于优化和细节的参数在Alluxio原理部分中涉及到,也可查阅Alluxio配置参数大全 Alluxio2.1.0版本官方介绍说使用ASYNC_THROUGH进行写入时防止数据丢失，所以我这里设置了ASYNC_THROUGH异步写磁盘，既能保证写入速度，又能将文件持久化之前配置Alluxio高可用，一直不稳定，心跳中断，Master和Worker掉线问题频发，Alluxio2.1版本官方说修复了各种心跳中断问题,当然Alluxio的高可用要求底层的Journal日志存储系统的稳定性很高，如果底层Journal存储系统不稳定（比如HDFS No More Good DataNode的情况），就会导致Master崩溃。 Alluxio常用命令Alluxio命令速查表包括缓存载入,驻留,释放,数据生存时间等重要命令Alluxio常用Shell命令速查表: #文件基本操作 可以在执行命令时指定参数 方法: alluxio fs -D...指定参数 copyFromLocal .... alluxio fs cat &lt;path/file&gt; # 打开文件 alluxio fs ls [-d|-f|-p|-R|-h|--sort=option|-r] &lt;path&gt; #查看目录 alluxio fs copyFromLocal [--thread &lt;num&gt;] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;remoteDst&gt; # 从本地上传文件到Alluxio alluxio fs copyToLocal [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;localDst&gt; #从Alluxio载文件到本地 alluxio fs count &lt;path&gt; # 统计Alluxio目录的文件数,文件夹数和总大小 alluxio fs du [-h|-s|--memory] &lt;path&gt; # 文件大小 alluxio fs cp [-R] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;dst&gt; #复制文件 alluxio fs mv &lt;src&gt; &lt;dst&gt; # 移动文件 alluxio fs rm [-R] [-U] [--alluxioOnly] &lt;path&gt; # 删除文件 alluxio fs mkdir &lt;path1&gt; [path2] ... [pathn] # 创建文件夹 alluxio fs touch &lt;path&gt; #创建一个空文件 alluxio fs setTtl [--action delete|free] &lt;path&gt; &lt;time to live&gt; # 设置一个文件的TTL时间 alluxio fs unsetTtl &lt;path&gt; # 删除文件的TTL值 alluxio fs checksum &lt;Alluxio path&gt; # 得到文件的MD5值 alluxio fs stat &lt;path&gt; # 显示文件路径信息 alluxio fs tail &lt;path&gt; # 显示文件最后1KB的内容 alluxio fs location &lt;path&gt; # 输出包含某个文件数据的主机,使用location命令可以调试数据局部性 alluxio fs help &lt;command&gt; # 查看命令介绍和用法 alluxio fs distributedMv &lt;src&gt; &lt;dst&gt; # 并行移动文件或目录 alluxio fs distributedCp &lt;src&gt; &lt;dst&gt; # 并行复制文件或目录 alluxio fs distributedLoad [--replication &lt;num&gt;] &lt;path&gt; # 在alluxio空间中加载文件或目录，使其驻留在内存中 #与底层存储的交互操作 alluxio fs load [--local] &lt;path&gt; # load命令可为数据分析编排数据,加快数据分析的效率,load命令将底层文件系统中的数据载入到Alluxio中,如果运行该命令的机器上正在运行一个Alluxio worker,那么数据将移动到该worker上,否则数据会被随机移动到一个worker上。 如果该文件已经存在在Alluxio中,设置了--local选项,并且有本地worker,则数据将移动到该worker上。否则该命令不进行任何操作。如果该命令的目标是一个文件夹,那么其子文件和子文件夹会被递归载入。 alluxio fs persist [-p|--parallelism &lt;#&gt;] [-t|--timeout &lt;milliseconds&gt;] [-w|--wait &lt;milliseconds&gt;] &lt;path&gt; [&lt;path&gt; ...] # 持久化Alluxio中的数据到底层存储 alluxio fs checkConsistency [-r] [-t|--threads &lt;threads&gt;] &lt;Alluxio path&gt; # 检查Alluxio与底层存储系统的元数据一致性(确定文件在底层存储还是在under storage system.) alluxio fs free -f &lt;&gt; # 已经持久化到底层存储,但内存中还保留着的文件可以通过free从内存中释放,未被持久化的文件不能被free alluxio fs mount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; &lt;ufsURI&gt;] # 将底层文件系统的&quot;ufsURI&quot;路径挂载到Alluxio命名空间中的&quot;alluxioPath&quot;路径下，&quot;path&quot;路径事先不能存在并由该命令生成。 没有任何数据或者元数据从底层文件系统加载。当挂载完成后，对该挂载路径下的操作会同时作用于底层文件系统的挂载点。monut命令可以挂载linux服务器上的某个文件夹alluxio fs mount /demo file:///tmp/alluxio-demo alluxio fs unmount &lt;alluxioPath&gt; # 取消挂载 alluxio fs updateMount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; # 保留元数据的同时更改挂载点设置 alluxio fs pin &lt;path&gt; media1 media2 media3 ... 如果管理员对作业运行流程十分清楚，那么可以使用pin命令手动提高性能。pin命令对Alluxio中的文件或文件夹进行标记。该命令只针对元数据进行操作，不会导致任何数据被加载到Alluxio中。如果一个文件在Alluxio中被标记了，该文件的任何数据块都不会从Alluxio worker中被剔除。如果存在过多的被锁定的文件，Alluxio worker将会剩余少量存储空间，从而导致无法对其他文件进行缓存。 alluxio fs unpin &lt;path&gt; # 将Alluxio中的文件或文件夹解除标记。该命令仅作用于元数据，不会剔除或者删除任何数据块。一旦文件被解除锁定，Alluxio worker可以剔除该文件的数据块。 alluxio fs startSync &lt;path&gt; # 启动指定路径的自动同步进程 alluxio fs stopSync &lt;path&gt; # 关闭指定路径的自动同步进程 alluxio fs setReplication [-R] [--max &lt;num&gt; | --min &lt;num&gt;] &lt;path&gt; # 设置给定路径或文件的最大/最小副本数 (-1表示不限制最大副本数) -R递归 #权限相关操作及管理员命令 alluxio fs chgrp [-R] &lt;group&gt; &lt;path&gt; # 换组 alluxio fs chmod [-R] &lt;mode&gt; &lt;path&gt; # 更改读写执行等权限 alluxio fs chown [-R] &lt;owner&gt;[:&lt;group&gt;] &lt;path&gt; # 所有者 alluxio fsadmin backup [directory] [--local] # 备份Alluxio的元数据到备份目录(默认目录由alluxio.master.backup.directory决定) alluxio fsadmin doctor [category] # 显示错误和警告 alluxio fsadmin report [category] [category args] # 报告运行集群的信息 alluxio fsadmin ufs --mode &lt;noAccess/readOnly/readWrite&gt; &quot;ufsPath&quot; # 更新挂载的底层存储系统的属性 alluxio formatMaster 初始化Master元数据 alluxio formatWorker 初始化Worker数据，Worker数据会被清空 alluxio getConf [key] 查看各个组件的参数和配置 key:[--master / --source] alluxio runJournalCrashTest 测试Alluxio 高可用日志系统（会停止服务一段时间） alluxio runUfsTests --path &lt;ufs path&gt; alluxio validateConf 使修改的配置生效 alluxio validateEnv &lt;args&gt; 使运行环境生效 alluxio copyDir &lt;PATH&gt; 类似于xsync脚本，可以向各个节点分发文件 #集群相关信息 alluxio fs masterInfo # 获得master节点的信息 alluxio fs leader # 打印当前Alluxio的leader master节点主机名。 alluxio fs getCapacityBytes # 获取Alluxio总容量 alluxio fs getSyncPathList # 获取同步路径列表 alluxio fs getUsedBytes # 获取已用空间大小 alluxio fs getfacl &lt;path&gt; # 显示访问控制列表(ACLs) alluxio fs setfacl [-d] [-R] [--set | -m | -x &lt;acl_entries&gt; &lt;path&gt;] | [-b | -k &lt;path&gt;] # 设置访问控制列表(ACLs) 上面的命令不能帮到你? 那就戳这里:Alluxio命令使用示例管理员命令使用示例 Alluxio WEB UI介绍及使用Alluxio master提供了Web界面以便用户管理Alluxio master Web界面的默认端口是19999:访问 http://MASTER IP:19999 即可查看Alluxio worker Web界面的默认端口是30000:访问 http://WORKER IP:30000 即可查看WEB UI官方介绍很明确-&gt;戳这里:Alluxio Web UI Alluxio与计算框架整合 Alluxio+Hive频繁使用的表存在Alluxio上，可通过内存读文件获得更高的吞吐量和更低的延迟 准备工作:cd /opt/module/hive vim conf/hive-env.sh export HADOOP_HOME=/opt/module/hadoop-2.7.2 # 添加 export HIVE_AUX_JARS_PATH=$ALLUXIO_HOME/client:$HIVE_AUX_JARS_PATH 四种情况: 创建一个Hive表并指定其存储在Alluxio bin/hive create table alluxio_test( id int, name string, color string ) row format delimited fields terminated by &#39;\\t&#39; LOCATION &quot;alluxio://hadoop101:19998/user/hive/warehouse/alluxio_test&quot;; # 查看表位置 describe extended alluxio_test; 已存在HDFS的内部表 bin/hive describe extended table_name; # 查看Hive表存储位置 alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; msck repair table table_name; # 确定alluxio对应位置存在表数据后修复Hive表元数据 第一次访问alluxio中的文件默认会被认为访问hdfs的文件，一旦数据被缓存在Alluxio中，之后的查询数据都会从Alluxio读取。 已存在HDFS的外部表 bin/hive describe extended table_name; # 将表数据改为Alluxio存储 alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; # 还原表数据到HDFS alter table table_name set location &quot;hdfs://hadoop101:9000/user/hive/warehouse/table_name&quot; describe extended table_name; Hive使用Alluxio作为默认存储系统 vim conf/hive-site.xml # 添加以下属性 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;alluxio://hadoop101:19998&lt;/value&gt; &lt;description&gt;Hive Use Alluxio As Default FileSystem&lt;/description&gt; &lt;/property&gt; # 对Hive指定的Alluxio配置属性，将它们添加到每个结点的Hadoop配置目录下core-site.xml中。例如，将alluxio.user.file.writetype.default 属性由默认的MUST_CACHE修改成CACHE_THROUGH: &lt;property&gt; &lt;name&gt;alluxio.user.file.writetype.default&lt;/name&gt; &lt;value&gt;CACHE_THROUGH&lt;/value&gt; &lt;/property&gt; # Alluxio中为Hive创建目录 alluxio fs mkdir /tmp alluxio fs mkdir /user/hive/warehouse alluxio fs chmod 775 /tmp alluxio fs chmod 775 /user/hive/warehouse # 检查Hive与Alluxio的集成情况 integration/checker/bin/alluxio-checker.sh -h # 查看该命令帮助 integration/checker/bin/alluxio-checker.sh hive -hiveurl [HIVE_URL] 注:CM集群设置Hive连接Alluxio Client的方式: 排坑: 安全认证问题: alluxio-site.properties中添加要模拟的用户: alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* Alluxio+SparkSpark可以在进行简单配置后直接使用Alluxio作为数据访问层，Spark应用程序可以通过Alluxio透明地访问许多不同类型的持久化存储服务（例如，AWS S3 bucket、Azure Object Store buckets、远程部署的 HDFS 等）的数据，也可以透明地访问同一类型持久化存储服务不同实例中的数据。为了加快I/O性能，用户可以主动获取数据到Alluxio中或将数据透明地缓存到Alluxio中，尤其是在Spark部署位置与数据相距较远时特别有效。此外，通过将计算和物理存储解耦，Alluxio 能够有助于简化系统架构。当底层持久化存储中真实数据的路径对 Spark 隐藏时，对底层存储的更改可以独立于应用程序逻辑；同时Alluxio作为邻近计算的缓存，仍然可以给计算框架提供类似 Spark 数据本地性的特性。 配置参数配置（spark-defaults.conf中添加）spark.driver.extraClassPath /&lt;PATH_TO_ALLUXIO&gt;/client/alluxio-2.0.1-client.jarspark.executor.extraClassPath /&lt;PATH_TO_ALLUXIO&gt;/client/alluxio-2.0.1-client.jar或者Jar包拷贝cp client/alluxio-2.0.1-client.jar $SPARK_HOME/jars/如果高可用的Alluxio,还需在spark-default中指定: spark.driver.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true spark.executor.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true 或者配置Hadoop文件core-site.xml如下 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.address&lt;/name&gt; &lt;value&gt;zkHost1:2181,zkHost2:2181,zkHost3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 自定义Spark作业中Alluxio的属性：spark-submit…. –driver-java-options “-Dalluxio.user.file.writetype.default=CACHE_THROUGH” 而不是–conf val s = sc.textFile(&quot;alluxio://192.168.1.101:19998/LICENSE&quot;) val double = s.map(line =&gt; line + line) double.saveAsTextFile(&quot;alluxio://192.168.1.101:19998/out&quot;) df = spark.table(&quot;select ...&quot;) df.format.parquet(&quot;alluxio://xxxxx&quot;) 官方Alluxio+Spark配置设置 检查配置是否正确在$ALLUXIO_HOME运行 integration/checker/bin/alluxio-checker.sh spark spark://sparkMaster:7077 使用 存储 RDD 到 Alluxio 内存中就是将 RDD 作为文件保存到 Alluxio 中: saveAsTextFile：将 RDD 作为文本文件写入，其中每个元素都是文件中的一行 saveAsObjectFile：通过对每个元素使用 Java 序列化，将 RDD 写到一个文件中 // as text file rdd.saveAsTextFile(&quot;alluxio://localhost:19998/rdd1&quot;) rdd = sc.textFile(&quot;alluxio://localhost:19998/rdd1&quot;) // as object file rdd.saveAsObjectFile(&quot;alluxio://localhost:19998/rdd2&quot;) rdd = sc.objectFile(&quot;alluxio://localhost:19998/rdd2&quot;) 缓存 Dataframe 到 Alluxio 中(将 DataFrame 作为文件保存到 Alluxio 中): df.write.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) df = sqlContext.read.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) Alluxio对Shuffle的提升目前三种方案:一是基于Alluxio-Fuse客户端,无需修改源码,直接挂载Shuffle目录,但Alluxio-Fuse目前的性能不是很好二是重写Spark Shuffle Service底层源码实现基于Alluxio Client的Shuffle三是可以Splash Shuffle Manager插件,我的另一篇文章有讲到 -&gt; QCon总结-Splash Shuffle Manager当然也可以选择等Spark3.0的Remote Shuffle Service Alluxio+HadoopMR运行HadoopMR程序: bin/hadoop jar ../libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount -Dalluxio.user.file.writetype.default=CACHE_THROUGH -libjars /opt/module/alluxio/client/alluxio-2.0.1-client.jar \\&lt;INPUT FILES&gt; &lt;OUTPUT DIRECTORY&gt; Alluxio+Presto后续更新… 性能测试使用官方提供的沙箱申请官方测试沙箱Sandbox：ALLUXIO SANDBOX申请成功后，按照邮件的指引操作，注意，bin/sandbox setup &amp;的过程中千万不要Ctrl+C中止,部署完成状态如下图： 运行基准测试（TPC-DS），耐心等待后的测试结果：已安装TPC-DS基准套件，用于运行性能测试。Spark已安装为TPC-DS用来将其作业发送到的计算框架。TPC-DS的比例因子为100，这与26GB的数据集大小相关。由索引单独标识的基准按不同的使用方案分组，并且将结果报告为每个方案的汇总。其中 w/o是without，即只是用S3为直接底层存储的情况；w/是with，即使用了Alluxio作为中间件下的性能从图中测试结果可以看出,当计算数据存储在公有云虚拟机实例中时，Alluxio作为存储与计算框架的中间件，能够有1.5-3倍左右的性能提升受到各方面限制，以上测试结果并非Alluxio的最佳预期。其他人的试过程 自测Spark Sql做测试时候多次重复作业输入数据位于OS的高速缓冲区,Alluxio没有加速效果甚至变慢我的测试环境是三台机器,每台101GB内存,16核,同台机器部署CM Hadoop,Spark,Hive,AlluxioWorker,AlluxioClientAlluxio读参数CACHE_PROMOTE,写参数CACHE_THROUGH 测试方法 测试操作 运行时间(HDFS) 运行时间(Alluxio) 表结构 SparkSQL select count(1) from table; 4s 6s 13.5GB 17字段 SparkSQL select count(1) from table; 5s 6s 13.5GB 17字段 SparkSQL select count(1) from table; 6s 8s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 80s 80s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 77s 52s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 60s 73s 13.5GB 17字段 SparkSQL select count(1) from test.table group by language; 11.5s 11.5s 13.5GB 17字段 Spark Persist df.write.parquet(Path) 3.0min 4.0min 13.5GB 17字段 Spark Persist spark.read.parquet(Path).count() 4s 5s 13.5GB 17字段 Spark Persist spark.read.parquet(Path).count() 6s 6s 13.5GB 17字段 后来又做了Spark Dataframe的Persist到MEMORY_ONLY和Persist到Alluxio,效果也不是很好,究其原因,我认为是我的HDFS DataNode已经和计算框架Spark部署在一起了,而且磁盘IO没有瓶颈,所以这不符合Alluxio的应用场景,从而没有令人满意的效果.至于HDFS更快的原因,我想是Spark要读取的数据很可能已经存在OS的高速缓冲区Alluxio还是要用对场景才行. Alluxio FUSE什么是Alluxio FUSEAlluxio-FUSE可以在一台Unix机器上的本地文件系统中挂载一个Alluxio分布式文件系统。通过使用该特性，一些标准的命令行工具（例如ls、 cat以及echo）可以直接访问Alluxio分布式文件系统中的数据。此外更重要的是用不同语言实现的应用程序如C, C++, Python, Ruby, Perl, Java都可以通过标准的POSIX接口(例如open, write, read)来读写Alluxio，而不需要任何Alluxio的客户端整合与设置。 Alluxio FUSE局限性 文件只能顺序地一次写入,不能修改和覆盖,如果要修改就要删除原文件再创建 不支持soft-link和hard-link(即ln) alluxio.security.group.mapping.class选项设置为ShellBasedUnixGroupsMapping的值时,用户与分组信息才与Unix系统的用户分组对应 与直接使用Alluxio客户端相比，使用挂载文件系统的性能会相对较差 Alluxio FUSE使用 挂载 挂载alluxio_path到本地mount_point,mount_point必须是本地文件系统中的一个空文件夹，并且启动Alluxio-FUSE进程的用户拥有该挂载点及对其的读写权限。可以多次调用该命令来将Alluxio挂载到不同的本地目录下。所有的Alluxio-FUSE会共享$ALLUXIO_HOME\\logs\\fuse.log这个日志文件。 integration/fuse/bin/alluxio-fuse mount mount_point [alluxio_path] 卸载 integration/fuse/bin/alluxio-fuse umount mount_point 检查挂载点运行信息 integration/fuse/bin/alluxio-fuse stat 注意事项要使用启动master和worker的用户来挂载fuse，比如使用hdfs用户启动的Alluxio，则要用hdfs来挂载，可以正常使用，如果使用root用户挂载，目录信息会乱码且无法正常使用。hdfs用户下成功mount后，切换到root用户也会看到挂载点信息乱码。Alluxio相关服务未启动，挂载点信息也会乱码。Alluxio默认只能写本地worker，如果明确知道要写入的文件大小的范围，可以使用ASYNC_THROUGH并加大worker的缓存大小，或者配置多级缓存使worker的缓存空间大于写入文件的大小，才能防止被置换，从而提高效率如果不确定写的文件大小的范围，就不要使用ASYNC_THROUGH这个参数，因为如果本地Worker缓存空间不够就会写入失败，这时，为了保险起见可以使用写参数CACHE_THROUGH边缓存边写或写参数THROUGH只写底层存储，来防止写入文件失败。当然，还有一种比较好的方案，写参数设为ASYNC_THROUGH配合更大的Worker缓存来提高效率，同时设置alluxio.user.file.write.location.policy.class=alluxio.client.file.policy.RoundRobinPolicy参数来保证写入不会失败。如果只写一次，可以及时free掉无用的缓存，减少后面写数据时发生的缓存置换。 Alluxio 客户端APIJava APIAlluxio提供了两种不同的文件系统API：Alluxio API和与Hadoop兼容的API,Alluxio API提供了更多功能，而Hadoop兼容API为用户提供了使用Alluxio的灵活性，无需修改使用Hadoop API编写的现有代码.Maven项目依赖设置 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.my.alluxio&lt;/groupId&gt; &lt;artifactId&gt;AlluxioTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- alluxio-fs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.alluxio&lt;/groupId&gt; &lt;artifactId&gt;alluxio-core-client-fs&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hdfs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;7&lt;/source&gt; &lt;target&gt;7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打jar插件 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--Jar包运行时的主类--&gt; &lt;mainClass&gt;IOTestUtil&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Java读写文件API import alluxio.AlluxioURI; import alluxio.client.file.FileInStream; import alluxio.client.file.FileOutStream; import alluxio.exception.AlluxioException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; /** * HDFS &amp; Allxuio IO读取文件测试工具 IO接口 文件API */ public class IOTestUtil { public static void main(String[] args) throws IOException, AlluxioException { String filePath = args[0]; HDFSUtil h = new HDFSUtil(&quot;hdfs://192.168.1.101:8020&quot;); h.readFile(filePath); AlluxioUtil a = new AlluxioUtil(); a.readFile(filePath); System.out.println(&quot;读文件测试 Finished&quot;); System.out.println(&quot;------------------------&quot;); if (args.length != 1){ String fileToWritePath = args[1]; a.writeFile(fileToWritePath); System.out.println(&quot;写文件测试 Finished&quot;); } } } class HDFSUtil{ private Configuration conf = new Configuration(); public HDFSUtil(String HDFSURL){ conf.set(&quot;fs.defaultFS&quot;,HDFSURL); System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;hdfs&quot;); } public void readFile(String path) throws IOException { FileSystem fs = FileSystem.get(conf); fs.getFileStatus(new Path(path)); FSDataInputStream in = fs.open(new Path(path)); try{ long hdfsStartTime=System.currentTimeMillis(); in = fs.open(new Path(path)); byte[] buffer = new byte[1024]; int byteRead = 0; while ((byteRead = in.read(buffer)) != -1) { System.out.write(buffer, 0, byteRead); //输出字符流 } long hdfsEndTime=System.currentTimeMillis(); System.out.println(&quot;HDFS读取运行时间:&quot;+(hdfsEndTime-hdfsStartTime)+&quot; ms&quot;); }catch (Exception e){ e.printStackTrace(); } finally { in.close(); } } } class AlluxioUtil{ private static final alluxio.client.file.FileSystem fs = alluxio.client.file.FileSystem.Factory.get(); public AlluxioUtil(){} public FileInStream readFile(String AlluxioPath) throws IOException, AlluxioException { AlluxioURI path = new AlluxioURI(AlluxioPath); //封装Alluxio 文件路径的path FileInStream in = fs.openFile(path); try{ long startTime=System.currentTimeMillis(); in = fs.openFile(path); // 调用文件输入流FileInStream实例的read()方法读数据 byte[] buffer = new byte[1024]; int byteRead = 0; // 读入多个字节到字节数组中，byteRead为一次读入的字节数 while ((byteRead = in.read(buffer)) != -1) { System.out.write(buffer, 0, byteRead); //输出字符流 } long endTime=System.currentTimeMillis(); System.out.println(&quot;Alluxio读取运行时间:&quot;+(endTime-startTime)+&quot; ms&quot;); }catch (IOException | AlluxioException e){ e.printStackTrace(); }finally { in.close(); } in.close(); //关闭文件并释放锁 return in; } public void writeFile(String AlluxioPath) throws IOException, AlluxioException { AlluxioURI path = new AlluxioURI(AlluxioPath); // 文件夹路径 FileOutStream out = null; try { out = fs.createFile(path); //创建文件并得到文件输入流 out.write(&quot;qjj1234567&quot;.getBytes()); // 调用文件输出流FileOutStream实例的write()方法写入数据 }catch (IOException | AlluxioException e){ e.printStackTrace(); }finally { out.close(); // 关闭和释放文件 } } } Python APIAlluxio的Python库基于REST API实现的CentOS6和Windows的环境下安装alluxio的python库失败，最终在CentOS7 Python2.7.5的环境下成功执行了pip install alluxio if __name__ == &#39;__main__&#39;: print(&quot;后续用到API再更新&quot;) pass Q&amp;A 加速不明显? Alluxio通过使用分布式的内存存储以及分层存储,和时间或空间的本地化来实现性能加速。如果数据集没有任何本地化, 性能加速效果并不明显。 速度反而更慢了? 测试时尽量多观察集群的CPU占用率,Yarn内存分配和网络IO等多种因素,可能瓶颈不在读取数据的IO上。 确保要读取的数据缓存在Alluxio中,才能加速加速数据的读取。 一定要明确应用场景,Alluxio的设计主要是针对计算与存储分离的场景。在数据远端读取且网络延迟和吞吐量存在瓶颈的情况下,Alluxio的加速效果会很明显,但如果HDFS和Spark等计算框架已经共存在一台机器(计算和存储未分离),Alluxio的加速效果并不明显,甚至可能出现更慢的情况。 多次重复作业输入数据位于OS的高速缓冲区,Alluxio没有加速效果甚至变慢。 内存爆炸，副本过多内存占用过大？ 两种方案：关闭被动缓存alluxio.user.file.passive.cache.enabled=false关闭被动缓存对于不需要数据本地性但希望更大的Alluxio存储容量的工作负载是有益的，或者通过命令alluxio fs setReplication -R –max 5 限制某个目录的文件最大副本数 一些官方的Q&amp;A Alluxio官方问题与答案 总结 对新技术的调研，最重要的是了解它的应用场景，只有场景对了，效果才会很明显 一定要多看官方文档，虽然Alluxio文档不是很详细，但也有帮助，要自己找细节 对自己遇到的难以解决的问题要积极与社区沟通和讨论 自己遇到的问题可能别人也遇到了，有可能是版本的BUG，或许已经有人提交Issue了，一定多留意 新的稳定版发行，一定要了解它的新特性以及修复了哪些漏洞","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"Alluxio","slug":"Alluxio","permalink":"https://shmily-qjj.top/tags/Alluxio/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"我的2019年度总结博客","slug":"我的2019年度总结博客","date":"2020-01-01T00:08:08.000Z","updated":"2020-04-12T14:15:48.001Z","comments":true,"path":"2d8df45c/","link":"","permalink":"https://shmily-qjj.top/2d8df45c/","excerpt":"","text":"我的2019年度总结2019对我来说是很有意义的一年，离开学校步入社会，找到了人生的第一份工作，第一次建立了个人博客，技术也是突飞猛进，还结交了许多新伙(da)伴(lao),头发也少了几根哈哈哈哈….感谢2019，感谢她带给我这么多，一向高冷的她刚刚竟然用不舍的语气跟我说：“去吧，2020在等着你呢，她更需要你！我会望着你的背影渐渐远去，期待着2020使你更优秀，我也会默默替你高兴的！我也舍不得你，时光不等人，我们只好就此别过了。”还有几个小时就是2020年了，我还真有点舍不得她(2019),可她终究还是要离开我，所以我把对她(2019)的怀念，以及对她(2020)的憧憬都记录在这篇博客里吧！ (吃瓜群众:tui，你个渣男!!!) 回顾2019主持人：欢迎今晚的嘉宾，佳境同学！2019即将结束，2020将如期而至，说说你现在的感受？我：2019年，我收获了好多，离开学校，步入社会，这才感觉自己真正长大了，肩上的责任开始重了，生活节奏也完全不一样了，我觉得步入社会的感觉挺好，没有想象中的那么可怕，工作虽不轻松，但能充实自己，身边也都是一些有趣的同事，很聊得来，每天都挺嗨皮的！感谢2019给予我的这些！主持人：说说2019你都经历了什么吧！我：三月四月，天天泡在曲府(学校老校区东区的607自习室，我的府上)，效率不高，但也能学学习，同学经常来光顾，一起学习，挺欢乐的。还有可嘉和健哥我们一起去吃饭，去学校南区看小姑娘，哈哈。 五月，劳动节假期去了五常玩，吃到了纯正的五常大米，爬了山，坐着小电驴放着乡村爱情主题曲，玩的很尽兴！吃过了东区最后一顿早餐，上过了东区最后一节课，我们离开了生活三年的老校区，虽然老校区很破，但生活了这么久，也有感情了，离开还挺感慨的，就像现在我要离开2019一样！不过来了新校区，也算是体验到了大学生活该有的样子，跟骏儿他们坐在操场看晚会，还跟俄罗斯小姐姐合照，hin开心的一段时光！ 六月，端午回家，第一次和爸妈去歌厅，唱到《时间都去哪了》，瞬间泪崩，感叹岁月不饶人。六月的英语六级考试虽然没过没，但却收获了一枚女朋友，还趁着年轻玩了一场异地恋奔现。没能最终走到一起，但也感谢她的出现使我成长，她是个善良的好姑娘，我在这里也祝她幸福，每天开心吧！ 七月，那段时间坚持早起跑步，瘦了，瘦的挺快的，从一个小胖墩变得不那么胖了！那段时间我努力地复习着学过的知识，同时在哈行开发部门实习，什么也不让我们实习生碰，所以我想早点离开，于是紧张地准备秋招，总觉得心里有块石头一直悬着似的，因为还没面试。 八月辞掉了那边，来上海这边工作，由于我的专业限制，只有在北上广深才有发展，离家那么远，还是毅然决然地来了，趁着年轻，就是要闯荡一番！一开始对魔都很陌生，以为是一座冷冰冰的城市，现在好多了。 九月，第一次参加部门团建，吃阳澄湖大闸蟹+打卡苏州园林，玩的好开心，跟同事也熟悉了！ 十月，网恋奔现，去天津玩了几天，跟她各种游荡，去了世纪钟广场，喝了玫瑰酒，看了《中国机长》，去了猫咖撸猫，还去了“分手轮”天津之眼，最终还是没能逃过它的诅咒哈哈！劝广大情侣们去天津之眼要慎重啦！中旬，沾Leader的光，有机会参加QCon全球软件开发大会，收获了很多！ 十一月到现在，写代码，改代码，修BUG，测试，调研，调优… 主持人：那2019年对你来说还真是有意义的一年啊！那你觉得2019年你最大的变化是什么呢？我：假期变少了算吗。。(主持人一脸无奈：不算！！！)好叭…身体上，我比以前瘦了，哈哈，减肥成功不易！心理上呢。。。我觉得我更加独立了，更加成熟了，上进心和求知欲都UP UP!主持人：嗯，你一定可以在保持好身材的同时，保持上进行和求知欲的！2019年，有没有发现自己的不足？我：有的！上班才发现原来工作中有好多提高效率的方法我不会，有个编码前思考过多的毛病，还有有时候想问题会想当然，而且一些原理相关的东西掌握不熟还有一些关键问题会拿捏不准，算法基础也比较薄弱，要学要补的东西很多！干就完了！有优秀的老哥们带着，不怕了！主持人：那生活方面和做事方面呢？我：我觉得还好，以前我是内向型的，但是跟身边人熟悉了就会开朗许多了！奥，对了，有个坏习惯，一定要改，那就是熬夜，希望在2020我能逐渐改掉熬夜的坏毛病，有规律的生活。还有要多锻炼身体，毕竟身体是革命的本钱嘛！主持人：娱乐方面呢？记得你喜欢弹吉他？我：诶，半年没弹了，生疏了，不过会捡起来的。最近只靠打游戏来娱乐了，在王者峡谷里他们都夸我是我国服元歌。。。主持人：2019年的音乐方面作品少了很多，原来时间花在了玩游戏上！我：不不，2019是很忙的一年，没有时间做后期…不过，游戏确实也玩了，玩了一直很喜欢的一款《地铁离去》，里面主角阿尔乔姆弹的吉他很动听，喏，我还自己扒了谱子《Metro Exodus》。现在一想，确实比去年少了很多作品，在云村只发了有一首翻弹《少年的梦》，和两首翻唱《懂了就懂了》、《浮生未歇》。其实还有两首已经录好了，还没来得及混音，哈哈！主持人：此时此刻，你有什么想对朋友和亲人说的？我：对我的朋友们说：朋友们等我回去嗨！啤酒踩着箱子喝，一天三顿小烧烤给我安排上！然后对爸妈说，过年等我回去过个团圆年！我会注意身体少熬夜的！等我发达了带你们旅游去！我不在身边，你们可要注意身体呀！主持人：你的朋友们听了很开心吧，你的父母，也会为你感动，为你骄傲，希望你在2020年，无论事业还是生活，都能更上一层楼！我：谢谢您！主持人：那在节目的最后，你还有什么要给大家分享的吗？我：嗯，今天准备了几张照片，把2019最美好的回忆分享给大家！ 展望2020挥手告别2019，2020我来啦！2020我们一起加油！ 小目标 代码能力提高，深入理解设计模式 框架底层原理掌握，开始逐步向源码层面深入 提高对大数据技术栈的整体认识，先有深度，后有广度 工作效率提高，熟悉快捷键 养成好的写文档习惯，只要值得深入学习的东西都尽量写博客 吉他能弹得更好吧，网易云作品的质量提高 改掉熬夜坏习惯 交几个好朋友啥的 凑九条，希望能久久铭记我的2020小目标 小期待 期待早睡早起健康的自己 期待求知进取爱拼的自己 期待天天向上进步的自己 期待争分夺秒高效的自己 期待悠然自得快乐的自己 期待……..一夜暴富哈哈哈 小愿望第一个愿望就是家人能健健康康，自己也养成健康的生活习惯第二个愿望是在大数据方面有更深入的学习和实践第三个愿望是结交好朋友，认识新朋友最后一个愿望就是要正在看我博客的你每天都开心呀 小想法真该早睡了，我想早睡，我好想早睡呀！可是一到晚上就贼精神。。。有没有小伙伴可以互相监督的呀！可以在下方评论区给我留言吼！最后，祝大家在2020年，身体倍儿棒，吃嘛嘛香，学业有成，生意兴旺…还有一夜暴富昂，最重要的是暴富了可别忘了我昂，哈哈哈哈。","categories":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/生活/"}],"tags":[{"name":"个人总结","slug":"个人总结","permalink":"https://shmily-qjj.top/tags/个人总结/"},{"name":"2019","slug":"2019","permalink":"https://shmily-qjj.top/tags/2019/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/生活/"}]},{"title":"数据库事务ACID理解","slug":"数据库事务ACID理解","date":"2019-12-28T12:22:00.000Z","updated":"2020-04-12T14:15:48.001Z","comments":true,"path":"1f7eb1b3/","link":"","permalink":"https://shmily-qjj.top/1f7eb1b3/","excerpt":"","text":"Intro对于事务ACID，知道大概的意思，但总觉得对这个概念还有点模糊，所以写一篇博客加深一下印象。 数据库的事务一个事务中可能有多个操作，当所有操作都成功了的情况下这个事务才会被提交，如果其中一个操作失败，整个事务都将回滚(Rollback)到事务开始前的状态，好像这个事务从未执行过。简单来说就是:要么什么都不做，要么做全套（All or Nothing） ACIDACID是指数据库事务正确执行的四个基本特征的缩写通过上图可以大概了解ACID的基本特征，下面做详细介绍 原子性（Atomicity）事务中包含的操作集合，要么全部操作执行完成，要么全部都不执行。即当事务执行过程中，发生了某些异常情况，如系统崩溃、执行出错，则需要对已执行的操作进行回滚，清除所有执行痕迹。例子：A向B转账100，这个事务包括两步(A失去100，B得到100)，原子性保证这两步都成功或者都失败。 一致性（Consistency）事务执行前和事务执行后，数据库的完整性约束不被破坏。即事务的执行是从一个有效状态转移到另一个有效状态。例子：A向B转账100，这个事务包括两步(A失去100，B得到100)，A和B所在的表收入和支出存在外键约束，若A支出增加而B收入未增加，则违反了一致性约束。 隔离性（Isolation）数据库允许多个并发事务同时对数据进行读写和修改的能力，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。多个事务并发执行时，彼此之间不应该存在相互影响。隔离程度不是绝对的，每个数据库都提供有自己的隔离级别，每个数据库的默认隔离级别也不尽相同。例子：A向B转账100，交易还未完成时，B查询不到100元入账。 持久性（Durability）事务正常执行完毕后，对数据库的修改是永久性的，即便系统故障也不会丢失。即事务的修改操作已经记录到了存储介质中。例子：A向B转账100，A永久失去了100元而B永久得到100元，不能赖账。 总结ACID 原子性：事务操作的整体性。 一致性：事务操作下数据的正确性。 隔离性：事务并发操作下数据的正确性。 持久性：事务对数据修改的可靠性。 事务隔离级别上面说过“每个数据库都提供有自己的隔离级别，每个数据库的默认隔离级别也不尽相同”，事物隔离级别分为四种，下面一一介绍。首先简述共享锁（S）和排它锁（X），方便后续理解：多个共享锁(S)可以同时获取，但是排它锁(X)会阻塞其它所有锁 未提交读(Read Uncommitted)指一个事务读取到了另外一个事务未提交的数据。即事务的修改阶段未加排他锁，对其他事务可见。例如事务T1可能读取到只是事务T2中某一步的修改状态，即存在脏读的现象。脏读：事务读取到的数据可能是不正确、不合理或者处于非法状态的数据，例如在事务T1读取后，事务T2可能又对数据做了修改，或者事务T2中某些操作违反了一致性约束，做了回滚操作，该情况下事务T1读取到的数据称之为脏数据，该行为称之为脏读。 提交读(Read Committed)一个事务过程中只能读取到其他事务对数据的提交后修改，即事务的修改阶段加了排它锁，直到事务结束才释放，执行读命令那一刻加了共享锁，读完即释放，以此维持事务修改阶段对其他事务的不可见。例如事务T2读取到的只能是事务T2提交完成后的状态。该隔离级别避免了脏读现象，但正是由于事务T1可能读取到的是事务T2修改完成后的数据，以致出现了不可重复读现象。不可重复读：对于同一个事务的前后两次读取操作，读取到的内容不同。例如在事务T1读取操作后，事务T2可能对数据做了修改，事务T2修改完成提交后，事务T1又做了读取操作，因为内容已被修改，导致读取到的内容与上一次不同，即存在不可重复读现象。 可重复读(Repeatable Reads)一个事务过程中不允许其他事务对数据进行修改。即事务的读取过程加了共享锁，事务的修改过程加了排它锁，并一直维持锁定状态直到事务结束。因为事务的读取或修改都需要维持整个阶段的锁定状态，所以避免了脏读和不可重复读现象。但是因为只对现有的记录上进行了锁定，并未维持间隙锁/范围锁，导致某些数据记录的插入未受阻拦（结果多了一行），即存在幻读现象。幻读：事务中前后相同的查询语句，返回的结果集不同。例如在事务T1查询表记录后，事务T2向表中增加了一条记录，当事务T1再次执行相同的查询时，返回的结果集可能不同，即存在幻读现象。 可串行化(Serializable)一个事务过程中不允许其他事务对指定范围数据进行修改。即事务过程中若指定了操作集合的范围，在可重复读的锁基础上增加了对操作集合的范围锁，通过增加范围锁避免了幻读现象。 四种隔离级别设置: 级别 说明 Serializable 可避免脏读、不可重复读、虚读情况的发生 Repeatable read 可避免脏读、不可重复读情况的发生 Read committed 可避免脏读情况发生 Read uncommitted 最低级别，以上情况均无法保证 锁的使用是为了在并发环境中保持每个业务流处理结果的正确性，这样的概念在计算机领域中很普遍，但是都必须要基于一个前提，或者称之为约定：在执行操作前，首先尝试去获取锁，获取成功则可以执行，若获取失败，则不执行或等待重复获取。因为无论任何类型的操作，有没有锁都不影响程序本身的执行流程，但只有遵从这个约定才能体现出其价值。就像红绿灯并不影响车辆本身的行驶能力，只有声明所有个体皆遵守相同的规则，所以一切才变得有序。在数据库的并发环境下，隔离程度越高，也就意味着并发程度越低，所以各个数据库中一般设置的都是一个折中的隔离级别。 基于Mysql测试隔离级别 SELECT @@global.tx_isolation; # 查看全局事物隔离级别 SELECT @@session.tx_isolation; # 查看会话事物隔离级别 SELECT @@tx_isolation; # 查看当前事务隔离级别 SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; # 可避免脏读、不可重复读、虚读情况的发生 SET SESSION TRANSACTION ISOLATION LEVEL read committed; # 可避免脏读情况发生 SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; # 可避免脏读、不可重复读情况的发生 SET SESSION TRANSACTION ISOLATION LEVEL serializable; # 可避免脏读、不可重复读、幻读情况的发生 start transaction; --建表 drop table AMOUNT; CREATE TABLE `AMOUNT` ( `id` varchar(10) NULL, `money` numeric NULL ) ; --插入数据 insert into amount(id,money) values(&#39;A&#39;, 800); insert into amount(id,money) values(&#39;B&#39;, 200); insert into amount(id,money) values(&#39;C&#39;, 1000); --测试可重复读，插入数据 insert into amount(id,money) values(&#39;D&#39;, 1000); --设置事务 SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; SELECT @@tx_isolation; --开启事务 start transaction; --脏读演示，读到其他事务未提交的数据 --案列1，事务一：A向B转200，事务二：查看B金额变化，事务一回滚事务 update amount set money = money - 200 where id = &#39;A&#39;; update amount set money = money + 200 where id = &#39;B&#39;; --不可重复读演示，读到了其他事务提交的数据 --案列2，事务一：B向A转200，事务二：B向C转200转100 SET SESSION TRANSACTION ISOLATION LEVEL read committed; --开启事务 start transaction; --两个事务都查一下数据(转账之前需要，查一下金额是否够满足转账) select * from amount; --事务一：B向A转200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --事务二：B向C转200转100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --从事务二的角度来看，读到了事务一提交事务的数据，导致金额出现负数 --幻读演示 --案列3，事务一：B向A转200，事务二：B向C转200转100 SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; --开启事务 start transaction; --两个事务都查一下数据(转账之前需要，查一下金额是否够满足转账) select * from amount; --事务一：B向A转200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --事务二：B向C转200转100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --从事务二的角度来看，读到了事务一提交事务的数据，导致金额出现负数 参考资料事务的ACID事务ACID理解事务ACID属性与隔离级别","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://shmily-qjj.top/tags/数据库/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"Hive3.x来了","slug":"Hive3.x新特性","date":"2019-12-27T07:18:25.000Z","updated":"2020-04-12T14:15:47.998Z","comments":true,"path":"7fbbfd34/","link":"","permalink":"https://shmily-qjj.top/7fbbfd34/","excerpt":"","text":"Hive3.x新特性新特性简述 执行引擎变更为TEZ,不使用MR 成熟的ACID大数据事务支持 LLAP用于妙极，毫秒级查询访问 基于Apache Ranger的统一权限管理 默认开启HDFS ACLs Beeline代替Hive Cli，降低启动开销 不再支持内嵌Metastore Spark Catalog不与Hive Catalog集成，但可以互相访问9.批处理使用TEZ，实时查询使用LLAP 架构原理 TEZ执行引擎Apache TEZ是一个针对Hadoop数据处理应用程序的分布式计算框架，基于Yarn且支持DAG作业的开源计算框架。Tez产生的主要原因是绕开MapReduce所施加的限制，逐步取代MR，提供更高的性能和灵活性。Apache TEZ的核心思想是将Map和Reduce拆分成若干子过程，即Map被拆分成Input、Processor、Sort、Merge和Output， Reduce被拆分成Input、Shuffle、Sort、Merge、Processor和Output等，分解后可以灵活组合成一个大的DAG作业。Apache TEZ兼容MR任务，不需要代码层面的改动。Apache TEZ提供了较低级别的抽象，为了增强Hive/Pig的底层实现，而不是最终面向用户的。Hive3的TEZ+内存查询结合的性能据说是Hive2的50倍(也有文章说是100倍，这个数字是不是很熟悉，它到底能不能与Spark内存计算速度媲美呢)。 上图是Hive On MR和Hive On Tez执行任务流程对比图，解释： Hive On MR Hive On Tez 计算需要多个MR任务而且中间结果都要落盘 只有一个作业，只写一次HDFS 没有资源重用 资源复用 处理完释放资源 Applications Manager资源池启动若干Container，处理完不释放直接分配给未运行任务 Map:Reduce = 1:1 不再是一个Map只对应一个Reduce 在磁盘处理数据集 小的数据集完全在内存中处理以及内存Shuffle 新的HiveQL执行流程Hive编译查询-&gt;Tez执行查询-&gt;Yarn分配资源-&gt;Hive根据表类型更新HDFS或Hive仓库中的数据-&gt;Hive通过JDBC连接返回查询结果 LLAPLLAP(Live Long and Process)实时长期处理，是Hive3的一种查询模式，由一个守护进程和一个基于DAG的框架组成，LLAP不是执行引擎(MR/Tez),它用来保证Hive的可伸缩性和多功能性，增强现有的执行引擎。LLAP的守护进程长期存在且与DataNode直接交互，缓存，预读取，某些查询处理和访问控制功能包含在这个守护程序中用于直接处理小的查询，而计算与IO较大的繁重任务会提交Yarn执行。守护程序不是必须的，没有它Hive仍能正常工作。对LLAP节点的请求都包含元数据信息和数据位置，所以LLAP节点无状态。可以使用Hive on Tez use LLAP来加速OLAP场景(OnLine Analytical Processing联机分析处理)LLAP为了避免JVM内存设置的限制，使用堆外内存缓存数据以及处理GROUP BY/JOIN等操作，而守护程序仅使用少量内存。Hive3支持两种查询模式Container和LLAP 如图LLAP执行示例，TEZ作为执行引擎，初始阶段数据被推到LLAP，LLAP直接与DataNode交互。而在Reduce阶段，大的Shuffle数据在不同的Container容器中进行，多个查询和应用能同时访问LLAP。 更成熟的ACID支持Hive的UPDATE一直是大数据仓库的一个问题，虽然在Hive3.x之前也支持UPDATE操作，但是性能很差，还需要进行分桶。Hive3.x支持全新的更成熟的ACID。Hive3默认对内部表支持事务和ACID特性。默认情况下启用ACID不会导致性能或操作过载。 物化视图重写和自动查询缓存多个查询可能需要用到相同的中间表，可以通过预先计算和将中间表缓存到视图中来避免重复计算。查询优化器会自动利用预先计算的缓存来提高性能。例如加速仪表盘中的join数据查询速度。 元数据映射表Hive会从JDBC数据源创建两个数据库：information_schema和sys。所有Metastore表都映射到表空间，并在sys中可用。information_schema数据显示系统的状态。 Hive 3.0其他特性1、连接Kafka Topic，简化了对Kafka数据的查询2、执行查询所需的少量守护进程简化了监视和调试3、工作负载管理(会话资源限制)：用户会话数，服务器会话数，每个服务器每个用户会话数等限制，防止资源争用导致资源不足4、会话状态，内部数据结构，密码等驻留在客户端而不是服务器上5、黑名单可以限制内存配置以防止HiveServer不稳定，可以使用不同的白名单和黑名单配置多个HiveServer实例，以建立不同级别的稳定性 优缺点 优点：性能，安全性，对ACID事物的支持，对任务资源调度的优化。 缺点：目前最新的CDH6.3还不兼容Hive3，自己安装坑点多；目前相关文献较少，排错难。 实践https://link.zhihu.com/?target=https%3A//hortonworks.com/tutorial/interactive-sql-on-hadoop-with-hive-llap/https://link.zhihu.com/?target=https%3A//dzone.com/articles/3x-faster-interactive-query-with-apache-hive-llaphttps://link.zhihu.com/?target=https%3A//community.hortonworks.com/articles/149486/llap-sizing-and-setup.html 参考资料Hive3新特性Apache Tez 了解Hive 3.x 功能介绍","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"Hive","slug":"Hive","permalink":"https://shmily-qjj.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"人声混音总结","slug":"人声混音总结","date":"2019-12-21T07:06:06.000Z","updated":"2020-04-12T14:15:48.000Z","comments":true,"path":"d2c4813b/","link":"","permalink":"https://shmily-qjj.top/d2c4813b/","excerpt":"","text":"人声混音总结前言2019年因为比较忙，出的音乐作品并不多，我记得只是翻弹了一首岸部真明的指弹曲《少年的梦》和翻唱了一首自己很喜欢的古风歌曲《浮生未歇》…2018年比较活跃，记得2018年的暑假买了录音笔，开始录吉他，录翻唱，对录音笔爱不释手，我的设备很简陋，只有一个ZOOM H1N小型录音笔和一台音质不好的笔记本电脑（这个电脑对我的混音造成了很大影响，用它听起来不错的音色，放在手机上挺就是另一个样子…），但是通过自己摸索，发现效果还是可以的。但现在当我又面对一条一条的音轨，眼花缭乱的效果器时，我有点不知所措的感觉，的却，将近大半年时间没混音，听感和对效果器的掌握已经忘掉太多，所以决定写这篇博客，督促自己学习混音知识，做到精益求精。通过深入混音技术，我相信我的混音作品能够变得更好！网易云歌手页：佳境Shmily 基本步骤也是我人声部分的插件效果器顺序，当然也不绝对 录音（一切始于源头，如果录的干声有问题，后期再强也无法修复，所以有瑕疵的地方要反复录制） 降噪干声的空白部分的噪音缓解，多多少少对干声音质有损可以使用比较强大的iZotope RX7，也可以使用Waves家的X-Noise和Z-Noise 修音（使用WavesTune修复音准，轻微跑调是基本可以无损修复的） 人声动态调整人声的动态很大，这里可以为每段音频调整输出的动态，有Waves的也可以使用Vocal Rider来解决人声音量不均匀的问题Vocal Rider可以侦测音量什么时候该高，什么时候该低，并自动进行调整，Vocal Rider不是压缩器，不会对声音音色产生影响用法：①调节“目标”选一个理想的默认音量 ②调节range自动音量调节范围，也就是动态大小 均衡器这步应该是对人声音色影响最大的，可以使用多个均衡器串联 齿音缓解齿音不能被完全消除，应该在保证人声音色无损的前提下尽可能减少齿音使用RDeEsser或DeEsser 人声齿音6khz左右 压限压缩和限制，防止信号过载，防止人声忽大忽小，防止人声动态过大不同压缩器有不同的音染特色，能使音色更优美压缩器种类很多，我常用CLA-76 C1等Waves家的压缩器 限制器一般也是使用Waves家的L3LL Ultra Stereo、L1、L2等 饱和度与失真很小的饱和度与失真能让人声更厚实，添加泛音让人声更突出可用NLS Channel的失真部分，也有推荐Scheps 73，不过我的Waves版本没有这个插件 混响和延迟增加空间感，混响和延迟对单声道可以增加深度感，对双声道可以增加宽度感混响的时间短，产生的空间小，时间长则产生的空间也较长混响使用Waves RVerb延迟使用H-Delay，用来强调某些词 激励器对一些音轨进行音量增益，干湿度增益等操作，为了方便，使用Waves中的OneKnob系列插件，一个旋钮解决问题！ 母带母带处理包括总体激励、均衡、混响、音量提升等，推荐使用Ozone 8。我目前对母带处理还不是很了解。 知识点 均衡器EQ 均衡器EQ注意事项网上有推荐FabFilter Pro-Q这个插件，不过我用的Waves的一套 EQ黄金定律： 0、减窄增宽，在用减法eq时要用较高的Q值，用加法EQ时采用较低的Q值。 1、如果声音浑浊，请衰减250hz附近的频段 2、如果声音听起来有喇叭音，请衰减800hz附近的频段 3、当你试图让声音听起来更好，请考虑用衰减 4、当你试图让声音听起来与众不同，请考虑用提升 5、不要放大原先没有的声音 减法均衡器F6 Dynamic EQ Waves Q10100hz以下切掉 加法均衡器 常用效果器的使用","categories":[{"name":"音乐","slug":"音乐","permalink":"https://shmily-qjj.top/categories/音乐/"}],"tags":[{"name":"人声混音","slug":"人声混音","permalink":"https://shmily-qjj.top/tags/人声混音/"}],"keywords":[{"name":"音乐","slug":"音乐","permalink":"https://shmily-qjj.top/categories/音乐/"}]},{"title":"程序猿是怎么表白的","slug":"程序猿是怎么表白的","date":"2019-12-11T13:10:21.000Z","updated":"2020-04-12T14:15:48.003Z","comments":true,"path":"d1c9241f/","link":"","permalink":"https://shmily-qjj.top/d1c9241f/","excerpt":"","text":"程序猿是怎么表白的Hi Dear,要看看我的代码吗？不看怎么知道我爱你！程序猿是怎么表白的？当然也离不开敲代码啦！就缺个懂代码的女朋友了。。。 一.我的世界只有太阳、月亮和你/** * I love three things in this world.Sun, moon and you. Sun for morning, moon for night, and you forever. */ class LoveThreeThings extends Me { const loveFirstThings = &#39;Sun&#39;; const loveSecondThings = &#39;Moon&#39;; const loveThirdThings = &#39;You&#39;; public function MyLove() { return &#39;I Love&#39; . self::loveThirdThings . &#39;forever. Never change!&#39;; } } 二.百年好合while(&#39;ILoveyou&#39;): for IBeWithYou in range(0,50*365): time.sleep(60*60*24) //程序能一直执行，执行完50年，若我们还有50年，余生继续。 //从前的日色变得很慢 //车马邮件都慢 //一生只够爱一人 三.谁都不能掌控全世界，但你至少可以掌控我，这是我的温柔 world.controlledBy(NoOne) withMyGentle() { you.control(me).equals(true) } //谁都不能掌控全世界，但你至少可以掌控我，这是我的温柔 四.若爱，请深爱 if(love ==1) { while(1) { love_depth ++; } } 五.将我手上的温度全部给予你，换取你幸福的脸庞if(you.hand==cold&amp;&amp;weather==winter): //如果冬天里你的手是冰冷的 giveyoulove(myhand.temp,yourhand.temp); //将我手上的温度全部给予你 return you.happyface; //换取你幸福的脸庞 六.我一直在找你，当我找到你，也就找到了整个世界while (i.findYou()) { if (i.get() == you) { System.out.print(&quot;Hello,Word!&quot;); } } 七.如果彼此相爱，那就白头偕老if(you.love(me) &amp;&amp; I.love(you)){ // 如果彼此相爱 this.liveToOld();// 那就白头偕老 }else{// 否则 this.bestWishesToYou(); // 祝你一切安好 } 八.陪伴是最长情的告白StringBuilder love = newStringBuilder(&quot;&quot;); for(;;)｛ love.append(Math.random()&gt;0.5?1:0); ｝ 九.自从遇见你，就不停地想你public void missing_you(meet_you) int time = 0 for（time=meet_you;;time++）｛ missing_you(); ｝ 十.你若不来，我便不弃public Me waitYou(){ if(!appear){ this.waitYou(); }else{ this.waitYou(); } } 十一.听说你要走，站在雨里，任凭身体被水珠撕裂成一个个没有意义的字母public void hearYouLeave(){ String body = myself.toString(); body.split(&quot;.&quot;); } 十二.每一世，我都会在这等你！就算容颜变迁，就算时光流转public Me findYou(){ for(int age = 0;age &lt;= 120; age++){ try{ Thread.sleep(60); if(this.waitMyLove() != null){ return this.waitMyLove(); }cache(InterruptedException e){ System.out.println(&quot;Time is nothing...&quot;); } } return null; } 十三.待更新待更新... 参考资料：如何用你的专业来表白？","categories":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/生活/"}],"tags":[{"name":"表白","slug":"表白","permalink":"https://shmily-qjj.top/tags/表白/"},{"name":"程序猿","slug":"程序猿","permalink":"https://shmily-qjj.top/tags/程序猿/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/生活/"}]},{"title":"Mysql Event Scheduler","slug":"Mysql Event Scheduler","date":"2019-11-15T13:25:04.000Z","updated":"2020-04-12T14:15:47.999Z","comments":true,"path":"3c26421b/","link":"","permalink":"https://shmily-qjj.top/3c26421b/","excerpt":"","text":"Mysql事件调度器工作的时候遇到一张表需要每天Truncate，就想到了Mysql的Event Scheduler，但是又忘了它的语法了，所以这里来复习一下。 什么是Event Scheduler事件调度器，可以作为定时调度器，类似于Crontab，可以取代部分操作系统任务调度器的定时任务工作。Mysql在5.1版本后新增了事件调度器，它可以支持秒级调度，很实用方便。时间调度器也可以看作是一个触发器，是针对某个表进行操作的，时间调度器执行采用了单独一个线程，可通过SHOW PROCESSLIST命令查看 Event Scheduler语法CREATE [DEFINER = { user | CURRENT_USER }] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT &#39;string&#39;] DO event_body; schedule: AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...] interval: quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND} 语法说明： DEFINER：指定可执行该定时器的MySQL账号，user的格式是’user_name’@’host_name’，CURRENT_USER或CURRENT_USER()，单引号是需要在语句中输入的。如果不指定，默认是DEFINER = CURRENT_USER。 event_name：事件名称，最大64个字符，不区分大小写，MyEvent和myevent是一样的，命名规则和其他MySQL对象是一样的。 ON SCHEDULE schedule：ON SCHEDULE指定事件何时执行，执行的频率和执行的时间段，有AT和EVERY两种形式。 [ON COMPLETION [NOT] PRESERVE]：可选，preserve是保持的意思，这里是说这个定时器第一次执行完成以后是否还需要保持，如果是NOT PRESERVE，该定时器只执行一次，完成后自动删除事件；没有NOT，该定时器会多次执行，可以理解为这个定时器是持久性的。默认是NOT PRESERVE。 [ENABLE | DISABLE | DISABLE ON SLAVE]：可选，是否启用该事件，ENABLE-启用，DISABLE-禁用，可使用alter event语句修改该状态。DISABLE ON SLAVE是指在主备复制的数据库服务器中，在备机上也创建该定时器，但是不执行。 COMMENT: 注释，必须用单引号括住。 DO event_body：事件要执行的SQL语句，可以是一个SQL，也可以是使用BEGIN和END的复合语句，和存储过程相同。 ON SCHEDULE时间类型两种时间类型AT timestamp和Every interval AT timestamp用于只执行一次的事件。执行的时间由timestamp指定，timestamp必须包含完整的日期和时间，即年月日时分秒都要有。可以使用DATETIME或TIMESTAMP类型，或者可以转换成时间的值，例如“2018-01-21 00:00:00”。如果指定是时间是过去的时间，该事件不会执行，并生成警告。 mysql&gt; create table test(id int,name varchar(255)); Query OK, 0 rows affected (0.01 sec) mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:30:59 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:30:59&#39; DO show tables; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; show warnings\\G; *************************** 1. row *************************** Level: Note Code: 1588 Message: Event execution time is in the past and ON COMPLETION NOT PRESERVE is set. The event was dropped immediately after creation. 1 row in set (0.00 sec) ERROR: No query specified mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:34:59&#39; DO show tables; Query OK, 0 rows affected (0.01 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:37:59&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) 时间过后我发现我的test表里仍然没数据 mysql&gt; show variables like &quot;event_scheduler&quot;; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | event_scheduler | OFF | +-----------------+-------+ 1 row in set (0.00 sec) 原因是我没开启event_schedulervim /etc/my.cnf 在[mysqld]这一栏下添加event_scheduler = ON来永久启用event_scheduler重启mysql服务systemctl restart mysqld.service mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:40:37 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:41:37&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test; +------+------+ | id | name | +------+------+ | 1 | qjj | +------+------+ 1 row in set (0.00 sec) mysql&gt; show events; Empty set (0.00 sec) # 一小时后执行 命令示例 mysql&gt; CREATE EVENT update_test ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOUR DO UPDATE test SET id = 2; Query OK, 0 rows affected (0.00 sec) 上述结果说明:必须先开启event_scheduler之后event才会生效，AT timestamp的方式只会在指定时间点执行一次，然后这个event就会被销毁，如果指定的时间是过去的是时间点，则这个event会有警告，且不执行也不保留event。 Every interval让事件定期执行，每多久执行一次ON SCHEDULE后面时间写法的几个栗子：EVERY 6 WEEK 每六周EVERY 20 second 每20秒EVERY 3 MONTH STARTS CURRENT_TIMESTAMP + INTERVAL 1 WEEK 一周以后开始，每隔三个月EVERY 2 WEEK STARTS CURRENT_TIMESTAMP + INTERVAL ‘6:15’ HOUR_MINUTE 6小时15分钟以后开始，每隔两周执行EVERY 1 DAY STARTS CURRENT_TIMESTAMP + INTERVAL 5 MINUTE ENDS CURRENT_TIMESTAMP + INTERVAL 2 WEEK 5分钟以后开始，每隔一天执行，两周后结束 举个栗子 mysql&gt; create event daily_truncate_test -&gt; ON SCHEDULE -&gt; EVERY 1 DAY -&gt; COMMENT &#39;每天执行一次清空test表数据&#39; -&gt; DO -&gt; truncate test; Query OK, 0 rows affected (0.00 sec) mysql&gt; mysql&gt; show events; +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | Db | Name | Definer | Time zone | Type | Execute at | Interval value | Interval field | Starts | Ends | Status | Originator | character_set_client | collation_connection | Database Collation | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | test | daily_truncate_test | root@CDH066 | SYSTEM | RECURRING | NULL | 1 | DAY | 2019-11-16 12:10:36 | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | | test | update_test | root@CDH066 | SYSTEM | ONE TIME | 2019-11-16 12:58:52 | NULL | NULL | NULL | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ 2 rows in set (0.00 sec) mysql&gt; SHOW PROCESSLIST; +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | 1 | event_scheduler | localhost | NULL | Daemon | 721 | Waiting for next activation | NULL | | 7 | root | CDH066:34902 | test | Query | 0 | starting | SHOW PROCESSLIST | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ 2 rows in set (0.00 sec) # 示例2 指定每天具体时间点的event事件 CREATE EVENT truncate_with_time ON SCHEDULE EVERY 1 day STARTS date_add(concat(current_date(), &#39; 00:00:00&#39;), interval 0 second) ON COMPLETION PRESERVE ENABLE COMMENT DO TRUNCATE test; 操作和查看事件show events; # 查看事件及其状态 ALTER EVENT daily_truncate_test DISABLE; # 禁用指定事件 ALTER EVENT daily_truncate_test ENABLE; # 启用指定事件 ALTER EVENT daily_truncate_test RENAME TO daily_truncate; # 重命名事件 ALTER EVENT test.daily_truncate_test RENAME TO qjj_test.daily_truncate_test; # 事件是数据库层面的，可以把事件从一个数据库移动到另一个数据库(另一个数据库要有对应的表) DROP EVENT daily_truncate; # 删除事件 总结Mysql作为最热门的关系型数据库之一，有很多东西值得我们去探索，好记性不如烂笔头，写了博客，对事件调度器的理解更加深刻了。","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://shmily-qjj.top/tags/Mysql/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"浅谈group by与distinct去重","slug":"浅谈group by与distinct去重","date":"2019-11-13T12:45:37.000Z","updated":"2020-05-04T12:53:02.527Z","comments":true,"path":"96009187/","link":"","permalink":"https://shmily-qjj.top/96009187/","excerpt":"","text":"前言今天带我的老哥让我改一下报警模块，把一些联系方式等信息存在Mysql里，方便以后管理和维护，很简单的东西，为了减少Mysql并发压力，我想每次报警只查询一次数据库，但子查询返回的结果有重复记录，于是我写了个类似SELECT col1,col2,col3,col4 FROM (select…) GROUP BY col1;的语句来去重，可以正常执行不报错且达到了目的就直接使用了，也没深究。直到老哥看了代码说找我说老弟呀你这个逻辑，有点问题呀。。。 按理说，GROUP BY都是要与聚合函数搭配使用的，所以确实是逻辑有问题，写代码规范很重要，规范的同时还要弄清楚原理，于是就有了这篇博客，详细说一下使用GROUP BY和DISTINCT去重… DISTINCT去重DISTINCT可多字段去重，每个字段的值都完全相同的情况下使用DISTINCT去重DISTINCT也可以单个字段值去重 select(name),id from table; mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel from person_info where wxid in (&#39;SCALA&#39;); +-------+ | tel | +-------+ | 10010 | +-------+ 1 row in set (0.00 sec) mysql&gt; select distinct name from person_info where wxid in (&#39;SCALA&#39;); +--------+ | name | +--------+ | scala | | scala1 | +--------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +-------+-------+ | tel | wxid | +-------+-------+ | 10010 | SCALA | +-------+-------+ 1 row in set (0.00 sec) 通过上面的实验可以看出，DISTINCT的去重效果是 如果取出的所有字段的值都完全相同则可以去重，如果取出的字段不完全相同，就无法去重。 GROUP BYGROUP BY与聚合函数连用，主要用于分组聚合，但它也可以用来去重，与DISTINCT相反，它支持多个字段值不完全相同的情况下去重，但会舍弃一些值。假如A,B,C三个字段，A和B两个字段在多条记录中值都相同，但C不同，使用GROUP BY去重后只会得到一条记录，C的值只保留一个，其余记录C字段不同的值舍弃。 mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) # GROUP BY的正确用法 分组聚合 mysql&gt; select count(name),tel,wxid from person_info group by tel; +-------------+-------+--------+ | count(name) | tel | wxid | +-------------+-------+--------+ | 1 | 10000 | SBT | | 3 | 10010 | SCALA | | 1 | 10086 | PYTHON | | 1 | 110 | QJJ | | 1 | 114 | JAVA | | 1 | 119 | MAVEN | | 1 | 120 | JJQ | +-------------+-------+--------+ 7 rows in set (0.00 sec) # 报警模块不希望重复报警，所以只想获取一个电话号码 下面的语句逻辑有问题，不符合GROUP BY的使用规范，但是能执行 mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) # 改成符合使用规范的 mysql&gt; select max(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | max(name) | tel | wxid | +-----------+-------+-------+ | scala2 | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) mysql&gt; select min(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | min(name) | tel | wxid | +-----------+-------+-------+ | scala | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) # mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,name; +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) 从上面实验可以得出的结论:如果只需要tel和wxid两个字段，无所谓name的字段值，就可以用GROUP BY的方式去重，但是也要尽量写得规范。如果需要name字段的值，就不能用GROUP BY来去重了。 问题情景重现报警有两种方式，一种是传人名，还有一种是传组名，人与组是多对多关系联系方式信息存为两张表，person_info和group_info,大致如下name 人名，tel是电话，wxid是微信，groupname是组名 mysql&gt; show tables; +----------------+ | Tables_in_test | +----------------+ | group_info | | persion_info | +----------------+ 2 rows in set (0.00 sec) mysql&gt; desc person_info; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | tel | varchar(255) | NO | | NULL | | | wxid | varchar(255) | NO | | NULL | | +-------+--------------+------+-----+---------+-------+ 3 rows in set (0.00 sec) mysql&gt; desc group_info; +-----------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | groupname | varchar(255) | NO | | NULL | | +-----------+--------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) mysql&gt; select * from person_info; +--------+-------+--------+ | name | tel | wxid | +--------+-------+--------+ | java | 114 | JAVA | | jjq | 120 | JJQ | | maven | 119 | MAVEN | | python | 10086 | PYTHON | | qjj | 110 | QJJ | | sbt | 10000 | SBT | | scala | 10010 | SCALA | +--------+-------+--------+ 7 rows in set (0.00 sec) mysql&gt; select * from group_info; +--------+-------------+ | name | groupname | +--------+-------------+ | java | languages | | jjq | person | | maven | build-tools | | python | languages | | qjj | person | | sbt | build-tools | | scala | languages | +--------+-------------+ 7 rows in set (0.00 sec) 报警接口传进来的参数可能是多个人名或者组名的组合列表，我想通过一次查询获取到所有报警人信息，于是我先写了内部子查询: SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;); # 结果: +------+------+------+-----------+ | name | tel | wxid | groupname | +------+------+------+-----------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | | jjq | 120 | JJQ | groupname | | qjj | 110 | QJJ | groupname | +------+------+------+-----------+ 4 rows in set (0.00 sec) 因为有重复的人名和重复的联系方式会重复报警，所以为了避免重复报警，我又加了外面的一层: SELECT name,tel,wxid,max(groupname) FROM (SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;)) a GROUP BY name; # 结果: +------+------+------+----------------+ | name | tel | wxid | max(groupname) | +------+------+------+----------------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | +------+------+------+----------------+ 2 rows in set (0.01 sec) 准确地拿到了name,tel,wxid，但是groupname字段呢，到底是哪个被舍弃了？不同情况下不一定。如果我们只要name,tel,wxid这三个字段，在groupname字段上加个max()好了，这样逻辑也说得通，也比较规范，如果要求精确拿到groupname字段的值，就不能使用group by去重。 关于效率DISTINCT和GROUP BY同时适用的场景下，不能说一定是谁的效率更高DISTINCT就是字段值对比的方式，要遍历整个表。GROUP BY类似于先建索引再查索引。小表DISTINCT去重效率高，大表GROUP BY去重效率高 总结 认清DISTINCT和GROUP BY的去重场景 某些场景下DISTINCT与GROUP BY去重同时适用，但DISTINCT效率更高 代码要规范且符合逻辑 要对SQL每个语法的使用场景有明确的认识 多总结问题","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://shmily-qjj.top/tags/SQL/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"王者荣耀-元歌教学","slug":"王者荣耀-元歌教学","date":"2019-11-03T15:05:06.000Z","updated":"2020-04-12T14:15:48.002Z","comments":true,"path":"26e6fc90/","link":"","permalink":"https://shmily-qjj.top/26e6fc90/","excerpt":"","text":"一波元歌攻略分享有些英雄，看似很秀，看似很难，其实只要理解了它的每个步骤，就会发现它并不难。今天我为大家带来一波干货满满的元歌攻略分享！元歌，是一个低风险，高回报的强势英雄，虽然经历过天美无情的剥削，但依然能在峡谷里绽放光芒！很多玩家一看到元歌，就想起那个口诀“1433223”，但如果只会记口诀，那就Out啦！其实很简单，我们只要明确它每一步都是干什么的，就更透彻地理解元歌，就能运用自如！如果一直靠记口诀玩，那将毫无意义！下面我们开始吧！什么？没有积极性？要不看一下秀操作视频==&gt;链接 出装解析 傀儡冷却较长，推荐冷却鞋 主要针对脆皮，推荐暗影战斧 秒杀型刺客，自带斩杀效果，配合破军，能秒杀任何脆皮 伤害足够秒杀敌方，为了确保万无一失，推荐名刀 对面战坦出了肉装，可能也有脆皮出了肉装，这时推荐碎星锤 伤害完全足够，但元歌基础血量太低，为了不被秒杀以及续航能力，推荐霸者重装 召唤师技能，推荐闪现 口诀解析不得不说“1433223”是个很好的例子，我来解析一下每一步都做了什么吧！ 1技能释放傀儡，傀儡弹出的路径上会对英雄造成伤害和半秒击飞(眩晕)。 傀儡交换的路径上都有伤害和不到半秒的击飞。 4技能拉动傀儡和真身一段距离。当然，路径上也有伤害，但没控制。 3技能这两个三技能是关键，要将傀儡靠近敌方英雄，三技能第一段是减少敌方英雄500移速持续两秒，3秒内可释放第二段。多么强大的减速！ 第二个3技能三技能第二段是元歌的核心控制技能。用傀儡贴近敌方英雄并释放三技能第二段，控制敌方约2.5s。多么强大的控制！ 2技能在三技能二段释放后的三秒内释放二技能，将真身拉到敌人身边。 第二个2技能【即真身的2技能】此时敌人还在被控制的状态，如同待宰的羔羊，这时释放第二个二技能，直接使敌方英雄残血。 3技能【即真身的3技能】此时敌人还在被控制的状态，如同待宰的羔羊，三技能是由两条线组成，每条线都有伤害，交叉点命中英雄会对其造成已损生命16%的额外伤害，和持续两秒的50%减速并安全退出战场一段距离，同时得到一个护盾和瞬间加速效果。如果是脆皮或者残血就直接带走了，如果经济高出1000，这一套操作下来将秒杀一切！ 真身4技能自己按一下就知道，解控+位移+免伤 如何练习根据上面的口诀解析，我们已经大致知道元歌的玩法，下面就是入门的练习方法。 一开始还不熟悉，肯定会很懵，所以，先尽可能慢地按出1-3-3-2-2-3，越慢越好，千万不要急于求成，体会每一步做了什么，然后再逐渐加快速度。 玩上几把后就会对1-3-3-2-2-3这套操作恍然大悟，虽然可能按不熟悉，但也理解了每一步的作用。 之后，我们加入4技能，1-4-3-3-2-2-3，就会发现4技能的目的就是让傀儡接近敌方英雄，来提高控住敌方英雄的概率。 再玩上几把或十几把后，你就会对4技能的距离有一个初步的把握，口诀是死的，人是活的，聪明的你还会发现，对方在你首次按3技能减速的时候位移了怎么办，这时，4技能的作用就大了，你会想到1-3-4-3-2-2-3，即先减速，对方位移，4技能让傀儡贴近，3技能控制，2技能将真身拉过来，2技能打伤害，3技能收割+退出战场。 怎么样，行云流水的操作！这时你就会体会到，理解每个步骤的作用的重要性。相信后面你肯定会更加灵活！ 你会发现，傀儡除了可以探视野，吓唬人，骗技能之外，还可以蹲在草丛里守株待兔，敌方英雄进草丛后，一套3-3-2-2-3，就能直接带走，是不是像极了小妲己！ 再多打些场次，你就会发现，手速不够呀！别灰心，多打就能找到感觉，保持紧张，手速自然也提上来了！ 久经峡谷，因为你深刻理解了元歌的每一步操作，所以你自然有10000种方式逃跑，比如4-1-4-2-1-3-2，比如1-2-1，再比如3-4-1-2-1等等…没有人能轻易搞死你，想死都难… 进阶恭喜你入门了！想必也发现了元歌这个英雄的强大了。下面我还要介绍一些独门秘籍！ 15度角Surprise亲自截图给你萌看！ 元歌的三技能是交叉的，交叉点很近，但线的攻击范围很远 如果想打正前方的敌人，则手动施法像偏离15度角的方向释放3技能 敌方残血会被3技能的线刮到，虽然不如交叉点疼，但后期也有至少1000+的伤害，足以收割 这个技巧较难，需要多去感受和观察 越塔Surprise注意：手速不够快请忽略此条！ 众所周知，元歌傀儡能抗2-3次防御塔攻击。 2次防御塔攻击大约2秒。 2秒内要做什么：1-4-3-3-1(释放傀儡，靠近敌方，减速瞬间控住敌方，傀儡要没血了赶快收回傀儡) 因为收回傀儡也有击飞0.5秒，0.5秒之内做什么：3-2或2-3 恭喜完成了越塔秒杀 残血收割机看到残血，利用傀儡的冲撞和收回直接收割。具体操作：1-4-3-1(-2-4)括号里是可选操作。 1技能释放傀儡，4技能冲撞对方造成少量伤害，对方可能还没死 3技能第一段伤害很高，直接收割 1技能收回傀儡 如果残血没死可选补个2技能，如果有追兵，可选4技能逃跑 持续输出其实这不算啥技巧，就是一定要多参团。元歌后期2，3技能距离长，冷却短，跟在团后面持续消耗，收割，配合15度角，谁也顶不住！输出最高非你莫属！ End感谢你看完我的元歌攻略分享，有什么心得体会，欢迎在下方评论区留言！B站元歌视频：链接加油，下一个国服元歌，就是你！","categories":[{"name":"其他","slug":"其他","permalink":"https://shmily-qjj.top/categories/其他/"}],"tags":[{"name":"王者荣耀","slug":"王者荣耀","permalink":"https://shmily-qjj.top/tags/王者荣耀/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"https://shmily-qjj.top/categories/其他/"}]},{"title":"Sqoop学习笔记","slug":"Sqoop学习笔记","date":"2019-11-03T05:15:27.000Z","updated":"2020-04-12T14:15:47.999Z","comments":true,"path":"26078/","link":"","permalink":"https://shmily-qjj.top/26078/","excerpt":"","text":"什么是SqoopSqoop是一款开源工具，用于Hadoop(Hive)与mysql等传统数据库间进行数据传递，可以将关系型数据库mysql,Oracle等中的数据导入HDFS中，也可以把HDFS中的数据导入到关系型数据库中。Sqoop2与Sqoop1完全不兼容，一般生产环境使用Sqoop1，这里主要说Sqoop1 Sqoop原理Sqoop原理很简单，就是将导入导出的命令翻译成MapReduce程序，Sqoop的操作主要目的（工作）是对MR程序的inputformat和outputformat进行定制.下图是Sqoop原理架构图图上意思很明确，这里不多赘述。戳官方文档了解更多 Sqoop安装部署去官网下载Sqoop的二进制包 tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt/module/ cd /opt/module/ mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop vim /etc/profile export SQOOP_HOME=/opt/module/sqoop export PATH=$PATH:$SQOOP_HOME/bin source /etc/profile cd sqoop/conf cp sqoop-env-template.sh sqoop-env.sh vim sqoop-env.sh 文件末尾加入如下配置： #Set the path for where zookeper config dir is #export ZOOCFGDIR= export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2 export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2 export HIVE_HOME=/opt/module/hive export HBASE_HOME=/opt/module/hbase export ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.13 export ZOOCFGDIR=/opt/module/zookeeper-3.4.13/conf 拷贝mysql驱动到Sqoop的lib目录下 cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/ 运行sqoop help命令 部署完成，测试： 查看Mysql表 sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password 000000 如果一切正常 - 则安装没问题了 Sqoop操作Sqoop的导入和导出导入：数据从RDBMS到HDFS的过程，数据源是RDBMS，目标是HDFS导出：数据从HDFS到RDBMS的过程，数据源是HDFS，目标是RDBMS 导数据到HDFS准备数据： 创建数据库和表 create database test; create table sqoop_test(id int primary key not null auto_increment,name varchar(255),sex varchar(255)); insert into test.sqoop_test(name,sex) values(&#39;qjj&#39;,&#39;male&#39;); insert into test.sqoop_test(name,sex) values(&#39;abc&#39;,&#39;female&#39;); use test; select * from sqoop_test; 导入HDFS方式分为全部导入/查询导入/导入指定列/筛选导入/增量更新 全部导入–num-mappers 1 设置一个map，输出文件个数也为1–null-string 指定字段为空时用什么代替 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --null-string &quot;-&quot; \\ --target-dir /user/sqoop/out \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 查询导入不通过- -table来指定，而是通过写- -query来指定$CONDITIONS是必须加的，为了在多个Map的情况下，可以传递参数，以保证导出数据的顺序不变。 where $CONDITIONS必备 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --target-dir /user/sqoop/out \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; \\ --query &#39;select name,sex from sqoop_test where id &lt;= 1 and $CONDITIONS;&#39; (这里如果用双引号，则$CONDITIONS需要转义) 导入指定列–delete-target-dir -&gt; 如果HDFS目录已经存在则删除–columns指定多个列 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --columns id,sex \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 筛选导入关键字筛选/字段筛选–where “条件”而且–where与–columns可以同时使用，但不能与–query同时使用 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --where &quot;id=1&quot; \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 增量更新表更新时重新导入浪费时间和资源增量更新三个重要参数–incremental append 指定增量导入–check-column col_name 以一个列作为增量导入的标准，这个列变化才会触发增量导入–last-value 指定上次导入的参考列的最后一个值（比如check-column为id，上次导入的id值为4，则增量导入要指定last-value为4） Sqoop官方用户文档: Sqoop User Guide 导数据到Hivemysql数据导入Hive过程分两步： Mysql先导入到HDFS 已被导出到HDFS的数据移动到hive仓库 hive-import指定target为hivetarget hive表会自动创建 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --hive-import \\ --fields-terminated-by &quot;\\t&quot; \\ --hive-overwrite \\ --hive-table sqoop_hive Sqoop官方参考: Importing Data Into Hive 导数据到HBase–columns指定source表中哪几列–column-family指定列族名称–split-by按指定列名字段做切分注意：需要手动创建HBase目标表（以前1.0老版本HBase自动创建） sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --columns &quot;id,name,sex&quot; \\ --column-family &quot;info&quot; \\ --hbase-create-table \\ --hbase-row-key &quot;id&quot; \\ --hbase-table &quot;hbase_company&quot; \\ --split-by id Sqoop官方参考: Importing Data Into HBase Sqoop数据导出Hive/HDFS导出数据到RDBMS过程是将表数据每行都编程字符串，然后插入mysql，所以必须指定–input-fields-terminated-by来把字段切割开注意，RDBMS作为target表，需要手动创建target表–export-dir指定了数据仓库中表数据位置 sqoop export \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --export-dir /user/hive/warehouse/sqoop_hive \\ --input-fields-terminated-by &quot;\\t&quot; 脚本操作Sqoop公司一般会使用调度工具定期执行脚本，比如获取前一天的数据要在凌晨一两点进行抽取数据，这就需要定时任务，所以为了方便定时任务，Sqoop参数也要写在脚本里，类似于hive -f hql_file touch hdfs_to_mysql_job vim hdfs_to_mysql_job 内容如下: export --connect jdbc:mysql://localhost:3306/test --username root --password 000000 --table sqoop_test --num-mappers 1 --export-dir /user/hive/warehouse/sqoop_hive --input-fields-terminated-by &quot;\\t&quot; 执行该任务: sqoop --options-file hdfs_to_mysql_job Sqoop参数中文参考文档Sqoop参数中文文档，里面还包括了参数的实现类类名，供参考和深入学习，点击链接下载:Sqoop参数-PDF版 总结 强行结束MR任务后，不急着再启MR任务，MRAppMaster任务需要kill掉再运行新任务 多看官方文档，里面很详细 要根据实际使用场景学习官方文档中重要的常用的部分 Sqoop毕竟是基于MapReduce的，而MR的运算速度已经不能满足我们的需求，所以导数据和抽取数据的流程完全可以用Spark来代替Sqoop，Spark2.4版本后稳定性和效率都有提升，且能兼容多种数据源，能完成99%的Sqoop任务，当然有一些追求稳定而非速度的抽取数据的任务仍然可以使用Sqoop","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"Sqoop工具","slug":"Sqoop工具","permalink":"https://shmily-qjj.top/tags/Sqoop工具/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"CentOS7安装CDH6安装与排坑","slug":"CentOS7安装CDH6全程记录","date":"2019-10-29T02:50:40.000Z","updated":"2020-08-08T14:07:39.932Z","comments":true,"path":"38328/","link":"","permalink":"https://shmily-qjj.top/38328/","excerpt":"","text":"前言一开始搭集群时，都是装Apache原生的Hadoop，Spark包，一个一个装，一个一个配，好麻烦，而且通过命令或REST监控还很不直观，直到我遇到了Cloudera Manager，这东西简直就是神器。Cloudera Manager（简称CM），是Cloudera开发的一款大数据集群部署神器，而且它具有集群自动化安装、中心化管理、集群监控、报警等功能，通过它，可以轻松一键部署，大大方便了运维，也极大的提高集群管理的效率。一开始因为CentOS8出来了，想尝鲜，发现ClouderaManager没有el8的版本，所以暂时还不能用CentOS8来安装CM，请千万不要尝试使用CentOS8。 CM的主要功能： 管理：对集群进行管理，如添加、删除节点等操作 监控：监控集群的健康情况，对设置的各种指标和系统运行情况进行全面监控 诊断：对集群出现的问题进行诊断，会针对集群问题给出建议的方案 集成：对hadoop生态的多种组件和框架进行整合，减少部署时间和工作量 兼容：与各个生态圈的兼容性强 总结一下就是：方便搭建和运维，提供全面监控 CDH架构CDH的组件： Agent：在每台机器上安装，该代理程序负责启动和停止服务和角色的过程，拆包配置，触发装置和监控主机。 Management Service：负责执行各种监控，警报和报告功能角色等服务。 Database：存储配置和监视信息。通常情况下，多个逻辑数据库在一个或多个数据库服务器上运行。例如，Cloudera的管理服务器和监控角色使用不同的逻辑数据库。 Cloudera Repository：软件由Cloudera管理分布存储库。 Clients：是用于与服务器进行交互的接口 CDH中都有哪些服务? 组件名称 用途 Zookeeper Apache ZooKeeper 是用于维护和同步配置数据的集中服务。 HDFS HDFS是 Hadoop 应用程序使用的主要存储系统。 yarn Apache Hadoop MapReduce 2.0 (MRv2) 或 YARN 是支持 MapReduce 应用程序的数据计算框架。依赖HDFS服务。 HBase 支持随机读/写访问的Hadoop数据库(HBase是一个分布式、面向列的开源数据库，) Hive 在大数据集合上的类SQL查询和表。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 impala Impala是一个新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。 solr Solr是一个分布式服务，用于编制存储在 HDFS 中的数据的索引并搜索这些数据。 spark Spark是强大的开源并行计算引擎，基于内存计算，速度更快；接口丰富，易于开发；集成SQL、Streaming、GraphX、MLlib，提供一栈式解决方案。 flume 高可靠、可配置的数据流集合。 storm Storm是一个分布式的、容错的实时计算系统。 kafka Kafka是一种高吞吐量的分布式发布订阅消息系统。 Hue 可视化Hadoop应用的用户接口框架和SDK。。 Sqoop 以高度可扩展的方式跨关系数据库和HDFS移动数据 oozie Oozie是一种框架，是用于hadoop平台的作业调度服务。 Avro 数据序列化：丰富的数据结构，快速/紧凑的二进制格式和RPC。 Crunch Java库，可以更轻松地编写，测试和运行MR管道。 DataFu 用于进行大规模分析的有用统计UDF库。 Mahout 用于群集，分类和协作过滤的库。 Parquet 在Hadoop中提供压缩，高效的列式数据表示。 Pig 提供使用高级语言批量分析大型数据集的框架。 MapReduce 强大的并行数据处理框架。 Pig 数据流语言和编译器 Sqoop 利用集成到Hadoop的数据库和数据仓库 Sentry 为Hadoop用户提供精细支持，基于角色的访问控制。 Kudu 完成Hadoop的存储层，以实现对快速数据的快速分析。 安装部署在虚拟机环境上部署Cloudera Manager，可能达不到预期的效果，但是基本的功能可以实现。我的电脑内存16GB勉强可以使用，如果电脑16GB以上的可以考虑折腾CDH，16GB以下的想都不要想… 环境物理机i7-6700hq 16GB内存 1T HDD虚拟机四台 8个逻辑核心 内存分配分别是 5GB 3GB 2GB 2GB （可以说是榨干了物理机性能）建议如果没有i7-8th及以上CPU或没有32G+的电脑，就不要尝试了。还是直接装Apache版的好些。VMWare 15SecureCRT 8.1.4FileZilla 3.40.0CentOS 7 以上是旧配置，后面再有更新均使用新的配置：物理机i7-9750h 64GB内存 2T HDD+2T SSD虚拟机四台 12个逻辑核心 内存分配分别是 20GB 14GB 14GB 10GBHyper-V虚拟机SecureCRT 8.5.3FileZilla 3.40.0CentOS 7能够同时运行所有服务。 一.基础配置下载CentOS7:CentOS7 Minimal下载 虚拟机配置采用NAT格式网卡,按如下配置虚拟网卡设置（编辑-虚拟网络编辑器）点击NAT设置:点击DHCP设置:以后我们的虚拟机都使用NAT网卡安装CentOS7文件-&gt;新建虚拟机-&gt;选择自定义(高级)-&gt;下一步-&gt;下一步-&gt;稍后安装操作系统-&gt;选择Linux/CentOS7 64位-&gt;下一步-&gt;虚拟机名称CDH066-&gt;下一步-&gt;根据自己电脑设置核心数-&gt;下一步-&gt;虚拟机内存5120MB-&gt;网络类型选NAT-&gt;下一步…-&gt;磁盘分配80GB-&gt;下一步-&gt;下一步-&gt;自定义硬件-&gt;选择CentOS7的安装镜像,如图:关闭-&gt;完成-&gt;开启此虚拟机开始安装安装Minimal版的CentOS，感觉很清爽！但是后续需要自己手动装一些依赖包，不过这样也好，可以避免安装过多无用的依赖。时区选择ShangHai。 在这步安装时指定root密码123456安装时指定一个管理员用户shmily 密码123456 ===2020更新===推荐使用Hyper-V win10自带不说，轻量级的检查点超好用，就算崩了还能从检查点恢复。感觉甩VMWare一百条街。配置方法大体相同，要注意网络那块在Hyper-V管理器中的虚拟交换机管理器新建内部网络，然后如果要指定IP，需要去电脑的网络设置IPV4，然后设置把Wifi网络共享给这个网卡。IPV4：192.168.x.1 (x均替换为你喜欢的值 1-254)网关255.255.255.255.0然后配置虚拟机ifcfg-eth0时IPADDR=192.168.x.101NETMASK=255.255.255.0DNS1=192.168.x.1DNS2=192.168.x.2……..很方便 安装完成后Reboot，按步骤进行如下配置 rm -rf * vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=cdh066 vi /etc/sysconfig/network-scripts/ifcfg-ens33 修改以下几项的值 BOOTPROTO=static ONBOOT=yes NM_CONTROLLED=yes IPADDR=192.168.1.66 GATEWAY=192.168.1.2 DNS1=192.168.1.2 vi /etc/sudoers 添加以下，必要的话可以加其他用户权限控制策略，这里我对root和shmily两个用户赋权 root ALL=(ALL) ALL 下面添加： shmily ALL=(ALL) ALL systemctl start NetworkManger systemctl enable NetworkManger service NetworkManager status systemctl status firewalld.service # 查看防火墙状态 systemctl stop firewalld.service # 关闭防火墙 systemctl disable firewalld.service # 关闭防火墙开机启动 systemctl is-enabled firewalld.service # 查看防火墙是否开机启动 # 关闭selinux vi /etc/selinux/config 配置文件中的 SELINUX=disabled # 开启SSH服务，用于使用SecureCRT连接 # 检查ssh服务是否开启（CentOS7默认开启） ps -e | grep sshd # 修改Hostname vi /etc/hostname localhost.localdomain改为cdh066 vi /etc/hosts # 添加如下记录 192.168.1.66 cdh066 192.168.1.67 cdh067 192.168.1.68 cdh068 192.168.1.69 cdh069 reboot # 重启机器CDH066 # 检查22端口是否开启 （CentOS87默认开启） yum install net-tools netstat -an | grep 22 SecureCRT连接测试SSHSecureCRT创建New Session -&gt; SSH2 -&gt; Hostname是CDH066 username是root发现还是会提示Hostname lookup failed: host not found需要修改Windows的C:\\Windows\\System32\\drivers\\etc\\hosts添加如下并保存192.168.1.66 cdh066192.168.1.67 cdh067192.168.1.68 cdh068192.168.1.69 cdh069 重新用SecureCRT连接出现如下图:Accept &amp; Save,输入密码并勾选Save password完成FileZilla也能连接了: 检查一下网络:ping 8.8.8.8能ping通即可进行下一步，如果ping不通，需要仔细检查网络配置文件: 安装python:CentOS7 Minimal默认带Python2.7.5版本，已经满足需求，为了开发方便，还是安装个ipython吧 yum -y install epel-release yum install python-pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests # 安装必要的库可以指定源 以安装requests库为例 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ipython # 安装ipython 完成后，命令行执行python即可运行python2.7.5，命令行执行ipython即可使用ipython 安装一些必要的常用命令[必要]yum install bind-utilsyum -q install /usr/bin/iostatyum install vim wget iotop lsofyum install -y gityum install dstat (全面的系统监控工具-推荐)yum install nload 安装一些CDH所需的必要依赖[必要] yum -y install chkconfig bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs redhat-lsb httpd httpd-tools unzip ntp systemctl start httpd.service # 启动httpd服务 systemctl enable httpd.service # 设置httpd开机启动 yum -y install httpd createrepo # createrepo是安装CDH6集群必备 vim /etc/rc.local 添加 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled chmod +x /etc/rc.d/rc.local 安装JDK1.8[千万不要自行更换版本]去Oracle官网下载1.8版本8u181的安装包JDK 1.8历史版本下载如果安装最新版本，后续CDH安装服务会无法启动，遇到各种问题。要明确CDH6.3支持的JDK版本： 这里注意，JDK目录一定是/usr/java/jdk_1.8.x_xx，这样CM服务才能检测到JDK，否则服务无法启动 mkdir /opt/software # 通过FileZilla上传到CDH066节点的 &lt;u&gt;/opt/software&lt;/u&gt;目录下 cd /opt/software mkdir /usr/java/ tar -zxvf jdk-8u181-linux-x64.tar.gz -C /usr/java/ cd .. vim /etc/profile 添加 #JAVA_HOME export JAVA_HOME=/usr/java/jdk1.8.0_181 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar source /etc/profile java -version 还有一些其他的监控命令Linux监控命令汇总 Mysql安装(CDH必备)首先查看Cloudera Manager官网要求的Mysql版本：Database Requirements参考CDH6.x兼容的版本，我们选择Mysql5.7版本注意:mysql-server依赖mysql-clientmysql-client依赖mysql-community-libsmysql-community-libs依赖mysql-community-common所以安装Server会默认安装其全部依赖 rpm -qa|grep mariadb rpm -e --nodeps mariadb-libs-5.5.64-1.el7.x86_64 # 卸载MariaDB 虽然CDH6支持了MariaDB，但还是推荐Mysql wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum -y install mysql-community-server 如图安装完成，接着我们对其进行一些配置 systemctl start mysqld.service systemctl enable mysqld.service # 设置开机启动 systemctl status mysqld.service # 查看mysql运行状态 grep &#39;temporary password&#39; /var/log/mysqld.log # 找到root初始密码，我的是cWgrI9:14%=_ mysql -uroot -p # 登陆mysql # 提示Enter Password cWgrI9:14%=_ set global validate_password_policy=LOW; # 没有这项会提示Your password does not satisfy the current policy requirements 如果不是生产环境需要修改密码安全策略等级为LOW set global validate_password_length=6; # 最低密码长度，因为测试所以设为了6 生产环境则不需要修改 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; # 修改数据库密码为123456 CREATE USER &#39;mysql&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # root登陆然后创建用户及其密码（用户名mysql为例） GRANT ALL ON mysql.* TO &#39;mysql&#39;@&#39;%&#39;; # 赋予mysql用户所有权限 flush privileges; # 刷新配置 status; # 通过这个命令发现Mysql目前不是UTF-8字符集 配置utf-8字符集vim /etc/my.cnf 添加如下配置注意顺序，client一定在mysqld属性的上方 [client] default-character-set=utf8 [mysqld] init_connect=&#39;SET collation_connection = utf8_unicode_ci&#39; init_connect=&#39;SET NAMES utf8&#39; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake 根据CDH官方推荐的Mysql参数配置,继续添加如下参数:如果生产环境，需要根据集群配置的实际情况来设定 [mysqld] transaction-isolation = READ-COMMITTED symbolic-links = 0 key_buffer_size = 32M max_allowed_packet = 32M thread_stack = 256K thread_cache_size = 64 query_cache_limit = 8M query_cache_size = 64M query_cache_type = 1 max_connections = 550 expire_logs_days = 10 max_binlog_size = 100M log_bin=/var/lib/mysql/mysql_binary_log server_id=1 binlog_format = mixed read_buffer_size = 2M read_rnd_buffer_size = 16M sort_buffer_size = 8M join_buffer_size = 8M # InnoDB settings innodb_file_per_table = 1 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 64M innodb_buffer_pool_size = 128M innodb_thread_concurrency = 8 innodb_flush_method = O_DIRECT innodb_log_file_size = 512M sql_mode=STRICT_ALL_TABLES # disable_ssl skip_ssl 重启Mysql服务systemctl restart mysqld.service 登录Mysql并查看是否修改成功mysql -hlocalhost -P3306 -uroot -p123456show variables like “%character%”;show variables like “%collation%”;如图即为配置成功 创建CM的数据库并增加数据库所属用户的远程登陆权限： CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;rman&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;sentry&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;nav&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;navms&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON scm.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; set global validate_password_policy=LOW; set global validate_password_length=6; GRANT ALL ON root.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # 让root用户可以在cdh066节点上登录 FLUSH PRIVILEGES; 关于如何查看和修改用户的远程登录权限：select user,host from mysql.user;host字段为%的则是允许远程登录的用户，是localhost的只能本地登录所以想给远程某台机器开通远程访问某个用户的权限： update mysql.user set host=’CDH066’ where user=’root’;或者想给某个用户所有局域网内机器的访问权限： update mysql.user set host=’%’ where user=’root’;然后重启服务或者刷新配置就可以通过mysql -hCDH066 -uroot -p123456来登录了远程其他节点可以通过制定-h来访问非root用户的mysql Mysql JDBC库配置：右键 链接另存为 进行下载下载mysql-connector-java-5.1.47-bin.jar，将mysql-connector-java-5.1.47-bin.jar文件上传到CDH066节点上的/usr/share/java/目录下并重命名为mysql-connector-java.jar（如果/usr/share/java/目录不存在，需要手动创建） 好玩的screenfetch(可选，用来娱乐…) cd /usr/local/src git clone https://github.com/KittyKatt/screenFetch.git cp screenFetch/screenfetch-dev /usr/local/bin/screenfetch chmod 777 /usr/local/bin/screenfetch 我的配置暂时这样，如果后续有什么需要改进我再更新。更多安全与防火墙配置参考安全与防火墙配置有关linux用户和组的详细文章:Linux用户和组 二.克隆虚拟机克隆CDH所需的另外三台虚拟机右键CDH066这台已关闭的虚拟机，右键-&gt;管理-&gt;克隆选择虚拟机中当前状态 下一步选择创建完整克隆 下一步虚拟机名称 CDH067 完成同样方法克隆 CDH068 CDH069克隆完成对机器进行设置:CDH067 3GB内存CDH068 2GB内存CDH069 2GB内存 开启CDH067机器确保 /ect/hosts里已经添加了其他机器的ip和hostvim /etc/sysconfig/network-scripts/ifcfg-ens33删除UUID和HWADDRIPADDR重新分配为192.168.1.67修改后如图 vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=CDH067 vi /etc/hostnameCDH066改为CDH067 重启 reboot 同样方式修改CDH068,CDH069 检查： ping8.8.8.8能通 使用SecureCRT可以正常连接到机器 ifconfig 或 ip addr命令查看ip地址成功改过来了则配置成功 三.配置免密登录在四台机器分别操作：ssh-keygen 并连续敲三下回车 在66机器上ssh-copy-id cdh066 建立 cdh066自身免密ssh-copy-id cdh067 建立 cdh066 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh066 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh066 -&gt; cdh069单向免密 在67机器上ssh-copy-id cdh066 建立 cdh067 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh066自身免密ssh-copy-id cdh068 建立 cdh067 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh067 -&gt; cdh069单向免密 在68机器上ssh-copy-id cdh066 建立 cdh068 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh068 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh068自身免密ssh-copy-id cdh069 建立 cdh068 -&gt; cdh069单向免密 在69机器上ssh-copy-id cdh066 建立 cdh069 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh069 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh069 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh069自身免密 测试都能免密登录:至此免密登录配置完成 四.CDH6安装下面的是下载地址，因为我之前手动安装了JDK1.8，所以可以不下载oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm这个包，其余的包全部下载下来CDH6.3.1下载地址还需要一个asc文件，下载地址：allkeys.asc,右键另存为即可在cdh066节点上进行操作mkdir /opt/software/cloudera-repos将下载的所有文件通过FileZilla上传到/opt/software/cloudera-repos目录，目录结构如下:├── allkeys.asc├── cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm├── cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm├── cloudera-manager-server-db-2-6.3.1-1466458.el7.x86_64.rpm├── enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpm└── cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm CDH066节点执行如下命令 目的是建立本地存储库 搭建本地源 为了节省空间，也可以只在一台机器上搭建源 cd /opt/software/cloudera-repos createrepo . # 将cloudera-repos目录移动到httpd的html目录下 制作本地源 cd .. mv cloudera-repos /var/www/html/ cd /etc/yum.repos.d touch cloudera-manager.repo vim cloudera-manager.repo 添加如下 baseurl地址对应自己的主机host 如 cdh067节点:http://cdh067/cloudera-repos/ [cloudera-manager] name=Cloudera Manager 6.3.1 baseurl=http://cdh066/cloudera-repos/ gpgcheck=0 enabled=1 autorefresh=0 type=rpm-md yum clean all yum makecache 制作本地源后http://cdh066/cloudera-repos/这个链接可以访问到源的文件我们搭建的本地源，后面会用到 安装Cloudera Manager组件: # 在CDH066节点运行 yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server --skip-broken --nogpgcheck 下载parcel包，：Index of cdh6/6.3.1/parcels/下载其中的CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel和manifest.json这两个文件，将这两个文件上传到/opt/cloudera/parcel-repo目录 cd /opt/cloudera/parcel-repo sha1sum CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel | awk &#39;{ print $1 }&#39; &gt;CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha chown -R cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo/* /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm 123456 # 初始化数据库 systemctl start cloudera-scm-server.service # 启动CM服务systemctl status cloudera-scm-server.service # 查看启动状态 等待几分钟后访问http://cdh066:7180，默认帐号密码都是admin 这里选择免费版本 下面就是群集安装的步骤：主机名称填写cdh066,cdh067,cdh068,cdh069，然后点击搜索搜索这里搜到了67，68，69节点，但是66节点是灰色的，安装时，66节点不会被安装Agent，意味着后续安装的组件只能部署在67，68，69节点上运行，不过没有关系，可以在添加组件的步骤之前新开个页面将cdh066也加进去。 这步使用我们搭建的本地源 http://cdh066/cloudera-repos/ 如下设置 这步不要勾选 填入root用户的密码 这步耐心等待，不要手动刷新 这步勾选最后一项 开始安装服务 如图，选自定义服务根据集群环境和需求选择合适的服务和搭配。 填上之前建的数据库，选的服务不同要求也不同 最后部署成功，启动服务：因为我虚拟机搭建，物理机本身配置就很差，有内存不足和请求延迟高的问题，所以，虽然服务都能正常打开，跑一两个小的计算任务也还能勉强承受，但CDH都会报警告，大多都是提示分配内存低了，请求延迟高了，内存不足等信息。后续文章更新内容采用新电脑64GB内存i7-9750h物理机环境，能正常运行CDH服务： ClockOffset的报警：集群全红，提示ClockOffset 未检测到ntpd服务。这个时候就需要配置NTP时间同步服务，请参考：集群NTP服务配置 中的 配置内网NTP-Clients 部分。 五.功能扩展 自定义告警脚本 集成官方Flink-1.9.0到CDH管理下载相应的csd文件和parcels文件到本地：csd下载地址parcels下载地址下载后得到如下：FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jar FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.sha FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel manifest.json 将FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jar放入/opt/cloudera/csd中将FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel和FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.sha放入/opt/cloudera/parcel-repo中然后重启Cloudera Manager Server服务：sudo systemctl restart cloudera-scm-server重启完成后进入页面，主机-&gt;Parcel-&gt;检查新Parcel-&gt;找到Flink-&gt;分配完成分配后开始添加服务：好啦，Flink可以使用啦。 总结坑点： 如果遇到HDFS无法启动的问题，可能是因为/dfs/nn/,/dfs/dn/,/dfs/snn/这些目录和里面的文件权限不够，请检查每个节点的这几个目录，保证nn,dn,snn文件夹权限为drwx—— 3 hdfs hadoop，即hdfs用户hadoop组，里面的current文件夹的权限为drwxr-xr-x 3 hdfs hdfs。 提示Error: JAVA_HOME is not set and Java could not be found 先确保JDK安装路径在/usr/java/jdkxxxxx，再确定JAVA版本是当前CDH支持的JAVA版本，过高过低都不会兼容，就报这个错误。 The number of live datanodes 2 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. 不多说，关闭安全模式 hdfs dfsadmin -safemode leave 注意，需要sudo到hdfs用户操作 如果sudo到hdfs失败，就vim /etc/passwd 将hdfs用户对应的/sbin/nologin改成/bin/bash 即可sudo到hdfs","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/大数据/"},{"name":"CDH6+CentOS7","slug":"CDH6-CentOS7","permalink":"https://shmily-qjj.top/tags/CDH6-CentOS7/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"记一次参加QCon全球软件开发大会","slug":"记一次参加QCon全球软件开发大会","date":"2019-10-21T11:59:50.000Z","updated":"2020-04-12T14:15:48.004Z","comments":true,"path":"39595/","link":"","permalink":"https://shmily-qjj.top/39595/","excerpt":"","text":"QCon全球软件开发大会全球软件开发大会是为团队领导者、架构师、项目经理和高级软件开发人员量身打造的企业软件开发大会，其所覆盖的主题内容与InfoQ网站相同，关注架构与设计、真实案例分析等等。秉承”促进软件开发领域知识与创新的传播”原则，QCon各项议题专为中高端技术人员设计，内容源于实践并面向社区。演讲嘉宾依据各重点和热点话题，分享技术趋势和最佳实践。了解更多请戳👉 QCon官网 菜鸟可以去么当然可以啦，我zou是一只小菜鸟啊！虽说QCon是为团队领导者、架构师、项目经理和高级软件开发人员量身打造的企业软件开发大会，但如果你们公司有机会，又恰好把这宝贵的学习机会给你们了，那就要珍惜呀！看看还是很有好处的，如果可以，选一些自己正在做的技术相关的场次参加，可以扩宽思路，你会发现我们要解决的问题其实还有更多解决方案！哪怕有听不懂的地方，记下来回去查都很受益！ 分享我所听因为我只参加了18号下午场的QCon，所以也只听了不到四个分享会,但我会把我觉得很有用的技术或者思路分享出来！ Splash Shuffle Manager 关于Spark的ShuffleShuffle简而言之:下一个Stage向上一个Stage要数据这个过程，就称之为 Shuffle.学过Spark的童鞋都知道大多数Spark作业的运行时间主要浪费在Shuflle过程中,因为该过程包含了大量的本地磁盘IO,网络IO和序列化过程.而看过Spark源码的童鞋应该都知道Spark的ShuffleManager,虽然Spark2.x已经摒弃了HashShuffleManager,但是如果过大的表遇到”去重”,”聚合”,”排序”,”重分区”或”集合”操作等shuffle算子时还是会有大量文件落盘,而本地磁盘的性能会严重拖慢Spark计算的整体速度. 而且Shuffle发生的机器如果发生故障还会导致Stage重算,性能和稳定性都大大降低有些大规模的计算是Shuffle调优不能解决的 Splash介绍关于以上问题,我们可以通过更改Shuffle Manager的源码来实现自定义Shuffle的溢写文件存储位置,但是,改源码辣么难……咋办……Splash-支持指定Shuffle过程溢写文件的存储位置 可以指定Shuffle文件存储到高可靠的分布式存储中 ShuffleFile接口代替本地文件访问 可以使用不同的网络传输和后端存储协议来实现随机读取和写入 Splash Shuffle Manager优点 使Executor变为无状态 使添加删除节点更有灵活性,宕机无需重复计算整个shuffle文件 Shuffle文件提交符合原子性,未提交的文件可以轻松清理 随机存储和计算的分离,提供Shuffle存储介质的更多选择 Splash Shuffle Manager位于Executor上,降低部署难度 Shuffle Performance Tool可以检验存储介质性能 Splash结构和原理如图蓝色框为Splash实现类,橙色框是Spark定义的接口,绿色框是基本数据结构使用Splash后的Shuffle过程:ShuffleManager是入口,ShuffleWriter在map stage写shuffle数据,用SplashSorter或SplashUnsafeSorter将数据保存在内存中,内存不足时则会将数据溢写到TmpShuffleFile,等所有数据计算完成,SplashSorter或SplashUnsafeSorter,合并内存文件和溢写文件,SplashAggregator负责数据聚合,使用SplashAppendOnlyMap数据结构,内存不够时持久化到shuffle数据存储系统;ShuffleReader从shuffle数据存储系统收集reduce stage需要的数据,SplashShuffleBlockResolver用来查找随即数据,是无状态的. 我认为我认为Splash的设计比较符合Spark计算与存储分离的理念,所以Splash的思路是好的,但展望Spark3.0的新特性,也发现了其实Spark3.0也有一个新特性叫”Remote Shuffle Service”,Remote Shuffle Service的基本想法是，如果Map Task能将Shuffle数据写到独立的Shuffle服务，然后Reduce Task从这个Shuffle服务读Shuffle数据，这样计算节点就不再需要为Shuffle任务保留本地磁盘空间了。这个理念与Splash这个项目的理念很相近,所以我们可以尝试和部署Splash也可以期待Spark3.0的新特性! 最后附上:Splash项目的Git地址 英特尔持久内存Intel Optane DC Persistent Memory,专为数据中心使用而设计的新的内存和存储技术特性就是有媲美DRAM的性能(较DRAM略差)和有SSD一般的容量大小(目前单条512GB),一定程度消除吞吐量瓶颈对大数据计算还是有一定加成的，这里我就不做广告啦，毕竟没有广告费哈哈！ 其他 什么DBDK?这是我听到的名词,搜了一下觉得还挺厉害…不能怪我孤陋寡闻.DBDK出的时候我连HelloWorld都还不会…数据平面开发套件,可以极大提高数据处理性能和吞吐量，提高数据平面应用程序的工作效率。DPDK使用了轮询(polling)而不是中断来处理数据包。在收到数据包时，经DPDK重载的网卡驱动不会通过中断通知CPU，而是直接将数据包存入内存，交付应用层软件通过DPDK提供的接口来直接处理，这样节省了大量的CPU中断时间和内存拷贝时间。 什么推荐中台?推荐中台是个很新的名词吧…我没怎么接触推荐这块,听的要睡着了…嗯,这场分享就是爱奇艺推荐中台…上链接吧:爱奇艺推荐中台：从搭建到上线仅10天，效率提升超30% 什么数据中台?分三部分:数据仓库,大数据中间件,数据资产管理主要元素:累计接入应用数,服务调用,数仓核心表…主要作用:解决数据管控问题,知道谁用.用在哪数据中台能力: 数据资产管理,数据质量管理,数据模型管理,构建标签体系,数据应用规划及实现上链接吧:什么是数据中台,关于数据中台最好的解读 写在最后QCon让我学到了很多东西,扩展了思路,鼓励我在学习新技术的道路上奋勇向前!感觉对新技术更加感兴趣了,如果下次还有机会参加那该多好呀!话说是不是我下次再去就能听懂那些大佬说的东西了吧…做个简单的总结吧 对新技术要仔细调研，琢磨存在的问题 对于陌生的技术要先明白它是做什么的，对扩宽思路有很大帮助 编程不仅是写代码，更要考虑性能，安全性和稳定性 一个新架构的出现必然有其优势，仔细思考它带来的影响 我顺便在会场周边玩了一圈,贴几张自认为不是直男拍的照片:","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}],"tags":[{"name":"QCon全球软件开发大会","slug":"QCon全球软件开发大会","permalink":"https://shmily-qjj.top/tags/QCon全球软件开发大会/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/技术/"}]},{"title":"随想类博客-待更新","slug":"随想类模板","date":"2019-09-21T14:16:00.000Z","updated":"2020-04-12T14:15:48.005Z","comments":true,"path":"14419/","link":"","permalink":"https://shmily-qjj.top/14419/","excerpt":"","text":"今天的话题是…. 内容……..注意结尾两个空格 中标题0中标题1中标题2字颜色大小 This is some text!This is some text!This is some text! 更多内容: Writing 我认为（中标题）xxxxx 小标题小标题0 字体斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本 脚注 列表无序列表用* + -三种符号表示 列表嵌套 有序列表第一项： 第一项嵌套的第一个元素 第一项嵌套的第二个元素 有序列表第二项： 第二项嵌套的第一个元素 第二项嵌套的第二个元素 最多第三层嵌套 最多第三层嵌套 最多第三层嵌套 更多内容: Generating 总结","categories":[{"name":"随想","slug":"随想","permalink":"https://shmily-qjj.top/categories/随想/"}],"tags":[{"name":"标签1","slug":"标签1","permalink":"https://shmily-qjj.top/tags/标签1/"},{"name":"标签2","slug":"标签2","permalink":"https://shmily-qjj.top/tags/标签2/"}],"keywords":[{"name":"随想","slug":"随想","permalink":"https://shmily-qjj.top/categories/随想/"}]}]}