{"meta":{"title":"佳境的小本本","subtitle":"佳境Shmily的个人网站","description":"佳境Shmily的个人网站","author":"佳境Shmily","url":"https://shmily-qjj.top"},"pages":[{"title":"album","date":"2019-09-21T13:47:59.000Z","updated":"2020-04-12T14:15:48.006Z","comments":true,"path":"album/index.html","permalink":"https://shmily-qjj.top/album/index.html","excerpt":"","text":"该模块正在开发balabala","keywords":"相册模块正在开发"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-04-12T14:15:48.006Z","comments":false,"path":"about/index.html","permalink":"https://shmily-qjj.top/about/index.html","excerpt":"","text":"[佳境Shmily] 与&nbsp; 佳境&nbsp; （ Shmily ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-04-12T14:15:48.008Z","comments":false,"path":"client/index.html","permalink":"https://shmily-qjj.top/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"看剧","date":"2019-09-21T13:32:48.000Z","updated":"2020-07-25T03:02:05.166Z","comments":true,"path":"bangumi/index.html","permalink":"https://shmily-qjj.top/bangumi/index.html","excerpt":"","text":"","keywords":"好剧安利"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-04-12T14:15:48.009Z","comments":false,"path":"donate/index.html","permalink":"https://shmily-qjj.top/donate/index.html","excerpt":"","text":"","keywords":"感谢大佬的打赏，我会努力更博滴！"},{"title":"lab","date":"2019-09-21T13:47:59.000Z","updated":"2020-04-12T14:15:48.010Z","comments":false,"path":"lab/index.html","permalink":"https://shmily-qjj.top/lab/index.html","excerpt":"","text":"该模块正在开发balabala","keywords":"Lab实验室模块正在开发"},{"title":"links","date":"2019-11-04T06:11:06.000Z","updated":"2020-12-26T02:12:14.405Z","comments":true,"path":"links/index.html","permalink":"https://shmily-qjj.top/links/index.html","excerpt":"","text":"","keywords":"博友圈"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-04-12T14:15:48.008Z","comments":true,"path":"comment/index.html","permalink":"https://shmily-qjj.top/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-07-25T03:43:31.921Z","comments":false,"path":"music/index.html","permalink":"https://shmily-qjj.top/music/index.html","excerpt":"","text":"欢迎光顾佳境的音乐小窝！这里有我的原创及翻唱作品，欢迎收听呦！戳这里：Shmily_佳境猜你想听：吹梦到西洲 cover恋恋故人难神秘园之歌 指弹版风之诗 cover押尾桑Cries In The Drizzle我心永恒 指弹版拥抱 cover徐秉龙 所有作品：","keywords":"佳境音乐小窝"},{"title":"rss","date":"2019-09-21T15:59:59.000Z","updated":"2020-04-12T14:15:48.013Z","comments":true,"path":"rss/index.html","permalink":"https://shmily-qjj.top/rss/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-04-12T14:15:48.016Z","comments":false,"path":"video/index.html","permalink":"https://shmily-qjj.top/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/bangumi/linglong.jpg', title: 'aaa', status: '已追完', progress: 100, jp: '废土风格的国产动漫巅峰之作', time: '放送时间: 2019-07-13', desc: ' 不久的未来，人类的世界早已拥挤不堪，迈向星河、寻找新家园的行动迫在眉捷。正当一切有条不紊的推进之时，月相异动，脚下的大地爆发了长达数十年、剧烈的地质变化，人类在这场浩劫中所剩无几。当天地逐渐恢复平静，人们从废墟和深渊中重新踏上了这片熟悉而又陌生的大地。习惯了主宰一切的我们是否还是这个世界的主人？' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 影视安利 分享好电影和好剧 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"b占"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-04-12T14:15:48.014Z","comments":true,"path":"tags/index.html","permalink":"https://shmily-qjj.top/tags/index.html","excerpt":"","text":""},{"title":"技术分享","date":"2019-09-21T14:53:25.000Z","updated":"2020-04-12T14:15:48.015Z","comments":false,"path":"tech/index.html","permalink":"https://shmily-qjj.top/tech/index.html","excerpt":"","text":"开发中","keywords":"技术分享模块正在设计和开发"}],"posts":[{"title":"SeaTunnel开源数据同步平台","slug":"SeaTunnel开源数据同步平台","date":"2021-12-15T08:30:00.000Z","updated":"2022-01-09T01:47:59.365Z","comments":true,"path":"84534d72/","link":"","permalink":"https://shmily-qjj.top/84534d72/","excerpt":"","text":"SeaTunnel开源数据同步平台SeaTunnel简介SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.SeaTunnel是一个简单易用且高效的开源数据集成平台（前身是WaterDrop），支持离线和实时数据同步。支持多种Source、Output、Filter组件以及自行开发输入输出插件和过滤器插件。SeaTunnel配置简单，基于已有的Spark、Flink环境几分钟就可以部署完成。因其有各种灵活的插件支持，只需要花几分钟编写一个配置文件即可完成一个数据同步任务的开发。 SeaTunnel架构 SeaTunnel特性： 简单易用，配置灵活，低代码 支持实时数据流和离线数据同步 高性能分布式、海量数据处理能力 模块化、插件化，易于扩展 支持通过SQL做ETL操作 SeaTunnel支持的组件：Input plugin： Fake, File, HDFS, Kafka, S3, Hive, Kudu, MongoDB, JDBC, Alluxio, Socket, self-developed Input pluginFilter plugin： Add, Checksum, Convert, Date, Drop, Grok, Json, Kv, Lowercase, Remove, Rename, Repartition, Replace, Sample, Split, Sql, Table, Truncate, Uppercase, Uuid, Self-developed Filter pluginOutput plugin: Elasticsearch, File, Hdfs, Jdbc, Kafka, Mysql, S3, Stdout, self-developed Output plugin支持的所有组件可以参考SeaTunnel通用配置 使用SeaTunnel安装部署SeaTunnel使用SeaTunnel将Kudu数据导入ClickHouse下载SeaTunnel:SeaTunnel二进制包 unzip seatunnel-1.5.5.zip cd seatunnel-1.5.5 # 修改seatunnel环境配置 vim config/seatunnel-env.sh SPARK_HOME=/hadoop/bigdata/spark/spark-2.3.2-bin-hadoop2.6 SeaTunnel将Kudu表导入ClickHouse准备kudu表Kudu表kudu_db.kudu_table（在KuduWebUI中表名为impala::kudu_db.kudu_table） 预先创建目标ClickHouse表 CREATE TABLE test.ch_table ( `cust_no` String, `tag_code` String, `update_datetime` DateTime ) ENGINE = MergeTree ORDER BY cust_no; 参考seatunnel-docs-configuration 配置数据抽取任务vim config/kudu2ch.batch.conf内容如下 spark &#123; spark.app.name = &quot;kudu2ch&quot; # executor的数量 spark.executor.instances = 2 # 每个excutor核数 (并行度,数据量大可以适当增大到ClickHouse服务器核数一半以下,尽量不要影响ClickHouse) spark.executor.cores = 1 # 每个excutor内存 spark.executor.memory = &quot;1g&quot; &#125; input &#123; kudu&#123; kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot; kudu_table=&quot;impala::kudu_db.kudu_table&quot; # 对应输出中需要指定source_table_name=&quot;kudu_table_source&quot; result_table_name=&quot;kudu_table_source&quot; &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; # 指定从哪个源抽取数据 source_table_name=&quot;kudu_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;test&quot; table = &quot;ch_table&quot; fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;] username = &quot;default&quot; password = &quot;123456&quot; # 每批次写入ClickHouse数据条数 bulk_size = 20000 &#125; &#125; 执行抽取任务： /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf 排错1： Caused by: ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 210, host: ch_jdbc_ip, port: 8123; Connect to ch_jdbc_ip:8123 [/ch_jdbc_ip] failed: Connection refused (Connection refused) 原因：CH Server端未开启远程访问权限解决：开启CH Server支持远程访问的权限 排错2： 2021-12-22 15:23:47 ERROR TaskSetManager:70 - Task 2 in stage 0.0 failed 1 times; aborting job Exception in thread &quot;main&quot; java.lang.Exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.ClassCastException: java.sql.Timestamp cannot be cast to java.lang.String at io.github.interestinglab.waterdrop.output.batch.Clickhouse.renderBaseTypeStatement(Clickhouse.scala:351) at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatementEntry(Clickhouse.scala:373) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement$1.apply$mcVI$sp(Clickhouse.scala:403) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160) at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement(Clickhouse.scala:391) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:187) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:162) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 原因：如果Kudu中表字段格式为Timestamp，需要在写入ClickHouse前先将Timestamp类型数据转换为字符串格式否则会写入错误。相关Git Issue: SeaTunnel-848相关文档：ClickHouse类型对照表解决：写入ClickHouse之前需要通过SeaTunnel中的 Filter插件 中的 SQL 或者 Convert 插件将各字段转换为对应格式，否则会产生报错修改配置vim config/kudu2ch.batch.conf内容如下 spark &#123; spark.app.name = &quot;kudu2ch&quot; # executor的数量 spark.executor.instances = 2 # 每个excutor核数 (并行度,数据量大可以适当增大到ClickHouse服务器核数一半以下,尽量不要影响ClickHouse) spark.executor.cores = 1 # 每个excutor内存 spark.executor.memory = &quot;1g&quot; &#125; input &#123; kudu&#123; kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot; kudu_table=&quot;impala::kudu_db.kudu_table&quot; # 对应输出中需要指定source_table_name=&quot;kudu_table_source&quot; result_table_name=&quot;kudu_table_source&quot; &#125; &#125; filter &#123; sql &#123; sql = &quot;select cust_no,tag_code,date_format(update_datetime, &#39;yyyy-MM-dd&#39;) as update_datetime from kudu_k_tab_sb_source&quot; &#125; &#125; output &#123; clickhouse &#123; # 指定从哪个源抽取数据 source_table_name=&quot;kudu_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;test&quot; table = &quot;ch_table&quot; fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;] username = &quot;default&quot; password = &quot;123456&quot; # 每批次写入ClickHouse数据条数 bulk_size = 20000 &#125; &#125; 若使用Convert模块，Filter中内容 filter &#123; date&#123; source_field = &quot;update_datetime&quot; target_field = &quot;update_datetime&quot; source_time_format = &quot;UNIX&quot; target_time_format = &quot;yyyy-MM-dd HH:mm:ss&quot; &#125; &#125; 执行抽取任务： /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf 数据验证:Kudu:+———-+| count(1) |+———-+| 714218 |+———-+Fetched 1 row(s) in 2.39s ClickHouse:Query id: 8d6bc13d-c49d-408a-8e07-3d2691e3ebbb┌─count()─┐│ 714218 │└─────────┘1 rows in set. Elapsed: 0.003 sec. 但DateTime类型相差8小时，因为ClickHouse的DateTime时区问题，故可以在sql中对update_datetime字段值减去8*3600秒 filter &#123; sql &#123; sql = &quot;select cust_no, tag_code, date_format(cast(cast(update_datetime as int) - 8*3600 as timestamp), &#39;yyyy-MM-dd HH:mm:ss&#39;) as update_datetime from kudu_k_tab_sb_source&quot; &#125; &#125; 一开始想设置ClickHouse中DateTime时区为DateTime(‘Asia/Hong_Kong’)，但SeaTunnel不支持这格式，只能用默认的DateTime格式注意：SeaTunnel抽取Kudu的SparkTask数等于Kudu表的Tablet数，建议给定Spark程序并行度为Tablet数的三分之一或二分之一。 SeaTunnel将Impala表导入ClickHouseSeaTunnel支持Input类型没有Impala但有JDBC，支持任何JDBC数据源，Impala也属于JDBC数据源。通过SeaTunnel可以将Impala管理的Kudu表、Hive表数据导出到其他存储引擎。 准备Impala Hive表Impala表 default.qjj_test+——+——–+———+| name | type | comment |+——+——–+———+| id | int | || name | string | |+——+——–+———+ 创建对应目标ClickHouse表 CREATE TABLE default.qjj_test ( `id` int, `name` String ) ENGINE = MergeTree ORDER BY id; 参考SeaTunnel-docs-JDBC编写任务配置文件配置文件/opt/seatunnel-1.5.5/config/impala2ch.batch.conf如下: spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; spark.executor.instances = 2 spark.executor.cores = 1 # 每个excutor内存 spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; table = &quot;(select * from qjj_test) as source_table&quot; # 或者直接写表名也可以table = &quot;qjj_test&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # 每批次写入ClickHouse数据条数 bulk_size = 20000 &#125; &#125; 将jdbc-jar放入seatunnel目录的plugins/my_plugins/lib目录Impala-jdbc下载地址：Donwload ImpalaJDBC41.jar cd seatunnel-1.5.6/ mkdir -p plugins/my_plugins/lib cd plugins/my_plugins/lib cp /hadoop/bigdata/common/lib/ImpalaJDBC41.jar . 执行抽取任务： /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master yarn --deploy-mode cluster --config /opt/seatunnel-1.5.5/config/impala2ch.batch.conf 此时可以正常抽取数据了，但通过观察程序WebUI发现无论给了多少ExecutorCore，只有一个Task，这样低的并行度会极大影响数据抽取效率，所以需要在配置上做改进：参考SeaTunnel-Spark-jdbc-string 得知SeaTunnel支持SparkJDBC的所有参数:spark-sql-data-sources-jdbc 配置修改思路是将原来的只有一个并行度增加到多个并行度所以使用partitionColumn, lowerBound, upperBound和numPartitions这四个参数进行调优，注意要对分区字段值数据有一定了解，选择合适的分区字段和lowerBound, upperBound很关键。当然这样并行加载数据源也将并行初始化多个连接，Spark源码中提醒到不要并行度过大，否则容易把外部存储搞垮。 partitionColumn, lowerBound, upperBound和numPartitions这四个参数能决定Spark读取JDBC数据源的并行度及策略，lowerBound是分区字段取值的下限(包含)，upperBound是上限(不包含)，numPatitions是我们希望按照多少分区来加载JDBC。注意第0个分区和最后一个分区加载的数据不被lowerBound, upperBound所限制，仍然会把所有数据加载出来。具体实现逻辑可以看Spark中JdbcRelationProvider和JDBCRelation两个核心类。 根据配置样例SeaTunnel-JDBC-Example 修改配置如下： spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; spark.executor.instances = 5 spark.executor.cores = 2 # 每个excutor内存 spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; table = &quot;(select * from qjj_test) as source_table&quot; # 或者直接写表名也可以table = &quot;qjj_test&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; jdbc.partitionColumn = &quot;id&quot; jdbc.numPartitions = &quot;20&quot; jdbc.lowerBound = 0 jdbc.upperBound = 2000000 &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # 每批次写入ClickHouse数据条数 bulk_size = 20000 &#125; &#125; 再次执行，观察WebUI发现并行度已经提高了，写入速度也变快了。 跑到后面发现有发生数据倾斜，可能是因partitionColumn参数设置不合理导致数据倾斜，要注意尽量选择不同范围数据分布均匀的字段作为分区字段，否则极易发生数据倾斜。但通过观察原表数据，发现没有数据在不同范围内分布均匀的字段，所以需要自己造一个分布均匀的字段。可以对字段做MOD(ASCII(SUBSTR(字段名,-1)), 分区数)操作。修改配置如下： spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; # 提高了分区数 相应的在jdbc允许的jdbc连接数范围内调大executor核数 以更高的并行度跑数据 spark.executor.instances = 30 spark.executor.cores = 2 # 每个excutor内存 spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; # 注意table的值是交给数据源jdbc去运行的而非Spark，不能使用SparkSQL函数，只能使用数据源支持的函数 次数将数据打散成300个区 可以使用不同的数据打散方式 最好先groupby测一下是否将数据均匀打散 table = &quot;(select id,name,(cast(rand() * 300 as int)) as spark_partition_column from qjj_test) as source_table&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; jdbc.partitionColumn = &quot;spark_partition_column&quot; jdbc.numPartitions = &quot;300&quot; jdbc.lowerBound = 0 jdbc.upperBound = 300 &#125; &#125; filter &#123; sql &#123; # 上面处理后多出来个字段，忽略掉该字段 sql = &quot;select id,name from impala_table_source&quot; &#125; &#125; output &#123; clickhouse &#123; source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # 每批次写入ClickHouse数据条数 bulk_size = 20000 &#125; &#125; 对于使用Impala JDBC进行数据抽取的情况，查询的并行度需要根据服务器数量和资源情况设置，连接并行度不应过大，Impalad对单池内存大小有限制。并行度太高会报如下错误： Caused by: java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: ExecQueryFInstances rpc query_id=42464c52f2e2c5dc:fe9ecfe800000000 failed: Failed to get minimum memory reservation of 272.00 MB on daemon data02.smycluster.sa:22000 for query 42464c52f2e2c5dc:fe9ecfe800000000 due to following error: Failed to increase reservation by 272.00 MB because it would exceed the applicable reservation limit for the &quot;Process&quot; ReservationTracker: reservation_limit=39.10 GB reservation=38.91 GB used_reservation=0 child_reservations=38.91 GB The top 5 queries that allocated memory under this tracker are: Query(8a4d40e3a6968443:7ae87ca100000000): Reservation=28.67 GB ReservationLimit=36.80 GB OtherMemory=21.24 MB Total=28.69 GB Peak=28.79 GB Query(bb4dc7b08c698bc3:f4036eb000000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=93.62 MB Total=1.15 GB Peak=2.39 GB Query(8a41df2c931faaec:ae30808c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=68.75 MB Total=1.13 GB Peak=1.37 GB Query(604eddfbd1fd2de5:b7493a7400000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=66.37 MB Total=1.13 GB Peak=1.38 GB Query(4c4ff283b5e12385:903c399c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=47.71 MB Total=1.11 GB Peak=1.39 GB Memory is likely oversubscribed. Reducing query concurrency or configuring admission control may help avoid this error. 在海量数据且资源配置不佳的情况下，使用Impala JDBC导出数据并不是很好的选择，Impala本身不适合跑批，跑批稳定性差，无容错机制。对于这样的场景可以将Impala表数据导出成Parquet文件，再Load到ClickHouse。也可以导出Parquet表到HDFS，再使用ClickHouse映射HDFS引擎表从而获取数据。 2022.1月-SeaTunnel正式进入Apache孵化器，我认为这是个比较优秀的项目，是个低代码实现数据抽取的高效平台，有兴趣可以多关注这个项目。 参考:SeaTunnel-githubSeaTunnel-docs-configuration使用WaterDrop将Kudu数据抽取到Clickhouse","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"WaterDrop","slug":"WaterDrop","permalink":"https://shmily-qjj.top/tags/WaterDrop/"},{"name":"SeaTunnel","slug":"SeaTunnel","permalink":"https://shmily-qjj.top/tags/SeaTunnel/"},{"name":"数据同步","slug":"数据同步","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"基于Manjaro KDE版打造美观舒适开发环境","slug":"基于Manjaro KDE版打造美观舒适开发环境","date":"2021-07-07T03:22:00.000Z","updated":"2021-12-13T15:17:16.756Z","comments":true,"path":"3f34ebe3/","link":"","permalink":"https://shmily-qjj.top/3f34ebe3/","excerpt":"","text":"基于Manjaro KDE版打造美观舒适开发环境系统安装与初始化配置安装系统到Manjaro官网下载最新ManjaroLinux发行版（本文基于Manjaro KDE Plasma 5.21.5版本）到Rufus官网下载镜像克隆工具，使用Rufus克隆Manjaro镜像到U盘，模式选择UEFI 系统BIOS设置项：Boot顺序将系统安装盘改为第一项关闭安全启动Security Boot =&gt; 否则无法引导进入LinuxSATA模式由Raid On切换为AHCI =&gt; 若系统有NVME硬盘则需要此操作，避免Linux无法识别到NVME硬盘（双系统用户先进入Windows-&gt;cmd运行msconfig-&gt;引导-&gt;勾选安全引导-&gt;重启的过程中会修复硬盘的AHCI驱动避免因切换AHCI导致无法启动Windows系统-&gt;重启后再取消勾选安全引导） 双显卡用户注意事项(单显卡忽略此步骤)：Nvidia+Intel双显卡笔记本安装需要这步：安装前给内核传参=&gt;按e在quiet后加：acpi_osi=! acpi_osi=”Windows 2009” 按F10启动，否则会卡死无法进入桌面 双击 Install Manjaro Linux打开安装向导时区选择Asia/ShangHai键盘默认即可接下来是关键步骤 磁盘要选手动分区磁盘空间规划：/boot/efi分区挂载到原EFI分区，共384G空闲空间，根分区xfs格式192G，home分区xfs格式160G，var分区ext4格式24G，swap给8G（xfs读取效率和断电容错较好但写效率略微低于ext4，ext4写效率高些读效率低于xfs）在空闲区域创建分区 步骤如下：数据分区最终结果如下：启动分区(EFI分区) 如下设置：最后设置系统管理员用户安装完成后 可以重启 双显卡用户注意事项(单显卡忽略此步骤)：重启第一次进入系统也需要按e在quiet后加：acpi_osi=! acpi_osi=”Windows 2009” 按F10启动，否则会卡死无法进入桌面进入系统后：sudo vim /boot/grub/grub.cfg 在所有quiet后加acpi_osi=! acpi_osi=”Windows 2009”参数，下次开机则不需要再加内核参数（若系统更新了内核，grub.cfg也会被更新，需要重新加内核参数进入系统，重新修改grub.cfg文件）建议每次更新系统执行如下脚本(update_grub.sh)自动增加内核参数： #!/bin/bash echo &quot;双显卡笔记本更新Manjaro系统后需要添加grub参数避免无法开机&quot; if [[ $(whoami) != root ]]; then echo -e &quot;\\033[41;37m[ERROR] Need sudo or root privilege.\\033[0m&quot; exit 1 fi GRUB_CFG=&quot;/boot/grub/grub.cfg&quot; GRUB_CFG_BACKUP=&quot;$GRUB_CFG&quot;_bak echo &quot;Backup path: $GRUB_CFG_BACKUP&quot; cp $GRUB_CFG $GRUB_CFG_BACKUP if [ -n &quot;$(grep &quot;Windows 2009&quot; $GRUB_CFG)&quot; ]; then echo &quot;Grub config is ok,no update.&quot; else echo &quot;Grub config need to be updated.Go to update it.&quot; # 防止不能加载显卡不能进桌面 sed -i &#39;s/quiet/quiet acpi_osi=! acpi_osi=&quot;Windows 2009&quot;/g&#39; $GRUB_CFG # 开机等待界面超时时间设为3s sed -i &#39;s/timeout=10/timeout=3/g&#39; $GRUB_CFG fi echo -e &quot;\\033[42;3mAll Done. Result:\\033[0m&quot; cat $GRUB_CFG | grep &quot;quiet&quot; cat $GRUB_CFG | grep &quot;timeout=&quot; 双显卡用户目前无论安装任何Linux发行版都很坑，对于Manjaro,奉上设置显卡切换的教程：Manjaro 笔记本配置Intel与Nvidia双显卡切换，防踩坑教程 至此 Manjaro Linux系统安装完成 初始化系统安装必备系统镜像源 cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup # 更新镜像排名 sudo pacman-mirrors -i -c China -m rank cp /etc/pacman.conf /etc/pacman.conf.backup # 添加ArchLinux中文社区源 sudo vi /etc/pacman.conf [archlinuxcn] SigLevel = Optional TrustedOnly #清华源 Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch #中科大源 #Server = https://mirrors.ustc.edu.cn/archlinuxcn/$arch # 配置生效 sudo pacman-mirrors -g # 更新pacman数据库全面更新系统并签名 sudo pacman -Syyu &amp;&amp; sudo pacman -S archlinuxcn-keyring # 安装一些常用的包 sudo pacman -S acpi vim 根据个人习惯创建一些目录（我的习惯并不好，把一部分项目文件放在/opt下，/opt本身是用于安装一些大型软件的。正常情况下，一个用户的个人文件放在家目录下比较规范） su root chmod 1777 /opt # 存放我的应用 mkdir /opt/apps chmod 1777 -R /opt/apps/ usermod -a -G root shmily # 用户代码项目存放目录 su shmily sudo mkdir -p /opt/Projects/ sudo mkdir -p /opt/Projects/OpenSourceProjects sudo mkdir -p /opt/Projects/MyProjects sudo mkdir -p /opt/Projects/EnterpriseProjects sudo chmod 1777 -R /opt/Projects/ # 系统环境所需目录 sudo mkdir -p /opt/Env/ # 工具目录 sudo mkdir -p /opt/Tools/ 配置免密 # sudo免密 避免频繁输入密码 以我用户名shmily为例(sudo su只切用户不带root的环境变量 sudo su -带root环境变量跟root用户一样) sudo vim /etc/sudoers shmily ALL=(ALL) NOPASSWD: ALL 清徐%sudo ALL=(ALL) ALL前的注释# sudo vim /etc/sudoers.d/10-installer 在%wheel ALL=(ALL) ALL行后添加如下配置 shmily ALL=(ALL) NOPASSWD: ALL %shmily ALL=(ALL) NOPASSWD: ALL 软件商店勾选启用AUR和Snap源 中文输入法安装输入法首选安装:sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-qt fcitx5-gtk kcm-fcitx5 fcitx5-material-color sudo vim ~/.pam_environment （在当前桌面登陆的用户下执行） INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=\\@im=fcitx5 后续如果其他用户需要中文输入法 也需要在每个用户的家目录下加以上环境变量注销重新登陆后生效配置输入法：将拼音上移，作为默认输入法设置shift按键为切换中英文输入的按键共享输入状态 避免输入法切换混乱如何更新主题 换主题Fcitx5-Material-Color说明：fcitx5为主体，fcitx5-chinese-addons中文输入方式支持fcitx5-qt，对Qt5程序的支持fcitx5-gtk，对GTK程序的支持fcitx5-qt4-gitAUR，对Qt4程序的支持kcm-fcitx5是KDE下的配置工具，不过在gnome下也可以正常使用。提示：一般情况下，只安装fcitx5-qt和fcitx5-gtk就可以了，配置工具fcitx5的配置文件位于~/.local/share/fcitx5，尽管您可以使用文本编辑器编辑配置文件，但是使用 GUI 配置显然更方便，kcm-fcitx5集成到 KCM 中的配置工具，专为KDE而生fcitx5-config-qt-git AUR：Qt前端的fcitx5配置工具，与kcm-fcitx5相冲突。注意：对于非 KDE 界面，可以使用 fcitx5-config-qt-gitAUR,该软件包与 kcm-fcitx5 相冲突，你需要手动卸载它环境变量。其他可选输入法组件：sunpinyin+sunpinyin-datafcitx-sunpinyinibus-sunpinyinkcm-fcitx Git配置git config --global user.name &quot;shmily&quot; git config --global user.email 710552907@qq.com git config --global http.version HTTP/1.1 git config --global core.autocrlf false git config --global core.safecrlf false git config --global core.autocrlf input #提交时转换为LF，检出时不转换 git config http.proxy socks5://127.0.0.1:7891 # 因为我的Clash代理sock端口是7891 git config --global --add remote.origin.proxy &quot;&quot; git config --global core.editor &quot;vim&quot; 开机自启脚本部署su rootvim /etc/systemd/system/rc-local.service 创建该文件 [Unit] Description=&quot;/etc/rc.local Compatibility&quot; [Service] Type=oneshot ExecStart=/etc/rc.local start TimeoutSec=0 StandardInput=tty RemainAfterExit=yes SysVStartPriority=99 [Install] WantedBy=multi-user.target vim /etc/rc.local 创建该文件 #!/bin/sh # /etc/rc.local if test -d /etc/rc.local.d; then for rcscript in /etc/rc.local.d/*.sh; do test -r &quot;$&#123;rcscript&#125;&quot; &amp;&amp; sh $&#123;rcscript&#125; done unset rcscript fi chmod a+x /etc/rc.localmkdir /etc/rc.local.dsystemctl enable rc-local.service自定义脚本放在/etc/rc.local.d/里就可以了 系统常规优化# 1.启用TRIM会帮助清理SSD中的块，从而延长SSD的使用寿命 sudo systemctl enable fstrim.timer # 2.安装中文字体 sudo pacman -S wqy-zenhei sudo pacman -S wqy-bitmapfont sudo pacman -S wqy-microhei sudo pacman -S ttf-wps-fonts sudo pacman -S adobe-source-han-sans-cn-fonts sudo pacman -S adobe-source-han-serif-cn-fonts # ls -l命令简写ll sudo vim /etc/profile和~/.bashrc alias ls=&#39;ls --color&#39; alias ll=&#39;ls -l --color&#39; 双系统优化Windows+Linux双系统可以加如下参数使Windows把硬件时间当作UTC（避免双系统切换导致的时间错乱）Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1 安装应用和工具常用软件安装# 微信、TIM sudo pacman -S yay yay --aururl https://aur.tuna.tsinghua.edu.cn --save sudo pacman -Sy base-devel yay -S com.qq.weixin.spark yay -S com.qq.tim.spark 增大dpi避免窗口和字体过小（在打开的窗口中设置 2k屏幕建议值168-192）： env WINEPREFIX=/home/shmily/.deepinwine/Spark-WeChat/ deepin-wine5 winecfg env WINEPREFIX=/home/shmily/.deepinwine/Spark-TIM/ deepin-wine5 winecfg ------------------------------------------------------------------------------------------------------- sudo pacman -S google-chrome # Chrome sudo pacman -S netease-cloud-music # 网易云音乐 yay -S tenvideo # 腾讯视频 sudo pacman -S unrar unzip p7zip # 解压 ### 安装WPS：软件商店安装如下包：wps-office-cn wps-office-mui-zh-cn wps-office-mime-cn ttf-wps-fonts sudo pacman -S gimp # 修图 sudo pacman -S neofetch screenfetch # 输出系统信息 yay -S todesk;sudo systemctl enable todeskd.service;sudo systemctl start todeskd.service;sudo systemctl status todeskd.service #远程桌面工具 ### 远程桌面连接工具remmina sudo pacman -S remmina 安装工程会提示 remmina 的可选依赖 freerdp: RDP plugin libsecret: Secret plugin [已安装] libvncserver: VNC plugin libxkbfile: NX plugin [已安装] nxproxy: NX plugin spice-gtk: Spice plugin telepathy-glib: Telepathy plugin xorg-server-xephyr: XDMCP plugin gnome-terminal: external tools 选择自己想要的依赖，如RDP远程桌面连接: sudo pacman -S freerdp ------------------------------------------------------------------------------------------------------- # 企业微信安装 https://aur.archlinux.org/packages/com.qq.weixin.work.deepin/ 下载deb包 用Ark打开deb包 解压出data.tar.xz 再解压data.tar.xz中的opt/apps/com.qq.weixin.work.deepin解压到/opt/apps/ cd /opt/apps/com.qq.weixin.work.deepin 修改/opt/apps/com.qq.weixin.work.deepin/entries/applications/com.qq.weixin.work.deepin.desktop中Icon的值：/opt/apps/com.qq.weixin.work.deepin/entries/icons/hicolor/48x48/apps/com.qq.weixin.work.deepin.svg sudo cp /opt/apps/com.qq.weixin.work.deepin/entries/applications/com.qq.weixin.work.deepin.desktop /usr/share/applications 增大dpi避免窗口和字体过小（在打开的窗口中设置 2k屏幕建议值168-192）： env WINEPREFIX=/home/shmily/.deepinwine/Deepin-WXWork/ deepin-wine5 winecfg ------------------------------------------------------------------------------------------------------- 软件仓库安装：Typora，Shotcut，laptop-mode-tools(可选 有tlp可以不用) 软件仓库安装:timeshift (系统可能已经自带了) 软件仓库安装:深度影院 深度相机 BaiduNetDisk百度网盘 # 安装文件同步工具 多端同步 sudo pacman -S syncthing # 参考https://github.com/syncthing/syncthing/tree/main/etc/linux-desktop创建快捷方式 # 启动Syncthing的快捷方式syncthing-start.desktop [Desktop Entry] Name=Start Syncthing GenericName=File synchronization Comment=Starts the main syncthing process in the background. Exec=/usr/bin/syncthing serve --no-browser --logfile=default Icon=syncthing Terminal=false Type=Application Keywords=synchronization;daemon; Categories=Network;FileTransfer;P2P # 查看Syncthing UI的快捷方式syncthing-ui.desktop [Desktop Entry] Name=Syncthing Web UI GenericName=File synchronization UI Comment=Opens Syncthing&#39;s Web UI in the default browser (Syncthing must already be started). Exec=/usr/bin/syncthing -browser-only Icon=syncthing Terminal=false Type=Application Keywords=synchronization;interface; Categories=Network;FileTransfer;P2P # 生效快捷方式 sudo desktop-file-install syncthing-start.desktop sudo desktop-file-install syncthing-ui.desktop Clash科学上网下载Clash cd ~/下载 gunzip clash-linux-amd64-v1.6.5.gz mkdir /opt/apps/Clash mv clash-linux-amd64-v1.6.5 /opt/apps/Clash/ cd /opt/apps/Clash chmod +x clash-linux-amd64-v1.6.5 ./clash-linux-amd64-v1.6.5 直到出现INFO[0003] Mixed(http+socks5) proxy listening at: 127.0.0.1:7890即可关闭 ls ~/.config/clash 会有config.yaml Country.mmdb 如果没出现上述INFO日志则可能是Country.mmdb下载失败，可以手动下载 sudo touch /usr/share/applications/Clash.desktop chmod a+x /usr/share/applications/Clash.desktop cat&gt;/usr/share/applications/Clash.desktop&lt;&lt;EOF [Desktop Entry] Name=Clash For Linux Comment=clash-for-linux Encoding=UTF-8 Exec=/opt/apps/Clash/clash-linux-amd64-v1.6.5 Icon=/opt/apps/Clash/logo_64.png Categories=System;Application;Network; StartupNotify=true Terminal=false Type=Application EOF # 生效我们的代理配置文件 cp ~/下载/Clash_1625991739.yaml ~/.config/clash/config.yaml 使用WebUI管理连接：根据cat ~/.config/clash/config.yaml | grep external-controller的结果，通过http://clash.razord.top进行策略组节点的切换 只浏览网页推荐使用Chrome浏览器插件Proxy SwitchyOmega:必要时可以使用系统全局代理：进入系统设置-&gt;网络设置-&gt;使用系统代理服务器配置(或使用手动设置的代理服务器)-&gt;http代理设为127.0.0.1:7890 Socks代理设置为127.0.0.1:7891配置Clash开机自启： cp /usr/share/applications/Clash.desktop ~/.config/autostart/ 截屏录屏 深度录屏sudo pacman -S deepin-screen-recorderln -s /usr/bin/deepin-screen-recorder /usr/bin/sr 运行sr命令使用添加截图功能到系统全局快捷方式:设置-&gt;快捷键-&gt;自定义快捷键-&gt;编辑-&gt;新建-&gt;全局快捷键-&gt;命令/URL-&gt;命令/usr/bin/deepin-screen-recorder 触发器Ctrl+Alt+A kazamsudo pacman -S kazam 可以截图和录屏的工具 深度截图sudo pacman -S deepin-screenshot 自带截图工具Spectacle日常截图自带截图工具就足够了，只是与Windows端我们常用的Ctrl+Alt+A不太一样，可以记住它的快捷键，用起来也很方便主要常用的就是Meta+Print （即Win+PrtScn） 截取当前活动的窗口 开发环境安装sudo pacman -S net-tools dnsutils inetutils iproute2 stress python-pip screen htop bat tree ncdu tig tldr sudo pacman -S nodejs sudo pacman -S npm sudo pacman -S make sudo pacman -S cmake sudo pacman -S clang sudo pacman -S maven ---------------------------------------------------------------------------------- # Java、Scala环境安装 # 下载jdk-8u181-linux-x64.tar.gz # 卸载系统默认jdk（系统默认使用Java8会导致部分依赖java运行的软件出现不兼容现象，建议将系统JAVA_HOME改为jdk11+版本） sudo archlinux-java unset # 否则不会从环境变量读java地址 sudo tar -zxvf jdk-8u181-linux-x64.tar.gz -C /opt/Env/ # 下载scala-2.12.12.tgz sudo tar -zxvf scala-2.12.12.tgz -C /opt/Env/ sudo vim /etc/profile export JAVA8_HOME=/opt/Env/jdk1.8.0_181 export JAVA_HOME=/opt/Env/jdk-11.0.11 export PATH=$PATH:$JAVA_HOME/bin export SCALA_HOME=/opt/Env/scala-2.12.12 export PATH=$PATH:$SCALA_HOME/bin source /etc/profile ------------------------------------------------------------------- Python源切换 sudo pip config --global set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple sudo pip config --global set install.trusted-host pypi.tuna.tsinghua.edu.cn ---------------------------------------------------------------------------------------- Mysql安装： pacman -Si mysql # 查看仓库中的MySQL版本号 sudo pacman -S mysql # 安装mysql sudo mkdir /opt/mysql-data sudo chmod 1777 /opt/mysql-data sudo mysqld --initialize --user=shmily --basedir=/usr --datadir=/opt/mysql-data --character-set-server=UTF8MB4 vim /etc/mysql/my.cnf 修改datadir=/opt/mysql-data chmod -R 777 /opt/mysql-data sudo systemctl start mysqld.service systemctl status mysqld.service 初始密码登陆 alter user &#39;root&#39;@&#39;localhost&#39; identified with mysql_native_password by &#39;123456&#39; mysql -uroot -p123456 sudo systemctl enable mysqld.service # 设置开机启动mysql server (可选) 支持Ubuntu系安装包(.deb包)git clone https://github.com/helixarch/debtapcd debtapsudo cp debtap /usr/local/bin（前三步可以用yay -S debtap或yaourt -S debtap代替）sudo debtap -u 更新软件包如果sudo debtap -u过程下载很慢，需要换源 vim /usr/bin/debtap 替换：http://ftp.debian.org/debian/dists https://mirrors.ustc.edu.cn/debian/dists 替换：http://archive.ubuntu.com/ubuntu/dists https://mirrors.ustc.edu.cn/ubuntu/dists/ 然后就可以操作安装.deb包了debtap xxx.deb （一路下一步，证书选GPL）得到解析后的安装包(Final Package)sudo pacman -U 解析后的安装包 开发工具安装# JetBrains全家桶 命令行方式安装 sudo pacman -S intellij-idea-ultimate-edition # 安装IDEA最新旗舰版 sudo pacman -S pycharm-community-edition # 安装PyCharm sudo pacman -S goland # 安装Goland # JetBrains全家桶 手动下载方式安装 [推荐] # 以GoLand为例 tar -zxvf goland-xxx.tar.gz -C /opt/apps/ mv /opt/apps/goland-2021.3.4 /opt/apps/GoLand cd /opt/apps/GoLand touch GoLand.desktop 内容如下 [Desktop Entry] Name=GoLand Comment=GoLand Exec=/opt/apps/GoLand/bin/goland.sh Icon=/opt/apps/GoLand/bin/goland.png Terminal=false Type=Application Categories=Development 然后执行sudo desktop-file-install GoLand.desktop 安装快捷方式 ----------------------------------------------------------------------- # 安装VSCode(yay -S visual-studio-code-bin)： # 首先官网去下载安装包vscode官网https://code.visualstudio.com 得到code-stable-xxxxxxx.tar.gz tar -zxvf code-stable-x64-1623937300.tar.gz -C /opt/apps/ sudo chmod +x /opt/apps/VSCode-linux-x64/code ln -s /opt/apps/VSCode-linux-x64/code /usr/local/bin/code touch /usr/share/applications/VSCode.desktop chmod +x /usr/share/applications/VSCode.desktop cp /opt/apps/VSCode-linux-x64/resources/app/resources/linux/code.png /usr/share/icons/ cat&gt;/usr/share/applications/VSCode.desktop&lt;&lt;EOF [Desktop Entry] Name=Visual Studio Code Comment=Multi-platform code editor for Linux Exec=/opt/apps/VSCode-linux-x64/code Icon=/usr/share/icons/code.png Type=Application StartupNotify=true Categories=TextEditor;Development;Utility; MimeType=text/plain; EOF ----------------------------------------------------------------------- # Typora markdown工具 yay -S typora # Sublime安装 参考https://www.sublimetext.com/docs/3/linux_repositories.html#pacman 激活码： ----- BEGIN LICENSE ----- Member J2TeaM Single User License EA7E-1011316 D7DA350E 1B8B0760 972F8B60 F3E64036 B9B4E234 F356F38F 0AD1E3B7 0E9C5FAD FA0A2ABE 25F65BD8 D51458E5 3923CE80 87428428 79079A01 AA69F319 A1AF29A4 A684C2DC 0B1583D4 19CBD290 217618CD 5653E0A0 BACE3948 BB2EE45E 422D2C87 DD9AF44B 99C49590 D2DBDEE1 75860FD2 8C8BB2AD B2ECE5A4 EFC08AF2 25A9B864 ------ END LICENSE ------​ 虚拟机软件安装安装VirtualBox:mhwd-kernel -li (当前系统是linux510，则安装linux510-virtualbox-host-modules)sudo pacman -Syu virtualbox linux510-virtualbox-host-modules重启或执行sudo vboxreload 安装KVM（备选）：pacman -S qemu libvirt ovmf virt-manager（kvm负责CPU和内存的虚拟化，qemu向Guest OS模拟硬件，ovmf为虚拟机启用UEFI支持，libvirt提供管理虚拟机和其它虚拟化功能的工具和API，virt-manager是管理虚拟机的GUI）systemctl enable libvirtdsystemctl start libvirtdusermod -a -G kvm shmily启动qem/virt-manager 安卓应用支持参考UOS(Deepin)对于安卓应用支持的解决方案，采用XDroid作为安卓应用支持软件。先下载XDroid软件：官网下载XDroid执行tar -zxvf xDroidInstall-x86_64-vxxxx.tar.gz 解压执行 ./xDroidInstall-x86_64-vxxxx.run 安装XDroid安装后重启一到两次即可完成安装应用商城已安装应用使用Android APP 系统界面美化Manjaro Linux是可以随用户心情随意定制的，可定制化程度极高，是桌面控的福音。下面做一些简单的界面设置。 Dock栏sudo pacman -S latte-dock根据偏好设置latte dock效果 oh-my-zsh sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; #安装powerlevel10k主题 sudo pacman -Sy --noconfirm zsh-theme-powerlevel10k #配置powerlevel10k echo &#39;source /usr/share/zsh-theme-powerlevel10k/powerlevel10k.zsh-theme&#39; &gt;&gt;! ~/.zshrc #使配置立即生效 source ~/.zshrc # 按提示设置即可 设置时建议不要带图标，因为在其他用到zsh的终端上可能会出现乱码或方框，很不美观，可以在其他用到zsh的终端上重新执行p10k configure命令来设置更合适的样式。 # 安装语法高亮插件 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting echo &quot;source $ZSH_CUSTOM/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot; &gt;&gt; $&#123;ZDOTDIR:-$HOME&#125;/.zshrc source ~/.zshrc 命令行终端可选代替Konsole的更好看的命令行终端Tabby(原Terminus)：https://github.com/Eugeny/tabby继续配置Konsole: 全局主题 打开设置-&gt;外观-&gt;全局主题之所以喜欢用Mac的全局主题（McSur-dark）是因为它的任务栏比较好看比较精致 窗口窗口选择了这款，按钮比较简洁，是和Mac相近的，但按钮位置在右边，我还是比较习惯这种按键位置。点击主题上的按钮可以调节主题按键大小，下方可以调节窗口边框大小窗口配色方案我选的Ambiance-ISH，亮色看起来更敞亮，心情更好。 登陆页登陆页面只有每次开机时才会出现，锁屏是单独的锁屏页面。打开设置-&gt;开机与关机-&gt;登录屏幕(SDDM) 在这里设置效果： 欢迎屏幕开机在登陆页面输入密码后会进入欢迎屏幕，大概有2秒左右停留在欢迎屏幕，随便选个好看的就可以了。 锁屏界面每次锁屏(Meta+L)后都会显示这个页面。打开设置-&gt;工作区行为-&gt;锁屏-&gt;外观：配置 桌面特效这里会有一些神奇的界面效果，如窗口惯性拖动，最小化神灯效果，窗口切换效果等。打开设置-&gt;工作区行为-&gt;桌面特效主要改动的地方：气泡相关、窗口背景虚化、窗口透明度、窗口惯性晃动、最小化过渡动画(神灯)、窗口后滑特效、窗口打开\\关闭动效、虚拟桌面切换动效 系统使用小技巧与问题处理解决无法写和更新NTFS盘数据的问题：创建 /usr/bin/fix_ntfs_disk_rw.sh 内容： #!/bin/bash # Fix NTFS Disk which can not be writen on linux system. # Usage: sh fix_ntfs_disk_rw.sh /run/media/shmily/Entertainment /Entertainment DEFAULT_MOUNT_POINT=$1 TARGET_MOUNT_POINT=$2 if [ &quot;$(whoami)&quot; != &quot;root&quot; ];then echo User root is necessary. exit 1 fi current_point=$(df -h | grep $DEFAULT_MOUNT_POINT | awk &#39;&#123;print $1&#125;&#39;) echo &quot;Remounting point $current_point from $DEFAULT_MOUNT_POINT to $TARGET_MOUNT_POINT&quot; sudo ntfsfix $current_point sudo umount $DEFAULT_MOUNT_POINT sudo mkdir -p $TARGET_MOUNT_POINT sudo chmod 1777 $TARGET_MOUNT_POINT sudo mount -t ntfs -o rw $current_point $TARGET_MOUNT_POINT echo &quot;All Done&quot; 将系统默认挂载点重新挂载为自定义的挂载点 用法sh fix_ntfs_disk_rw.sh /run/media/shmily/Entertainment /Entertainment 内存清理sudo echo 1 &gt; /proc/sys/vm/drop_cachessudo echo 2 &gt; /proc/sys/vm/drop_cachessudo echo 3 &gt; /proc/sys/vm/drop_caches 搜索工具Alt+Space 全局搜索工具 会在桌面上方弹出搜索框 可以搜索应用、文件、目录、服务、设置等 解决thermal误报导致自动关机报错kernel: thermal thermal_zone3: critical temperature reached (125 C), shutting down 直接被关机sudo chmod 665 /sys/class/thermal/thermal_zone3/modesudo echo “disabled” &gt; /sys/class/thermal/thermal_zone3/mode这个参数需要每次系统启动时重新写入,放入开机启动脚本路径/etc/rc.local.d/ 必须掌握的系统备份和恢复技巧Linux各个依赖包之间存在复杂的依赖关系，同时我们经常使用较高的权限操作，可能会因为种种原因导致系统出现各种问题，所以备份还原是必备的技巧，能在系统宕机或滚挂后可以还原到某个先前的时间节点，来保护我们辛辛苦苦调教了很久的系统不会出意外。注意：系统检测到有大更新时，不要急于更新，要首先使用timeshift做一个快照，再更新。原因是部分情况下，系统更新后看似没问题，但实际上软件的依赖库版本发生了变化，导致有部分软件无法正常运行了，这种情况不易发现。要养成先快照再升级的习惯。系统备份和还原两种方式：使用tar压缩包打包备份系统 https://www.cnblogs.com/smlile-you-me/p/13601039.html使用timeshift的快照备份和还原系统 按照向导设置：选择快照类型:RSYNC-&gt;选择快照位置(选一个分区，注意只能选Linux文件系统的分区，不支持远程、NTFS等)-&gt;选择快照等级(根据重要性和磁盘空间选择备份周期和保留快照数)-&gt;用户主目录(默认全部) 点击创建 会立刻运行快照创建程序，创建完如图 家目录有些文件可能不需要备份，需要排除一部分文件：设定-&gt;筛选 可以自定义不对特定模式的文件创建快照 恢复快照: 选中要恢复的快照 点击恢复即可 当错误操作导致系统崩溃无法进入界面时，需要进入命令行使用timeshift相关命令恢复:通过Ctrl+Alt+F1（一般是F1-F6都可）进入tty终端 输入用户和密码登录 # 查看可还原的还原点 sudo timeshift --list /dev/nvme0n1p9 is mounted at: /run/timeshift/backup, options: rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota Device : /dev/nvme0n1p9 UUID : d4fa3365-62fe-4488-ba18-b36ddac64c4d Path : /run/timeshift/backup Mode : RSYNC Status : OK 2 snapshots, 75.5 GB free Num Name Tags Description ------------------------------------------------------------------------------ 0 &gt; 2021-08-12_12-24-49 O 1 &gt; 2021-08-12_14-00-01 M # 还原快照 --skip-grub选项为跳过grub安装，一般来说grub不需要重新安装，除非bios启动无法找到正确的grub启动项，才需要安装 sudo timeshift --restore --snapshot &#39;2021-08-12_14-00-01&#39; --skip-grub 无法进入系统也无法进入tty命令行参照文章开始的部分创建Manjaro安装盘，进入LiveCD桌面，安装timeshift 按上一步的步骤进行恢复恢复完成后桌面无法加载程序快捷方式-&gt;解决：yay -Syuu执行系统更新即可 使用Wine运行Windows程序Wine （“Wine Is Not an Emulator” 的首字母缩写）是一个能够在多种POSIX-compliant操作系统（诸如Linux，macOS及BSD等）上运行Windows应用的兼容层。Wine不是像虚拟机或者模拟器一样模仿内部的Windows逻辑，而是將Windows API调用翻译成为动态的POSIX调用，免除了性能和其他一些行为的内存占用，让你能够干净地集合Windows应用到你的桌面。安装wine 忽略 之前的步骤已经有wine了sudo pacman -S wine wine_gecko wine-monosudo pacman -S lib32-mesa lib32-nvidia-utils中文显示问题修复： 在windows下拷贝字体文件——simsun.ttc（c:\\windows\\fonts\\simsun.ttc），复制到~/.wine/drive_c/windows/Fonts下；然后，编辑reg文件，文件内容如下： REGEDIT4 [HKEY_LOCAL_MACHINE\\Software\\Microsoft\\NT\\CurrentVersion\\FontSubstitutes] &quot;Arial&quot;=&quot;simsun&quot; &quot;Arial CE,238&quot;=&quot;simsun&quot; &quot;Arial CYR,204&quot;=&quot;simsun&quot; &quot;Arial Greek,161&quot;=&quot;simsun&quot; &quot;Arial TUR,162&quot;=&quot;simsun&quot; &quot;Courier New&quot;=&quot;simsun&quot; &quot;Courier New CE,238&quot;=&quot;simsun&quot; &quot;Courier New CYR,204&quot;=&quot;simsun&quot; &quot;Courier New Greek,161&quot;=&quot;simsun&quot; &quot;Courier New TUR,162&quot;=&quot;simsun&quot; &quot;FixedSys&quot;=&quot;simsun&quot; &quot;Helv&quot;=&quot;simsun&quot; &quot;Helvetica&quot;=&quot;simsun&quot; &quot;MS Sans Serif&quot;=&quot;simsun&quot; &quot;MS Shell Dlg&quot;=&quot;simsun&quot; &quot;MS Shell Dlg 2&quot;=&quot;simsun&quot; &quot;System&quot;=&quot;simsun&quot; &quot;Tahoma&quot;=&quot;simsun&quot; &quot;Times&quot;=&quot;simsun&quot; &quot;Times New Roman CE,238&quot;=&quot;simsun&quot; &quot;Times New Roman CYR,204&quot;=&quot;simsun&quot; &quot;Times New Roman Greek,161&quot;=&quot;simsun&quot; &quot;Times New Roman TUR,162&quot;=&quot;simsun&quot; &quot;Tms Rmn&quot;=&quot;simsun&quot; （注：按照windows的格式，最后一行之后要敲回车符）保存文件名为fonts.reg，保存在/.wine下；然后导入regedit：打开gnome-terminal，输入指令 cd ~/.wine ; regedit fonts.reg 最后，打开regedit，/.wine/drive_c/windows/regedit.exe,依次找到 HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows NT\\CurrentVersion\\FontSubstitutes，将该键下的MS Shell Dlg和MS Shell Dlg2键值删除。 Wine使用Wine运行程序: wine xxx.exemsi安装包运行: msiexec -i &lt;msi安装包&gt; Deepin-wineDeepin-Wine是Deepin团队移植的Wine，在其基础上移植的很多软件如微信、TIM/QQ、网易云音乐等有着更好的兼容性和使用体验。注意，Deepin-Wine是32位的，并且其依赖于Wine，因此本机上安装的Wine最好是32位的，否则Deepin-Wine使用命令时会有不便。安装(忽略 之前的步骤已有此依赖) yaourt deepin-wine使用方法与wine基本一致 更多Wine进阶使用可以了解Wine官方网站 Linux使用Wine 参考链接Manjaro Gnome 下fcitx5的安装Fcitx5-Material-ColorArchLinux WikiSyncthingManjaro安装Mysql8.0（血泪篇）archlinux Timeshift系统备份与还原轻松上手Manjaro之Manjaro下使用Wine","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://shmily-qjj.top/tags/Linux/"},{"name":"Manjaro","slug":"Manjaro","permalink":"https://shmily-qjj.top/tags/Manjaro/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Presto-基于内存的高效SQL交互查询引擎","slug":"Presto-基于内存的高效SQL交互查询引擎","date":"2021-03-12T06:46:00.000Z","updated":"2021-04-05T05:13:27.452Z","comments":true,"path":"4c197c46/","link":"","permalink":"https://shmily-qjj.top/4c197c46/","excerpt":"","text":"Presto-高效SQL交互式查询引擎Presto简介&emsp;&emsp;Presto是Facebook开源的分布式SQL查询引擎，适用于交互式分析查询的场景（OLAP），响应时间在小于 1 秒到几分钟的场景，数据量支持GB到PB字节。类似的工具有Impala、ClickHouse等… Presto优缺点优点： 架构清晰，可不依赖任何外部系统独立运行。 Presto自身提供了对集群的监控。 基于纯内存计算，不需要写磁盘，效率高 自身更加轻量级资源调度，线程级别的Task，效率高 轮询查询结果并立刻返回结果，效率高 解耦数据源，统一查询入口，支持多个数据源不同表的联邦查询分析 MPP架构的优势-扩展性，节点独立，无锁资源竞争，无IO冲突，无共享数据 简单的数据结构，列式存储，逻辑行，大部分数据都可以轻易的转化成Presto所需要的这种数据结构。 丰富的接口，可完美对接外部存储系统，以及添加自定义的函数。 缺点： 无容错能力，无重试机制 不支持数据类型隐式转换 与Hive相比存在不小的语法差异、函数和UDF差异以及运算结果差异(如1/2在Hive结果为0.5在Presto结果为0) Hive views are not supported.需要创建Presto视图 因为纯内存计算，不适合多个大表Join(聚合操作边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高；但关联操作可能产生大量临时数据，可能比Hive慢) Coordinator单点问题（常见方案：ip漂移、Nginx代理动态获取等） Presto原理在学习Presto原理前推荐先看看我之前关于Impala的文章：《Impala-基于内存的高效SQL交互查询引擎》 Presto架构和进程&emsp;&emsp;Presto与Impala的架构极其相似，都是采用Master-Slave模型以及MPP架构,而且Presto的工作角色也与ImpalaDaemon的角色基本相同，Presto有三种工作角色：Coordinator,Worker和DiscoveryServer： Coordinator：即Master，负责管理Meta元数据，Worker节点，SQL的解析和调度，生成Stage和Task分发给Workers，负责合并结果集并返回给客户端。相当于结合了Impalad的Coordinator角色和Planner角色的功能，区别是每个Impalad节点都可以是Coordinator，而Presto只能有一个Coordinator，多个协调者进程会导致脑裂，查询任务会死锁。 Worker：负责计算和读写数据。相当于Impalad的Executor角色的功能。 DiscoveryServer：通常内嵌于Coordinator节点，也可以独立出来部署，功能类似ZK，类似Impala中的ImpalaStateStore，用于监控节点心跳，一般DS和Coordinator在同一节点。Worker启动会向DS进程注册，Coordinator可以从DS获取到所有正常提供服务的Worker。Coordinator 与 Worker、Client 通信是通过 REST API。 Presto数据模型Presto使用Catalog、Schema和Table这3层结构来管理数据： Catalog：每个数据源都有一个名字，一个Catalog可包含多个Schema。通过show catalogs命令查看Presto已连接的所有数据源 Schema：相当于一个数据库实例，一个Schema(数据库)中有多个Table表，通过show schemas from hive命令查看hive数据源所有库 Table：相当于一张表，通过show tables from catalog_name.schema_name来查看库下有哪些表。定位一张表：数据源的类别.数据库.数据表 Presto有两种存储单元：Page和Block Page：多行数据的集合，包含多个列的数据，这里的多行数据是逻辑行，实际是以列式存储。 Block：一列数据，根据不同类型的数据，通常采取不同的编码方式。（Kudu也有类似的思想） array类型的Block：应用于固定长度的类型如int、long、double，由两部分组成 boolean valueIsNull[]表示每一行是否有值。 T values[] 每一行的具体值。 可变长度的Block：String类型，由三部分组成： Slice：所有行数据拼接起来的字符串 int offsets[]：每行数据的起始偏移位置(每一行的长度等于下一行的起始偏移量减去当前行的起始偏移量) boolean valueIsNull[]：是否空值，如果无值，偏移量与上一行相等 固定长度的string类型的block：所有行的数据拼接成一长串Slice，每一行的长度固定 字典类型的Block：可以嵌套任意类型的Block，由两部分组成： 字典 int ids[] 数据的编号，查找时先找到id，再从字典中拿到真实值 Presto插件了解了Presto的数据模型后，就可以利用插件来对接自己的系统。Presto提供了一套Connector接口，支持从自定义存储中读取元数据，以及列式存储数据。 ConnectorMetadata:管理表的元数据，表的元数据，partition等信息。在处理请求时，需要获取元信息，以便确认读取的数据的位置。Presto会传入filter条件，以便减少读取的数据的范围。元信息可以从磁盘上读取，也可以缓存在内存中。 ConnectorSplit:一个IO Task处理的数据的集合，是调度的单元。一个split可以对应一个partition，或多个partition。 SplitManager:根据表的meta，构造split。 SlsPageSource:根据split的信息以及要读取的列信息，从磁盘上读取0个或多个page，供计算引擎计算。 基于Presto的插件我们可以开发这些功能： 对接自己的存储系统。 添加自定义数据类型。 添加自定义处理函数。 自定义权限控制。 自定义资源控制。 添加query事件处理逻辑。 目前Presto已经支持很多类型的Connector，具体可见官方文档：Presto-Connectors Presto内存管理机制&emsp;&emsp;Presto作为基于内存的计算引擎，对内存的分配很精细。Presto采用逻辑上的内存池，来管理不同类型的内存需求。Presto把机器的内存划分成三个内存池，分别是System Pool,Reserved Pool,General Pool。 System Pool：保留给系统使用的内存，默认是Xmx的40% General Pool：大部分Query使用这个内存池中的内存，因为大部分Query消耗内存并不高 Reserved Pool：用于给消耗内存最大的一个Query使用，这个内存池默认占10%的总内存，也表示一个Query在一台机器上最大的内存使用量 &emsp;&emsp;为什么Presto会使用内存池机制？首先，System Pool为了系统正常运行以及数据传输时系统缓存消耗；在资源不充足时，一个消耗内存较大的Query开始运行，因为没足够空间所以会挂起等待执行，等一些消耗内存小的Query执行完，又有新的Query请求，内存一直不充足，如果没有Reserved Pool，这个消耗内存大的Query就会一直被挂起直到失败。为了防止这种情况，预留出Reserved Pool内存池供大Query执行。Presto每秒钟挑出来一个内存占用最大的query，允许它在所有机器上都能使用Reserved pool，避免一直没有可用内存供大Query使用。 &emsp;&emsp;如果大Query不在某些节点使用Reserved Pool就会浪费那台节点的预留内存，所以为什么不是单台机器中挑出占用内存最大的Task来使用Reserved Pool？这样设计会死锁，假设一个大Query的一个Task在某台机器可用Reserved Pool很快执行完，而另外一台机器的Task还是挂起状态，这个Query也会一直处于挂起状态，效率降低。 &emsp;&emsp;Presto内存管理分为两部分：Query内存管理和机器内存管理，是由Coordinator负责的Query内存管理：Query会划分为多个Task，每个Task会有一个线程循环获取Task状态包括内存使用情况，Query内存管理就是汇总这些Task的内存使用情况。机器内存管理：Coordinator有一个线程定时轮询每台机器的内存状态 Presto执行计划Presto与Spark、Hive一样，都是使用Antlr进行语法解析，一条SQL经过如下步骤最终生成在每个节点执行的LocalExecutionPlan逻辑计划。 样例： select c1.rank, count(*) from dim.city c1 join dim.city c2 on c1.id = c2.id where c1.id &gt; 10 group by c1.rank limit 10; 生成的逻辑计划： 物理执行计划：逻辑计划的每一个SubPlan都会提交到一个或者多个Worker节点上执行，一个SubPlan也可以理解为一个Stage，SubPlan有几个重要的属性planDistribution、outputPartitioning、partitionBy属性。 planDistribution有三种类型 Source：数据源，会根据数据源大小确定分配多少个节点 Fixed：分配到固定个数的节点执行（Config配置中的query.initial-hash-partitions参数配置，默认是8） None：这个SubPlan只分配到一个节点执行 outputPartitioning有两种类型，表示这个SubPlan的输出是否按照partitionBy属性的key值对数据进行Shuffle。 Hash：发生Shuffle None：不进行Shuffle 在下面的执行计划中，SubPlan1和SubPlan0 PlanDistribution=Source，这两个SubPlan都是提供数据源的节点，SubPlan1所有节点的读取数据都会发向SubPlan0的每一个节点；SubPlan2分配8个节点执行最终的聚合操作；SubPlan3只负责输出最后计算完成的数据。只有SubPlan0的OutputPartitioning=HASH（存在AggregateNode计划），所以SubPlan2接收到的数据是按照rank字段Partition后的数据 SQL提交并解析为SubPlan后的执行流程：比如一条SQL最终生成4个SubPlan（0-3），其中0，1并行执行Join或聚合操作，其余串行执行，每个SubPlan都会分发到多个工作节点执行。 Coordinator通过HTTP协议调用Worker节点的/v1/task接口将执行计划分配给所有Worker节点（图中蓝色箭头） SubPlan1的每个节点读取一个Split的数据并过滤后将数据分发给每个SubPlan0节点进行Join或聚合操作 SubPlan1的每个节点计算完成后按GroupBy Key的Hash值将数据分发到不同的SubPlan2节点 所有SubPlan2节点计算完成后将数据分发到SubPlan3节点 SubPlan3节点计算完成后通知Coordinator结束查询，并将数据发送给Coordinator 总结一条SQL在Presto执行的完整流程： 客户端通过HTTP协议发送一个查询语句给Presto集群的Coordinator Coordinator接收到客户端传来的查询语句，对该语句进行解析、生成查询执行计划，并根据查询执行计划依次生成SqlQueryExecution -&gt; SqlStageExecution -&gt; HttpRemoteTask Coordinator将每个Task分发到所需要处理的数据所在的Worker上进行分析 执行Source Stage的Task，这些Task通过Connector从数据源中读取所需要的数据 处于下游Stage中用的Task会读取上游Stage产生的输出结果，并在该Stage的每个Task所在的Worker内存中进行后续的计算和处理 Coordinator从分发的Task之后，一直持续不断的从最后的Stage中的Task获得计算结果，并将结果写入到缓存中，直到所有的计算结束 Client从提交查询后，就一直监听Coordinator中的本次查询结果集，立即输出。直到所有的结果都返回，本次查询结束 部署与使用Presto集群部署下载Presto使用GUID生成工具生成所需机器数量相等的GUID，用于配置node.properties cd $PRESTO_HOME mkdir etc # node.properties配置节点信息(每个节点不同) vim etc/node.properties node.environment=cdh 一个Presto集群有相同的env名称，我这里起名叫cdh node.id=AB1C6EAC-8CF6-B397-EFD3-77C8AB041CD5 刚刚生成的GUID，每个节点都要不同的GUID node.data-dir=/var/presto/data 存放数据的目录 # JVM配置(每个节点相同) vim etc/jvm.config -server -Xmx4G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError # Presto配置 Coordinator节点 非生产集群单个节点可以既为Coordinator又为Worker可设node-scheduler.include-coordinator=true vim etc/config.properties(每个节点不同) coordinator=true 是否为Coordinator node-scheduler.include-coordinator=false 是否在Coordinator节点执行计算（会影响性能，不建议true） http-server.http.port=8080 请求发送的HTTP端口 query.max-memory=4GB 单条Query占用集群内存的最大值 query.max-memory-per-node=1GB 单条Query单个节点占用内存的最大值 query.max-total-memory-per-node=2GB 单条Quey单个节点占用的执行内存和系统内存(readers, writers, and network buffers, etc.)总和的最大值 discovery-server.enabled=true 整合Coordinator和DiscoveryServer为一个进程，使用同一个端口 discovery.uri=http://cdh101:8080 # Presto配置 Worker节点 vim etc/config.properties coordinator=false http-server.http.port=8080 query.max-memory=4GB query.max-memory-per-node=1GB query.max-total-memory-per-node=2GB discovery.uri=http://cdh101:8080 指定集群中已有的DS服务HTTP地址 # 日志等级设置(每个节点相同) vim etc/log.properties com.facebook.presto=INFO # 配置支持Hive数据源 (每个节点相同)(其他数据源可参考https://prestodb.io/docs/current/connector) mkdir etc/catalog vim etc/catalog/hive.properties connector.name=hive-hadoop2 hive.metastore.uri=thrift://cdh101:9083 hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml # ############################################### ln -s /var/presto/data/var/log log 将日志链接到安装目录 在各个节点后台启动Presto：bin/launcher start 也可以在前台运行,查看具体的日志：bin/launcher run 停止服务进程命令：bin/laucher stop 启动脚本编写 #!/bin/bash # 需要root用户免密 # 使用 sh presto-server.sh start PRESTO_HOME=/opt/modules/presto-server-0.248 OP=$1 if [ &quot;$OP&quot; == &quot;start&quot; ] || [ &quot;$OP&quot; == &quot;stop&quot; ] || [ &quot;$OP&quot; == &quot;status&quot; ] || [ &quot;$OP&quot; == &quot;restart&quot; ]; then echo &quot;Begin to $OP Presto Coordinator and Workers.&quot; for((host=101; host&lt;=104; host++)); do echo --- &quot;$OP&quot; presto server on cdh$host --- ssh -l root cdh$host $PRESTO_HOME/bin/launcher &quot;$OP&quot; done else echo &quot;Usage: ./presto-server.sh [start|stop|restart|status]&quot; exit 1 fi # ############################################### 根据自己的版本下载presto客户端：https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.248/presto-cli-0.248-executable.jar chmod a+x presto-cli-0.248-executable.jar mv presto-cli-0.248-executable.jar presto ln -s /opt/modules/presto-server-0.248/presto /usr/bin/presto 进入Presto客户端(指定数据源hive，指定库名default) presto --server cdh101:8080 --catalog hive --schema default # ############################################### # 配置MySQL数据源 vim etc/catalog/mysql.properties connector.name=mysql connection-url=jdbc:mysql://cdh102:3306 connection-user=root connection-password=123456 Presto On Yarn部署待补充 Presto语法：SHOW CATALOGS; 查看Presto集群当前可用数据源 SHOW SCHEMAS; 查看当前数据源有哪些库 SHOW SCHEMAS FROM hive; 查看hive数据源的所有库 （FROM可以用IN替换) SHOW TABLES; 查看当前Schema库下有哪些表 SHOW TABLES FROM hive.default; 查看hive数据源下default库下的所有表 CREATE SCHEMA hive.web WITH (location = &#39;hdfs:///user/hive/warehouse/web/&#39;) # 建库 ALTER SCHEMA old_db_name RENAME TO new_db_name # 改库名 -- 建表 CREATE TABLE hive.test.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = &#39;Parquet&#39;, partitioned_by = ARRAY[&#39;ds&#39;, &#39;country&#39;], bucketed_by = ARRAY[&#39;user_id&#39;], bucket_count = 50 ); -- 查看表 desc hive.test.page_views; -- 查看表字段 SHOW COLUMNS IN hive.test.page_views; -- 统计表信息（类似于Spark的Analyzed table）（数据大小，行数，最大值，最小值，无重复值个数，NULL值占比） SHOW STATS FOR table SHOW STATS FOR ( SELECT * FROM table [ WHERE condition ] ) -- 查看逻辑计划（相比Spark的逻辑计划，多了每个节点的行数及数据大小,CPU操作数,内存消耗,网络传输大小） EXPLAIN sql EXPLAIN ANALYZE sql -- 更全面 但会触发计算 -- 枚举表 SELECT * FROM ( VALUES (1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;) ) t (id, name); SELECT * FROM ( VALUES (1, &#39;a&#39;, ARRAY[1, 2, 3]), (2, &#39;b&#39;, ARRAY[4, 5, 6]), (3, &#39;c&#39;, ARRAY[7, 8, 9]) ) AS t (id, name, arr); Presto支持事务，相关命令有COMMIT,START TRANSACTION,ROLLBACK更多语法：SQL Statement Syntax Presto WEBUI访问WEBUI地址即为DiscoveryServer地址：http://cdh101:8080/ui/透过WEB UI可以查看到每个SQL Query的执行相关状态信息以及Presto集群的运行状态信息。 任务状态 原因 QUEUED 等待执行 PLANNING 正在转成执行计划 RUNNING 正在运行Query BLOCKED 阻塞中，等待内存、Buffer等资源 FINISHING 即将完成执行，正在返回数据 FINISHED 执行完成 FAILED 执行失败 BLOCKED状态是正常的，但持续很长时间都是这个状态就需要排查下原因，有很多可能的原因：1.内存不足2.磁盘或网络I/O瓶颈3.数据倾斜(所有数据都转移到几个worker上)4.并行度低(只有几个worker可用)5.某个Stage查询开销较高（如select *操作数据过多）对于某个Query的执行过程相关监控信息，可以在WebUI上点那个Query ID即可查看 连接Presto用户连接Presto的主要方式：Presto-Cli,JDBC,PyHive,PrestoOnSpark等。Presto-Cli: wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.248/presto-cli-0.248-executable.jar mv presto-cli-0.248-executable.jar presto chmod a+x presto presto --server cdh101:8080 --catalog hive --schema default --user admin JDBC: &lt;dependency&gt; &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt; &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt; &lt;version&gt;0.248&lt;/version&gt; &lt;/dependency&gt; public class PrestoConnetToJDBC &#123; public static void main(String[] args) throws SQLException &#123; // 1.简单创建连接 // String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users&quot;; // Connection connection = DriverManager.getConnection(url, &quot;root&quot;, null); // connection.prepareStatement(&quot;show tables&quot;); // 2.带参数创建连接 String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users&quot;; Properties properties = new Properties(); properties.setProperty(&quot;user&quot;, &quot;root&quot;); properties.setProperty(&quot;password&quot;, &quot;&quot;); properties.setProperty(&quot;SSL&quot;, &quot;false&quot;); Connection connection = DriverManager.getConnection(url, properties); // 3.带参数创建连接 // String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users?user=root&amp;password=secret&amp;SSL=true&quot;; // Connection connection = DriverManager.getConnection(url); // 读数据或做其他操作 Statement stmt = connection.createStatement(); ResultSet rs = stmt.executeQuery(&quot;select * from tb_user_info limit 10&quot;); while (rs.next())&#123; System.out.println(rs.getString(1) + &quot;--&quot; + rs.getString(2)); &#125; &#125; &#125; python:pip3 install saslpip3 install thriftpip3 install thrift-saslpip3 install PyHivepip3 install sqlalchemypip3 install requests from sqlalchemy import * from sqlalchemy.engine import create_engine from sqlalchemy.schema import * import pandas as pd # Presto engine = create_engine(&#39;presto://admin:123456@cdh101:8080/mysql/db_users&#39;) # 密码连接 engine = create_engine(&#39;presto://cdh101:8080/mysql/db_users&#39;) df = pd.read_sql(&quot;select * from tb_user_records limit 10&quot;,engine) print(df) PrestoOnSpark:Presto on Spark即利用Spark作为Presto查询的执行框架操作：Executing Presto on Spark 最佳实践Presto参数调优：Properties Reference，官方详细介绍了Presto的config.properties中的常规参数如join参数，内存管理参数，Spilling溢出磁盘相关参数，数据网络交换参数（一个查询任务不同Stage会有不同节点交换数据，这些参数提高网络利用率），任务参数，节点调度参数，优化器参数以及正则相关参数 Presto不是纯内存计算吗？为什么要溢写到磁盘？正常情况Presto执行Query请求的内存资源超过query_max_memory或query_max_memory_per_node这个Query就会被终止。溢写磁盘机制：Presto节点空闲时Query会利用全部内存资源，如果没足够内存，Query被迫使用磁盘来存储中间数据，写入磁盘再从磁盘读取回来，有较高的IO开销。解决IO开销高的方法：spiller-spill-path可设置多个磁盘多个路径，并行读写提高IO效率；spill-encryption-enabled启用压缩用CPU开销换IO开销局限：系统无法将中间数据划分成足够小的块，导致从磁盘加载块数据时发生OOM；只有Join和聚合操作可以落盘 资源隔离机制？Presto可以像Yarn一样将全部资源分为多个资源组（通过配置文件etc/resource-groups.properties），资源组也可以有子组。配置可参考：Resource Groups Session配置管理通过配置etc/session-property-config.properties可以将任务分为不同类型（如即时查询，etl，高消耗etl等…），然后对不同类型的任务配置不同的资源，参数（configuration property）。具体见：Session Property Managers（Presto参数分为configuration property和session property） 分布式排序需要排序数据超过单节点query.max-memory-per-node大小限制，默认会启用分布式排序（参数distributed-sort）。排序速度不会随节点数量增加而线性加快，因为排序后数据在单个节点合并 使用Alluxio基于内存缓存热点数据和降低远程机房网络IO影响注意:计算与存储节点共置的场景下，Alluxio对Presto的加速效果并不明显Alluxio分布式缓存数据湖相关知识可以参考我的另一篇文章：Alluxio-基于内存的虚拟分布式存储系统Presto结合Alluxio配置和使用可以参考官方文档：Alluxio Cache Service 基于成本的优化Join操作对查询性能影响大，Presto也会像Spark一样，评估Join的表的顺序，自动选择最低成本的Join表顺序。对应的ConfigurationProperty(optimizer.join-reordering-strategy)对应的SessionProperty(join_reordering_strategy)参数值：AUTOMATIC全自动的Join优化，ELIMINATE_CROSS_JOINS默认参数消除不必要的笛卡尔积，NONE按SQL语法的顺序Join 分布式Join算法选择Presto的Join是基于Hash的，分为两种方式：Partitioned和BroadcastPartitioned：每个节点都持有一部分Hash后的数据，然后JoinBroadcast：一个表被广播到所有参与Join计算节点对应的ConfigurationProperty(join-distribution-type)对应的SessionProperty(join_distribution_type)参数值：AUTOMATIC全自动的Join算法选择，BROADCAST，PARTITIONED(默认) Presto会根据元数据信息读取分区数据，合理设置分区能减少Presto数据读取量，提升查询性能 Presto对ORC格式文件的读取进行了特定优化，相对于Parquet，Presto对ORC支持更好(Impala对Parquet支持更好) 数据压缩可以减少节点间数据传输对网络带宽的压力，对于即席查询需要快速解压，建议采用Snappy压缩算法 预先排序以提高性能对于已经排序的数据，在查询的数据过滤阶段，ORC格式支持跳过读取不必要的数据。比如对于经常需要过滤的字段可以预先排序。INSERT INTO table nation_orc partition(p) SELECT * FROM nation SORT BY n_name;如果需要过滤 n_name 字段，则性能将提升：SELECT count(*) FROM nation_orc WHERE n_name=’AUSTRALIA’; 一些Presto优化常识 因为列式存储，尽量避免select *，而是只查询有用字段 对于有分区的表，where语句中优先使用分区字段进行过滤 合理安排__group by字段的顺序__有助于提高查询效率，这些字段按照每个字段distinct数据多少进行降序排列 多表Join时，数据越多的表越往后放，Left join时，条件过滤尽量在ON阶段完成，而少用WHERE，Join左边尽量放小数据量的表，而且最好是重复关联键少的表 将使用频繁的表作为一个子查询抽离出来，避免多次读取IO Order by时使用limit：Order by需要扫描数据到单个worker节点进行排序，导致单个worker需要大量内存。如果是查询Top N或者Bottom N，使用limit可减少排序计算和内存压力 精度要求低的场景使用近似聚合函数：Presto有一些近似聚合函数，对于允许有少量误差的查询场景，使用这些函数对查询性能有大幅提升。比如使用approx_distinct()函数比Count(distinct x)有大概2~3%的误差。 用regexp_like代替多个like语句：Presto查询优化器没有对多个like语句进行优化，使用regexp_like对性能有较大提升 [GOOD] SELECT xxx FROM access WHERE regexp_like(method, &#39;GET|POST|PUT|DELETE&#39;) [BAD] SELECT xxx FROM access WHERE method LIKE &#39;%GET%&#39; OR method LIKE &#39;%POST%&#39; OR method LIKE &#39;%PUT%&#39; OR method LIKE &#39;%DELETE%&#39; 使用Rank函数代替row_number函数来获取Top N:在进行一些分组排序场景时，使用rank函数性能更好 [GOOD] SELECT checksum(rnk) FROM ( SELECT rank() OVER (PARTITION BY l_orderkey, l_partkey ORDER BY l_shipdate DESC) AS rnk FROM lineitem ) t WHERE rnk = 1 [BAD] SELECT checksum(rnk) FROM ( SELECT row_number() OVER (PARTITION BY l_orderkey, l_partkey ORDER BY l_shipdate DESC) AS rnk FROM lineitem ) t WHERE rnk = 1 使用Presto分析统计数据时，可考虑把多次查询合并为一次查询，用Presto提供的子查询完成。 WITH subquery_1 AS ( SELECT a1, a2, a3 FROM Table_1 WHERE a3 between 20180101 and 20180131 ), /*子查询subquery_1,注意：多个子查询需要用逗号分隔*/ subquery_2 AS ( SELECT b1, b2, b3 FROM Table_2 WHERE b3 between 20180101 and 20180131 ) /*最后一个子查询后不要带逗号，不然会报错。*/ SELECT subquery_1.a1, subquery_1.a2, subquery_2.b1, subquery_2.b2 FROM subquery_1 JOIN subquery_2 ON subquery_1.a3 = subquery_2.b3; 字段名与关键字冲突：MySQL对于关键字冲突的字段名加反引号，Presto对与关键字冲突的字段名加双引号。 Hive分析任务如何迁移PrestoPresto使用ANSI标准的SQL语法，Hive使用类SQL语法HQL官方案例：Migrating From Hive -- 1.Presto使用下标取数组元素 下标从1开始 select id, arr[1] as arr2, arr[3] as arr3 from (SELECT * FROM ( VALUES (1, &#39;a&#39;, ARRAY[1, 2, 3]), (2, &#39;b&#39;, ARRAY[4, 5, 6]), (3, &#39;c&#39;, ARRAY[7, 8, 9]) ) AS t (id, name, arr)) a; -- 2.不支持隐式数据类型转换，需要手动转换 SELECT CAST(x AS varchar) , CAST(x AS bigint) , CAST(x AS double) , CAST(x AS boolean) FROM ... SELECT CAST(5 AS DOUBLE) / 2;SELECT 5 / 2; -- 3.WITH AS语法 WITH a AS (SELECT uploader,videos FROM tb_user_info LIMIT 10) select * from a; -- 4.UNNEST关键字代替LATERAL VIEW explode()进行行转列 Hive写法: SELECT student, score FROM tests LATERAL VIEW explode(scores) t AS score; Presto写法: SELECT student, score FROM tests CROSS JOIN UNNEST(scores) AS t (score); -- 5.Hive视图不支持通过Presto查询，所以要在Presto创建同名视图（即在presto读取视图定义(StatementAnalyzer.java)的时候，解析原始的sql定义的语句，转换成presto的视图结构） -- 6.cast as string不支持，因为Presto的是Varchar，需要在ASTBuilder.java中把string替换为了varchar类型 -- 7.select 1 = &#39;1&#39;;在Hive和Presto计算结果分别为true,cannot be applied to integer, varchar(1) 需要额外操作实现透明的隐式转换 -- 8.UDF支持、null值处理 -- 9.对于timestamp类型字段做where条件比较，hive可以直接比较，presto需要加timestamp关键字 /*Hive的写法*/ SELECT t FROM a WHERE t &gt; &#39;2021-01-01 00:00:00&#39;; /*Presto中的写法*/ SELECT t FROM a WHERE t &gt; timestamp &#39;2021-01-01 00:00:00&#39;; -- 10.Presto的MD5传入binary类型则会返回binary类型，所以对字符串的MD5需要转换： SELECT to_hex(md5(to_utf8(&#39;abcd&#39;))); -- 11.Presto不支持INSERT OVERWRITE，只能先DELETE再INSERT Hive数仓的数据安全性和权限参考Built-in System Access Control在我看来hive.security=file形式的授权比较灵活先配置全局的Catalog访问权限：user:可选参数，正则匹配用户名，默认.*catalog:可选参数，正则匹配Catalog名，默认.*.allow:必选参数，用户是否对calalog有访问权限true\\false # 启用基于文件的权限控制 vim /opt/modules/presto-server-0.248/etc/access-control.properties access-control.name=file security.config-file=/opt/modules/presto-server-0.248/etc/rules.json security.refresh-period=10s # 配置权限自动刷新时间间隔 10s # 设置权限控制规则：允许只admin用户有mysql catalog的权限，所有用户有hive catalog权限，所有用户无system catalog权限 vim /opt/modules/presto-server-0.248/etc/rules.json &#123; &quot;catalogs&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;catalog&quot;: &quot;(mysql|system)&quot;, &quot;allow&quot;: true &#125;, &#123; &quot;catalog&quot;: &quot;hive&quot;, &quot;allow&quot;: true &#125;, &#123; &quot;catalog&quot;: &quot;system&quot;, &quot;allow&quot;: false &#125; ] &#125; 分发access-control.properties和rules.json，重启PrestoServer生效 连接PrestoClient并指定用户presto --server cdh101:8080 --catalog hive --schema default --user qjj 配置hive数据源的权限，参考Hive Security Configuration有legacy,read-only,file,sql-standard四种形式，仍然是file的授权形式比较灵活 vim etc/catalog/hive.properties hive.security=file security.config-file=/opt/modules/presto-server-0.248/etc/catalog/hive-security.json vim /opt/modules/presto-server-0.248/etc/catalog/hive-security.json &#123; &quot;schemas&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;schema&quot;: &quot;.*&quot;, &quot;owner&quot;: true &#125;, &#123; &quot;user&quot;: &quot;staging&quot;, &quot;owner&quot;: false &#125;, &#123; &quot;user&quot;: &quot;test&quot;, &quot;schema&quot;: &quot;test&quot;, &quot;owner&quot;: false &#125; ], &quot;tables&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;, &quot;OWNERSHIP&quot;] &#125;, &#123; &quot;user&quot;: &quot;staging&quot;, &quot;table&quot;: &quot;(staging_db_users|staging_db_videos).*&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;] &#125;, &#123; &quot;user&quot;: &quot;test&quot;, &quot;table&quot;: &quot;test.*&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;] &#125; ] &#125; 分发catalog/hive.properties、catalog/hive-security.json 重启Presto-server 总结 Hive\\Spark的SQL任务迁移到Presto在语法、计算结果、视图使用、类型转换、UDF及空值处理上有差异 Hive\\Spark任务迁移Presto，如果要做到对业务透明，还有很长的路要走 参考资料Presto Documentation深入理解PrestoPresto实现原理和美团的使用实践Hive迁移Presto在OPPO的实践零基础熟悉 Presto的概念、安装、使用及优化","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"交互式分析","slug":"交互式分析","permalink":"https://shmily-qjj.top/tags/%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%88%86%E6%9E%90/"},{"name":"SQL引擎","slug":"SQL引擎","permalink":"https://shmily-qjj.top/tags/SQL%E5%BC%95%E6%93%8E/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据平台常见异常处理汇总","slug":"大数据平台常见异常处理汇总","date":"2021-01-30T04:50:00.000Z","updated":"2022-01-06T04:54:50.547Z","comments":true,"path":"BigdataExceptionsSummary/","link":"","permalink":"https://shmily-qjj.top/BigdataExceptionsSummary/","excerpt":"","text":"大数据平台常见异常处理汇总本博客记录工作中遇到的，大数据相关各个组件的异常处理过程，养成良好的问题归纳总结习惯，累积问题解决经验与思路。 Spark相关 Shuffle异常导致任务失败报错：org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1原因：shuffle分为shuffle write和shuffle read两部分。shuffle write的分区数由上一阶段的RDD分区数控制，shuffle read的分区数则是由Spark提供的一些参数控制。shuffle write可以简单理解为类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上。shuffle read的时候数据的分区数则是由spark提供的一些参数控制。可以想到的是，如果这个参数值设置的很小，同时shuffle read的量很大，那么将会导致一个task需要处理的数据非常大。结果导致JVM crash，从而导致取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。有时候即使不会导致JVM crash也会造成长时间的gc。解决思路：减少shuffle的数据量和增加处理shuffle数据的分区数①spark.sql.shuffle.partitions控制分区数，默认为200，根据shuffle的量以及计算的复杂度提高这个值 shuffle并行度②提高spark.executor.memory③map side join或是broadcast join来规避shuffle的产生④分析数据倾斜 解决数据倾斜⑤增加失败的重试次数和重试的时间间隔通过spark.shuffle.io.maxRetries控制重试次数，默认是3，可适当增加，例如10。通过spark.shuffle.io.retryWait控制重试的时间间隔，默认是5s，可适当增加，例如10s。⑥类似RemoteShuffleService的服务，解决Shuffle单台机器IO瓶颈，记录Shuffle状态，大批量提升Shuffle效率和稳定性。 SparkSQL报awaitResult异常报错：org.apache.spark.SparkException: Exception thrown in awaitResult原因：广播数据超时解决：spark.sql.broadcastTimeout=1200 默认大小300 HiveOnSpark不能创建SparkClient及Return Code 1异常报错：FAILED：SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask原因：以上报错证明初始化Spark失败，而以前不会失败，所以大概率是资源问题而不是代码问题，查看Yarn队列发现所提交的队列已满且已超过能申请资源的上限（虚线部分），故任务启动失败解决：CM界面-&gt;群集-&gt;动态资源池配置-&gt;提高队列的资源权重（上限也会响应提高）-&gt;刷新动态资源池配置 ExecutorLost、Task Lost报错：1.[executor lost] WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, aa.local): ExecutorLostFailure (executor lost)2.[task lost] WARN TaskSetManager: Lost task 69.2 in stage 7.0 (TID 1145, 192.168.47.217): java.io.IOException: Connection from /xx.xxx.xx.xxx:xxxxx closed3.[timeout] java.util.concurrent.TimeoutException: Futures timed out after [120 secondERROR TransportChannelHandler: Connection to /xxx.xxx.xx.xxx:xxxxx has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong原因：节点资源不足、网络延迟波动、GC导致Executor运行慢等原因解决：①spark.network.timeout的值（默认为120s,配置所有网络传输的延时），根据情况改成300(5min)或更高 ②分别增加各类超时参数spark.core.connection.ack.wait.timeoutspark.akka.timeoutspark.storage.blockManagerSlaveTimeoutMsspark.shuffle.io.connectionTimeoutspark.rpc.askTimeout or spark.rpc.lookupTimeout SparkThriftServer无法链接jdbc,后台报错Task has been rejected by ExecutorService报错：2021-08-01 02:26:32.028 WARN [Thread-43] [10.139.53.62] org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:185) :- Task has been rejected by ExecutorService 9 times till timedout, reason: java.util.concurrent.RejectedExecutionException: Task org.apache.thrift.server.TThreadPoolServer$WorkerProcess@6b4f8abf rejected from java.util.concurrent.ThreadPoolExecutor@48ca75d3[Running, pool size = 500, active threads = 500, queued tasks = 0, completed tasks = 917]原因：每次连接都是一个socket连接，都会提交一个Runnable对象到ExecutorService线程池，线程池默认最大500,连接不使用且未关闭就会占用一个线程，占满就无法再连接解决：hive-site.xml调整：hive.server2.session.check.interval 6h-&gt;1hhive.server2.idle.session.timeout 7d-&gt;1dhive.server2.thrift.max.worker.threads (500-&gt;800) (根据用户量大概估，一个用户可能多个Socket/JDBC连接)目的：避免因socket连接对象线程池被占满导致无法连接jdbc &lt;property&gt; &lt;name&gt;hive.server2.session.check.interval&lt;/name&gt; &lt;value&gt;1h&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.idle.session.timeout&lt;/name&gt; &lt;value&gt;1d&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt; &lt;value&gt;800&lt;/value&gt; &lt;/property&gt; 注意：hive.server2.session.check.interval &lt; hive.server2.idle.operation.timeout &lt; hive.server2.idle.session.timeout HDFS相关 数据块丢失且命令无法修复起因：多张表查询发现如下报错，提示块丢失分析：CM界面看HDFS丢失块，发现有2500多，大批量块丢失可能的原因： 1.DataNode与NameNode未通信，DataNode进程未启动 2.DataNode数据磁盘损坏，数据丢失 3.一个文件的全部副本丢失解决过程：尝试修复丢失块：hdfs debug recoverLease -path -retries 显示修复成功，但使用hadoop fs -text 还是报MissingBlock无法读取使用fsck检测坏块 hdfs fsck /user/hive/warehouse发现绝大多数block名称都带有172.xxx.xxx.11 定位到可能是172.xxx.xxx.11节点的DataNode可能存在问题通过CM日志和机器上进程状态判断172.xxx.xxx.11的DataNode已与NameNode保持心跳，运行正常，进而怀疑磁盘坏了（概率太小）查看CM配置和机器磁盘，发现少配置了些硬盘路径，原因是在配置新节点磁盘路径时误修改整个配置组的磁盘路径，导致该配置组中所有DataNode缺少磁盘，进而出现块丢失且无法修复的问题。解决：还原磁盘配置，滚动重启该配置组中的DataNode，将不同机器配置分成多个配置组，重新修改配置重启过程中丢失块数一直在减少：最终恢复正常总结：1.CM上修改配置一定要慎重，注意修改配置组中某台节点的配置会影响整个配置组中所有节点的配置2.CM显示DataNode重启成功只是进程启动成功，但日志出现“Total time to add all replicas to map”字眼才是真正完成启动3.https://hdfs-site/dfshealth.html#tab-overview 从HDFS WebUI获取更多信息（丢失块的信息一目了然） Win开发Hadoop环境winutils错误：Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries解决：将winutils.exe放在HADOOP_HOME\\bin下，然后代码里System.setProperty(“hadoop.home.dir”, “D:\\Programming\\Env\\Hadoop\\hadoop-2.7.2\\“)或设置环境变量HADOOP_HOME和PATH后重启电脑winutils.exe下载地址：winutils-master Namenode is not formatted错误Namenode is not formatted分析：一般新部署的HDFS才会提示需要格式化(hadoop namenode -format)，而生产环境Namenode一旦有异常，也可能出现这样的问题，但我不能格式化生产环境啊！NN日志如下：可知识由于加载fsimage异常，去NameNode的存储路径/dfs/nn/发现没有文件，而其他节点/dfs/nn目录下有current目录，current目录下有fsimage。所以可以判定该Namenode无法启动就是因为fsimage丢失解决：scp这个current目录，权限与之前的一致（一般为hdfs:hdfs权限） 修复HDFS坏块查看坏块可通过 hdfs fsck / hadoop dfsadmin -report 修复坏块除了上面讲过的hdfs debug recoverLease -path -retries 命令外，还可以通过hadoop fs -setrep -R 3 / 命令，设置副本，副本小于3的情况会自动恢复三副本，修复后还有坏块那就是一个副本都没有了，只能丢了。 两个NN均为Standby，重启后过一会又都StandbyNN均报错：java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!Caused by: java.io.IOException: Cannot find any valid remote NN to service request!Exception from remote name node RemoteNameNodeInfo [nnId=namenode37, ipcAddress=xxxx, httpAddress=xxxx], try next.org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error解决：强制切换主备sudo -uhdfs hdfs haadmin -transitionToActive –forcemanual namenode37（nnId在hdfs-site有配） 切换后重启Failover Controller或ZKFC。 基于Sentry认证的HDFS集群NN启动异常NN进程启动成功但是一直处于忙碌状态，查看NN的日志 晚上9点53:24.664分 INFO SentryAuthorizationInfo Refresh interval [500]ms, retry wait [30000] 晚上9点53:24.664分 INFO SentryAuthorizationInfo stale threshold [60000]ms 晚上9点53:24.669分 INFO HMSPaths HMSPaths:[/user/hive/warehouse, /user/hive/xxxx] Initialized 晚上9点53:24.681分 INFO SentryTransportPool Creating pool for xxx:8038,xxx:8038 with default port 8038 晚上9点53:24.683分 INFO SentryTransportPool Adding endpoint xxx:8038 晚上9点53:24.683分 INFO SentryTransportPool Adding endpoint xxx:8038 晚上9点53:24.683分 INFO SentryTransportPool Connection pooling is disabled 日志卡在这不继续了，卡在这 与此同时两个SentryServer有一台发生OOM，不稳定原因：由于基于Sentry认证，HDFS的NN在启动时，Sentry会全量收集HMSPaths下的ACL信息，会驻留在Sentry内存中，如果Sentry内存不足容易OOM，需要通过增加堆内存的方式解决。两个SentryServer虽然可以负载均衡高可用，当一个SentryServer宕掉请求才会转移到另一个节点，而OOM时，请求会一直卡住，不会自动转移，导致NN启动异常。解决：增加Sentry堆内存，不盲目加，根据当前Hive服务器数、库数、表数、分区数、列数及视图数进行评估，具体评估标准如下：如图推算：每百万个Hive对象（库、表、分区数）需要配置2.25G的Sentry最大堆内存 distcp数据拷贝报错distcp报错file might have been written to during copy, consider enabling HDFS Snapshots to avoid this error.hdfs-site.xml里增加dfs.namenode.snapshot.capture.openfiles 值为true 开启Immutable Snapshot，保证快照目录里所有文件的状态都是关闭的，文件大小都是创建快照时的状态，解决同步hdfs数据报错文件不存在，或者报不能复制一个打开的或正在写入的文件这些问题。 NameNode启动后无法RPC和切换状态并一直FullGC，几乎80%以上时间都在FullGC 2021-07-14 09:38:35,803 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47892ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48186ms 2021-07-14 09:39:25,780 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47976ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48428ms 2021-07-14 09:40:15,526 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47744ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47840ms 2021-07-14 09:41:04,652 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47126ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47437ms 2021-07-14 09:41:55,106 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 48452ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48767ms 2021-07-14 09:42:48,440 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 51332ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=51719ms 2021-07-14 09:43:40,527 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 50086ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=50450ms 2021-07-14 09:44:34,750 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 52222ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=52462ms 2021-07-14 09:45:30,759 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 53999ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=54179ms 原因：加载FSImage到堆内存的过程中由于堆内存不足导致无法启动。解决： 增加NameNode堆内存，启动后查看HDFS WebUI-&gt;Overview-&gt;Summary查看主NN堆内存使用情况和目前Blocks数量，对NN内存需求重新做评估，调整合适的堆大小。根据Block数量和增量精细化计算NN的堆内存，避免再次OOM。 如果可能，可以减小HDFS垃圾回收时间。 查看HDFS WebUI-&gt;Snaoshot栏，删除无用的快照，避免快照变化文件占用大量空间和Blocks。 HDFS DataNode磁盘切换停止DN、JN -&gt; 将/dfs目录移至目标磁盘，并修改hdfs对应的磁盘为目标路径（对应节点的dfs.journalnode.edits.dir和dfs.datanode.data.dir） -&gt; 启动DN、JN HDFS远程Kerberos客户端连接非Kerberos集群报错：hdfs dfs -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/ ls: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.; Host Details : local host is: &quot;client_host/client_ip&quot;; destination host is: &quot;remote_nn_ip&quot;:8020; 原因:本地客户端采用Kerberos认证，但远程Server未开启Kerberos认证，远程为SIMPLE认证解决:设置ipc.client.fallback-to-simple-auth-allowed=true参数即可 参数加在dfs后面 如下 hdfs dfs -D ipc.client.fallback-to-simple-auth-allowed=true -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/ Hive相关 HiveMetaStore状态不良导DDLSQL耗时200s以上HMS进程报错：hive metastore server Failed to sync requested HMS notifications up to the event ID xxx原因分析：查看sentry异常CounterWait源码发现传递的id比currentid大导致一直等待超时，超时时间默认为200s（sentry.notification.sync.timeout.ms）。开启了hdfs-sentry acl同步后，hdfs，sentry，HMS三者间权限同步的消息处理。当突然大批量的目录权限消息需要处理，后台线程处理不过来，消息积压滞后就会出现这个异常。这个异常不影响集群使用，只是会导致create，drop table慢需要等200s，这样等待也是为了追上最新的id。我们这次同时出现了HMS参与同步消息处理的线程被异常退出，导致sentry的sentry_hms_notification_id表数据一直没更新，需要重启HMS。如果积压了太多消息，让它慢慢消费处理需要的时间太长，可能一直追不上，这时可以选择丢掉这些消息。解决： ①可以通过设置sentry.notification.sync.timeout.ms参数调小超时时间，减小等待时间，积压不多的话可以让它自行消费处理掉。 ②丢掉未处理的消息，在sentry的sentry_hms_notification_id表中插入一条最大值(等于当前消息的id，从notification_sequence表中获取) ，重启sentry服务。（notification_log 表存储了消息日志信息） HBase外部表报Unexpected end-of-input起因：使用Hive创建HBase外部表时正常，但使用HBase外部表时报Unexpected end-of-input: was expecting closing分析过程：翻阅源码部分发现异常是在解析外部表创建JSON时发生，于是对比建表语句和Hive元数据库中的TABLE_PARAMS表信息得到原因原因：创建hbase外部表catalog太长导致schema太长，而hive元数据表mysql里的table_params字段param_value字段类型是varchar(4000)建表时由于schema太长，超过4000字符的部分被截断。而使用该表的时候会读元数据，但因为元数据不完整而报错。解决：①分多次建表②改varchar(4000)为longtext然后重建表（影响无法评估，没尝试） Hive和Spark查询Hive表报java.net.UnknownHostException: nameservice1分析：由于机器HDFS HA配置发生变动，关掉了高可用，所以nameservice变成了ip:8020，报这个错，首先检查了/etc/hadoop/conf/hdfs-site.xml里面无dfs.nameservices配置，证明配置文件没问题。使用hivecli，desc formatted某几张表，发现表的LOCATION均为hdfs://nameservice1/xx 找到了问题的原因，元数据错误，所以需要去Hive元数据库批量修改元数据。解决：连接到Hive metastore元数据库，批量修改LOCATION：update hive.SDS set LOCATION=concat(“hdfs://ip:8020”,substring(LOCATION,20,length(LOCATION)-19)) where LOCATION like “hdfs://nameservice1%”;update hive.DBS set DB_LOCATION_URI=concat(“hdfs://ip:8020”,substring(DB_LOCATION_URI,20,length(DB_LOCATION_URI)-19)) where DB_LOCATION_URI like “hdfs://nameservice1%”;无需重启服务即可生效，问题解决。 Yarn相关 应用提交报Retrying connect to server 0.0.0.0原因：应用没有认到yarn-site.xml或者yarn-site.xml配置不正确解决：①指定HADOOP_CONF_DIR ②确认yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; 内存不足Container退出报错：Diagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.Dump of the process-tree for container_e23_1574819880505_43157_01_000001分析：”physical memory used”为物理内存占用（应用已占满9G），”virtual memory used”为虚拟内存占用，18.9GB是取决于yarn.nodemanager.vmem-pmem-ratio（yarn-site.xml中设置的虚拟内存和物理内存比例，默认2.1），报错是因为物理内存不足，是任务设置的内存少了解决思路：①如果资源充足，增加任务并行度分担任务负载②增大任务可用资源（注意不要超过单台NM可分配上限yarn.scheduler.maximum-allocation-mb的值）③适当增大yarn.nodemanager.vmem-pmem-ratio，适当调高虚拟内存比例④[不建议]取消内存的检查：在yarn-site.xml或者程序中中设置yarn.nodemanager.vmem-check-enabled为false HBase相关 集群新增RS致另一RS异常退出上线的RegionServer会触发Region移动，报错前有Flush操作，因为Region移动前会先Flush Region异常相关源码：Flush Memstore到HFile这个过程未发生异常，但Flush一个Memstore后跟踪Memstore总大小未发生变化，即内存清理失败 超过5次就中止这个RS分析：版本BUG Couldn’t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxx导致Master不可用hbase shell执行list命令报ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing跟进Master日志看到Couldn’t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxx，Master状态不可用分析：由于HDFS的原因导致HBase的Snapshot文件丢失，详细看了一下，其他snapshot目录下有.snapshot以及data.manifest，而这个没有操作：删除这个snapshot目录 重启HMaster修复后scan操作又报Unknown table xxx，而且几乎所有表都不能scan操作：首先想到的就是修复HBase元数据hbase hbck -fixMeta修复后继续scan，报错ERROR: No server address listed in hbase:meta for region t1,,1536659773616.09db0b8b3b7f8cd81dde86c9f1e41306. containing row rowkey001: 1 time…分析：查询hbase:meta表scan ‘hbase:meta’,{LIMIT=&gt;10,FILTER=&gt;”PrefixFilter(‘test_ray’)”}发现表元数据中都没有regionserver的信息，正常情况是这样的：解决：重新对region分区：hbase hbck -fixAssignments 最终服务和数据均恢复正常 Kafka相关 ConsumerRebalanceFailedException异常解决报错：kafka.common.ConsumerRebalanceFailedException: group_xxx-1446432618163-2746a209 can’t rebalance after 5 retries分析：官网给出了异常的相关说明和解决方案consumer rebalancing fails (you will see ConsumerRebalanceFailedException): This is due to conflicts when two consumers are trying to own the same topic partition. The log will show you what caused the conflict (search for “conflict in “).If your consumer subscribes to many topics and your ZK server is busy, this could be caused by consumers not having enough time to see a consistent view of all consumers in the same group. If this is the case, try Increasing rebalance.max.retries and rebalance.backoff.ms.Another reason could be that one of the consumers is hard killed. Other consumers during rebalancing won’t realize that consumer is gone after zookeeper.session.timeout.ms time. In the case, make sure that rebalance.max.retries * rebalance.backoff.ms &gt; zookeeper.session.timeout.ms.简单来说就是同一个topic的同一个分区被多个消费者消费，发生冲突解决：①增加相关topic的partition数②提高kafka的consumer如下两项配置rebalance.backoff.ms=2000rebalance.max.retries=10 其他 Zookeeper日志和快照过大解决1：cd $ZOOKEEPER_HOME;bin/zkCleanup.sh {dataDir} 5;bin/zkCleanup.sh {snapshotDir} 5 并设置成定时调度解决2：从3.4.0开始，zookeeper提供了自动清理snapshot和事务日志的功能，通过配置autopurge.snapRetainCount和autopurge.purgeInterval这两个参数能够实现定时清理了。这两个参数都是在zoo.cfg中配置的： autopurge.purgeInterval 这个参数指定了清理频率，单位是小时，需要填写一个1或更大的整数，默认是0，表示不开启自己清理功能。 autopurge.snapRetainCount 这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。原理：Zookeeper不会删除旧的快照和日志文件，zkCleanup.sh可以帮助合理清理当前节点的旧的日志和快照文件解决磁盘空间。其中dataDir是特定服务集合的znode存储的永久副本，对znode的所有更改会附加到log日志中，snapshotDir是永久快照。zkCleanup.sh将保留最后一个快照和相关的log日志，清除其他的快照和日志。注意最后一个参数值要大于等于3，日志损坏的情况下保证有3个以上备份。参考:zookeeperAdmin.html#sc_maintenance","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据平台","slug":"大数据平台","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/"},{"name":"异常分析","slug":"异常分析","permalink":"https://shmily-qjj.top/tags/%E5%BC%82%E5%B8%B8%E5%88%86%E6%9E%90/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Impala-基于内存的高效SQL交互查询引擎","slug":"Impala-基于内存的高效SQL交互查询引擎","date":"2021-01-17T02:18:30.000Z","updated":"2021-12-13T15:17:16.755Z","comments":true,"path":"1ae37d82/","link":"","permalink":"https://shmily-qjj.top/1ae37d82/","excerpt":"","text":"Impala-基于内存的高效SQL交互查询引擎Impala简介&emsp;&emsp;Impala是Cloudera提供的一款高效率的SQL实时查询工具，官方测试性能比Hive快10到100倍，SQL查询性能甚至比SparkSQL还更加高效。Impala是基于Hive的大数据分析查询引擎，直接使用Hive的元数据库，意味着Impala元数据都存储在Hive的元数据库当中。Impala兼容绝大多数HiveQL语法。 Impala基于MPP（Massively Parallel Processing）[大规模并行处理]理念的查询引擎，什么是MPP？ MPP是一种海量数据实时分析架构,MPP理念是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果 特点： ● Shared Nothing架构（每一个节点都是独立的，自给的，在系统中不存在单点竞争，没有共享数据），私有资源 ● 任务分布式并行执行（数据无共享，无IO冲突，无锁资源竞争，计算速度快） ● 数据分布式存储(本地化) ● 横向扩展（易扩容）; ● 单个节点查询效率慢会影响整个查询（倾斜） 附： Shared Everthing：完全透明共享 CPU/Memory/IO，并行处理能力是最差的 Shared Storage：各个处理单元使用自己的私有CPU//Memory，但共享磁盘系统 Shared Nothing：各个处理单元都有自己私有的CPU/Memory/IO 基于SQL的计算引擎对比 引擎 开发语言 执行机制 资源调度 内存分配 容错 场景 Hive(MR) Java SQL-&gt;MR-&gt;Yarn-&gt;HDFS Yarn调度 内存不够则用磁盘 Hadoop容错机制包括重试和推测执行 离线分析和跑批任务 Spark Scala SQL-&gt;计划-&gt;Yarn-&gt;HDFS 支持多种调度 优先使用内存不够则用磁盘，可手动 Lineage+Checkpoint+失败重试 兼容多种场景 Presto Java SQL-&gt;计划-&gt;Workers-&gt;HDFS 自身 纯内存计算 无容错设计 交互式分析查询 ClickHouse C++ SQL-&gt;计划-&gt;读存储引擎数据向量化执行 自身 内存+连续IO 多主机跨数据中心异步复制，不怕节点宕机 存储数据库+OLAP分析的场景 Impala C++ SQL-&gt;计划-&gt;HDFS 自身 纯内存计算 无容错设计 交互式分析查询 Impala优缺点优点： 基于内存计算，低延迟，高吞吐，查询速度快，适用于秒级响应的OLAP交互式分析查询 提供窗口函数（聚合 OVER PARTITION, RANK, LEAD, LAG, NTILE等等）以支持高级分析功能 支持PB级数据量的实时分析 支持map、struct、array类型上的复杂嵌套查询，支持UDF和UDAF 可以使用Impala插入或更新HBase（类似Phoenix） 支持Parquet、Avro、Text、RCFile、SequenceFile、HFile等多种文件格式，支持Snappy（有效平衡压缩率和解压缩速度）、Gzip（最高压缩率的归档数据压缩）、Deflate（不支持文本文件）、Bzip2、LZO（只支持文本文件）等多种压缩编码格式 支持存储在HDFS、HBase、S3、Kudu上的数据操作 与CDH深度整合，支持查看查询任务的各项指标 支持Sentry和Kerberos 局限性： 不适用于跑批 查询时占用大量内存 不支持ORC “AnalysisException: Impala does not support modifying a non-Kudu table” 不支持非Kudu表的Update、delete操作 不支持Date数据类型 Impala原理Impala架构图 Impala Daemon&emsp;&emsp;Impala的核心进程Impalad，部署在所有的数据节点上，接收客户端的查询请求，读写数据，并行执行来自集群中其他节点的查询请求，将中间结果返回给调度节点。调用节点将结果返回给客户端。Impalad进程通过持续与StateStore通信来确认自己所在的节点是否健康以及是否可以接受新的任务请求。 Impalad包含三种角色： Query Coordinator：用户在Impala集群上的某个节点提交数据处理请求（例如impala-shell提交SQL），则该Impalad节点称为Coordinator Node（协调节点）,负责定位数据位置，拆分请求（Fragment），将任务分解为多个可并行执行的小请求，发送这些请求到多个Query Executor，接收Query Executor处理后返回的数据并构建最终结果返回给用户。 Query Planner：Java编写的，解析SQL生成QueryPlanTree执行计划树。 Query Executor：执行数据计算，比如scan，Aggregation，Merge等，返回数据。 Impala StateStore&emsp;&emsp;Statestored进程，状态管理进程（类似ZK），定时检查Impala Daemon的健康状况，协调各个运行Impalad进程之间的信息，Impala通过这些信息去定位查询请求所要的数据，如果Impala节点下线，StateStore会通知其他节点，避免查询任务分发到不可用的节点上。(定位Impala卡住以及各种异常状况，可以先从ImpalaStateStore下手，查看StateStore日志方便定位问题) Impala Catalog Service&emsp;&emsp;Catalogd进程，元数据管理服务，收集Hive等系统的元数据，将数据表变化的信息分发给各个进程。接收来自StateStore的所有请求，每个Impala节点在本地缓存所有元数据。当表创建、数据更新或Schema发生变化时，其他Impala后台进程必须更新元数据缓存，才能查询。 Schema变化时（Hive操作create table/drop table/alter table add columns）使用：invalidate metadata //重新加载所有库中的所有表（不推荐，还不如重启Catalogd进程）invalidate metadata [table] //重新加载指定的某个表 数据变化时（Hive操作insert into、load data、alter table add partition、Alter table drop partition或HDFS增删重命名文件）使用：refresh [table] //刷新某个表refresh [table] partition [partition] //刷新某个表的某个分区 注意：invalidate会清除表的缓存并从MetaStore重新同步元数据，代价较大；refresh会重用之前的元数据，仅仅执行文件刷新操作，它能够检测到表中分区的增加和减少，代价相对小些 Impala join算法1.HashJoin，等值Join采用Hash算法进行Join，具体分为Broadcast Hash Join和Shuffle Hash Join。Boradcast Join适合右表是小表的情景，Impala会广播小表到各个节点，再关联。Shuffle Join适合大表与大表Join的情景，Impala会将大表划分成多块，然后分别进行Hash Join。2.Nested Loop Join，非等值Join使用，非等值Join效率低，不支持Hint。 Impala Query Hint语法： SELECT STRAIGHT_JOIN select_list FROM join_left_hand_table JOIN [&#123; /* +BROADCAST */ | /* +SHUFFLE */ &#125;] join_right_hand_table remainder_of_query; -- -------------------------------------- INSERT insert_clauses [&#123; /* +SHUFFLE */ | /* +NOSHUFFLE */ &#125;] [/* +CLUSTERED */] SELECT remainder_of_query; -- -------------------------------------- SELECT select_list FROM table_ref /* +&#123;SCHEDULE_CACHE_LOCAL | SCHEDULE_DISK_LOCAL | SCHEDULE_REMOTE&#125; [,RANDOM_REPLICA] */ remainder_of_query; Hint会改变SQL的执行计划，使用Hint注意事项： 有两个地方需要加上hint关键字，select后面加上STRAIGHT_JOIN；join后面加上[shuffle]或者/* +shuffle */ 如果是多层嵌套的join方式，也需要在每一层加上STRAIGHT_JOIN和[shuffle]或者/* +shuffle */ 外层的hint对于内层的join子语句是不起作用的 如果select后面跟distinct之类的关键字，STRAIGHT_JOIN需要跟在关键字后面不同Hint标签的具体含义和场景见文档:impala_hints 在CDH使用ImpalaImpala相关进程：注意：考虑集群性能，一般将StateStore与CatalogService放在同一节点上，因之间要做通信 在StateStore的WEBUI http://cdh101:25010/ 可以查看Impala集群监控状态和配置信息： 在Catalog的WEBUI http://cdh101:25020/ 可以看到各个库表元数据信息、Schema及占用内存大小 在Impala Daemon的WEBUI http://cdh102:25000/ 可以看到该进程信息 在http://cdh102:25000/queries 可以查看该节点执行SQL的详情 在CDH Impala组件中可以查看执行SQL任务的详细信息 一些重要的常用的impala-shell使用命令 impala-shell -i host指定Coordinator -d 指定连接到哪个库 -q &quot;select ...&quot; 不进入impala-shell直接查询某语句返回结果到命令行 -f file 执行文件中的sql -p 获取执行计划 -o 保存执行结果到文件 -h 帮助 进入Impala-shell后： shell &lt;shell&gt; 不退出impala-shell执行系统命令 profile 分析Query执行，以便于性能调优（比-p更多的信息） 最佳实践 文件格式推荐parquet，查询效率高 避免碎片文件，注意文件的大小 根据实际的文件大小和个数选择分区的粒度 分区key选择最小的整数类型代替字符串类型，降低元数据占用内存大小 使用COMPUTE STATS命令进行表、分区的性能分析（收集统计信息），提高表的查询效率 COMPUTE STATS [db_name.]table_name COMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)] 最小化返回Client端的数据量 使用explain+SQL命令确认执行计划是否高效 执行查询后使用summary命令确认硬件消耗（物理性能特性），输出的信息包括哪个阶段耗时最多，以及每一阶段估算的内存消耗、行数与实际的差异 执行查询后使用profile命令显示详细性能信息，输出的信息包括内存、CPU、I/O以及网络消耗的详细信息，可根据该信息进行调优 使用profile查看是否有hdfs块倾斜，合理分配block大小 充分利用Impala Query Hint优化查询效率 Join时大表放在最左面；效率最高的Join放在最前面；定期对表收集统计信息, 或者在大量DML操作后主动收集统计信息；单条SQL的Join数尽量不超过4否则效率低下 impalad无法启动EERROR是WebServer: Could not start on address 0.0.0.0:25000 解决： lsof -i :25000 拿到PID并kill这个PID 节点访问负载均衡–之前提到每个Impalad都是Coordinator，所有任务都提交到一台Coordinator会存在单点性能及单点故障问题，所以为提高任务提交吞吐量避免单点问题，可配置负载均衡的地址访问impalaDaemon,参考Using Impala through a Proxy for High Availability官方文档或Configuring Impala Load Balancer博客，配置如下： yum -y install haproxy cp /etc/haproxy/haproxy.cfg /etc/haproxy/impala_haproxy.cfg vim /etc/haproxy/impala_haproxy.cfg 注释掉main frontend which proxys to the backends后面的所有内容 注释掉如下行： timeout http-requests 10s timeout queue 1m timeout http-keep-alive 10s timeout check 10s 修改如下行: timeout connect 5000 timeout client 3600s timeout server 3600s 增加如下行： listen stats :25002 balance mode http stats enable stats auth username:password listen impala :21001 mode tcp option tcplog balance leastconn server impalad_1 impalad1_ip:21000 check server impalad_2 impalad2_ip:21000 check server impalad_3 impalad3_ip:21000 check server impalad_4 impalad4_ip:21000 check server impalad_5 impalad5_ip:21000 check # server alias_name impaladIP:21000 check listen impalajdbc :21051 mode tcp option tcplog balance source server impalad_jdbc_01 impalad1_ip:21050 check server impalad_jdbc_02 impalad2_ip:21050 check server impalad_jdbc_03 impalad3_ip:21050 check server impalad_jdbc_04 impalad4_ip:21050 check server impalad_jdbc_05 impalad5_ip:21050 check # server alias_name impaladIP:21050 check 保存退出wq 运行haproxy –f /etc/haproxy/impala_haproxy.cfg 让代理生效 impala-shell -i haproxy_running_ip:21001 可连接代理 jdbc程序原来连接21050改为连接21051,地址改为haproxy_running_ip 保证haproxy运行稳定要做的灾备工作 1.将代理程序加入开机自动运行：root用户下chmod +x /etc/rc.d/rc.local并向该文件添加/sbin/haproxy –f /etc/haproxy/impala_haproxy.cfg 2.代理程序若因极端情况挂掉，写个自动拉起脚本以保证服务（root@cdh01 crontab */1 * * * * sh /app/impala/auto_impala_haproxy.sh） 总结： Impala是典型的MPP架构实时查询分析引擎，类似的引擎还有ClickHouse Impala非常适合即时报表展示的场景 使用Impala一定要注意元数据缓存问题以及所查询的表文件个数、倾斜问题，否则会严重拖慢效率 多参考以上最佳实践部分 相关链接Impala-3.4 PDF DocumentImpala介绍以及优劣Cloudera Impala WikiImpala Github repositoryImpala架构","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"实时SQL查询引擎","slug":"实时SQL查询引擎","permalink":"https://shmily-qjj.top/tags/%E5%AE%9E%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%BC%95%E6%93%8E/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"我的2020年度总结博客","slug":"我的2020年度总结博客","date":"2021-01-01T00:08:08.000Z","updated":"2021-04-05T05:03:42.533Z","comments":true,"path":"2020Summary/","link":"","permalink":"https://shmily-qjj.top/2020Summary/","excerpt":"","text":"我的2020年度总结2020是很特别的一年，独立完成了毕业设计，真正从大学毕业了，来到了离家很远的深圳工作，而且又认识了一群新的伙(dou)伴(bi)，如果说去年头发是少了几根，那今年就是少了几撮了吧…当然2020还是有些许的遗憾与不足…我的2019年度总结博客中有写到对自己2020年的一些期望，而2020真正做到了哪些，又有哪些没做到呢？(吃瓜群众:好像好多你都没做到吧!!!)不管做到多少，没做到多少，都要感谢2020，让我认清自己，更加成熟。2020有着很多值得回忆和记录的事，而2021又有很多新的期待，让我们一起看看吧。还有，今年佳境终于是有人陪跨年的佳境了，感谢康富同学，陪我一起跨年，做海鲜大餐给我吃！康老板666！套路老板发红包，还是你强！主持人 回顾2020主持人：欢迎来到《回顾2020》节目的现场，佳境同学！你还记得我吗？去年我们在《回顾2019》节目中见过。我：我记得你！主持人：2020你一定吃得很好吧？我：我怀疑你在说我变月半了！主持人：一个词简单总结下2020吧。我：Emmm，用跌宕起伏来形容比较合适。主持人：你说2020是跌宕起伏的一年，那讲一下你都经历了什么吧。我：三月，辞掉了实习工作，想找个更好的机会，但疫情下找工作很不方便，只能通过远程…找工作的过程中发现了自己的薄弱项，也算是好事吧。但裸辞真的还挺难受的，感觉会瞬间陷入迷茫，尤其是疫情之下，更加迷茫。四月，面试，毕业设计，双线程执行，那段时间有压力，但是爸妈给了我很多鼓励，才让我在重压下心情也能得到放松，感谢他们！买了一万多块钱的笔记本电脑，肉疼啊…五月，学习之余，陪爸妈看电视剧，一起出去溜达，去店里帮忙，一起做饭…想到很快要去工作了，陪他们的时间越来越少，虽然在家有时会跟他们吵，发脾气，但离开了又后悔让他们生气。人都是这样矛盾吧。这个月末，毕业设计也拉下了帷幕，我的毕设还被欢哥（毕设导师）评为优秀等级，学校还偷偷地奖励了400块钱，这是我学校工行卡的最后一笔入账，一开始钱看来自哈理工，都懵了，学校何时如此大方，后来才知道是毕设奖励…六月，可能是这一年中最清闲的一个月了，想到要离开家去那么远的深圳，抓紧时间再跟小伙伴们聚一聚，聚，无外乎就是恰饭、电影、KTV、溜达和打游戏，有点小遗憾就是没珍惜这段时间来一次说走就走的毕业旅行，但更大的遗憾是因为疫情，直到毕业也没能回到大学校园，跟那几个狐朋狗友再聚一下，去年曾约好回去后要踩着啤酒箱子喝，你们说要把我堵在宿舍门口喝完再让进去，但最后宿舍是老师收拾的，个人物品也是老师邮寄的，我们谁都没能回去，下次再相聚不知道有多难。20号，坐绿皮火车来深圳，29个小时的硬卧，还好是妈妈送我来的，一路上有陪伴，还有带了爸爸做的烤肉。这段漫长的旅途上，除了静静躺下思考人生和看车窗外的风景，更多的是离家的不舍，要知道我之前的二十二年都是从北方度过的，随着绿皮车咚咚声越来越远的，是家门口那条大凌河，是不眠的夜窗外的雪，是海河路那条破烂的大街，是那一群狐朋狗友…七月，对深圳很陌生，感觉在这边很难找到以前的玩伴了，没想到好几个大学同学和大学校友在这边，可以一起去玩，还参加了哈理工校友聚会活动，而且入职后发现公司里也有老乡，顿时觉得心里踏实多了。刚入职新公司不久，对公司业务和各个技术部门组成都不熟悉，也有些迷茫，好在阿杜和杰哥看好我，我觉得自己挺幸运的。第一次团建在公司附近吃的，也熟悉了新同事。就是没想到后面几次团建都是吃烧烤…哈哈哈！这个月，学习了Kudu并在团队内部分享，对一致性算法Raft有了更深的理解，给对接你部门培训了高效Python代码的方案，统一数据分析平台Linkis分布式部署、调优和解决了些BUG。八月，好像是很平平淡淡的一个月哦，想不起来可以说啥。九月，做了自己之前未尝做过的领域，数据治理和血缘关系，从0到1实现了一个血缘收集项目并上线了，对血缘收集、图数据库有了一定的了解和实践经验，对数据治理也有了一些自己的认识。十月，十一黄金周，请妈妈去三亚玩（三亚是她一直很想去的地方），打卡蜈支洲岛，三亚千古情，玫瑰谷，槟榔谷，天涯海角，大小洞天，在鹿回头看鹿城全貌，吃海鲜，喝最好喝的奶椰汁，还有清补凉，海南四大名粉…太开心了，没见过这么美的景色，而且是带着很重要的人–妈妈一起，妈妈也很开心，只是这一趟旅途中，我明显感觉妈妈真的老了，好想多带爸妈出去旅游，他们为我操劳太多，付出太多，一直在老家那边，没时间出来看看外面的世界，这个明年要搞起来！送妈妈回老家，回去也是坐了绿皮车，也是29小时，只是这趟没人陪了，送她到车站的路上两个人都沉默不语，但送她刷脸进站，看到弱小的背影慢慢消失在茫茫人海，我内心很沉重，我到家，灯黑着，很安静，眼眶就湿润了，妈妈让我看看枕头底下有她留的字条，都是鼓励的话，还是没忍住哭了一下，还好没人看到……部门来了个新总监，华哥，我觉得他人挺好的，给整个部门带来了巨大的改变，他很重视对员工的培养，希望我们能更专注于一些技术，学到一些新东西。华哥也挺器重我的，让我做了一些技术含量比较高的工作，这个月就是做了BadSQL的拦截，对Spark二次开发，对Spark的logicalPlan有了更深的认识；还做了Atlas部署和源码修改，对Atlas的工作原理和SparkListener有了一些了解。虽然这类工作很有挑战性，但可以挑战它不香吗？！十一月，参加了微众银行Dss有奖征文活动，拿到了最受欢迎征文奖，感谢所有投票给我文章的人，可是奖品大礼包到现在还没邮到，坐等。今年的双十一，花了有史以来最多的钱，1.1w，肉疼，买手机占一大笔，抢Mate40Pro抢到心累。转正评级A+，挺开心，感谢杰哥！十二月，又是在工作上有挑战的一个月，又是完全陌生的方向，做数据脱敏，华哥和磊哥指导下，从方案调研，到设计，评审再到实现，每一步都充满坎坷。这个过程让我熟悉了Spark任务执行流程，逻辑计划生成和转换，Parquet读写，RangerHivePlugin原理，设计思路也更加开阔了，虽然实现上有些小漏洞，但后面会一步一步完善的。说到这里，还是要感谢华哥器重，让我明年做数据平台组组长，我感觉我离当组长的能力还差得远，这也是一种激励吧，想担起这个重任就要先刻苦充实自己的知识水平和能力，新的一年给自己加油打气！华哥还给我面试他人的机会，才发现面试有好多学问，而且面试不仅是考察面试者的知识水平和能力，更考验面试官的能力、技巧，面试别人的过程中，遇到优秀的人，会发现自己的不足，自己需要加强的地方太多了。这个月参加了公司信息技术条线的2021工作规划讨论，跟大佬们一起讨论才觉得自己思路和眼界都比较窄。跟老乡干饭去，碰见个户外直播，问谁会唱歌，被老乡推出去了，蹭了主播热度唱了首《告白气球》，想问主播那几分钟掉了多少粉丝？跟同事几个人一起去爬了梧桐山，拍了很好看的照片，一会给大家分享下哈！很伤心的是我们部门唯一东北话贼溜的老乡离职了(我是东北话不行)，希望她后面一帆风顺把。圣诞节玩游戏拿了好多礼物，也是很开心！跨年晚上抢了老板的大红包，很巴适！元旦假期计划了跟张宇和康富去广州三日游，终于朋友圈有得写了，哈哈！主持人：经历过分别，面对过压力，遇到过困难，也受到过鼓励，2020年对你来说果然是跌宕起伏啊！那你觉得2020年你最大的变化是什么呢？我：学会做饭了，还挺好吃的（自我感觉良好，其实不咋的），更加独立了，更重要的是渐渐意识到该找个对象了，哈哈！主持人：会做饭了，确实进步很大啊。有找对象的意识很好，希望2020你能找到对的人吧。还记得去年的对自己的小目标吗，讲讲实现了哪些？我：框架底层原理深入了一点点，代码能力也有提高，写博客也在坚持，只是今年写的文章比较少。交到了几个好朋友。没实现的我也讲一下吧，不熬夜这个没实现，去年的总结，问大家有没有互相监督熬夜的在评论区留言，没人留言，没有监督，明年尽量少熬夜吧，能减少就行！网易云(说错了是网抑云)作品质量比原来强了，当然只出了很少几个作品：《吹梦到西洲》、《有幸》👈点击可以听哈哈哈，今年认为质量最好的翻唱作品就是这首《吹梦到西洲》了。主持人：那生活方面和做事方面呢？我：生活方面，感觉生活质量比实习时候更好了，毕竟挣钱比实习时多了，深圳挣钱深圳花嘛，不过我比较节约，还是攒了一些的。啊，我的心里只有一件事，就是搞钱。我觉得做事情比以前更稳重了，但有时还是会浮躁，已经改善很多了。主持人：此时此刻，你有什么想对朋友和亲人说的？我：对好朋友说，多数人都是匆匆过客，而你却留了下来，即使很久不联系，有事情也能随叫随到，有什么话都可以讲给你，认识你是我的幸运，2021继续有你，谢谢你的陪伴！对爸妈说，虽然离你们很远，过年我还是会回去的，没有团圆的年是不完整的，很快就会相见了！我不在时你们要多注意身体，加强锻炼，明年带你们一起去旅游！2021，我尽量少熬夜哈！不用担心我。主持人：你的朋友和亲人此刻应该也收到了你说的话。你的2021，充满希望和挑战，加油吧，你一定能行！我：谢谢您！主持人：刚才你提到了要给大家分享一些照片是吧？我：嗯，那下面我把2020的精彩瞬间分享给大家！ 展望2021挥手告别2020，2021我来啦！2021也要奥利给呀！ 小目标 数据治理、数据湖、实时计算等方面要掌握，技术要紧跟潮流，不能落伍 提高开发效率，能快速精确定位问题 学习时更专心，不浮躁 减少熬夜，有必要健身 小愿望 家人、朋友和自己都要身体健康！ 能担当起平台组组长的重任 正在看我博客的你每天都开心 小想法第一，今年是变革的一年，不断学习，改变自己，为自己投资，提升自己各方面的能力。第二，还是跟去年一样，有没有互相监督不熬夜的！评论区留言！","categories":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"个人总结","slug":"个人总结","permalink":"https://shmily-qjj.top/tags/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"},{"name":"2020","slug":"2020","permalink":"https://shmily-qjj.top/tags/2020/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"实现基于Spark的数据脱敏","slug":"实现基于Spark的数据脱敏","date":"2020-12-11T14:16:00.000Z","updated":"2021-06-11T13:15:49.451Z","comments":true,"path":"4cf161e5/","link":"","permalink":"https://shmily-qjj.top/4cf161e5/","excerpt":"","text":"实现基于Spark的数据脱敏前言&emsp;&emsp;Spark是当前大数据领域不可替代的重要组件，拥有成熟的生态、强大的性能和广泛的应用场景，但在数据安全越来越重要的今天，Spark在对数据权限的管控能力方面仍然没有进展。&emsp;&emsp;不仅仅是Spark，大多数大数据生态圈中的组件都缺乏对数据安全的管控，于是很多硬件资源较充裕的公司会将数据全量脱敏后分别存放，牺牲存储空间来达到数据脱敏的目的；而有些公司选择Apache Ranger作为权限管控组件，但Ranger(目前版本2.2)在设计上对各个大数据组件版本有着严格的依赖，且暂时不支持对Spark的权限管控。&emsp;&emsp;经过阅读Ranger源码，以及在测试环境试用后，确定了Ranger方案不可行，我决定在Spark基础上二次开发来实现数据脱敏功能。 知识准备 SparkSQL执行过程：SQL-&gt;Parser(Antlr)-&gt;AST-&gt;Catalyst-&gt;UnresolvedLogicalPlan-&gt;Analyzer-&gt;Optimizer-&gt;PhysicalPlan-&gt;执行计算和IO 以上过程直到物理计划都是继承自LogicalPlan，共四种：UnresolvedLogicalPlan: 也叫ParsedLogicalPlan，是根据语法树解析SQL后得到的逻辑计划，没有关联catalog，没有获取底层存储的元数据信息，也就是说SELECT *在这个阶段不会被解析为具体字段（AnalyzedLogicalPlan、OptimizedLogicalPlan、PhysicalPlan可看到具体字段）。AnalyzedLogicalPlan: 结合表的Catalog，绑定元数据，resolve化LogicalPlan，替换掉UnresolvedLogicalPlan，这里会检查表是否存在以及Schema完整性。绑定元数据是否成功主要有两点：1.子节点是否是resolved 2.输入的数据类型是否满足要求，具体可参考类：Expression，Analyzer类。OptimizedLogicalPlan:对AnalyzedLogicalPlan进行优化，有很多RuleExecutor，如谓词下推，Filter裁剪，WholeStageCodegen(大量类型转换和虚函数调用转为即时编译)，RemoveLiteralFromGroupExpressions移除group下的常量，RemoveRepetitionFromGroupExpressions移除重复的group表达式等…PhysicalPlan:将OptimizedLogicalPlan转换为实际执行的步骤，具体可参考SparkPlanner类。 // 拿到四种执行计划的方法 val sql:String = &quot;select * from qjj&quot; // 单独获取UnresolvedLogicalPlan，不用读元数据，效率最高 val unresolvedLogicalPlan:LogicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(sql) // 获取QueryExecution val qe:org.apache.spark.sql.execution.QueryExecution = sqlContext.sparkSession.sql(sql).queryExecution // 通过QueryExecution获取所有计划包括ParsedLogicalPlan，AnalyzedLogicalPlan，OptimizedLogicalPlan和PhysicalPlan val parsedLogicalPlan:LogicalPlan = qe.logical val analyzedLogicalPlan:LogicalPlan = qe.analyzed val optimizedLogicalPlan:LogicalPlan = qe.optimizedPlan val physicalPlan:LogicalPlan = qe.executedPlan val physicalPlan:LogicalPlan = qe.sparkPlan LogicalPlan包含三种子类型UnaryNode,BinaryNode和LeafNode，每种子类型下又有多种子类型，子类型下又包含子类型如：Project,GlobalLimit,LocalLimit,CreateTable,Distinct,SubqueryAlias,InsertIntoTable,Join,Aggregate,Union,Filter等。 Dataset和DataFrame的区别与联系：联系： 1.API统一，使用上没差别 2.DataFrame算是特殊类型的Dataset，是每个元素都为ROW类型的Dataset（DataFrame = Dataset[Row]）区别： 1.Dataset是强类型，编译时检查类型，DataFrame是弱类型，执行时才检查类型 2.Dataset是通过Encoder进行序列化，支持动态的生成代码，直接在bytes的层面进行排序过滤等的操作；而DataFrame是采用可选的java的标准序列化或是kyro进行序列化 Spark的TemporaryView：Spark的四种视图创建方法： ①df.createGlobalTempView(df.createOrReplaceGlobalTempView) 创建全局临时视图，多个SparkSession共享 SparkSQL写法：create global temporary view view_name(col1,col2…) as select (col1,col2…) from table_name; ②df.createTempView(df.createOrReplaceTempView) 创建Session级别的临时视图,多个SparkSession不共享 SparkSQL写法：create temporary view view_name(col1,col2…) as select (col1,col2…) from table_name;通过SQL创建视图时会有几种异常：It is not allowed to define a TEMPORARY view with IF NOT EXISTS. （创建视图不支持if not exists）It is not allowed to add database prefix test for the TEMPORARY view name. （创建视图不支持库名前缀）Not allowed to create a permanent view by referencing a temporary function. （不支持用带临时UDF的逻辑创建永久视图）视图删除：spark.catalog.dropTempView(‘view_name’)spark.catalog.dropGlobalTempView(‘global_view_name’)全局视图调用：spark.sql(“select * from global_temp.view_name”) 需要加global_temp前缀 从一条SQL到ThriftServer上的一个Job，如何生成：（该图引自SparkSQL并行执行多个Job的探索，文章不错，推荐有空看看） 基于SparkThriftServer的数据脱敏工作原理流程流程：原理： 实现细节数据库建表： -- 脱敏规则表 create table desensitization_rules( rule_name varchar(100) not null primary key comment &quot;脱敏规则名称&quot;, rule_type varchar(100) not null comment &quot;类型-可逆、不可逆、加密、解密&quot;, encrypt_column_type varchar(100) not null comment &quot;可加密的敏感数据分类如phone_num、id_card、bank_account、cust_name&quot;, encrypt_udf_name varchar(100) comment &quot;对应脱敏UDF名称&quot;, decrypt_udf_name varchar(100) comment &quot;解密UDF名称&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;规则创建时间&quot; )ENGINE=Innodb comment=&#39;脱敏规则库-用于配置脱敏规则与对应的加密UDF、解密UDF和加密数据类型的映射关系&#39;; -- 脱敏配置表 create table desensitization_conf( db_table varchar(255) comment &quot;库名.表名，为*代表对所有表都生效&quot;, column_name varchar(255) not null comment &quot;单个敏感字段名&quot;, column_type varchar(100) not null comment &quot;敏感字段数据分类&quot;, rule_name varchar(100) not null comment &quot;脱敏规则名称&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;创建时间&quot;, PRIMARY KEY (db_table,column_name) )ENGINE=Innodb comment=&#39;脱敏配置表-具体到字段的脱敏配置，该表决定如何脱敏，未配置的 默认是白名单&#39;; -- 角色权限表 create table desensitization_role_permissions( role varchar(100) not null primary key comment &quot;角色&quot;, authorized_dbs varchar(1000) NOT NULL comment &quot;有查敏感信息权限的库，逗号隔开，all表示全部库&quot;, authorized_tables text NOT NULL comment &quot;有查敏感信息权限的表，逗号隔开-库名.表名，all表示全部表&quot;, authorized_data_type varchar(1000) NOT NULL comment &quot;有权限的敏感数据类型如phone_num、id_card、account_num、cust_name，all表示数据类型&quot;, authorized_columns text NOT NULL comment &quot;有权限的字段名，多个字段逗号隔开，all表示全部字段&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;角色创建时间&quot; )ENGINE=Innodb comment=&#39;角色权限表-用于配置每个角色的权限&#39;; -- 用户权限表 create table desensitization_user_role( user varchar(100) not null primary key comment &quot;用户名&quot;, role varchar(255) not null comment &quot;角色，多个角色逗号隔开&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;添加时间&quot; )ENGINE=Innodb comment=&#39;用户角色映射关系表-默认无查询敏感数据的权限&#39;; -- ----------配置数据---------- insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_num&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_number&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile_phone&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile_no&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_no&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;cust_name&quot;,&quot;cust_name&quot;,&quot;HideUserName&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;bank_card_no&quot;,&quot;bank_account&quot;,&quot;HideBankCardNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;test_pn&quot;,&quot;phone_num&quot;,&quot;phone_num&quot;,&quot;HideMid4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;test_pn&quot;,&quot;phone&quot;,&quot;phone_num&quot;,&quot;HideMid4PhoneNumber&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;admin&quot;,&quot;all&quot;,&quot;all&quot;,&quot;all&quot;,&quot;&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;d_bd&quot;,&quot;d_bd&quot;,&quot;&quot;,&quot;&quot;,&quot;cust_name&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;d_qjj&quot;,&quot;&quot;,&quot;d_bd.test_qjj&quot;,&quot;&quot;,&quot;cust_name&quot;); insert into desensitization_user_role(user,role) values (&quot;admin&quot;,&quot;d_bd,d_qjj&quot;); insert into desensitization_user_role(user,role) values (&quot;bd_admin&quot;,&quot;admin&quot;); 这样设计权限考虑的点： 1.用户权限按角色管理 2.用户可以有多个角色 3.可以方便用户临时申请权限 4.敏感数据的字段可以支持默认脱敏配置 5.可以细粒度地指定某个用户查询某个表某个字段时如何脱敏 6.对数据进行敏感信息分类，方便按敏感数据类型控制权限（表设计了但实际没实现这块） 代码实现SQLAnalyzer类，工具类，主要是匹配SQL中的表以及判断SQL类型，下面列举主要方法 /** * Get all tables in a select sql * @param sql * @return List(table1,table2) or List() */ def getTablesInSelect(sql:String):List[String] = &quot;(?i)(?:from|join)\\\\s+[a-zA-Z0-9_.]+&quot; .r.findAllIn(sql) .map(x =&gt; x.replaceFirst(&quot;(?i)(?:from|join)\\\\s+&quot;, &quot;&quot;)) .toList /** * Determines SQL is a select statement or not. * @param sql * @return boolean */ def isSelectSQL(sql:String):Boolean = !StringUtils.isBlank(sql) &amp;&amp; sql.trim.replaceAll(&quot;\\r|\\n|\\r\\n&quot;,&quot; &quot;).matches(&quot;(?i)\\\\s*select.*&quot;) /** * Determines SQL is a Data transfer statement or not. * @param sql * @return boolean */ def isSelectInsertSQL(sql:String):Boolean = !StringUtils.isBlank(sql) &amp;&amp; sql.trim.replaceAll(&quot;\\r|\\n|\\r\\n&quot;,&quot; &quot;).matches(&quot;(?i)\\\\s*insert.*select.*&quot;) DesensitizationModule类，脱敏模块，实现了：读取并缓存脱敏配置；用户权限聚合，鉴权，创建临时视图并应用脱敏规则，替换SQL生成真正执行的SQL，其中getDesensitizedSQL是这个类提供给外部的方法，返回的是实际执行的SQL，用于替换原来的逻辑计划。loadDesensitizationMeta也是提供给外部的方法，用于加载脱敏配置和用户权限信息。 package org.apache.spark.sql.hive.thriftserver.desensitization import com.smy.exceptions.DesensitizationException import org.apache.spark.internal.Logging import org.apache.spark.sql.hive.thriftserver.xxxxx.getDF // 连接mysql，查询脱敏配置得到dataframe的方法 import org.apache.spark.sql.&#123;SQLContext, SparkSession&#125; import org.apache.spark.sql.hive.thriftserver.desensitization.SQLAnalyzer._ import scala.tools.scalap.scalax.util.StringUtil private[hive] object DesensitizationModule extends Logging&#123; // entities case class DesensitizationConf(columnName: String, columnType: String, ruleName:String) case class Permission(role: String, authorizedDBs: String, authorizedTables:String, authorizedDataTypes:String, authorizedColumns:String) object DesensitizationMeta &#123; var RULE_UDF_MAP: Map[String, String] = _ //desensitization_rules var DBTABLE_DESENSITIZATION_CONF_MAP: Map[String, Set[DesensitizationConf]] = _ //desensitization_conf var USER_PERMISSION_MAP: Map[String, Permission] = _ //desensitization_user_permissions var ROLE_PERMISSION_MAP: Map[String, Permission] = _ //desensitization_role_permissions var DEFAULT_COLUMN_UDF_MAPPING: Map[String,String] = _ &#125; private def getMergedParas(para1: String, para2: String, isRolePara: Boolean=false): String = &#123; if(isRolePara &amp;&amp; (para1.split(&quot;,&quot;).contains(&quot;admin&quot;) || para2.split(&quot;,&quot;).contains(&quot;admin&quot;))) return &quot;admin&quot; if (para1 != null &amp;&amp; para1.nonEmpty &amp;&amp; para2 != null &amp;&amp; para2.nonEmpty) &#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;) || para2.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; s&quot;$para1,$para2&quot; &#125; else if (para1 != null &amp;&amp; para1.nonEmpty) &#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; para1 &#125; else if (para2 != null &amp;&amp; para2.nonEmpty)&#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; para2 &#125;else&#123; &quot;&quot; &#125; &#125; private def permissionReduce(p1: Permission, p2: Permission): Permission = Permission( getMergedParas(p1.role,p2.role,true), getMergedParas(p1.authorizedDBs, p2.authorizedDBs), getMergedParas(p1.authorizedTables, p2.authorizedTables), getMergedParas(p1.authorizedDataTypes, p2.authorizedDataTypes), getMergedParas(p1.authorizedColumns, p2.authorizedColumns)) private def getFullTableName(tableName:String):String = if (tableName.isEmpty || tableName.equals(&quot;*&quot;)) &quot;*&quot; else if(tableName.contains(&quot;.&quot;)) tableName else s&quot;default.$tableName&quot; /** * Load desensitization metadata * @param sqlContext * @return true-&gt;succeed false-&gt;failed */ def loadDesensitizationMeta(sqlContext: SQLContext):Boolean = &#123; logWarning(&quot;Loading desensitization meta.&quot;) try &#123; // load RULE_UDF_MAP DesensitizationMeta.RULE_UDF_MAP = getDF(sqlContext, &quot;select rule_name,encrypt_udf_name from desensitization_rules&quot;) .collect() .map(x =&gt; (x.get(&quot;rule_name&quot;), x.get(&quot;encrypt_udf_name&quot;))).toMap // load DBTABLE_DESENSITIZATION_CONF_MAP val desensitizationConfMap = scala.collection.mutable.Map[String,scala.collection.mutable.MutableList[DesensitizationConf]]() getDF(sqlContext,&quot;select db_table,column_name,column_type,rule_name from desensitization_conf&quot;) .collect() .foreach&#123;x =&gt; val desensitizationConf = DesensitizationConf(x.get(&quot;column_name&quot;), x.get(&quot;column_type&quot;), x.get(&quot;rule_name&quot;)) if(desensitizationConfMap.get(x.get(&quot;db_table&quot;)).orNull==null)&#123; desensitizationConfMap.put(x.get(&quot;db_table&quot;),scala.collection.mutable.MutableList(desensitizationConf)) &#125;else&#123; desensitizationConfMap(x.get(&quot;db_table&quot;)) += desensitizationConf &#125; &#125; DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP = desensitizationConfMap.map(x =&gt; (getFullTableName(x._1),x._2.toSet)).toMap // load USER_PERMISSION_MAP and ROLE_PERMISSION_MAP DesensitizationMeta.ROLE_PERMISSION_MAP = getDF(sqlContext, &quot;select role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns from desensitization_role_permissions&quot;) .collect() .map &#123;x =&gt; (x.get(&quot;role&quot;), Permission(x.get(&quot;role&quot;), x.getOrDefault(&quot;authorized_dbs&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_tables&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_data_type&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_columns&quot;,&quot;&quot;))) &#125;.toMap DesensitizationMeta.USER_PERMISSION_MAP = getDF(sqlContext, &quot;select user,role from desensitization_user_role where user != &#39;&#39; and role != &#39;&#39;&quot;) .collect() .map&#123; x =&gt; (x.get(&quot;user&quot;), x.get(&quot;role&quot;) .split(&quot;,&quot;) .map(role =&gt; DesensitizationMeta.ROLE_PERMISSION_MAP.getOrElse(role,null)) .filter(x =&gt; x != null) .reduce(permissionReduce) ) &#125;.toMap // load DEFAULT_COLUMN_UDF_MAPPING (db_table is * means default global column desensitization configuration.) DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING = DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP.getOrElse(&quot;*&quot;,Set()) .map &#123; ddc =&gt; val udfName = DesensitizationMeta.RULE_UDF_MAP.getOrElse(ddc.ruleName,null) if (udfName == null) &#123; throw new DesensitizationException(s&quot;The mask rule $&#123;ddc.ruleName&#125; on column $&#123;ddc.columnName&#125; is not right or no Configured UDF.Please check desensitization_conf.&quot;) &#125; (ddc.columnName, udfName) &#125;.toMap logWarning(s&quot;## RULE_UDF_MAP ==&gt; $&#123;DesensitizationMeta.RULE_UDF_MAP&#125;&quot;) logWarning(s&quot;## DBTABLE_DESENSITIZATION_CONF_MAP ==&gt; $&#123;DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP&#125;&quot;) logWarning(s&quot;## USER_PERMISSION_MAP ==&gt; $&#123;DesensitizationMeta.USER_PERMISSION_MAP&#125;&quot;) logWarning(s&quot;## DEFAULT_COLUMN_UDF_MAPPING ==&gt; $&#123;DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING&#125;&quot;) true &#125;catch &#123; case e:Exception =&gt; throw new DesensitizationException(&quot;Exception when reloadAuth&quot;,e) false &#125; &#125; /** * create temporary view and get the view name. * The temporary view will clear when spark session exited. * @param spark SQLContext * @param tableName Full tableName with database prefix. * @param userName */ def createAndGetTempViewName(spark:SQLContext,userName:String,tableName:String):String=&#123; val columns = spark.table(tableName).columns val sourceCols = columns.mkString(&quot;,&quot;) val newViewName = tableName.replace(&quot;.&quot;,&quot;_&quot;) + &quot;_&quot; + System.currentTimeMillis() try&#123; val colUDFMap:Map[String,String] = getTableColUDFMapping(tableName, userName, columns) if(colUDFMap.isEmpty) return tableName logWarning(s&quot;### Desensitization table:$tableName user:$userName columnUDFMapping:$colUDFMap&quot;) val viewCols = columns.map &#123; col =&gt; if (!colUDFMap.contains(col)) &#123; col &#125; else &#123; colUDFMap(col) + s&quot;($col)&quot; &#125; &#125;.mkString(&quot;,&quot;) spark.sql(s&quot;create temporary view $newViewName($sourceCols) as select $viewCols from $tableName&quot;) newViewName &#125;catch &#123; case e:Exception =&gt; logError(s&quot;Failed to create a masked temporary view on table $tableName.&quot;,e) // If there is an exception, return the source table name. tableName &#125; &#125; /** * Determine whether the user has access to the table and get the desensitization strategy of table columns. * @param tableName FullTableName with database prefix * @param userName userName * @param tableColumns 表的所有字段 * @return columnUDFMap */ def getTableColUDFMapping(tableName:String,userName:String,tableColumns:Array[String]):Map[String,String] = &#123; logInfo(s&quot;Begin to get table($tableName) user($userName) auth and col-udf mapping.&quot;) // Judge User permission. val userPermission = DesensitizationMeta.USER_PERMISSION_MAP.getOrElse(userName,Permission(&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;)) if(userPermission.role.equals(&quot;admin&quot;))&#123; // user role is admin. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedDBs) || userPermission.authorizedDBs.split(&quot;,&quot;).contains(tableName.split(&quot;\\\\.&quot;)(0))) &#123; //User has permissions for this database. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedTables) || userPermission.authorizedTables.split(&quot;,&quot;).map(getFullTableName).contains(tableName)) &#123; //User has permissions for this table. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedDataTypes))&#123; return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedColumns))&#123; // User has permissions for all fields. return Map() &#125; val result:scala.collection.mutable.Map[String,String] = scala.collection.mutable.Map[String,String]() // Apply specified desensitization conf. // TODO: User has access to one dataType? val authorizedCols:Set[String] = userPermission.authorizedColumns.split(&quot;,&quot;).toSet val unauthorizedCols:Set[String] = DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING.keySet -- authorizedCols val tableSpecificDesensitizationConf:Set[DesensitizationConf] = DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP.getOrElse(tableName,Set()) if(tableSpecificDesensitizationConf.nonEmpty) &#123; logWarning(s&quot;Table $tableName has specific desensitization rules.&quot;) tableSpecificDesensitizationConf.foreach&#123; tsdc =&gt; val udfName = DesensitizationMeta.RULE_UDF_MAP.get(tsdc.ruleName).orNull if (udfName == null) &#123; throw new DesensitizationException(s&quot;The mask rule $&#123;tsdc.ruleName&#125; on table $tableName column $&#123;tsdc.columnName&#125; is not right or no Configured UDF.Please check desensitization_conf.&quot;) &#125; logInfo(s&quot;Apply specific desensitization udf: $&#123;tsdc.columnName&#125; =&gt; $udfName .&quot;) result.put(tsdc.columnName,udfName) &#125; &#125;else&#123; logInfo(s&quot;There is no specific desensitization conf in table $tableName.Use default desensitization conf.&quot;) &#125; // Apply default desensitization conf. unauthorizedCols.foreach&#123; uc =&gt; if(tableColumns.contains(uc) &amp;&amp; !result.contains(uc)) &#123; val udfName = DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING.get(uc).orNull // if (udfName == null) &#123; // throw new DesensitizationException(s&quot;There is no desensitization UDF associated with column $uc ,Please check desensitization_conf where db_table=&#39;*&#39;.&quot;) // &#125; logInfo(s&quot;Apply default desensitization udf: $uc =&gt; $udfName .&quot;) result.put(uc,udfName) &#125; &#125; result.toMap &#125; /** * Main method to generate desensitized sql. * @param spark sparkSession * @param userName request user of this sql * @param sql * @return Desensitized sql */ def getDesensitizedSQL(spark:SQLContext,userName:String,sql:String):String = &#123; if(isSelectSQL(sql))&#123; val tableList = getTablesInSelect(sql) if(tableList.isEmpty)&#123; return sql &#125; var outputSQL:String = sql tableList .foreach &#123; table =&gt; val viewName = createAndGetTempViewName(spark, userName,getFullTableName(table)) logInfo(s&quot;tableName: $table =&gt; viewName: $viewName&quot;) outputSQL = outputSQL.replace(table, viewName) &#125; outputSQL &#125;else if(isSelectInsertSQL(sql))&#123; //TODO: There is a vulnerability in the export of sensitive data. // &#39;insert select&#39; operation and &#39;create table as select&#39; operation. sql &#125;else&#123; sql &#125; &#125; &#125; 寻找修改切入点org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation类，是ThriftServer提供服务的入口，其中我们只需要将执行时的逻辑计划替换掉即可，该类中execute方法中可以找到源码： // Always set the session state classloader to `executionHiveClassLoader` even for sync mode if (!runInBackground) &#123; parentSession.getSessionState.getConf.setClassLoader(executionHiveClassLoader) &#125; sqlContext.sparkContext.setJobGroup(statementId, substitutorStatement, forceCancel) result = sqlContext.sql(statement) logDebug(result.queryExecution.toString()) HiveThriftServer2.eventManager.onStatementParsed(statementId, result.queryExecution.toString()) iter = if (sqlContext.getConf(SQLConf.THRIFTSERVER_INCREMENTAL_COLLECT.key).toBoolean) &#123; new IterableFetchIterator[SparkRow](new Iterable[SparkRow] &#123; override def iterator: Iterator[SparkRow] = result.toLocalIterator.asScala &#125;) &#125; else &#123; new ArrayFetchIterator[SparkRow](result.collect()) &#125; 我们可以明确的是statement是用户提交运行的SQL，后面触发计算操作时调用了result.collect()，则result就是这条SQL的结果集，我们需要修改的就是result这个dataframe对象。修改方法很简单，用我们脱敏后的SQL重新生成ParsedLogicalPlan，再用Dataset.ofRows得到新的Dataset： var logicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(statement) // Desensitization var sqlAfterDesensitization:String = statement try&#123; sqlAfterDesensitization = DesensitizationModule.getDesensitizedSQL(sqlContext, parentSession.getUserName, sql) if(!sqlAfterDesensitization.equals(statement))&#123; logWarning(s&quot;### SQL has changed after Desensitization Module. outputSQL: $sqlAfterDesensitization ###&quot;) logicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(sqlAfterDesensitization) &#125;else&#123; logInfo(s&quot;###经过脱敏模块处理后的SQL为: $statement 未发生改变###&quot;) &#125; &#125;catch &#123; case e:Exception =&gt; logError(&quot;***There may be some errors in DesensitizationModule.getDesensitizedSQL ***&quot;, e) // throw new DesensitizationException(&quot;Desensitization failed.&quot;,e) // Table not found and sql syntax error also throw this. &#125; result=Dataset.ofRows(sqlContext.sparkSession, logicalPlan) 这样在需要脱敏时逻辑计划就可以被替换并执行后续的操作了，返回给用户的数据也是脱敏后的数据。 UDF编写和注册DesensitizationUDFs类，注册脱敏UDF的统一入口，在org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager的OpenSession方法中调用：DesensitizationUDFs.register(ctx,username)，为正在登陆的用户调用注册UDF，保证UDF可用。但如果有用户恶意频繁登陆会触发频繁UDF注册，导致Thriftserver负载高，故可设置免UDF加载的白名单用户参数：–conf “spark.thrift.desensitization.load.udf.user.whitelist=user1,admin” // Register Desensitization UDFs var loadUDFWhitelist:Array[String] = Array() try&#123; // 提交任务时加--conf &quot;spark.thrift.desensitization.load.udf.user.whitelist=user1,admin&quot; 这些用户不加载脱敏UDF loadUDFWhitelist = ctx.sparkSession.conf.get(&quot;spark.thrift.desensitization.load.udf.user.whitelist&quot;).split(&quot;,&quot;) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() //如果没配置该参数 java.util.NoSuchElementException &#125; if(!loadUDFWhitelist.contains(username))&#123; DesensitizationUDFs.register(ctx,username) &#125; UDF类： package org.apache.spark.sql.hive.thriftserver.desensitization import org.apache.commons.lang.StringUtils import org.apache.spark.internal.Logging import org.apache.spark.sql.SQLContext import org.apache.spark.sql.hive.thriftserver.UDFUtil /** * Desensitization UDF * Created by Shmily on 2020/11/23. */ object DesensitizationUDFs extends Serializable with Logging&#123; // private val logger: Logger = LoggerFactory.getLogger(Desensitization.getClass) /** * [Irreversible] Hide the mid 4 digits of mobile phone number * @param phoneNumber * @return Encrypted phoneNumber */ def HideMid4PhoneNumber(phoneNumber:String): String = &#123; if(phoneNumber==null)&#123; return phoneNumber &#125; if(StringUtils.isBlank(phoneNumber))&#123; return &quot;&quot; &#125; phoneNumber.length match &#123; case 11 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*(\\\\w&#123;4&#125;)&quot;, &quot;$1****$2&quot;) case 7 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*&quot;, &quot;$1****&quot;) case _ =&gt; phoneNumber &#125; &#125; /** * [Irreversible] Hide the last 4 digits of mobile phone number * @param phoneNumber * @return Encrypted phoneNumber */ def HideLast4PhoneNumber(phoneNumber:String): String = &#123; if(phoneNumber==null)&#123; return phoneNumber &#125; if(StringUtils.isBlank(phoneNumber))&#123; return &quot;&quot; &#125; phoneNumber.length match &#123; case 11 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;7&#125;)\\\\w*&quot;, &quot;$1****&quot;) case 7 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*&quot;, &quot;$1****&quot;) case _ =&gt; phoneNumber &#125; &#125; /** * [Irreversible] Keep first char only. * @param name * @return Encrypted Name */ def HideUserName(name:String): String = &#123; if(name==null)&#123; return name &#125; if(StringUtils.isBlank(name))&#123; return &quot;&quot; &#125; val length = name.length name.substring(0,1).concat(&quot;*&quot; * (length-1)) &#125; /** * [Irreversible] ID Card keep 1-6 and last 3 digits. * @param id * @return Encrypted id */ def HideIDCard(id:String): String = &#123; if(id==null)&#123; return id &#125; if(StringUtils.isBlank(id))&#123; return &quot;&quot; &#125; id.length match &#123; case 15 =&gt; id.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1******$2&quot;) case 18 =&gt; id.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1*********$2&quot;) case _ =&gt; id &#125; &#125; /** * [Irreversible] ID Card keep 1-6 and last 3 digits. * @param bankCardId * @return Encrypted bankCardId */ def HideBankCardNumber(bankCardId:String): String = &#123; if(bankCardId==null)&#123; return bankCardId &#125; if(StringUtils.isBlank(bankCardId))&#123; return &quot;&quot; &#125; bankCardId.length match &#123; case 16 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1*******$2&quot;) case 17 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1********$2&quot;) case 19 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1**********$2&quot;) case _ =&gt; bankCardId &#125; &#125; /** * register all desensitization udf * @param ctx SQLContext */ def register(ctx:SQLContext,username:String):Unit = &#123; logWarning(s&quot;Registering desensitization UDFs for session [user: $username]&quot;) ctx.udf.register(&quot;hide_mid_4_phone_number&quot;,HideMid4PhoneNumber _) ctx.udf.register(&quot;hide_last_4_phone_number&quot;,HideLast4PhoneNumber _) ctx.udf.register(&quot;hide_user_name&quot;,HideUserName _) ctx.udf.register(&quot;hide_id_card&quot;,HideIDCard _) ctx.udf.register(&quot;hide_bank_card_number&quot;,HideBankCardNumber _) &#125; &#125; 实现启动时加载脱敏配置org.apache.spark.sql.hive.thriftserver.HiveThriftServer2类，含有ThriftServer的main方法，在启动ThriftServer服务时运行，在DeveloperApi注解下的startWithContext方法添加加载脱敏配置信息的方法loadDesensitizationMeta，保证每次启动ThriftServer生效配置。 @DeveloperApi def startWithContext(sqlContext: SQLContext): HiveThriftServer2 = &#123; val executionHive = HiveUtils.newClientForExecution( sqlContext.sparkContext.conf, sqlContext.sessionState.newHadoopConf()) DesensitizationModule.loadDesensitizationMeta(sqlContext) ...... 实现手动刷新脱敏配置org.apache.spark.sql.hive.thriftserver.server.SparkSQLOperationManager类，用于管理Spark的Operation，其中newExecuteStatementOperation是实际执行statement的方法，在这里判断SQL如果为”refresh_desensitization_auth”则调用loadDesensitizationMeta方法刷新脱敏配置信息 override def newExecuteStatementOperation( parentSession: HiveSession, statement: String, confOverlay: JMap[String, String], async: Boolean): ExecuteStatementOperation = synchronized &#123; val sqlContext = sessionToContexts.get(parentSession.getSessionHandle) Authentication.checkIp(parentSession) Authentication.checkUser(parentSession) require(sqlContext != null, s&quot;Session handle: $&#123;parentSession.getSessionHandle&#125; has not been&quot; + s&quot; initialized or had already closed.&quot;) val conf = sqlContext.sessionState.conf val hiveSessionState = parentSession.getSessionState setConfMap(conf, hiveSessionState.getOverriddenConfigurations) setConfMap(conf, hiveSessionState.getHiveVariables) val runInBackground = async &amp;&amp; conf.getConf(HiveUtils.HIVE_THRIFT_SERVER_ASYNC) var sql = statement.toLowerCase.trim var statement2 = statement if (sql.startsWith(&quot;create&quot;) &amp;&amp; sql.indexOf(&quot;options(&quot;) != -1) &#123; //remove &quot;\\n&quot; when creating external hbase table statement2 = statement.replaceAll(&quot;\\n&quot;, &quot; &quot;) &#125; // load desensitization if (&quot;refresh_desensitization_auth&quot;.equals(sql)) &#123; if (DesensitizationModule.loadDesensitizationMeta(sqlContext)) &#123; // 这里可以加限制admin权限的用户才能刷新 statement2 = &quot;select &#39;refresh_desensitization_auth succeed&#39;&quot; &#125; else &#123; statement2 = &quot;select &#39;refresh_desensitization_auth failed&#39;&quot; &#125; &#125; ...... 实现效果 总结优点： 1.无额外的硬件成本开销 2.性能损耗小 3.用户无感知 4.权限设计灵活缺点： 1.用户如果用敏感字段关联，结果不准确 2.如果用户将敏感数据创建临时表，且字段名称非通用敏感字段名称，就没办法脱敏了改进：设置跑批程序，遍历数仓的表，根据数据特征自动发现敏感字段，并自动迭代脱敏配置库 扩展&emsp;&emsp;上图是HiveServer2和SparkThriftServer的架构，可以看出两者架构相近。SparkThriftServer大量复用了HiveServer2的代码。&emsp;&emsp;HiveServer2的架构主要是通过ThriftCLIService监听端口，然后获取请求后委托给CLIService处理。CLIService又一层层的委托，最终交给OperationManager处理。OperationManager会根据请求的类型创建一个Operation的具体实现处理。比如Hive中执行sql的Operation实现是SQLOperation。&emsp;&emsp;Spark Thrift Server做的事情就是实现自己的CLIService——SparkSQLCLIService，接着也实现了SparkSQLSessionManager以及SparkSQLOperationManager。另外还实现了一个处理sql的Operation——SparkExecuteStatementOperation。这样，当Spark Thrift Server启动后，对于sql的执行就会最终交给SparkExecuteStatementOperation了。 基于Spark执行计划自定义Rule的数据脱敏未完待续…。。。！！！？？？","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://shmily-qjj.top/tags/Spark/"},{"name":"数据脱敏","slug":"数据脱敏","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E8%84%B1%E6%95%8F/"},{"name":"二次开发","slug":"二次开发","permalink":"https://shmily-qjj.top/tags/%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据脱敏方案调研","slug":"大数据脱敏方案调研","date":"2020-10-20T05:19:00.000Z","updated":"2021-01-31T09:52:36.515Z","comments":true,"path":"f5da73a2/","link":"","permalink":"https://shmily-qjj.top/f5da73a2/","excerpt":"","text":"大数据脱敏方案调研背景&emsp;&emsp;大数据发展速度飞快，大数据的价值也有目共睹，在大数据技术领域，对于分析性能，实时性等方面都有了很大的突破，但数据安全问题在数据业务建立初期很难被重视，而数据规模壮大后才开始重视，以致大多数企业大数据平台安全管控能力普遍缺失。。现今数据安全问题频发，而且一旦发生就会对公司造成很严重的利益甚至声誉损害。&emsp;&emsp;数据安全引发的问题代价极高，后果严重，而我们又不能保证服务器永远不会被攻击，所以，及时止损才是关键，大数据脱敏正是这关键的一个环节。有了数据脱敏，就可以随时保护用户隐私，防止重要信息泄露，即使服务器被攻击或有内鬼，也不担心敏感数据被带走。看来既要防外贼又要防内鬼，任重道远啊…&emsp;&emsp;所以数据脱敏是大数据处理链路中重要的一环，建立大数据脱敏体系平台迫在眉睫。 数据脱敏定义&emsp;&emsp;数据脱敏(Data Masking),又称数据漂白、数据去隐私化或数据变形。对敏感信息通过脱敏规则进行数据的变形，模糊化，伪装从而实现敏感隐私数据 的可靠保护。数据脱敏后，就可以在开发、测试和其它非生产环境以及外包环境中安全地使用脱敏后的真实数据集。 目标 针对大数据敏感数据信息，设计并落实敏感数据安全解决方案，实现敏感数据的模糊化，确保敏感数据信息安全可靠 通过大数据平台安全方案的建设，填补大数据平台数据安全防护方面的空缺，有效降低大数据安全管控方面的风险 发生数据泄露时风险可控 可管控的数据脱敏平台，结合用户认证和权限管理以及隐私数据级别实现基于审批模式的数据访问 数据分析与数据脱敏是矛盾的，要做到同时兼顾数据安全和数据使用，保证数据安全的同时最大化数据的分析价值 做到数据审计，发生数据泄露时方便快速定位泄露原因 难点 海量存量数据已经形成 主要涉及系统和数仓两个层面，应用多，应用环境复杂 主动发现敏感数据困难 原则 脱敏通常多数情况是不可逆的，但也有要求可以恢复原始数据的场景 脱敏后数据通常应具有原数据的特征，适用于开发和测试环境，而不是无意义的字符串，比如银行卡号前四位表示银行名称脱敏后这四位也保持不变；数据要求高时，可能要做到脱敏后数据与原始数据频率分布一致，字段唯一性等 数据关联关系要保留，业务规则关联性保证，如主键外键数据脱敏后在另一个表仍然能关联到，如账户类数据往往会贯穿主体的所有关系和行为信息需要特别注意保证所有相关主体信息的一致性 所有可能生成敏感数据的非敏感字段同样需要脱敏，要面对根据非敏感字段能推导出敏感信息的场景 脱敏过程自动化可重复，脱敏结果稳定准确，多次脱敏后数据始终一致 数据脱敏流程分为敏感数据发现-&gt;敏感数据梳理-&gt;脱敏方案制定-&gt;脱敏任务执行 敏感数据发现敏感数据发现分为人工和自动两种，一般是以自动为主结合人工辅助。人工可以指定数据脱敏规则、敏感数据特征和不同数据的脱敏策略。自动识别是根据人工指定的敏感数据特征，借助敏感数据信息库和分词系统自动识别敏感信息，相对于人工方式，自动识别可以减少工作量和防止疏漏。敏感数据发现是一个闭环过程，不断优化和完善敏感数据信息库。 敏感数据梳理在敏感数据发现的基础上，梳理敏感数据列，敏感数据关联关系，不同类型数据的不同脱敏方式，保证清晰的脱敏后关联关系。敏感信息字段的名称、敏感级别、字段类型、字段长度、赋值规范等内容在这一过程中明确，用于下面脱敏策略制定的依据。 脱敏方案制定针对不同业务的数据脱敏需求，在已有脱敏算法基础上定制脱敏策略。该步主要通过脱敏策略复用不同脱敏算法实现。 脱敏任务执行安排脱敏任务，定时跑批，并行处理，断点续延，容错… 脱敏算法有几种通用算法，也有要根据业务需求和数据来定制的脱敏算法如k-匿名，L-多样性，数据抑制，数据扰动，差分隐私等… 脱敏规则分为可恢复和不可恢复两种类型的脱敏规则目标：建立数据脱敏规则算法库 替代用伪装数据完全替换源数据中的敏感数据，替代是最常用的数据脱敏方法具体操作： 常数替代（所有敏感数据都替换为唯一的常数值） 查表替代（从中间表中随机或按照特定算法选择数据进行替代，中间表的设计非常关键） 参数化替代（以敏感数据作为输入，通过特定函数形成新的替代数据）具体使用哪种操作取决于效率、业务需求等因素间的平衡特点：能够彻底的脱敏单类数据，但也会使相关字段失去业务含义 随机变换对待脱敏数据通过随机函数调整，是一种常用脱敏方法随机函数逻辑： 数值类型随机增减百分比 日期类型随机增加天数 String类型数字变随机数字，字母变随机字母特点：能保持数据特征和业务含义例子：abc123 -&gt; drh428 混洗对敏感字段数据跨行随机互换来破坏原有数据实现脱敏特点：保证了字段的数据范围，数据特征和业务含义，但牺牲了安全性，有被还原的可能例子：20201024 -&gt; 20180112 加密加密待脱敏的数据，使用方通过不同的密钥来解密得到原始数据，使用较少特点：不保证数据特征和业务含义，存在安全隐患如密钥泄露和加密强度不够，耗费集群算力例子：abc -&gt; TH3Wwi2wif51ga 遮挡对敏感数据用*或x等字符遮挡从而加密数据，是常用脱敏方式特点：保持数据特征格式，脱敏效果好例子：18612346666 -&gt; 186xxxx66xx Hash映射将数据映射为Hash值特点：不能保证数据特征和业务含义，可以将长度不一的数据变为相同长度例子：zwdwf -&gt; 710057965 偏移类似于加盐，对数据增加一个固定的偏移量特点：隐藏数值的部分特征例子：253 -&gt; 1253 截断只保留数据的某几位，其余位截断例子：0421-88888 -&gt; 0421 唯一值映射将数据映射为唯一的一个值，通过映射表找回原有的值 局部混淆前几位不变，后面位置数据混淆 脱敏环境数据脱敏环境细分为生产环境和非生产环境（开发、测试、预发布、外包、数据分析等）根据脱敏环境的具体场景将脱敏分为： 静态数据脱敏SDM(Static Data Masking)一般用在非生产环境，数据是从生产环境经过脱敏后再在非生产环境使用。解决的问题：测试环境开发库需要生产库的数据量和数据格式方便排查问题或进行分析，但又不能将敏感数据放在非生产环境的场景 动态数据脱敏DDM(Dynamic Data Masking)一般同在生产环境，访问敏感数据时进行动态脱敏。解决的问题：生产环境根据不同场景不同用户对敏感数据采用不同脱敏级别和脱敏算法进行脱敏 大数据脱敏技术方案先上一个总体的脑图：数据脱敏脑图XMIND 批量数据脱敏 先将数据同步到数据仓库的一个中间表，然后通过一些自定义的脱敏函数udf进行数据脱敏，然后将未脱敏的中间表删除或通过权限管控起来即可。对于存量数据也是经过UDF处理后得到脱敏数据。手机号脱敏UDF示例: package top.shmily.qjj; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDF; // 上传udf jar到集群 hdfs dfs -put udf-1.0-SNAPSHOT-jar-with-dependencies.jar /tmp/udf_path/ // 修改文件权限 hdfs dfs -chmod -R 777 hdfs:///tmp/udf_path/ // 注册udf函数 create function tmp.pul as &#39;top.shmily.qjj.PhoneUnlookUdf&#39; using jar &#39;hdfs:///tmp/udf_path/udf-1.0-SNAPSHOT-jar-with-dependencies.jar public class PhoneUnlookUdf extends UDF &#123; //重写evaluate方法 public String evaluate(String phone)&#123; if (phone.length() == 11)&#123; String res = phone.substring(0, 3) + &quot;****&quot; + phone.substring(7, phone.length()); return res; &#125; else &#123; return phone; &#125; &#125; &#125; 使用ApacheRanger进行数仓Hive表数据进行脱敏Apache Ranger对Hive数据支持两种脱敏方式：行过滤(Row Filter)和列屏蔽(Column Masking)。它可对Select结果进行行列级别数据脱敏，从而达到对用户屏蔽敏感信息的目的。脱敏更多的是用到Ranger的列屏蔽，可用不同策略对不同列脱敏，列屏蔽支持的策略：Redact策略: 用x屏蔽所有字母字符，用n屏蔽所有数字字符。Partial mask-show last 4 策略: 仅显示最后四个字符,其他用x代替。Partial mask-show first 4 策略: 仅显示前四个字符,其他用x代替。Hash策略: 用值的哈希值替换原值。Nullify策略: 用NULL值替换原值。Unmasked策略: 原样显示。Date-show only year策略: 仅显示日期字符串的年份部分，并将月份和日期默认为01/01。Custom策略: 可使用任何有效Hive UDF(返回与被屏蔽的列中的数据类型相同的数据类型)来自定义策略。但Ranger对系统各个组件依赖较严格，版本有差异都会编译失败，可以参考Ranger的Plugin中权限相关实现类的实现方法，利用Ranger的思想自己编写权限控制和脱敏逻辑。Ranger实现脱敏的方式在RangerWiki有详细说明：Row level filtering and column-masking using Apache Ranger policies in Apache Hive 使用Apache ShardingSphere实现数据脱敏&emsp;&emsp;Apache ShardingSphere是一套开源的分布式数据库中间件解决方案由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（规划中）这3款相互独立，却又能够混合部署配合使用的产品组成。它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。&emsp;&emsp;数据脱敏模块属于ShardingSphere分布式数据治理这一核心功能下的子功能模块。它通过对用户输入的SQL进行解析，并依据用户提供的脱敏配置对SQL进行改写，从而实现对原文数据进行加密，并将原文数据(可选)及密文数据同时存储到底层数据库。在用户查询数据时，它又从数据库中取出密文数据，并对其解密，最终将解密后的原始数据返回给用户。Apache ShardingSphere分布式数据库中间件自动化&amp;透明化了数据脱敏过程，让用户无需关注数据脱敏的实现细节，像使用普通数据那样使用脱敏数据。此外，无论是已在线业务进行脱敏改造，还是新上线业务使用脱敏功能，ShardingSphere都可以提供一套相对完善的解决方案。&emsp;&emsp;具体可参考详细文档和官网。&emsp;&emsp; 修改ThriftServer源码，在ThriftServer端使用Antlr4解析SQL，匹配脱敏规则库后针对敏感字段自动套用UDF并提交执行。（JDBC端修改不知道是不是也可行，感觉ThriftServer源码改起来更简单点）通过修改SparkThriftServer源码实现数据脱敏的方法见我的另一篇博客：实现基于Spark的数据脱敏 实时数据脱敏 实时数据结合历史数据通过中间件进入Flink或Storm程序，这时只有实时数据，不适用基于全量数据的脱敏算法，所以结合历史数据和相应算法进行实时脱敏。 数据安全审计数据安全审计，目前业界常用的就是ELK(Es+LogStash+Kibana)，下面是用于大数据审计的流程 总结 脱敏的过程就是一个在数据安全性和数据可用性之间平衡的过程 选择或设计一种既能满足开发测试外包的要求，又能保证安全性的脱敏算法特别重要 脱敏后数据的关联关系和业务关系不能被破坏 要结合数据、业务需求实际情况制定适合的脱敏规则 参考美团技术团队数据脱敏大数据脱敏方案数据脱敏原理及方法简析手机号码脱敏Apache ShardingSphere数据脱敏全解决方案详解","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"数据脱敏","slug":"数据脱敏","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E8%84%B1%E6%95%8F/"},{"name":"数据安全","slug":"数据安全","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"使用PySpark优化Pandas","slug":"使用PySpark优化Pandas","date":"2020-08-10T03:26:08.000Z","updated":"2020-09-05T02:33:11.404Z","comments":true,"path":"pyspark_pandas/","link":"","permalink":"https://shmily-qjj.top/pyspark_pandas/","excerpt":"","text":"前言&emsp;&emsp;Pandas一直是非常受欢迎的数据分析利器，它基于Numpy，专为解决数据分析任务。因其基于Python，只能单节点单核心运行，所以在大数据分析场景下，瓶颈很明显。PySpark是基于Spark JavaClient的上层接口，可以结合Python语言以及Spark分布式运行的特点，来解决Pandas在大数据下的瓶颈。本篇文章主要对比Pandas API与PySparkAPI，总结一些Pandas应用场景下使用PySpark提高效率的方案。&emsp;&emsp;本篇主要是对比Pandas和PySpark的API使用，但不能对它们众多API做一一对比介绍，所以对于PySpark的更多API使用请参考：**pyspark.sql官方使用文档** 对比 特点 Pandas PySpark 运行方式 单机单核 分布式 并行机制 不支持 支持 数据位置 单机内存 多节点内存和磁盘 大数据支持 差 优 数据处理方式 无懒加载 懒加载+优化无用操作 DataFrame 可变 不可变 基本原则 需要对大量数据进行分析的场景下，在大数据处理的源头必须使用PySpark 数据经过一系列操作、聚合后数据量减少，且迫不得已用Pandas的情况下再使用Pandas(用Pandas处理的数据尽量更少) 如果可以，尽量全程使用PySpark进行分析操作 需要对计算复杂且耗时的Sparkdataframe进行cache避免重算提高效率 尽可能将一段处理逻辑写到一段SQL中，而非得到多个Dataframe然后进行join 数据创建文中所有Spark Dataframe对象简称df,Pandas的Dataframe对象简称pd_df。 Pandas pd_df = pd.read_csv(&#39;/datas/root/csv_data/csv_file.csv&#39;) # 1.读本地csv数据源 pd_df = spark.sql(&quot;select col1,col2 from table&quot;).to_pandas # 2.读Hive数据源 pd_df = spark.sql(&quot;select * from table&quot;).to_pandas # 3.读Hive整个表 # 4.读MySQL表数据 pd_df = pd.read_sql(&#39;select * from table&#39;, con=pymysql.connect(host=&quot;localhost&quot;,user=username,passwd=password,db=database_name,charset=&quot;utf8&quot;)) # 5.从list，set，dict创建dataftame pd_df = pd.DataFrame(&#123;&quot;id&quot;:[1,2,3,4,5],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;,np.nan]&#125;) # 6.读json pd_df = pd.read_json(&#39;/datas/root/csv_data/json_file&#39;) # zeros创建指定shape的带0的ndarray pd_df = np.zeros((5,3), dtype=&#39;int64&#39;) #5 行 3 列 PySpark df = spark.read.option(&#39;inferSchema&#39;,&quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&#39;/data/data_test/csv_file.csv&#39;) # 1.读HDFS上csv数据源 df = spark.read.csv(&quot;file:///a.csv&quot;) # 读本地csv 路径/a.csv df = spark.sql(&quot;select col1,col2 from table&quot;) # 2.读Hive数据源 df = spark.table(&#39;table&#39;) # 3.读Hive整个表 # 4.读MySQL表数据 conf = &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh101:3306/&quot;, &quot;dbtable&quot;: &#39;test.a&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, &#125; df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() # 5.从list，set，dict创建dataftame df = spark.createDataFrame(pd.DataFrame(&#123;&quot;id&quot;:[1,2,3,4,5],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;,None]&#125;)) 或 df = spark.createDataFrame([(1,&#39;qjj&#39;),(2,&#39;zxw&#39;),(3,&#39;zzz&#39;),(4,&#39;abc&#39;)], [&#39;id&#39;, &#39;name&#39;]) # 6.读json文件 df = spark.read.json(&#39;/datas/root/csv_data/json_file&#39;) # 7.从Parquet创建数据 df = spark.read.parquet(&quot;...&quot;) df = spark.read.format(&#39;parquet&#39;).load(&#39;parquet_file&#39;),opt...) # 8.从ORC创建数据 df = spark.read.orc(&#39;...&#39;) # 9.从text创建数据 df = spark.read.text(&#39;...&#39;) # 10.创建指定shape的带0的dataframe df = spark.createDataFrame([[0 for i in range(3)] for i in range(5)]) #5 行 3 列 # 创建数据并指定字段名(Schema) from pyspark.sql.types import * schema = StructType().add(&#39;col1&#39;, StringType(), True).add(&#39;col2&#39;, IntegerType()) # True是否可以为空 df = spark.createDataFrame([(&#39;aaa&#39;, 1),(&#39;bbb&#39;, 2)], schema=schema) 数据结构 Pandasindex索引：自动创建行结构：Series结构，属于Pandas DataFrame列结构：Column结构，属于Pandas DataFrame pd_df[&#39;col&#39;] = 0 # 列添加 pd_df[&#39;col&#39;] = 1 # 列修改 pd_df.rename(columns=&#123;&#39;col&#39;:&#39;new_col&#39;,&#39;xx&#39;:&#39;xxx&#39;&#125;) # 重命名列名 pd_df.columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;] # 重命名列名 pd_df.dtypes # 查看字段和类型 pd_df.drop(columns=[&#39;col&#39;, &#39;name&#39;]) # 删除字段col PySparkindex索引：无行结构：Row对象，属于Spark DataFrame列结构：Column对象，属于Spark DataFrame from pyspark.sql.functions import lit df = df.withColumn(&quot;col&quot;, lit(0)) # 列添加 df = df.withColumn(&quot;col&quot;, lit(1)) # 列修改 df = df.withColumnRenamed(&#39;col&#39;, &#39;new_col&#39;).withColumnRenamed(&#39;col1&#39;, &#39;new_col1&#39;) # 重命名列名 df.dtypes # 查看字段和类型 df.printSchema() # 打印字段和类型-树形 df.drop(&#39;col&#39;, &#39;name&#39;) # 删除字段col 数据显示 Pandas pd.set_option(&#39;max_rows&#39;,1024) # 最多显示1024行不隐藏 pd.set_option(&#39;max_columns&#39;,1024) # 最多显示1024列不隐藏 pd_df或print(pd_df) PySpark df.show() # 打印前20行且每个字段打印不超过20字符 df.show(30) # 打印前30行且每个字段打印不超过20字符 df.show(100,False) # 打印前100行且每个字段打印字符数不限（不隐藏） 数据排序 Pandas pd_df.sort_index(by=&#39;score&#39;, ascending=False) # 按轴（字段score）进行倒序排序 pd_df.sort_index(by=&#39;score&#39;, ascending=False).reset_index() # 按轴（字段score）进行倒序排序,排序后index会乱序，重设index为顺序 pd_df.sort_values(by=&#39;score&#39;) # 在列中按值进行排序 PySpark df.sort(&#39;score&#39;, ascending=False) # 按列（score字段）倒序排序 df.orderBy(&#39;score&#39;) # 按列（score字段）顺序排序 交集并集差集 Pandas pd.merge(pd_df1, pd_df2, on=[&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;]) # 交集 pd.merge(pd_df1,pd_df2,on=[&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;], how=&#39;outer&#39;) # 并集 pd_df1=pd_df1.append(pd_df2);pd_df1=pd_df1.drop_duplicates(subset=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;],keep=False);pd_df1 # 差集 PySpark df = df1.intersect(df2) # 交集 df = df1.union(df2) # 并集 df = df1.subtract(df2) # 差集 数据选择或切片 Pandas # 1.取一列 pd_df.col_name # 2.取多列 pd_df[[&#39;id&#39;,&#39;score&#39;]] # 3.取第一行 pd_df.ix[0] # 4.取前两行 pd_df.head(2) # 5.按条件取数据 pd_df.loc[pd_df.name==&#39;qjj&#39;] # 取pd_df的name字段值为qjj记录 pd_df.loc[pd_df.name==&#39;qjj&#39;, &#39;col&#39;] # 取pd_df的name字段值为qjj的记录中name字段和col字段的值 # 6.数据随机抽样 pd_df.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) # n行数 frac抽取比例 replace=False无放回 ... PySpark # 1.取一列 df.select(&#39;score&#39;).show() # 2.取多列 df.select(&#39;id&#39;,&#39;score&#39;).show() df.select(df[&#39;id&#39;],df[&#39;score&#39;]).show() # 2.取多列 每个值加20 df.select(df[&#39;id&#39;] + 20,df[&#39;score&#39;]).show() # 3.取第一行 df.first() # 4.取前两行 df.head(2) 或 df.take(2) # 5.按条件取数据 df.filter(&quot;name=&#39;qjj&#39;&quot;) # 取df的name字段值为qjj记录 df.filter(&quot;name=&#39;qjj&#39;&quot;).select(&#39;name&#39;, &#39;col&#39;) # 取df的name字段值为qjj的记录中name字段和col字段的值 # 6.数据随机抽样 df=df.sample(withReplacement=False, fraction=0.01) # withReplacement为False抽出数据不放回，fraction为抽取比例范围0-1，seed参数为随机数种子，默认即可 数据过滤 Pandas pd_df[pd_df[&#39;score&#39;]&gt;=60] pd_df[pd_df[&#39;score&#39;]&gt;=60][pd_df[&#39;id&#39;]&gt;=5] pd_df.query(&#39;score &gt;= 60&#39;) PySpark df.filter(&#39;score&gt;=60&#39;) 或 df.where(&#39;score&gt;=60&#39;) df.filter(&#39;score&gt;=60 and id&gt;=5&#39;) 或 df.where(&#39;score&gt;=60 and id&gt;=5&#39;) 数据去重 Pandas pd_df.drop_duplicates(&#39;col&#39;) PySpark df.drop_duplicates() # data中一行元素全部相同时才去除 df.drop_duplicates([&#39;a&#39;,&#39;b&#39;]) # data根据’a&#39;,&#39;b&#39;组合列删除重复项，默认保留第一个出现的值组合（first）。 取唯一值 Pandas pd_df[&#39;col&#39;].unique() PySpark df.select(&#39;col&#39;).distinct().count() 或df.drop_duplicates([&#39;col&#39;]).count() 分组聚合 Pandas pd_df.groupby(&#39;col&#39;).mean() PySpark df.groupBy(&#39;col&#39;).mean().show() df.groupBy(&#39;col&#39;).avg(&#39;score&#39;).show() from pyspark.sql import functions df.groupBy(&#39;col&#39;).agg(functions.avg(&#39;score&#39;), functions.min(&#39;score&#39;), functions.max(&#39;score&#39;)).show() # 使用SQL分组聚合 spark.sql(&quot;select name,first(col) as col,sum(score) from table group by name&quot;).show() 数据计算 Pandaspd_df[&#39;col&#39;].apply(lambda x: round(math.log(7,2),2)) # 计算2为底7的log，精确小数点后2位 pd_df[&#39;col&#39;].apply(lambda x: sum(x)) # 求和 PySparkspark.sql(&quot;select round(log(2,7),2) as r&quot;).show() # 计算2为底7的log，精确小数点后2位 spark.sql(&quot;select sum(col) from df&quot;).show() # 求和 数据统计 Pandas pd_df.count() # 输出每一列的非空行数 pd_df.describe() # 描述某些列的count, mean, std, min, 25%, 50%, 75%, max pd_df[&#39;col&#39;].value_counts() # 统计某列的数据量 PySpark df.count() # 输出总行数 df.describe().show() # 描述某些列的count, mean, stddev, min, max df.select(&#39;col&#39;).filter(&#39;col is null&#39;).count() # 统计某列的数据量 数据合并TODO:待完善测试 Pandas pd.concat([pd_df,pd_df1], axis=0) # 数据横向合并axis=0 纵向合并axis=1 Pandas下有merge方法，支持多列合并 同名列自动添加后缀，对应键仅保留一份副本 pd_df.join() 支持多列合并 pd_df.append() 支持多行合并 # 根据一定计算规则计算得到新增列 PySpark df.withColumn(新列名，df[列名]**2) # 数据简单操作后横向合并 df.union(df1) # 数据纵向合并-自动去除重复数据 df.unionAll(df1) # 数据纵向合并-不去除重复数据 # 可以使用sql实现concat、merge功能 df.join(df1,df.id==df1.id) # inner join df.join(df1,df.id==df1.id, &#39;left&#39;) # left join df.join(df1,df.id==df1.id, &#39;left&#39;) # right join df.join(df1,df.id==df1.id, &#39;outer&#39;) # full outer join 任何一边不存在填充null # 根据UDF计算得到新增列 udf+withColumn+闭包 from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] for i in l: my_udf = udf(lambda x: x.count(i) if x else 0, IntegerType()) df = df.withColumn(&#39;col_&#39; + i, my_udf(&#39;array_type_col&#39;)) 数据修改对应pd.apply(f)方法 即给df的每一列应用函数f Pandas pd_df.apply(f) # 可作用于Series或整个Dataframe，并对每个元素应用函数f pd_df.apply(f, axis=1) # axis=0 表示按列，axis=1 表示按行 pd_df.replace(&#123;1:10, 2:20&#125;) # 将dataframe中值为1的都替换成10,2替换成20 pandas支持替换为不同类型 PySpark df.foreach(f) 或者 df.rdd.foreach(f) # 将df的每一列应用函数f df.foreachPartition(f) 或者 df.rdd.foreachPartition(f) # 将df的每一分区数据应用函数f pd_df.replace(&#123;1:10, 2:20&#125;) # 将dataframe中值为1的都替换成10,2替换成20 spark不支持替换为不同类型 注意：Spark的apply方法会触发全量数据Shuffle，如果数据量过大会有shuffle异常和ExecutorOOM等错误，任务失败概率会增加，而且需要消耗更多计算资源 空值处理 Pandas # 对缺失数据自动添加NaNs pd_df.fillna(1) # fillna函数 将NaN的地方替换为1.0 pd_df.dropna() # dropna函数 将含有NaN的行删除 pd_df[&#39;col&#39;]=np.where(pd.isnull(pd_df[&#39;col&#39;], &quot;unknown&quot;, pd_df[&#39;col&#39;])) # 某个字段出现空时替换为unknown pd_df[&#39;col&#39;]=np.where(pd_df[&#39;col&#39;]==&#39;&#39;, &quot;unknown&quot;, pd_df[&#39;col&#39;]) # 某个字段出现空字符串时替换为unknown pd_df.isna() # 非空值变为False，有空值变为True PySpark 不自动添加NaNs，且不抛出错误 df.na.fill(1).show() # fillna函数 将null的地方替换为1.0 df.na.drop().show() # dropna函数 将含有null值字段的行删除 df.dropna(subset=[&#39;col1&#39;, &#39;col2&#39;]) # 扔掉col1或col2中任一一列包含null的行 df=df.na.fill(subset=&#39;col&#39;, value=&#39;unknown&#39;) # 某个字段出现空时替换为unknown select if(col=&#39;&#39;,&#39;unknown&#39;,col) as col # 某个字段出现空字符串时替换为unknown df.fillna(&#39;True&#39;) # 有空值变为True 还可使用case when或if处理空值 SQL支持 Pandas import pymysql con = pymysql.connect(host=&quot;localhost&quot;, user=&quot;root&quot;, password=&quot;123456&quot;, database=&quot;test&quot;, charset=&#39;utf8&#39;, use_unicode=True) sql_cmd = &quot;SELECT * FROM a&quot; # a是test库下的表名 pd_df = pd.read_sql(sql_cmd, con) PySpark # sql操作 df.registerTempTable(&#39;score_table&#39;) # 将已有数据注册成临时表（关闭SparkSession这个表就会消失） df.createOrReplaceTempView(&#39;score_table&#39;) # 与registerTempTable功能相同，是较新的API df.createOrReplaceGlobalTempView(&#39;score_table&#39;) # 上面两个是创建SparkSession级别的临时表 这个是Application级别的临时表 spark.sql(&quot;desc score_table&quot;).show() spark.sql(&quot;&quot;&quot;select count(1) as count from score_table&quot;&quot;&quot;).show() # UDF高级功能函数注册操作 from pyspark.sql.types import StringType # 引入返回值类型 spark.udf.register(&quot;get_length&quot;, lambda x: len(x), StringType()) # 注册UDF函数 spark.sql(&quot;select get_length(&#39;name&#39;) from score_table&quot;).show() # 使用UDF函数 # 对特征进行操作 df.selectExpr(&quot;a*2+b as a&quot;,&quot;b*3 as b&quot;) # a字段值改为原始值*2加b字段值 可以有多个运算操作 df = df.selectExpr(&quot;*&quot;,&quot;b*3 as b_3&quot;) # 原始字段不变，新增b_3字段值为b字段*3 互相转换 Pandas df = spark.createDataFrame(pandas_df) # Pandas转Spark df df = spark.createDataFrame(pandas_df[[&#39;col1&#39;, &#39;col2&#39;]]) # Pandas某几个字段的df转Spark df PySpark pandas_df = spark_df.toPandas() # Spark转Pandas df pandas_df = spark_df.select(&#39;col1&#39;, &#39;col2&#39;).toPandas() # Spark某几个字段的df转Pandas df 注：Spark转Pandas df会将Spark df全部数据拉到Driver端单机单节点运行，性能差且网络IO占用高，尽量避免将大量数据转成Pandas DataFrame。 透视表透视表与逆透视表：透视Pivot：按不需要转换的字段分组（groupBy） -&gt; pivot函数进行透视，可选第二个参数指定输出字段数据项 -&gt; 聚合汇总数据项得到结果逆透视unpivot：列形式且无重复值的数据转成行形式且有重复值得数据 Pandas l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] for tag in l: pivot_table = pd.pivot_table(pd_df, index=[&#39;col1&#39;, &#39;col2&#39;], values=&#39;list_type_col&#39;, aggfunc=lambda x: sum(tag==j for i in x for j in i)) # 统计数组值等于tag计数True个数 pivot_table.columns=[tag] PySpark # 注意：pivot只能跟在groupBy之后 l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] pivot_table = df.selectExpr(&#39;*&#39;, &#39;explode(list_type_col)&#39;, &#39;1 as tmp&#39;).groupBy(&#39;col1&#39;, &#39;col2&#39;).pivot(&quot;list_type_col&quot;, l).sum(&quot;tmp&quot;).fillna(0) # 注意：不指定pivot的第二个参数所需字段会降低效率 # 相关逻辑可以直接使用spark sql编写 diff操作 Pandas pd_df.diff() # diff函数是用来将数据进行某种移动之后与原数据进行比较得出的差异数据 PySpark 没有diff操作（Spark的上下行是相互独立，分布式存储的） 数据保存pd_df.to_csv(&quot;/data/path_to_file&quot;) # 写本地csv文件 PySparkdf.write.csv(&quot;file:///data/path&quot;) # 数据写本地csv，可能写多个文件 df.coalesce(1).write.csv(&quot;file:///data/path&quot;) # 数据写本地，写1个csv文件 df.coalesce(1).mode(&quot;overwrite&quot;).option(header=True).csv(&#39;/data/hdfs_path&#39;,sep=&#39;\\t&#39;) # 写一个csv文件到hdfs，带header，默认覆盖，分隔符为\\t df.write.insertInto(&#39;exist_hive_table&#39;) # 追加写数据到已存在的hive表 字段与df中字段名称顺序类型要对应 df.write.insertInto(&#39;exist_hive_table&#39;, overwrite=True) # 覆盖写数据到已存在的hive表 字段与df中字段名称顺序类型要对应 df.write.jdbc(url=&quot;jdbc:mysql://xxx.xxx.xxx.xxx:3306/db_name&quot;, table=&quot;table_name&quot;, mode=&quot;overwrite&quot;, properties=&#123;&quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;123456&quot;&#125;) # 将数据overwrite到mysql 注意数据量不能太大且并行度不能太高，可能会把mysql搞垮，建议并行度不超过10==&gt;NumExecutors*ExecutorCores &lt;= 10 写表时观察mysql端的负载和压力:show status;和show processlist; df.write.saveAsTable(“hive_table”, mode=”append”) # 直接写数据到hive表 无论表是否已经存在都可以 还有options，partitionBy，format等参数影响表结构df.write.format(‘parquet’).bucketBy(100,’year’,’month’).sortBy(‘day’).mode(‘overwrite’).saveAsTable(‘sorted_bucketed_table’) # 数据排序分区存储成parquetdf.coalesce(1).write.save(path,format,mode,partitionBy,**Options) # 存储数据df.coalesce(1).write.json(“file:///data/path”,mode=’overwrite’,) # 写数据到单个json文件 注：文件写到hdfs也不要紧，可以通过挂载NFS或者FUSE等方式将hdfs目录挂载到本地，同样方便后续处理 ## 高级用法（优化） * PySpark连续编写转换函数 ```python spark.table(&#39;ods_test.test&#39;).filter(&#39;age=22&#39;).where(&#39;dt=&quot;20200524&quot;&#39;).groupBy(&#39;id&#39;).avg(&#39;age&#39;).registerTempTable(&#39;tmp&#39;) for i in spark.sql(&quot;select id,&#39;avg(age)&#39; as avg_age from tmp&quot;).collect(): print(i[0], i[1]) 读取MySQL大表优化partitionColumn：分区字段，需要是数值类的（partitionColumn must be a numeric column from the table in question.），经测试，除整型外，float、double、decimal都是可以的lowerBound：下界，必须为整数，不能大于upperBound否则报错upperBound：上界，必须为整数，与lowerBound一起确定分区数据量步长，lowerBound和upperBound并不会过滤数据。numPartitions：最大分区数量，必须为整数，当为0或负整数时，实际的分区数为1；并不一定是最终的分区数量，例如“upperBound - lowerBound&lt; numPartitions”时，实际的分区数量是“upperBound - lowerBound”；以上四个参数必须同时制定否则报错。在分区结果中，分区是连续的，虽然查看每条记录的分区，不是顺序的，但是将rdd保存为文件后，可以看出是顺序的。 conf = &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh102:3306/&quot;, &quot;dbtable&quot;: &#39;db_users.tb_user_records&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, &quot;partitionColumn&quot;: &quot;duration&quot;, # 这个字段为int类型 &quot;lowerBound&quot;: &quot;0&quot;, &quot;upperBound&quot;: &quot;10000&quot;, &quot;numPartitions&quot;: &quot;5&quot; &#125; df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() df1.rdd.getNumPartitions() # 会得到5个分区 该操作的目的是增加并行JDBC连接数，增加读取速度以及增加DataFrame的分区数从而增加计算的并发度。并发度即为Spark的Task数，这个数量一般根据总core数（executor_coresnum_executors）来计算：Task数≈总core数（2~3倍）如果数据量较少，则不需要以这种方式读取，否则可能降低效率伪代码，帮助理解原理： # 情况一： if partitionColumn || lowerBound || upperBound || numPartitions 有任意选项未指定，报错 # 情况二： if numPartitions == 1 忽略这些选项，直接读取，返回一个分区 # 情况三： if numPartitions &gt; 1 &amp;&amp; lowerBound &gt; upperBound 报错 # 情况四： numPartitions = min(upperBound - lowerBound, numPartitions) if numPartitions == 1 同情况二 else 返回numPartitions个分区 delta = (upperBound - lowerBound) / numPartitions 分区1数据条件：partitionColumn &lt;= lowerBound + delta || partitionColumn is null 分区2数据条件：partitionColumn &gt; lowerBound + delta &amp;&amp; partitionColumn &lt;= lowerBound + 2 * delta ... 最后分区数据条件：partitionColumn &gt; lowerBound + n*delta 也就是说，需要合理设置numPartitions和upperBound和upperBound的值，避免某个分区数据量过大。尽量使用范围基本确定且分区字段值分布相对均匀的Int类型字段做分区字段。 多个UDF作用于同一列数据Demo:multi_udf_one_col.py 其他Python三方库：SparklingPandasSparklingPandas 参考PySpark.sql modulepandas与pyspark对比Spark：使用partitionColumn选项读取数据库原理PySpark-DataFrame操作指南","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://shmily-qjj.top/tags/pandas/"},{"name":"pyspark","slug":"pyspark","permalink":"https://shmily-qjj.top/tags/pyspark/"},{"name":"优化","slug":"优化","permalink":"https://shmily-qjj.top/tags/%E4%BC%98%E5%8C%96/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"高效运行Python方案","slug":"高效运行Python方案","date":"2020-07-26T03:16:00.000Z","updated":"2020-07-31T16:39:36.935Z","comments":true,"path":"2ed52290/","link":"","permalink":"https://shmily-qjj.top/2ed52290/","excerpt":"","text":"高效运行Python&emsp;&emsp;Python以其简洁的语法，丰富的三方库，强大的功能而受到越来越多人的欢迎，但没有十全十美的编程语言，Python的运行效率一直被人们诟病。在一些场景下，我们希望Python也能够高效率运行，充分利用系统资源，所以这篇文章记录一些加快Python程序运行效率的方法，让我们的Python更高效！ 加速已有代码&emsp;&emsp;这部分介绍的方案主要针对已有Python代码在不想做太大改动的情况下的优化方案。 使用numba加速numba官方网站 优点： 无学习成本，只加一行代码（高级用法和调优除外） 动态编译，直接翻译机器码，不走Python虚拟机，性能达到C语言水平 支持GPU加速 兼容常用的科学计算库 局限： 我测试时有些场景会报WARN，需要调一下参数，也可能环境原因 对部分第三方库有兼容性问题 测试： pip install numba 扩展： from numba import jit @jit 在方法前加装饰器-常用做法，object模式：默认nopython模式，但如果遇到不兼容的第三方库会退化成python模式，保证能运行但不能提速。 @jit(nopython=True,fastmath=True) 牺牲一点数学精度来提高速度（默认精度高） @jit(nopython=True,parallel=True) 自动进行并行计算 原理：numba加速Python代码的原理是使用jit即时编译直接将Python代码翻译成机器码（上图左侧流程），避免了编译成Python字节码pyc再走Python虚拟机（上图右侧流程），直接提高了运行效率。 结论：从上面的测试结果可以看到有将近300倍的效率提升，能大幅加速Python脚本的执行效率，对大量数据友好，对循环友好。我测试即使在低端处理器环境运行，也能有100+倍的性能提升。这个方案提速效果相当明显，而且对原有代码和环境改动很小，推荐哦！ 使用modin加速pandasmodin官方网站pandas是很常用的数据分析库，功能强大，但它有个缺点就是对大数据的支持并不好，不适合大规模数据。优点： 无学习成本，只改一行代码 可以分布式跑，基于ray 支持GPU加速 局限性： 目前支持93%的Pandas API 分布式运行功能为为实验性功能 随着运行核心数增加，会占用更多内存 安装时可能会更改原有pandas版本，需留意 需要安装ray或dask依赖包，还有一些其他依赖包 测试： def pandas_test(): import pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x: x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x: 1 if x.a%2==0 else 0, axis=1) print(&#39;pandas_df.apply Time: {:5.2f}s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg({&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]}) print(&#39;pandas_df.groupby Time: {:5.2f}s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;pandas_df.read_csv Time: {:5.2f}s&#39;.format(time() - start)) def modin_pandas_test(): import modin.pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x:x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x:1 if x.a%2==0 else 0, axis=1) print(&#39;modin_pandas_df.apply Time: {:5.2f}s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg({&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]}) print(&#39;modin_pandas_df.groupby Time: {:5.2f}s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;modin_pandas_df.read_csv Time: {:5.2f}s&#39;.format(time() - start)) if __name__ == &#39;__main__&#39;: pandas_test() modin_pandas_test() 单机跑Apply API速度大概快了3.5倍多。分布式还没测试。 结论：使用modin模块的pandas代替普通的pandas，本质是将单机单核跑的任务负载分散到多核心甚至多机器来加速运算。基本可以满足使用pandas的业务需求场景，而且核心数越多，机器数越多，运行效率提升越高，但相应需要更大的内存。适合对大量数据操作的场景。此外，pandas官网给出了一些优化效率的建议，参考：Enhancing performance 使用pandarallel加速pandaspandarallel官方网站优点： 无学习成本，只添加1-2行代码 充分利用CPU 局限性： 理论上只提速物理核心数倍的效率。 有使用成本（实现新进程，通过共享内存发送数据等等），因此只有计算量足够高时，才更有效。 使用：pandarallel-example 结论：对于非常少量的数据，不值得使用。对大量数据，可以尝试该方案，不会像modin一样依赖pandas版本，可以在原有pandas版本上操作。 编写高效代码&emsp;&emsp;除了上面已经提到的方案，在我们平时编码时也要注意编码效率，这部分主要介绍编写Python代码时一些提高运行效率的方法、技巧和工具。 使用PySpark优点： 使用Pyspark的dataframe进行数据操作数据分析简单高效，有较低的学习成本。 只需要一行代码即可实现pyspark dataframe和pandas dataframe互相转换。 Pyspark dataframe可以直接registerTempTable，然后可以很容易地使用pyspark.sql对这个表做sql分析。 分布式运行，分析效率效率高，对大量数据很友好。 功能强大，支持udf。 局限： 写代码要注意，避免小文件，减少driverResultSet（注意尽量避免让driver单点运算全部数据） 需要更多内存做计算 使用： # 例如以前的pandas分析作业，可以移植到pyspark # ①pandas dataframe转pyspark dataframe： df = spark.createDataFrame(pandas_dataframe) # ②pyspark dataframe转pandas dataframe: pandas_dataframe = spark_dataframe.toPandas() # ③代码中将spark dataframe注册成临时表（随sparkSession销毁，不占空间） df.registerTempTable(‘tmp’) # ④对数据做SQL分析 df = spark.sql(“””select * from tmp limit 10”””) 结果为新的dataframe # ⑤结果输出 df.show() / df.writeInsertInto(table_name) / df.write.option(‘header’,True),csv(file) # …… 很多种输出方式，也可以继续转回pandas dataframe做后续操作 PySpark使用文档 结论：在数据量特别大的情况下，分布式计算是首选，所以对于大规模数据分析，目前PySpark是比较推荐的方式。 使用DaskDask官方网站优点： 高效处理大量数据 支持分布式局限： 只有来自pandas的某些功能才能移植到Dask上执行 仅在不适合主存储器的数据集上，才建议使用Dask 示例： # 低速： import numpy as np import pandas as pd df = pd.Dataframe(np.random.randint(0, 6, size=(100000000, 5)), columns = list(&#39;abcde&#39;) df.groupby(&#39;a&#39;).mean() # 高速： import dask.dataframe as dd df_dask = dd.from_pandas(df, npartitions=50) df_dask.groupby(&#39;a&#39;).mean().compute() 详细了解Dask 使用多线程优点：能提高IO密集型Python程序效率。因为在一个线程因IO阻塞等待时，CPU切换到其他线程，CPU利用率高。局限：由于GIL(Global Interpreter Lock)机制限制Python解释器任何时刻都只能执行一个线程，在计算密集型Python程序并不能提高执行效率，反而可能因线程切换降低效率。使用： # 用法1 import threading import time class myThread(threading.Thread): def __init__(self,threadID,name,counter): threading.Thread.__init__(self) self.threadId = threadID self.name = name self.counter = counter def run(self): # 线程创建执行run函数 while self.counter &lt; 8: time.sleep(2) self.counter += 1 print(self.threadId,self.name,self.counter,time.ctime(time.time())) print(&quot;Thread Stop&quot;) thread1 = myThread(1, &quot;Thread-1&quot;, 1) thread2 = myThread(2, &quot;Thread-2&quot;, 2) thread1.start() thread2.start() # 用法2 import threading from queue import Queue import time def testThread(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(5): t = threading.Thread(target=testThread, arg=(i, )) t.start() GIL：GIL是CPython解释器引入的锁，GIL在解释器层面阻止了真正的并行运行。解释器在执行任何线程之前，必须等待当前正在运行的线程释放GIL，事实上，解释器会强迫想要运行的线程必须拿到GIL才能访问解释器的任何资源，例如栈或Python对象等，这也正是GIL的目的，为了阻止不同的线程并发访问Python对象。这样GIL可以保护解释器的内存，让垃圾回收工作正常，不会出现运行死锁。但事实上，这却造成了程序员无法通过并行执行多线程来提高程序的性能。如果我们去掉GIL，就可以实现真正的并行。GIL并没有影响多处理器并行的线程，只是限制了一个解释器只能有一个线程在运行。结论：IO包括磁盘IO和网络IO，所以可以在磁盘IO密集型Python任务或网络延迟是瓶颈的Python任务中使用Python多线程。 使用多进程优点：可以提高计算密集型Python程序执行效率。会用到多个CPU核心。绕过GIL机制，充分利用CPU。核心原理是以子进程的形式，平行的运行多个python解释器，从而令python程序可以利用多核CPU来提升执行速度。由于子进程与主解释器相分离，所以他们的全局解释器锁也是相互独立的。每个子进程都能够完整使用一个CPU内核。 局限： 进程间进行数据的交互会产生额外的I/O开销。 整个内存空间被复制到每个子进程中，这样对于比较复杂的程序造成的额外开销也很大。 使用： # 用法1 import multiprocessing def method(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() # 用法2 from multiprocessing.pool import ThreadPool # 可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。 # 如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 def my_print(item): print(item[0]+item[1]) pool_size = 10 # 进程池大小 items = [(1,2),(2,3),(3,4),(4,5)] pool = ThreadPool(pool_size) # 创建一个进程池 pool.map(my_print, items) # 往进程池中填进程 pool.close() # 关闭进程池，不再接受进程 pool.join() # 等待子进程结束以后再继续往下运行，通常用于进程间的同步 等待进程池中进程全部执行完 # 共享内存-共享变量 import multiprocessing from ctypes import c_char_p import time int_val = multiprocessing.Value(&#39;i&#39;, 0) # int类型共享变量 s = (c_char_p, &#39;str&#39;) # str类型共享变量 def method(num): for i in range(10): time.sleep(0.1) with int_val.get_lock(): # 仍然需要使用 get_lock 方法来获取锁对象 int_val.value += num print(int_val.value) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() 结论：如果Python程序瓶颈在CPU数量或是CPU密集型，都可采用多进程。 使用Cython优点： Python代码可通过一定工具转Cython代码 性能达到C语言水平 局限： 需要修改转换工具 高级用法学习成本高 使用：学习Cython：cython-book pip install cython 使用concurrent.futures介绍：对threading和multiprocessing进一步封装的包，方便实现线程池和进程池。使用： # 线程池 import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ThreadPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) # 进程池 import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ProcessPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) 扩展： 在两个CPU核心的机器上运行多进程程序，比其他两个版本都快。 这是因为，ProcessPoolExecutor类会利用multiprocessing模块所提供的底层机制，完成下列操作： 1. 把numbers列表中的每一项输入数据都传给map。 2. 用pickle模块对数据进行序列化，将其变成二进制形式。 3. 通过本地套接字，将序列化之后的数据从煮解释器所在的进程，发送到子解释器所在的进程。 4. 在子进程中，用pickle对二进制数据进行反序列化，将其还原成python对象。 5. 引入包含gcd函数的python模块。 6. 各个子进程并行的对各自的输入数据进行计算。 7. 对运行的结果进行序列化操作，将其转变成字节。 8. 将这些字节通过socket复制到主进程之中。 9. 主进程对这些字节执行反序列化操作，将其还原成python对象 10.最后，把每个子进程所求出的计算结果合并到一份列表之中，并返回给调用者。 multiprocessing开销比较大，原因就在于：主进程和子进程之间通信，必须进行序列化和反序列化的操作。 详细参考：python concurrent.futures 常见代码优化 在set中查找比在list查找快 list_data = list(data) set_data = set(data) # 低速： 789 in list_data # 高速： 789 in set_data 用dict而非两个list进行匹配查找 # 已知list_a,list_b # 低速： list_b[list_a.index(123)] # 高速： dict(zip(list_a,list_b)).get(123,None) 优先用for循环，比while略快 在循环体中避免重复计算 用循环机制代替递归函数 # 低速： def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # 高速： def fib(n): if n in (1,2): return 1 a, b = 1, 1 for i in range(2,n): a,b = b, a+b return b 使用缓存机制加速递归函数 # 低速： def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # 高速： from functools import lru_cache @lru_cache(100) def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) 使用collections.Counter加速计数 import time data = [x**2 % 1989 for x in range(2000000)] # 低速 st = time.time() values_count = {} for i in data: i_cnt = values_count.get(i, 0) values_count[i] = i_cnt + 1 print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) # 高速 st = time.time() from collections import Counter values_count = Counter(data) print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) 使用collections.ChainMap加速字典合并 # 低速 dict_a = {i: i + 1 for i in range(1, 1000000, 2)} dict_b = {i: i * 2 + 1 for i in range(1, 1000000, 3)} dict_c = {i: i * 3 + 1 for i in range(1, 1000000, 5)} dict_d = {i: i * 4 + 1 for i in range(1, 1000000, 7)} result = dict_a.copy() result.update(dict_b) result.update(dict_c) result.update(dict_d) print(result.get(9999)) # 高速 from collections import ChainMap chain = ChainMap(dict_a, dict_b, dict_c, dict_d) print(chain.get(9999)) 使用map代替推导式进行加速 a = [x**2 for x in range(1, 1000000, 3)] # 低速 a = map(lambda x: x**2, range(1, 1000000, 3)) # 高速 使用filter代替推导式进行加速 a = [x for x in range(1, 1000000, 3) if x % 7 == 0] # 低速 a = filter(lambda x: x % 7 == 0, range(1, 1000000, 3)) # 高速 numpy向量化加速-使用np.array代替list集合 a = range(1, 1000000, 3) b = range(1, 1000000, -3) c = [3 * a[i] - 2 * b[i] for i in range(0, len(a)] # 低速 import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.arange(1, 1000000, -3) array_c = 3 * array_a - 2 * array_b # 高速 使用np.ufunc代替math.func # 低速 import math a = range(1, 1000000, 3) b = [math.log(x) for x in a] # 高速 import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.log(array_a) pandas df.to_excel效率低于df.to_csv 查看Python性能日志使用profilerpython中的profiler可以帮助我们测量程序执行过程中详细的时间和空间复杂度。使用时通过-o参数传入可选输出文件以保留性能日志。 python -m cProfile [-o output_file] my_python_file.py 使用profile导入profile监控python程序整体执行耗时。 import profile profile.run(&#39;main()&#39;) 使用line_profiler监控方法耗时。 # pip install line_profiler def a(): pass def main(): a() from line_profiler import LineProfiler lp = LineProfiler(a,main) lp.run(&#39;main()&#39;) lp.print_stats() 在ipython中获取代码耗时%time code 获取执行code这一行代码的耗时 %%time 获取耗时 %%timeit -n 10 获取执行10次的平均耗时 %prun method() 获取执行method方法的耗时详情，输出与profiler一样","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://shmily-qjj.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Apache Kudu总结","slug":"Apache Kudu总结","date":"2020-07-05T04:26:08.000Z","updated":"2021-12-13T15:17:16.753Z","comments":true,"path":"5f26355/","link":"","permalink":"https://shmily-qjj.top/5f26355/","excerpt":"","text":"Apache Kudu前言&emsp;&emsp;在Kudu出现前，由于传统存储系统的局限性，对于数据的快速输入和分析还没有一个完美的解决方案，要么以缓慢的数据输入为代价实现快速分析，要么以缓慢的分析为代价实现数据快速输入。随着快速输入和分析场景越来越多，传统存储层的局限性越来越明显，Kudu应运而生，它的定位介于HDFS和HBase之间，将低延迟随机访问，逐行插入、更新和快速分析扫描融合到一个存储层中，是一个既支持随机读写又支持OLAP分析的存储引擎。本篇文章研究一下Kudu，对其应用场景，架构原理及基本使用做一个总结。 Kudu介绍 在Kudu出现前，无法对实时变化的数据做快速分析： 以上设计方案的缺陷： 1.数据存储多份造成冗余，存储资源浪费。 2.架构复杂，运维成本高，排查问题困难。 而Kudu就融合了动态数据与静态数据的处理，同时支持随机读写和OLAP分析。 Kudu与HDFS,HBase的对比： 适用场景 既有随机读写随机访问，又有批量扫描分析的场景(OLAP) HTAP（Hybrid Transactional Analytical Processing）混合事务分析处理场景 要求分析结果实时性高（如实时决策，实时更新）的场景 实时数仓 支持数据逐行插入、更新操作 同时高效运行顺序读写和随机读写任务的场景 Kudu作为持久层与Impala紧密集成的场景 解决HBase(Phoenix)大批量数据SQL分析性能不佳的场景 跨大量历史数据的查询分析场景（Time-series场景） 特点及缺点 特点 基于列式存储 快速顺序读写 使用 LSM树 以支持高效随机读写 查询性能和耗时较稳定 不依赖Zookeeper 有表结构，需要定义Schema，需要定义唯一键，支持SQL分析（依赖Impala，Spark等引擎） 支持增删列,单行级ACID（不支持多行事务-不满足原子性） 查询时先查询内存再查询磁盘 数据存储在Linux文件系统，不依赖HDFS存储 缺点 暂不支持除PK外的二级索引和唯一性限制 不支持多行事务 不支持BloomFilter优化join 不支持数据回滚 不能修改PK，不支持AUTO INCREMENT PK 每表最多不能有300列，每个TServer数据压缩后不超8TB 数据类型少，不支持Map，ARRAY，Struct等复杂类型 与相似类型存储引擎对比&emsp;&emsp;本文重点说Kudu，但我们也需要了解其他类似组件，了解它们各自擅长的地方，才能更好地做技术选型。这里简单对比一下Kudu，Hudi和DeltaLake这三种存储方案，因为它们都具有相似的特性，能解决类似的问题。 特性 Kudu Hudi Delta Lake 行级别更新 支持 支持 支持 schema修改 支持 支持 支持 批流共享 支持 支持 支持 可用索引 是 是 否 多并发写 支持 不支持 支持 版本回滚 不支持 支持 支持 实时性 高 近实时 差 使用HDFS 不支持 支持 支持 空值处理 默认null error 默认null 并发读写 支持 不支持并发写 支持 云存储 不支持 支持 支持 兼容性 Spark，Impala，Presto Spark，Presto，Hive，MR 较好 依赖Spark，有限支持Hive，Presto 选择建议：考虑实时数仓方案以及SQL支持方面可选Kudu，数据湖方案及可回滚可选DeltaLake和Hudi，考虑兼容性高且应对读多写少读少写多都有很好的方案选Hudi，考虑并发写能力读多写少且与Spark紧密结合选DeltaLake。 Kudu架构原理Raft算法介绍&emsp;&emsp;为了更好地理解Kudu，需要简单了解一下Raft算法。Raft是一个一致性算法，在分布式系统中一致性算法就是让多个节点在网络不稳定甚至部分节点宕机的情况下能对某个事件达成一致。而Raft是一个用于管理日志一致性的协议，它将分布式一致性分解为多个子问题：LeaderElection，LogReplication，Safety，LogCompaction等。 &emsp;&emsp;Raft将系统中的角色分为Leader，Follower和Candidate。正常运行时只有Leader和Follower，选举时才会有Candidate。&emsp;&emsp;Leader:接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。&emsp;&emsp;Follower:接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。响应Candidate的邀请投票请求。把客户端请求重定向到Leader。&emsp;&emsp;Candidate:Leader选举过程中的临时角色，由于Follower转变而来。 Leader选举发生在Follower接收不到Leader的HeartBeat导致ElectionTimeout超时的情况下。①每个Follower都有一个时钟ElectionTimeout，是个随机值，表示Follower等待成为Leader的时间，谁的时钟先跑完则先发起Leader选举。（收到Leader心跳时会清零ElectionTimeout）②Follower将其任期(Term)加1然后转为Candidate状态，并且给自己投票，然后携Term_id和日志index给其他节点发起选举（RequestVote RPC）。有三种情况：①赢得半数以上选票，成为Leader②收到Leader消息，Leader被抢了，成为Follower③选举超时时，没有节点赢得多数选票，选举失败，Term_id自增1，进行下一轮选举 ③Raft协议所有日志都只能从Leader写入Follower，Leader节点日志只会增加（index+1），不会删除和覆盖。所以Leader必须包含全部日志，能被选举为Leader的节点一定包含了所有已经提交的日志。每个节点最多只能给一个候选人投票，先到先服务的原则。选举胜出规则：节点Term_id越大越新则可能胜出，但可能有Term_id相同的情况，Term_id相同，比较日志Index越大越新则胜出。这一点很像Zookeeper选举的规则。详细过程：首先会有一个Candidate选自己然后发起投票-&gt;Follower收到邀票，如果这个Follower还没给其他节点投票(一个节点只能一票),且对比Term_id比自己大，就把票投给这个Candidate，如果Term_id比自己小且自己还没投票，就拒绝请求，给自己投票。 概括：增加任期编号-&gt;给自己投票-&gt;重置ElectionTimeout-&gt;发送投票RPC给其他节点 日志复制过程：Leader接收来自客户端的请求并将其以日志的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致。 Raft同步日志由编号index、term_id和命令组成。=&gt;有助于选举和根据term持久化日志永远只有一个流向Leader-&gt;Follower 1.日志复制的保证： 1.如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的（原因：leader 最多在一个任期里的一个日志索引位置创建一条日志条目，日志条目在日志的位置从来不会改变）。 2.如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的（原因：每次 RPC 发送附加日志时，leader 会把这条日志条目的前面的日志的下标和任期号一起发送给 follower，如果 follower 发现和自己的日志不匹配，那么就拒绝接受这条日志，这个称之为一致性检查）。 2.网络故障或Leader崩溃时保证一致性： 网络崩溃讲集群分为两拨，没有Leader存在的另一波会重新选主，这时网络恢复，会出现两个Leadr的情况，这是会产生冲突的，这时会根据任期Term_id将任期低的Leader自动降级为Follower，Leader和Follower日志有冲突的时候，Leader将校验Follower最后一条日志是否和Leader匹配，如果不匹配，将递减查询，直到匹配，匹配后，删除冲突的日志。这样就实现了主从日志的一致性。 递减查询，直到匹配，强制覆盖 =&gt;Leader会强制Follower复制它的日志，Leader会从最后的LogIndex从后往前试，直到找到日志一致的index，然后开始复制，覆盖该index之后的日志条目。 场景：发生了网络分区或者网络通信故障，使得Leader不能访问大多数Follwer了，那么Leader只能正常更新它能访问的那些Follower，而大多数的Follower因为没有了Leader，他们重新选出一个Leader，然后这个 Leader来接受客户端的请求，如果客户端要求其添加新的日志，这个新的Leader会通知大多数Follower。如果这时网络故障修复 了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新（递减查询匹配日志）。 日志压缩日志不能无限增长，否则会导致重播日志时耗时很长。所以对日志进行压缩，定量Snapshot。 &emsp;&emsp;Raft参考：Raft算法详解&emsp;&emsp;Raft算法在Kudu中的应用：多个TMaster之间通过Raft协议实现数据同步和高可用–Raft负责在多个Tablet副本中选出Leader和Follower，Leader Tablet负责发送写入数据给Follower Tablet，大多数副本都完成了写操作则会向客户端确认。 Kudu的一致性模型相关资料不多，可以一起讨论 Kudu为用户提供了两种一致性模型(snapshot consistency和external consistency)。默认的一致性模型是snapshot consistency。这种一致性模型保证用户每次读取出来的都是一个可用的快照，但这种一致性模型只能保证单个client可以看到最新的数据，但不能保证多个client每次取出的都是最新的数据。另一种一致性模型external consistency(后开始的事务一定可以看到先提交的事务的修改。所有事务的读写都加锁可以解决这个问题，缺点是性能较差。)可以在多个client之间保证每次取到的都是最新数据，但是Kudu没有提供默认的实现，需要用户做一些额外工作。 为了实现external consistency，Kudu提供了两种方式： 1.在client之间传播timestamp token。在一个client完成一次写入后，会得到一个timestamp token，然后这个client把这个token传播到其他client，这样其他client就可以通过token取到最新数据了。不过这个方式的复杂度很高，基于HybridTime方案，这也就是为什么Kudu高度依赖NTP。 2.通过commit-wait方式，这有些类似于Google的Spanner。但是目前基于NTP的commit-wait方式延迟实在有点高。不过Kudu相信，随着[分布式事务实现-Spanner](https://blog.csdn.net/weixin_30650039/article/details/94998723)的出现，未来几年内基于real-time clock的技术将会逐渐成熟。 LSM树LSM树(Log-Structured Merge Tree)&emsp;&emsp;Kudu与HBase在写的过程中都采用了LSM树的结构，LSM树的主要思想就是随机写转换为顺序写来提高写性能，随机读写需要磁盘的机械臂不断寻道，延迟较高，而转换为顺序写后机械臂不会频繁寻址，性能较好。&emsp;&emsp;LSM树原理是把一棵大的树拆分成N棵小树，小树存在于内存中，随着更新和写入操作，小树存放数据达到一定大小后会写入磁盘，小树到了磁盘中，定期与磁盘中的大树做合并。&emsp;&emsp;大家都知道HBase的MemStore，Kudu在写入方面的设计与之类似，Kudu先将对数据的修改保留在内存中，达到一定大小后将这些修改操作批量写入磁盘。但读取的时候稍微麻烦些，需要读取历史数据和内存中最近修改操作。所以写入性能大大提升，而读取时要先去内存读取，如果没命中，则会去磁盘读多个文件。 压缩和编码&emsp;&emsp;我们都知道列式存储的压缩效果很好，那么为什么列式存储比行存储压缩效果好呢？&emsp;&emsp;比如一个列存的国家名，那只能包含“美国”，“日本”，“韩国”，“加拿大”等值，而这些值会存储在一起，而不是分散到包含很多不相关的其他列值之间。这样列式存储也就不需要将每个值都完完整整保存起来，所以压缩效果显著。&emsp;&emsp;编码对于列式存储的优化更加明显，编码和压缩作用相同，比如上面的例子，编码会将数据的值转换为一种更小的表现形式，比如，“美国”编码为1，“日本”编码为2，“韩国”编码为3，“加拿大”编码为4…则Kudu只存储1，2，3，4…而不存储长字符串，占用空间大大减少。 Kudu一些概念 Table：具有Schema和全局有序主键的表。一张表有多个Tablet，多个Tablet包含表的全部数据。Tablet：Kudu的表Table被水平分割为多段，Tablet是Kudu表的一个片段（分区），每个Tablet存储一段连续范围的数据（会记录开始Key和结束Key），且两个Tablet间不会有重复范围的数据。一个Tablet会复制（逻辑复制而非物理复制，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息）多个副本在多台TServer上，其中一个副本为Leader Tablet，其他则为Follower Tablet。只有Leader Tablet响应写请求，任何Tablet副本可以响应读请求。TabletServer：简称TServer，负责数据存储Tablet、提供数据读写服务、编码、压缩、合并和复制。一个TServer可以是某些Tablet的Leader，也可以是某些Tablet的Follower，一个Tablet可以被多个TServer服务（多对多关系）。TServer会定期（默认1s）向Master发送心跳。Catalog Table：目录表，用户不可直接读取或写入，仅由Master维护，存储两类元数据：表元数据（Schema信息，位置和状态）和Tablet元数据（所有TServer的列表、每个TServer包含哪些Tablet副本、Tablet的开始Key和结束Key）。Catalog Table只存储在Master节点，也是以Tablet的形式，数据量不会很大，只有一个分区，随着Master启动而被全量加载到内存。Master：负责集群管理和元数据管理。具体：跟踪所有Tablets、TServer、Catalog Table和其他相关的元数据。协调客户端做元数据操作，比如创建一个新表，客户端向Master发起请求，Master写入其WAL并得到其他Master同意后将新表的元数据写入Catalog Table，并协调TServer创建Tablet。WAL：一个仅支持追加写的预写日志，无论Master还是Tablet都有预写日志，任何对表的修改都会在该表对应的WAL中写入条目(entry)，其他副本在数据相对落后时可以通过WAL赶上来。逻辑复制：Kudu基于Raft协议在集群中对每个Tablet都存储多个副本，副本中的内容不是实际的数据，而是操作该副本上的数据时对应的更改信息。Insert和Update操作会走网络IO，但Delete操作不会，压缩数据也不会走网络。 存储与读写Kudu的存储结构：&emsp;&emsp;如图，Table分为若干Tablet；Tablet包含Metadata和RowSet，RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、AdhocIndex、BaseData、DeltaMem及若干个RedoFile和UndoFile（UndoFile一般情况下只有一个）。&emsp;&emsp;MemRowSet：插入新数据及更新已在MemRowSet中的数据，数据结构是B+树，主键在非叶子节点，数据都在叶子节点。MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。每次达到1G或者120s时生成一个DiskRowSet，DiskRowSet按列存储，类似Parquet。&emsp;&emsp;DiskRowSet：DiskRowSets存储文件格式为CFile。DiskRowSet分为BaseData和DeltaFile。这里每个Column被存储在一个相邻的数据区域，这个数据区域被分为多个小的Page，每个Column Page都可以使用一些Encoding以及Compression算法。后台会定期对DiskRowSet做Compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。&emsp;&emsp;BaseData：DiskRowSet刷写完成的数据，CFile，按列存储，主键有序。BaseData不可变，类似Parquet。&emsp;&emsp;BloomFile：根据一个DiskRowSet中的Key生成一个BloomFilter，用于快速模糊定位某个key是否在DiskRowSet中存在。&emsp;&emsp;AdhocIndex：存放主键的索引，用于定位Key在DiskRowSet中的具体哪个偏移位置。&emsp;&emsp;DeltaMemStore：每份DiskRowSet都对应内存中一个DeltaMemStore，负责记录这个DiskRowSet上BaseData发生后续变更的数据，先写到内存中，写满后Flush到磁盘生成RedoFile。DeltaMemStore的组织方式与MemRowSet相同，也维护一个B+树。&emsp;&emsp;DeltaFile：DeltaMemStore到一定大小会存储到磁盘形成DeltaFile，分为UndoFile和RedoFile。&emsp;&emsp;RedoFile：重做文件，记录上一次Flush生成BaseData之后发生变更数据。DeltaMemStore写满之后，也会刷成CFile，不过与BaseData分开存储，名为RedoFile。UndoFile和RedoFile与关系型数据库中的Undo日子和Redo日志类似。&emsp;&emsp;UndoFile：撤销文件，记录上一次Flush生成BaseData之前时间的历史数据，Kudu通过UndoFile可以读到历史某个时间点的数据。UndoFile一般只有一份。默认UndoFile保存15分钟，Kudu可以查询到15分钟内某列的内容，超过15分钟后会过期，该UndoFile被删除。 &emsp;&emsp;DeltaFile(主要是RedoFile)会不断增加，产生大量小文件，不Compaction肯定影响性能，所以就有了下面两种合并方式： Minor Compaction：多个DeltaFile进行合并生成一个大的DeltaFile。默认是1000个DeltaFile进行合并一次。 Major Compaction：RedoFile文件的大小和BaseData的文件的比例为0.1的时候，会将RedoFile合并进入BaseData，Kudu记录所有更新操作并保存为UndoFile。补充一下：合并和重写BaseData是成本很高的，会产生大量IO操作，Kudu不会将全部DeltaFile合并进BaseData。如果只更新几行数据，但要重写BaseData，费力不讨好，所以Kudu会在某个特定列需要大量更新时再把BaseData与DeltaFile合并。未合并的RedoFile会继续保留等待后续合并操作。 Kudu读流程： Client发送读请求，Master根据主键范围确定到包含所需数据的所有Tablet位置和信息。 Client找到所需Tablet所在TServer，TServer接受读请求。 如果要读取的数据位于内存，先从内存（MemRowSet，DeltaMemStore）读取数据，根据读取请求包含的时间戳前提交的更新合并成最终数据。 如果要读取的数据位于磁盘（DiskRowSet，DeltaFile），在DeltaFile的UndoFile、RedoFile中找目标数据相关的改动，根据读取请求包含的时间戳合并成最新数据并返回。 Kudu写流程： Client向Master发起写请求，Master找到对应的Tablet元数据信息，检查请求数据是否符合表结构。 因为Kudu不允许有主键重复的记录，所以需要判断主键是否已经存在，先查询主键范围，如果不在范围内则准备写MemRowSet。 如果在主键范围内，先通过主键Key的布隆过滤器快速模糊查找，未命中则准备写MemRowSet。 如果BloomFilter命中，则查询索引，如果没命中索引则准备写MemRowSet，如果命中了主键索引就报错：主键重复。 写入MemRowSet前先被提交到一个Tablet的WAL预写日志，并根据Raft一致性算法取得Follower Tablets的同意，然后才会被写入到其中一个Tablet的MemRowSet中。为了在MemRowSet中支持多版本并发控制(MVCC)，对最近插入的行(即尚未刷新到磁盘的新的行)的更新和删除操作将被追加到MemRowSet中的原始行之后以生成重做(REDO)记录的列表。 MemRowSet写满后，Kudu将数据每行相邻的列分为不同的区间，每个列为一个区间，Flush到DiskRowSet。 Kudu更新流程： Client发送更新请求，Master获取表的相关信息，表的所有Tablet信息。 Kudu检查是否符合表结构。 如果需要更新的数据在MemRowSet，B+树找到待更新数据所在叶子节点，然后将更新操作记录在所在行中一个Mutation链表中；Kudu采用了MVCC(多版本并发控制，实现读和写的并行，任何写都是插入)思想，将更改的数据以链表形式追加到叶子节点后面，避免在树上进行更新和删除操作。 如果需要更新的数据在DiskRowSet，找到其所在的DiskRowSet，前面提到每个DiskRowSet都会在内存中有一个DeltaMemStore，将更新操作记录在DeltaMemStore，达到一定大小才会生成DeltaFile到磁盘。 分区方式&emsp;&emsp;Kudu的分区即为Tablet，如果主键设计不好以及分区不合理都会造成数据发生单点读写问题，也就是热点问题。Kudu分区设计方案需要根据场景和读取写入的方式来制定。最好是将读写操作都能分散到大部分节点。&emsp;&emsp;在Kudu中只有主键才能被用来分区。分区模式有三种： 基于Hash分区(Hash Partitioning):&emsp;&emsp;哈希分区通过哈希值将行分配到许多Buckets(存储桶)之一,一个Bucket对应一个Tablet。&emsp;&emsp;优点：按ID哈希分区可以将数据均匀分布，写操作会分布在多个节点，减轻热点和Tablet大小不均匀问题。也就是基于ID哈希分区写效率高&emsp;&emsp;缺点：按ID查询数据会读取单个Tablet(Bucket)，单点读取效率低。 基于Range分区(Range Partitioning):&emsp;&emsp;由PK范围划分组成，一个区间对应一个Tablet。将数据按给定的主键范围的存储到各个TS节点上。&emsp;&emsp;优点：如果按日期范围分区，单个ID的读取会跨多个节点并行执行，效率高。也就是基于时间范围分区查询效率高。&emsp;&emsp;缺点：如果按日期范围分区会有写热点问题，而且一旦数据量超出最后一个Range，接下来的数据将全部写入最后一个Range分区，发生倾斜。 多级分区(Multilevel Partitioning):**可以在单表上组合分区类型。&emsp;&emsp;优点：结合以上两种分区方式，保留两种分区类型的优点–既可以数据分布均匀，又可以在每个分片中保留指定的数据。也就是基于ID哈希分区且基于时间范围分区组合方式读写效率都会提高**&emsp;&emsp;缺点：优点太多… 复制策略&emsp;&emsp;如果一个TServer出现故障，副本的数量由3减到2个，Kudu会尽快恢复副本数。两种复制策略：3-4-3：如果一个副本丢失，先添加替换的副本，再删除失败的副本，Kudu默认使用这种复制策略。3-2-3：如果一个副本丢失，先删除失败的副本，再添加替换的副本。 一些细节 为什么Kudu要比HBase、Cassandra扫描速度更快？&emsp;&emsp;HBase、Cassandra都有列簇(CF)，并不是纯正的列存储，那么一个列簇中有几个列，但这几个列不能一起编码，压缩效果相对不好，而且在扫描其中一个列的数据时，必然会扫描同一列簇中的其他列。Kudu没有列簇的概念，它的不同列数据都在相邻的数据区域，可以在一起压缩，也可以对不同列使用不同压缩算法，压缩效果很好；而且需要哪列读哪列不会读其他列，读取时不需要进行Merge操作，根据BaseData和Delta数据得到最终数据。Kudu扫描性能可媲美Parquet。还有，Kudu的读取方式避免了很多字段的比较操作，CPU利用率高。 Kudu一个Tablet中存很多很多DiskRowSet，怎么才能快速判断Key在哪个DiskRowSet？&emsp;&emsp;首先肯定不能遍历，O(n)的复杂度是很难受的。它使用二叉查找树，每个节点维护多个DiskRowSet的最大Key和最小Key，这样就可在O(logn)时间内定位Key所在DiskRowSet。 Kudu不同的列类型不同，使用的编码和压缩方式？&emsp;&emsp;Kudu每列都有类型，编码方式和压缩方式，编码方式根据数据类型不同有合适的默认值，压缩方式默认不压缩。 Kudu的部署&emsp;&emsp;Kudu有两种进程Master和TServer，Kudu服务是可以单独部署在集群的，但大多数情况可能是与Hadoop集群共置，不同的环境需要不同的部署方案，本节用数据化运营的思想来说Kudu服务的部署。 Master部署&emsp;&emsp;Master高可用，一般配置3或5个Master来保证HA，同一时刻只有一个Master工作，半数以上Master存活，服务都可正常运行。Master之间需要达成共识，大多数Master“投票”得到Leader，其他的为Follower。如果该Master出现问题，也是通过Raft一致性算法来做选举，既容错又高效。&emsp;&emsp;一般配置3或5个Master，7个就有点多了没必要。Master数目必须为奇数个。给定一组需要写N个副本（一般为3或5）的Tablet，可以接受(N-1)/2个写入错误。&emsp;&emsp;由于Mater中只保存元数据，数据量会一直比较小，即使被频繁请求，被全量加载到内存，也不需要占用大量系统资源。 TServer部署&emsp;&emsp;根据业务量，了解大概要存储多少数据。因为Kudu列式存储与Parquet相似，可以根据相同数据量Parquet占用磁盘大小来粗略估计需要多少存储空间。&emsp;&emsp;TServer数量和配置大致给多少呢？ 假设: Parquet格式存储的数据集大小60TB 每个TServer数据磁盘最大8T 给数据磁盘预留25%的磁盘空间 Tablet冗余副本3 TServer数量 = (Parquet格式存储的数据集大小 * 冗余数) / (TS磁盘容量 * ( 1 - 磁盘预留)) TServer数量 = (60 * 3) / (8 * (1 - 0.25)) = 30 存储介质&emsp;&emsp;在CM部署Master和TServer时，我们可以看到如下配置：&emsp;&emsp;Kudu设计时就对数据和WAL分开存储的，为什么呢？&emsp;&emsp;之前说过WAL仅支持追加写，单个操作乍一看会顺序写WAL，但同时执行多个任务时，更像是随机写WAL，这就很考验WAL底层存储的IOPS(IO Per Second)了。传统机械盘IOPS也就几百，而NVMe-SSD的IOPS能达到万级甚至百万级，所以Kudu WAL尽量存储在SSD中。&emsp;&emsp;那WAL的SSD盘大概要选多大呢？ Kudu的WAL日志是可以控制大小，日志段数量的。 默认日志段大小8M，数量1-80个，按默认的8M，80个算，如果共2000个Tablet，需要WAL的SSD大小： 8 * 80 * 2000 = 1280000MB 约1.3TB &emsp;&emsp;Tablet的数据可以用机械盘HDD来存储(SSD更好)，可以与DataNode处于同一块磁盘，这样更方便管理，因为这样可以充分利用磁盘负载和磁盘空间，不至于一个盘爆满另一个盘空余很多。 Kudu使用环境：四台机器CDH6.3.1集群，6核心12线程，内存分别为：20GB，14GB，14GB和10GB。**OS:**CentOS7;**Impala:**3.2.0-cdh6.3.1;**Kudu:**1.10.0-cdh6.3.1(3Master+3TServer);**Hive:**2.1.1-cdh6.3.1;依次启动HDFS、hive、Kudu、Impala。 Kudu + Impala&emsp;&emsp;Impala定位是一款实时查询引擎(低延时SQL交互查询)，快的原因：基于内存计算，无需MR，C++编写，兼容HiveQL和支持数据本地化。这与Kudu场景相吻合，Kudu官网也说Impala和Kudu可以无缝整合。&emsp;&emsp;进入Impala配置，Kudu服务处勾选Kudu即可。&emsp;&emsp;注意：=, &lt;=, ‘&lt;‘, ‘&gt;‘, &gt;=, BETWEEN, IN等操作会从Impala谓词下推到Kudu，性能高。而!=, like和其他Impala关键字会让Kudu返回所有结果再让Impala过滤，效率低下。因为Kudu没二级索引，所以没有主键的谓词也会造成全表扫描。 1.使用Impala创建Hash分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE DATABASE IF NOT EXISTS impala_kudu; -- 建内部表，Impala发生drop操作会删除Kudu上对应数据 CREATE TABLE impala_kudu.first_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 8 -- 使用Hash分区 STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- 查看Kudu中已能看到刚创建的表 kudu table list cdh102:7051,cdh103:7051,cdh104:7051 -- 查看表以及tablets kudu table list cdh102:7051,cdh103:7051,cdh104:7051 --list_tablets 在WebUI上可以看到该表对应8个Tablet以及每个Tablet信息。 2.使用Impala创建RANGE分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE TABLE impala_kudu.second_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY RANGE (id) ( -- 使用Range分区 只有主键可以做RANGE分区的字段 PARTITION 0 &lt;= values &lt;= 3, -- 如果id范围取多个值，则为values，如果分区字段按是单个值，则为value PARTITION 4 &lt;= values &lt;= 7, PARTITION 8 &lt;= values &lt;= 11, PARTITION 11 &lt; values ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- ------------------------------------------------------------------------------------ CREATE TABLE impala_kudu.third_kudu_table( state STRING, name String, PRIMARY KEY(state,name) ) PARTITION BY RANGE (state) ( -- 联合主键时，RANGE分区可以使用其中一个字段 PARTITION value = &#39;succeed&#39;, -- 如果分区字段按是单个值，则为value PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 3.使用Impala创建混合分区的Kudu表 impala-shell -i cdh102:21000 -- Impala Daemon在cdh102机器 CREATE TABLE impala_kudu.fourth_kudu_table( id INT, state String, name String, PRIMARY KEY(id,state) ) PARTITION BY HASH (id) PARTITIONS 4, -- 混合分区 HASH+RANGE RANGE (state) ( PARTITION value = &#39;succeed&#39;, PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; -- 最终Tablet数为HASH分区数乘以RANGE分区数 ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 4.在Impala映射已经存在的Kudu表 [kudu@cdh102 /]# kudu table list cdh102:7051,cdh103:7051,cdh104:7051 impala::impala_kudu.first_kudu_table impala::impala_kudu.test impala::impala_kudu.second_kudu_table impala::impala_kudu.fourth_kudu_table impala::impala_kudu.third_kudu_table CREATE EXTERNAL TABLE impala_kudu.fifth_kudu_table STORED AS KUDU -- Kudu表映射到Impala中，不能指定字段，主键和分区方式，由Kudu决定 TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39;, &#39;kudu.table_name&#39; = &#39;impala::impala_kudu.first_kudu_table&#39; ); drop table impala_kudu.fifth_kudu_table; -- 删除不会对Kudu中表有影响 5.转储一张Hive表到Kudu show create table default.test; CREATE TABLE impala_kudu.test( id INT, name STRING, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 2 -- 数据量少，分2个桶 STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); INSERT INTO impala_kudu.test SELECT * FROM default.test; SELECT * FROM impala_kudu.test; -- 这时会发现数据顺序发生变化了，因为Hash分区的原因 6.Kudu表备份到Hive实测在数据量不大 百G级别 使用Impala实现Kudu数据全量同步Hive，不会对Kudu服务造成影响 CREATE TABLE default.impala_kudu_test LIKE impala_kudu.test; INSERT INTO default.impala_kudu_test SELECT * FROM impala_kudu.test; 在官网了解更多：Using Apache Kudu with Apache Impala Kudu + Spark &lt;!-- scala.bin.version: 2.12, kudu.version: 1.10.0 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-spark2_$&#123;scala.bin.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-spark2-tools_$&#123;scala.bin.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt; &lt;/dependency&gt; Spark连接并操作Kudu表 import org.apache.kudu.client._ import collection.JavaConverters._ // Read a table from Kudu val df = spark.read .options(Map(&quot;kudu.master&quot; -&gt; &quot;kudu.master:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;kudu_table&quot;)) .format(&quot;kudu&quot;).load // Query using the Spark API... df.select(&quot;id&quot;).filter(&quot;id &gt;= 5&quot;).show() // ...or register a temporary table and use SQL df.createOrReplaceTempView(&quot;kudu_table&quot;) val filteredDF = spark.sql(&quot;select id from kudu_table where id &gt;= 5&quot;).show() // Use KuduContext to create, delete, or write to Kudu tables val kuduContext = new KuduContext(&quot;kudu.master:7051&quot;, spark.sparkContext) // Create a new Kudu table from a DataFrame schema // NB: No rows from the DataFrame are inserted into the table kuduContext.createTable( &quot;test_table&quot;, df.schema, Seq(&quot;key&quot;), new CreateTableOptions() .setNumReplicas(1) .addHashPartitions(List(&quot;key&quot;).asJava, 3)) // Insert data kuduContext.insertRows(df, &quot;test_table&quot;) // Delete data kuduContext.deleteRows(filteredDF, &quot;test_table&quot;) // Upsert data kuduContext.upsertRows(df, &quot;test_table&quot;) // Update data val alteredDF = df.select(&quot;id&quot;, $&quot;count&quot; + 1) kuduContext.updateRows(filteredRows, &quot;test_table&quot;) // Data can also be inserted into the Kudu table using the data source, though the methods on // KuduContext are preferred // NB: The default is to upsert rows; to perform standard inserts instead, set operation = insert // in the options map // NB: Only mode Append is supported df.write .options(Map(&quot;kudu.master&quot;-&gt; &quot;kudu.master:7051&quot;, &quot;kudu.table&quot;-&gt; &quot;test_table&quot;)) .mode(&quot;append&quot;) .format(&quot;kudu&quot;).save // Check for the existence of a Kudu table kuduContext.tableExists(&quot;another_table&quot;) // Delete a Kudu table kuduContext.deleteTable(&quot;unwanted_table&quot;) Kudu + Hive与Hive MetaStore集成官网写的很详细。在官网了解更多：Using the Hive Metastore with Kudu Kudu APIsKudu常用Command LinesKudu客户端命令 kudu cluster ksck master1,master2,master3 查看表及表状态 HEALTHY正常 UNDER_REPLICATED缺失副本但不影响使用 UNAVAILABLE表无法使用 kudu cluster rebalance master1,master2,master3 -max_moves_per_server 3 -max_run_time_sec 10 Kudu数据重平衡避免热点TS，TS的service_queue被占满、内存占用过大 Kudu有很多命令，大致分几类： su - kudu kudu cluster 集群管理，包括健康状态检查，移动tablet，rebalance等操作 kudu diagnose 集群诊断工具 kudu fs 在本地Kudu文件系统做操作，检查一致性，列出元顺据，数据集更新，数据转储 kudu hms 操作HiveMetaStore，包括检查与Kudu元数据一致性，自动修复元数据，列出元数据 kudu local_replica 操作本地副本，包括从远程copy副本过来，获取空间占用情况，删除Tablet，获取副本列表，转储本地副本等 kudu master 操作KuduMaster，可以运行master，获取master状态，时间戳，flag等信息， kudu pbc protobuf容器文件操作 kudu perf 集群性能测试，运行负载，显示本地Tablet行数等 kudu remote_replica 操作远程TServer上的副本，远程复制，删除，转储，列出Tablet kudu table 操作Kudu表，包括添加范围分区，设置blockSize，设置列的压缩类型，编码类型，默认值，注释，复制表数据到另一表，建表，删除列，删表，描述表，删除范围的分区，获取和更改表其他配置，列出表，找到Row所在Tablet，列重命名，表重命名，scan，获取表的统计信息 kudu tablet 操作Kudu的Tablet 包括更换Tablet的Leader，Raft配置 kudu test 测试 kudu tserver 操作TabletServer包括运行，设置Flag，获取状态，时间戳，列出TServers等 kudu wal 操作Kudu WAL，转储WAL日志文件 Kudu常用Java API： Maven Dep： &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;/dependency&gt; //KuduDDL.java Kudu数据定义API包括：建表，删表，增加字段和删除字段 package top.qjj.shmily.operations; import org.apache.kudu.ColumnSchema; import org.apache.kudu.Schema; import org.apache.kudu.Type; import org.apache.kudu.client.*; import org.apache.kudu.shaded.com.google.common.collect.ImmutableList; import java.util.LinkedList; public class KuduDDL&#123; public static void main(String[] args) &#123; String kuduMasterAddrs = &quot;cdh102,cdh103,cdh104&quot;; KuduDDLOperations kuduDDLOperations = KuduDDLOperations.getInstance(kuduMasterAddrs); //创建Kudu表 String tableName = &quot;kudu_table_with_hash&quot;; //1.Schema指定 LinkedList&lt;ColumnSchema&gt; schemaList = new LinkedList&lt;&gt;(); schemaList.add(kuduDDLOperations.newColumn(&quot;id&quot;, Type.INT32, true)); schemaList.add(kuduDDLOperations.newColumn(&quot;name&quot;, Type.STRING, false)); Schema schema = new Schema(schemaList); //2.设置建表参数-哈希分区 // CreateTableOptions options = new CreateTableOptions(); // options.setNumReplicas(1); //设置存储副本数-必须为奇数否则会抛异常 // List&lt;String&gt; hashKey = new LinkedList&lt;String&gt;(); // hashKey.add(&quot;id&quot;); // options.addHashPartitions(hashKey,2); //哈希分区 设置哈希键和桶数 //2.设置建表参数-Range分区 CreateTableOptions options = new CreateTableOptions(); options.setRangePartitionColumns(ImmutableList.of(&quot;id&quot;)); //设置id为Range key int temp = 0; for(int i = 0; i &lt; 10; i++)&#123; //id 每10一个区间直到100 PartialRow lowLevel = schema.newPartialRow(); //定义用来分区的列 lowLevel.addInt(&quot;id&quot;, temp); //与字段类型对应 INT32则addInt INT64则addLong PartialRow highLevel = schema.newPartialRow(); temp += 10; highLevel.addInt(&quot;id&quot;, temp); options.addRangePartition(lowLevel, highLevel); &#125; //3.开始建表 boolean result = kuduDDLOperations.createTable(tableName, schema, options); System.out.println(result); //添加字段 kuduDDLOperations.addColumn(tableName, &quot;test&quot;, Type.INT8); //删除字段 kuduDDLOperations.deleteColumn(tableName, &quot;test&quot;); //删除Kudu表 boolean delResult = kuduDDLOperations.dropTable(tableName); System.out.println(delResult); //关闭连接 kuduDDLOperations.closeConnection(); &#125; &#125; class KuduDDLOperations &#123; private static volatile KuduDDLOperations instance; private KuduClient kuduClient = null; private KuduDDLOperations(String masterAddr)&#123; kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); &#125; public static KuduDDLOperations getInstance(String masterAddr)&#123; if(instance == null)&#123; synchronized (KuduDDLOperations.class)&#123; if(instance == null)&#123; instance = new KuduDDLOperations(masterAddr); &#125; &#125; &#125; return instance; &#125; public void closeConnection()&#123; try &#123; kuduClient.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public ColumnSchema newColumn(String name, Type type, boolean isKey)&#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(isKey); return column.build(); &#125; /** * 创建表 * 注意：Impala DDL对表字段名大小写不敏感，但Kudu层已经转为小写，且Kudu API中字段名必须小写； * 注意：Impala DDL建表表名大小写敏感且到Kudu层表名不会被转成小写，且Kudu API对表名大小写敏感。 * @param tableName 表名 * @param schema Schema信息 * @param tableOptions 建表参数 TableOptions对象 * @return boolean */ public boolean createTable(String tableName, Schema schema, CreateTableOptions tableOptions)&#123; try &#123; kuduClient.createTable(tableName, schema, tableOptions); System.out.println(&quot;Create table successfully!&quot;); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 删库跑路 * @param tableName 要删的表名 * @return boolean 是否需要跑路 */ public boolean dropTable(String tableName)&#123; try &#123; kuduClient.deleteTable(tableName); System.out.println(&quot;Drop table successfully!&quot;); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 给Kudu表添加字段 * @param tableName 表名 * @param column 字段名 * @param type 类型 * @return */ public boolean addColumn(String tableName, String column, Type type) &#123; AlterTableOptions alterTableOptions = new AlterTableOptions(); alterTableOptions.addColumn(new ColumnSchema.ColumnSchemaBuilder(column, type).nullable(true).build()); try &#123; kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;成功添加字段&quot; + column + &quot;到表&quot; + tableName); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 删除Kudu表指定字段 * @param tableName 表名 * @param column 字段名 * @return */ public boolean deleteColumn(String tableName, String column)&#123; AlterTableOptions alterTableOptions = new AlterTableOptions().dropColumn(column); try &#123; kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;成功删除表&quot; + tableName + &quot;的字段&quot; + column); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; &#125; // ------------------------------------------------------------------------------------------------------------- //KuduDML.java Kudu数据操作API包括：CRUD package top.qjj.shmily.operations; import org.apache.kudu.client.SessionConfiguration.FlushMode; import org.apache.kudu.client.*; public class KuduDML &#123; public static void main(String[] args) &#123; String masterAddr = &quot;cdh102,cdh103,cdh104&quot;; KuduDMLOperations kuduDMLOperations = KuduDMLOperations.getInstance(masterAddr); //插入数据 kuduDMLOperations.insertRows(); //更新一条数据 kuduDMLOperations.updateRow(); //删除一条数据 kuduDMLOperations.deleteRow(); //查询数据 kuduDMLOperations.selectRows(); //关闭Client连接 kuduDMLOperations.closeConnection(); &#125; &#125; class KuduDMLOperations &#123; private static volatile KuduDMLOperations instance; private KuduClient kuduClient = null; private KuduDMLOperations(String masterAddr)&#123; kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); &#125; public static KuduDMLOperations getInstance(String masterAddr)&#123; if(instance == null)&#123; synchronized (KuduDMLOperations.class)&#123; if(instance == null)&#123; instance = new KuduDMLOperations(masterAddr); &#125; &#125; &#125; return instance; &#125; public void closeConnection() &#123; try &#123; kuduClient.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; /** * 以kudu_table_with_hash表(id INT,name STRING)为例 插入数据 * 注意：写数据时数据不支持为null，需要对进来的数据判空 */ public void insertRows()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); //打开表 KuduSession kuduSession = kuduClient.newSession(); //创建会话Session kuduSession.setFlushMode(FlushMode.MANUAL_FLUSH); //设置数据提交方式 /** * 1.AUTO_FLUSH_SYNC（默认） 目前比较慢 * 2.AUTO_FLUSH_BACKGROUND 目前有BUG * 3.MANUAL_FLUSH 目前效率最高 远远高于其他 * 关于这三个参数测试调优可看这篇：https://www.cnblogs.com/harrychinese/p/kudu_java_api.html */ int numOps = 3000; kuduSession.setMutationBufferSpace(numOps); //设置MANUAL_FLUSH需要设置缓冲区操作次数限制 如果超限会抛异常 int nowOps = 0; //记录当前操作数 for(int i = 0; i &lt;= 100; i++)&#123; Insert insert = table.newInsert(); //字段数据 insert.getRow().addInt(&quot;id&quot;, i); insert.getRow().addString(&quot;name&quot;, &quot;小&quot;+i); nowOps += 1; if(nowOps == numOps / 2)&#123; //所以缓冲区操作次数达到一半时进行flush提交数据，避免抛异常 kuduSession.flush(); //提交数据 nowOps = 0; //计数器归零 &#125; kuduSession.apply(insert); &#125; kuduSession.flush(); //保证最后都提交上去了 kuduSession.close(); System.out.println(&quot;数据成功写入Kudu表&quot;); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;数据写入失败，原因：&quot; + e.getMessage()); &#125; &#125; /** * 以kudu_table_with_hash表(id INT,name STRING)为例 查询数据 */ public void selectRows()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); // 打开表 KuduScanner scanner = kuduClient.newScannerBuilder(table).build(); //创建Scanner while (scanner.hasMoreRows())&#123; for (RowResult r: scanner.nextRows()) &#123; System.out.println(r.getInt(&quot;id&quot;) + &quot; - &quot; + r.getString(1)); &#125; &#125; scanner.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;查询失败，原因：&quot; + e.getMessage()); &#125; &#125; /** * 以kudu_table_with_hash表(id INT,name STRING)为例 更新一条数据 */ public void updateRow()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); session.setFlushMode(FlushMode.AUTO_FLUSH_SYNC); Update update = table.newUpdate(); PartialRow row = update.getRow(); //定义用来分区的列 row.addInt(&quot;id&quot;, 66); row.addString(&quot;name&quot;, &quot;qjj&quot;); session.apply(update); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;数据更新失败，原因：&quot; + e.getMessage()); &#125; &#125; /** * 以kudu_table_with_hash表(id INT,name STRING)为例 删除一条数据 */ public void deleteRow()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); Delete delete = table.newDelete(); delete.getRow().addInt(&quot;id&quot;,18); //根据主键唯一删除一条记录 session.flush(); session.apply(delete); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;数据删除失败，原因：&quot; + e.getMessage()); &#125; &#125; &#125; Kudu常用Python API： import kudu from kudu.client import Partitioning from datetime import datetime # Connect to Kudu master server client = kudu.connect(host=&#39;cdh102,cdh103,cdh104&#39;, port=7051) # Define a schema for a new table builder = kudu.schema_builder() builder.add_column(&#39;key&#39;).type(kudu.int64).nullable(False).primary_key() builder.add_column(&#39;ts_val&#39;, type_=kudu.unixtime_micros, nullable=False, compression=&#39;lz4&#39;) schema = builder.build() # Define partitioning schema partitioning = Partitioning().add_hash_partitions(column_names=[&#39;key&#39;], num_buckets=3) # Create new table client.create_table(&#39;python-example&#39;, schema, partitioning) # Open a table table = client.table(&#39;python_example&#39;) # Create a new session so that we can apply write operations session = client.new_session() # Insert a row op = table.new_insert(&#123;&#39;key&#39;: 1, &#39;ts_val&#39;: datetime.utcnow()&#125;) session.apply(op) # Upsert a row op = table.new_upsert(&#123;&#39;key&#39;: 2, &#39;ts_val&#39;: &quot;2020-01-01T00:00:00.000000&quot;&#125;) session.apply(op) # Updating a row op = table.new_update(&#123;&#39;key&#39;: 1, &#39;ts_val&#39;: (&quot;2020-07-12&quot;, &quot;%Y-%m-%d&quot;)&#125;) session.apply(op) # Delete a row op = table.new_delete(&#123;&#39;key&#39;: 2&#125;) session.apply(op) # Flush write operations, if failures occur, capture print them. try: session.flush() except kudu.KuduBadStatus as e: print(session.get_pending_errors()) # Create a scanner and add a predicate scanner = table.scanner() scanner.add_predicate(table[&#39;ts_val&#39;] == datetime(2020, 7, 12)) # Open Scanner and read all tuples # Note: This doesn&#39;t scale for large scans result = scanner.open().read_all_tuples() Kudu优化1.使用SSD会显著提高Kudu性能。（因为如果取多个字段，列式存储在传统磁盘上会多次寻址，而使用SSD不会有寻址问题）2.kudu性能调优3.memory_limit_hard_bytes 该参数是单个TServer能够使用的最大内存量。如果写入量很大而内存太小，会造成写入性能下降。如果集群资源充裕，可以将它设得比较大，比如设置为单台服务器内存总量的一半。官方也提供了一个近似估计的方法，即：每1TB实际存储的数据约占用1.5GB内存，每个副本的MemRowSet和DeltaMemStore约占用128MB内存，（对多读少写的表而言）每列每CPU核心约占用256KB内存，另外再加上块缓存，最后在这些基础上留出约25%的余量。4.block_cache_capacity_mb Kudu中也设计了BlockCache，不管名称还是作用都与HBase中的对应角色相同。默认值512MB，经验值是设置1~4GB之间，我们设了4GB。5.memory.soft_limit_in_bytes/memory.limit_in_bytes这是Kudu进程组（即Linux cgroup）的内存软限制和硬限制。当系统内存不足时，会优先回收超过软限制的进程占用的内存，使之尽量低于阈值。当进程占用的内存超过了硬限制，会直接触发OOM导致Kudu进程被杀掉。我们设为-1，即不限制。6.maintenance_manager_num_threads单个TServer用于在后台执行Flush、Compaction等后台操作的线程数，默认是1。如果是采用普通硬盘作为存储的话，该值应与所采用的硬盘数相同。7.max_create_tablets_per_ts创建表时能够指定的最大分区数目（hash partition * range partition），默认为60。如果不能满足需求，可以调大。8.follower_unavailable_considered_failed_sec当Follower与Leader失去联系后，Leader将Follower判定为失败的窗口时间，默认值300s,判定为失败则认为数据丢失。9.max_clock_sync_error_usec NTP时间同步的最大允许误差，单位为微秒，默认值10s。如果Kudu频繁报时间不同步的错误，可以适当调大，比如15s。 Kudu异常处理Apache Kudu Troubleshooting1.报错”Remote error:Service unavailable: Scan request on kudu.tserver.TabletServerService from xx.xx.xx.xx:xx dropped due to backpressure.The service queue is full;it has 50 ites.” 原因：高峰期单个Tablet的rpc请求队列达到上限，导致TabletServer无法提供服务，临时解决方案是重启该TabletServer。 可在gflagfile增加参数：–rpc_service_queue_length=120 (适当调大，默认值100) 2.TS维护前需要健康检查，如果有任何副本不足的情况，需等待副本拷贝完成后再维护。可在gflagfile增加参数：–rpc_service_queue_length=3600 follower_unavailable_considered_failed_sec默认为300s，tablet失去联系超过300s后，该节点的数据就会在其他节点重建，为了避免维护造成的不必要的数据移动和拷贝，可以临时设置此时间为更长的时间（重启维护加上tablet重启后初始化需要的时间） KuduTS重启恢复速度更快。 3.Kudu JavaAPI客户端请求连接服务器Mater时报错：①Caused by: org.apache.kudu.client.RecoverableException: connection disconnected②你的主机中的软件终止了一个已建立的链接 unexpected exception from downstream on③Caused by: org.apache.kudu.client.NoLeaderFoundException: Master config (master1_ipaddr:7051,master2_ipaddr:7051,master3_ipaddr:7051) has no leader. Exceptions received: org.apache.kudu.client.RecoverableException: connection disconnected,org.apache.kudu.client.RecoverableException: connection disconnected,org.apache.kudu.client.RecoverableException: connection disconnected④org.apache.kudu.client.NonRecoverableException: cannot complete before timeout:KuduRpc(method=GetTableSchema,tablet=null,attemtp=7…,Sent:(master-master1_ip:7051,[ConnectToMaster,7 ])…Received(master-master1_ip:7051,[NERWORK_ERROR, 6]))在网上搜了一大堆：设置完如下的配置，也不起作用。–rpc_encryption=disabled–rpc_authentication=disabled–trusted_subnets=0.0.0.0/0原因：kudu客户端连接kudu服务器时,服务器返回master的主机名而非IP，告诉客户端谁是master,然后通信，但是主机名不是主机的ip，所以客户端会在本地hosts文件找这个主机名，但是本机没有配置，所以会失败，直到超时。解决：需要本地解析ip对应的host，修改本地host，增加Master节点的host映射即可解决。 4.Impala 查询Kudu报Error loading metadata for kudu table xxx 如下：原因：Impala操作Kudu超时解决：在Impala设置kudu_operation_timeout_ms = 1800000 5.Kudu客户端如下报错：RPC can not complete before timeout: KuduRpc(method=CreateTable, tablet=null, attempt=26, DeadlineTracker(timeout=30000, elapsed=29427)解决: session.setTimeoutMillis(60000); new KuduClient.KuduClientBuilder(&quot;cdh101&quot;).defaultAdminOperationTimeoutMs(600000).build(); 6.Kudu服务部分或全部挂，报错日志如下Check failed: _s.ok() Bad status: Service unavailable: Cannot initialize clock: Timed out waiting for clock sync: Error reading clock. Clock considered unsynchronized原因：ntp同步问题，可能是ntpd进程未启动、或/etc/sysconfig/ntpd带有-x参数、或所有节点都同步到一台公用ntp服务器但该ntp服务器有问题解决：1.检查ntpd进程是否存在 2.去掉/etc/sysconfig/ntpd中-x参数(默认只有-g) 3.配置/etc/ntp.conf将集群内所有节点时钟同步到集群内的某一台节点 再重启Kudu即可 7.Kudu客户端报错提示服务端需要认证具体堆栈如下： org.apache.kudu.client.NonRecoverableException: cannot re-acquire authentication token after 5 attempts (Couldn&#39;t find a valid master in (master1:7051,master2:7051,master3:7051). Exceptions received: [org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials, org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials, org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials]) at org.apache.kudu.client.KuduException.transformException(KuduException.java:110) at org.apache.kudu.client.KuduClient.joinAndHandleException(KuduClient.java:413) at org.apache.kudu.client.KuduScanner.nextRows(KuduScanner.java:72) at com.smy.crm.common.kudu.KuduApiHelperNew.select(KuduApiHelperNew.java:182) at com.smy.crm.common.kudu.KuduApiHelper.select(KuduApiHelper.java:161) at com.smy.crm.tag.util.TagKuduUtil.findCustAllTagFromNarrow(TagKuduUtil.java:302) at com.smy.crm.tag.manager.TagKuduManager.findCustAllTag(TagKuduManager.java:332) at com.smy.crm.tag.rule.TagRealTimeExecute.execute(TagRealTimeExecute.java:105) at com.smy.crm.tag.mq.AutoTagMqConsumer.lambda$startNewConsumer$0(AutoTagMqConsumer.java:78) at org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService$ConsumeRequest.run(ConsumeMessageConcurrentlyService.java:411) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) 查看kudu配置发现服务端–rpc_authentication参数值为required而未按官网说的默认是optional所以未认证或认证过期的长连接就会连接失败导致程序挂掉参考Kudu官网介绍如下： Kudu authentication with Kerberos Kudu can be configured to enforce secure authentication among servers, and between clients and servers. Authentication prevents untrusted actors from gaining access to Kudu, and securely identifies connecting users or services for authorization checks. Authentication in Kudu is designed to interoperate with other secure Hadoop components by utilizing Kerberos. Configure authentication on Kudu servers using the --rpc_authentication flag, which can be set to one of the following options: required - Kudu will reject connections from clients and servers who lack authentication credentials. optional - Kudu will attempt to use strong authentication, but will allow unauthenticated connections. disabled - Kudu will only allow unauthenticated connections. By default, the flag is set to optional. To secure your cluster, set --rpc_authentication to required. 解决：在gflagfile增加参数：–rpc_authentication=optional 8.Kudu新增节点无法连接客户端错误日志 org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: KuduRpc(method=GetTableSchema, tablet=null, attempt=10, TimeoutTracker(timeout=30000, elapsed=30020), Trace Summary(27441 ms): Sent(30), Received(27), Delayed(9), MasterRefresh(10), AuthRefresh(0), Truncated: false Sent: (master-xxx1:7051, [ ConnectToMaster, 10 ]), (master-xxx2:7051, [ ConnectToMaster, 10 ]), (master-xxx3:7051, [ ConnectToMaster, 10 ]) Received: (master-xxx1:7051, [ NETWORK_ERROR, 9 ]), (master-xxx2:7051, [ NETWORK_ERROR, 9 ]), (master-xxx3:7051, [ NETWORK_ERROR, 9 ]) Delayed: (UNKNOWN, [ GetTableSchema, 9 ])) Master日志 Failed RPC negotiation. Trace: 1203 17:18:29.304059 (+ 0us) reactor.cc:583] Submitting negotiation task for server connection from kudu_client_ip:41573 1203 17:18:29.304166 (+ 107us) server_negotiation.cc:184] Beginning negotiation 1203 17:18:29.304168 (+ 2us) server_negotiation.cc:373] Waiting for connection header 1203 17:18:29.367151 (+ 62983us) server_negotiation.cc:381] Connection header received 1203 17:18:29.368056 (+ 905us) server_negotiation.cc:337] Received NEGOTIATE NegotiatePB request 1203 17:18:29.368057 (+ 1us) server_negotiation.cc:420] Received NEGOTIATE request from client 1203 17:18:29.368073 (+ 16us) server_negotiation.cc:349] Sending NEGOTIATE NegotiatePB response 1203 17:18:29.368090 (+ 17us) server_negotiation.cc:205] Negotiated authn=SASL 1203 17:18:29.527892 (+159802us) server_negotiation.cc:337] Received TLS_HANDSHAKE NegotiatePB request 1203 17:18:29.528938 (+ 1046us) server_negotiation.cc:349] Sending TLS_HANDSHAKE NegotiatePB response 1203 17:18:29.555620 (+ 26682us) server_negotiation.cc:337] Received TLS_HANDSHAKE NegotiatePB request 1203 17:18:29.555806 (+ 186us) server_negotiation.cc:349] Sending TLS_HANDSHAKE NegotiatePB response 1203 17:18:29.555827 (+ 21us) server_negotiation.cc:589] Negotiated TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(256) Mac=AEAD 1203 17:18:29.649572 (+ 93745us) negotiation.cc:304] Negotiation complete: Network error: Server connection negotiation failed: server connection from kudu_client_ip:41573: BlockingRecv error: failed to read from TLS socket (remote: kudu_client_ip:41573): Cannot send after transport endpoint shutdown (error 108) Metrics: &#123;&quot;server-negotiator.queue_time_us&quot;:79,&quot;thread_start_us&quot;:42,&quot;threads_started&quot;:1&#125; 原因及解决kudu-master_trusted_subnetskudu-tserver_trusted_subnets客户端所在机器不在默认信任子网内（白名单），导致连接无权限。如果不考虑安全性可以设置–trusted_subnets=0.0.0.0/0，如若考虑安全性，可以在默认值基础上增加新节点所在子网地址。例：–trusted_subnets=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16,172.17.0.0/8注：需要设置在“gflagfile 的 Kudu 服务高级配置代码段（安全阀）” HTAP混合事务分析处理HTAP，即Hybrid Transactional Analytical Processing，我们知道OLAP、OLTP，而HTAP就是结合两者场景，既需要联机事务处理有需要联机分析处理，这也是Kudu的场景。HTAP的场景举例： 管理层希望看到实时的数据汇总报表 客服人员希望能够尽快访问某设备的最新数据以便尽快排除故障 乘车线路拥堵立刻感知并立刻规划线路 车联网，物联网 总结&emsp;&emsp;Kudu–Fast Analytics on Fast Data.一个Kudu实现了整个大数据技术栈中诸多组件的功能，有分布式文件系统（好比HDFS），有一致性算法（好比Zookeeper），有Table（好比Hive表），有Tablet（好比Hive分区），有列式存储（如Parquet），有顺序和随机读取（如HBase），所以看起来kudu像一个轻量级的，结合了HDFS+Zookeeper+Hive+Parquet+HBase等组件功能并在性能上进行平衡的组件。它轻松地解决了随机读写+快速分析的业务场景，解决了实时数仓的诸多难点，同时降低了存储成本和运维成本。&emsp;&emsp;学Kudu时让我想到曾经看过的终结者系列电影，万物互联，主角不小心被街边一个不起眼的监控探头拍到，就会立刻引来终结者的追杀，任何有网络的地方留下任何痕迹都会立刻被终结者感知…很明显，这就是在快速变化的数据上进行快速分析，如果没有Kudu，大量的物联网数据就只能批处理了，就没了时效性，主角就可以随便浪了。没准”天网”系统里就部署了Kudu节点呢？！哈哈哈！&emsp;&emsp;在实时数仓、实时计算和物联网蓬勃发展的今天，你确定不学一下Kudu吗？ 参考资料1.《Kudu:构建高性能实时数据分析存储系统》2.Apache Kudu - Fast Analytics on Fast Data3.Kudu专注于大规模数据快速读写，同时进行快速分析的利器4.Kudu基础入门5.Kudu、Hudi和Delta Lake的比较6.迟到的Kudu设计要点面面观7.迟到的Kudu设计要点面面观-前篇8.kudu-列式存储管理器-第四篇（原理篇）9.Kudu configuration reference","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Kudu","slug":"Kudu","permalink":"https://shmily-qjj.top/tags/Kudu/"},{"name":"实时","slug":"实时","permalink":"https://shmily-qjj.top/tags/%E5%AE%9E%E6%97%B6/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"基础数据结构与算法","slug":"基础算法学习","date":"2020-04-27T07:45:00.000Z","updated":"2020-06-01T10:32:27.836Z","comments":true,"path":"6a894937/","link":"","permalink":"https://shmily-qjj.top/6a894937/","excerpt":"","text":"前言算法与数据结构从现在开始重视还来得及！为何要重视算法与数据结构？ 算法能力代表了基本功水平，能代表一个程序员功底是否扎实。 算法和数据结构的思想以及时间复杂度空间复杂度的概念是提高工作效率，学习能力和成长潜力的重要途径。 算法能力是设计一个系统(造轮子)的重要基础。 下层基础决定上层建筑！算法与数据结构是我的薄弱项，需要多做总结！尽量对每个数据结构的经典题目做总结，尽量使用LeetCode原题来帮助理解。LeetCode主页先贴这里:Shmilyqjj的力扣主页，大家互相监督学习呦！ 基本概念学习算法有帮助的一些基本概念。 顺序存储与链式存储物理结构：是指数据的逻辑结构在计算机中的存储形式。逻辑结构：是指数据对象中数据元素之间的互相关系。 基础数据结构数据结构是算法的基石。要做到对算法的融会贯通和举一反三，熟悉各种数据结构是必备的。优秀的算法取决于采用哪种数据结构。重点：熟悉每种数据结构优缺点，应用场景，熟练掌握其思想并灵活运用。 数组和字符串特点：逻辑结构与物理结构一致优点：构建简单，索引某个元素的复杂度O(1)缺点：必须连续分配一段内存，查询过程遍历时间复杂度O(n)，删除过程时间复杂度O(n)场景：元素个数确定，经常按索引查询，不经常插入和删除 经典题目：翻转字符串字母异位词 链表分为单向链表和双向链表优点：能灵活分配内存空间，插入和删除元素效率高O(1)缺点：不能通过下标读取，读取第n个位置元素复杂度O(n)场景：元素个数不确定，经常插入和删除，不经常查询 经典题目：快慢指针-&gt;判断是否有环快慢指针-&gt;翻转链表快慢指针-&gt;寻找倒数第K元素快慢指针-&gt;找链表中间位置构建虚假链表头 栈后进先出(LIFO),可以采用单链表实现复杂度O(1)，虽然也可以用数组实现但时间复杂度可能较大场景：解决问题时只关心最近一次操作，解决完后要找之前的操作。 经典题目：匹配括号，判断是否有效 队列先进先出(FIFO),可以采用双链表实现。场景：按一定顺序处理过来的数据。 经典题目：广度优先搜索 双端队列和普通队列的区别是：可以在队头队尾都可以查看，添加和删除数据，复杂度都为O(1)，可以采用双链表实现。场景：实现长度动态变化的窗口或连续区间，动态窗口。 经典题目：返回滑动窗口最大值 树充分应用递归，树的问题基本都与递归有关面试常考：普通二叉树，平衡二叉树，完全二叉树，二叉搜索树，四叉树，多叉树需要了解：红黑树 ，AVL自平衡二叉搜索树 遍历方法：前序遍历：根-&gt;左子树-&gt;右子树中序遍历：左子树-&gt;根-&gt;右子树后序遍历：左子树-&gt;右子树-&gt;根深度优先遍历DFS：包含以上三种广度优先遍历BFS：即层次遍历，每一层从左向右输出 经典题目：二叉搜索树的第K个最小元素 如何学习数据结构与算法好的学习方法往往能起到事半功倍的效果，这里根据算法大佬们的学习经历总结一下算法学习方法，警示自己督促自己养成正确学习方法吧。 参考链接我是如何学习数据结构与算法的 斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://shmily-qjj.top/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"MySQL索引原理深入","slug":"MySQL索引原理深入","date":"2020-03-24T02:16:00.000Z","updated":"2020-05-04T12:53:02.526Z","comments":true,"path":"7c15e85/","link":"","permalink":"https://shmily-qjj.top/7c15e85/","excerpt":"","text":"MySQL索引原理深入索引可以大大提高Mysql检索速度，为什么能提高，怎么做到的？这些细节必须深入学习和分析，才能对技术运用了如指掌。今天来学习一下Mysql的索引原理与底层存储选型，为了能够对Mysql有更深入的了解。 索引定义索引是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息。索引相当于我们看书的目录。 优点1.快速检索数据2.保证数据记录唯一性3.实现表与表之间的参照完整性4.使用ORDER BY/GROUP BY子句进行数据检索时，利用索引可以减少排序和分组的时间 缺点1.索引需要占磁盘物理空间2.增删改操作时索引也要动态地维护，有性能开销3.创建索引耗时，数据量越大耗时也越大 分类1.普通索引：无唯一性限制2.唯一索引：UNIQUE，有唯一性限制3.主键索引：唯一索引的特殊类型，在主键上创建索引4.候选索引：唯一性，切决定记录的处理顺序5.聚集索引：Clustered Index聚簇索引，索引列的键值的物理顺序与逻辑顺序相同6.非聚集索引：Non-Clustered Index非聚簇索引，索引列的键值的物理顺序与逻辑顺序无关7.全文索引：主要针对文本的内容进行分词，加快查询速度8.联合索引：多列组成的索引，查询效率提升高于多个单列索引合并的效率 应用场景1.在经常搜索的列上创建索引2.在主键上创建索引3.在用来JOIN的列上创建索引4.在经常通过WHERE根据范围检索的列上创建索引5.在经常GROUP BY/ORDER BY的列上创建索引6.在经常DISTINCT的列上创建索引 索引字段要求1.列值的唯一性太小不适合建索引2.列值太长不适合建索引3.更新频繁的列不适合建索引 索引失效1.LIKE的使用（LIKE XXX%可以用索引，但LIKE %xxx不能）2.部分操作符（&lt;,&lt;=,=,&gt;,&gt;=,BETWEEN,IN可以用索引，但&lt;&gt;,not in,!=不能）3.判空操作（is null或is not null）4.int类型字段（如手机号没用varchar存，查186开头的，不能）5.联合索引（设置了col1和col2两个字段联合索引，WHERE col1=’xxx’或WHERE col1=’xxx’ AND col2=’xxx’或WHERE col2=’xxx’ AND col1=’xxx’都可用索引，但WHERE col2=’xxx’不能）6.对索引列操作（计算、函数、自动类型转换、手动类型转换都会使索引失效）7.SELECT *（尽量使用覆盖索引，尽量取用到的字段值而非使用星号,这样WHERE的时候覆盖索引效率高）8.字符串不加单引号引起索引失效9.尽量避免索引列有null值 Mysql索引数据结构选型该部分会详细说明：索引数据结构的选型过程以及各自的优缺点B树与B+树的区别为什么以B+树作为Mysql的索引数据结构Innodb引擎和MyISAM引擎的区别以及索引实现和存储区别聚簇索引与非聚簇索引区别 数据结构选型过程过程：哈希表-&gt;二叉查找树-&gt;红黑树-&gt;二叉平衡树AVL-&gt;B树-&gt;B+树 哈希表哈希算法把任意的key变换成固定长度的key地址。性能不错，但是有哈希冲突问题，一般用链地址法来解决（类似HashMap）。这样查数据的时候先计算Hash，然后遍历链表，直到拿到key。时间复杂度O(1),看来很理想。但是为什么没用哈希？如果用哈希表做索引的数据结构，select * from tb where id &gt; 1这样的范围查找场景，就要把索引数据全部加载到内存再筛选，太慢。虽然Hash做索引的数据结构可以快速定位key，但没法做到高效的范围查找。ps:当然，如果业务是经常使用where条件单条查询数据，Hash索引效率更高复杂度O(1)。使用Hash索引，InnoDB和MyISAM不支持，可用MEMORY，NDB引擎。 二叉查找树BinarySearchTree（BST）是支持数据快速查找的数据结构，复杂度O(log2n)~O(n)之间,也可以高速检索数据能不能解决范围查找呢？能！比如我找id &gt; 3，我只要找到比它大的根节点和右子树即可。那为啥二叉查找树不能做索引的数据结构？因为如果二叉排序树是平衡的，则n个节点的二叉排序树的高度为log2(n+1),其查找效率为O(log2n)，近似于折半查找。如果二叉排序树完全不平衡，则其深度可达到n，查找效率为O(n)，退化为顺序查找。而数据库中经常有以自增id为主键索引的场景，必然会线性查找，性能太低。 红黑树通过自动调整树形态让二叉树保持基本平衡，复杂度O(logn)，因为基本平衡，查询效率不会明显降低，不存在O(n)的情况。吃瓜群众：那就用这个做索引数据结构吧！万万不可用红黑树做索引的数据结构！还是自增id为主键索引的情况，如果红黑树按顺序插入数据，整个红黑树会明显右倾，查询效率会明显降低。像我这种数据结构渣渣，送自己一个宝贝：红黑树算法图形模拟 AVL树AVL树，也是通过调整形态保持二叉树平衡，它虽然在调整形态时会有更多性能开销，但它绝对平衡。它能根本解决红黑树的右倾问题。复杂度O(logn)。那AVL树这么好，为啥还是不能用于索引数据结构？AVL树每个节点只能存一个数据，每次比较只能加载一个数据到内存，查询较深的AVL树节点，就要有多次的磁盘IO开销，磁盘IO是数据库瓶颈，这样肯定不合理的呀！对磁盘IO的优化方案就是一次尽可能多读或者多写数据，读1b和读1kb速度基本一样的，希望磁盘能一次加载更多数据进内存，这就是B树，B+树原理了。 B树与B+树B树：又名多路查找树，特点：①节点数据递增，遵循左小右大②M阶B树，每个节点最多可以有M个子节点③根节点至少有两个儿子④除根结点之外的所有非叶子结点至少有ceil(M/2)个子节点(ceil(2.1) = 3)⑤所有叶子节点都在同一层次B树代替AVL树：让每个节点存的key数目适当增加，即增加M（B树的阶数），磁盘读取次数大大降低，尽可能减少磁盘IO，加快检索速度，还能支持范围查找。B+树：与B树区别：一是B树每个节点（包括非叶子节点）都存数据，B+树非叶子节点有索引作用，而数据存在叶子节点 –&gt; B树的每个节点存不了太多数据，B+树每个叶子节点能存很多索引（地址）。所以B+树高度更低，减少了磁盘IO。二是B树节点之间没索引，B+相邻叶子节点之间有索引指针 –&gt; WHERE范围查询性能很好。三是B+树的查询效率更稳定，因为数据都存在叶子节点，查数据的操作次数相同。 综上，Mysql基于B+树实现的索引。 InnoDB与MyISAM的区别 InnoDB MyISAM 默认支持ACID 不支持ACID 支持外键 不支持外键 性能好 性能好于Innodb 必须有主键 可没有主键 数据与索引存放在一起 数据与索引分开存放 聚集索引方式 非聚集索引方式 支持表级、行级(默认)锁 支持表级锁 崩溃易恢复 崩溃难恢复 聚集索引与非聚集索引(InnoDB与MyISAM实现索引的区别)mysql&gt; show global variables like &quot;%datadir%&quot;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------+ mysql&gt; create table innodb_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.02 sec) mysql&gt; create table myisam_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=myisam DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.00 sec) ll /var/lib/mysql/db_name/ 总用量 128 -rw-rw----. 1 mysql mysql 65 3月 24 04:30 db.opt -rw-rw----. 1 mysql mysql 8586 3月 24 04:31 innodb_table.frm -rw-rw----. 1 mysql mysql 98304 3月 24 04:31 innodb_table.ibd -rw-rw----. 1 mysql mysql 8586 3月 24 04:33 myisam_table.frm -rw-rw----. 1 mysql mysql 0 3月 24 04:33 myisam_table.MYD -rw-rw----. 1 mysql mysql 4096 3月 24 04:33 myisam_table.MYI 从建表后生成的文件可看出，InnoDB生成frm(建表语句)和ibd(数据+索引)，而MyISAM生成frm(建表语句),MYD(数据文件)和MYI(索引文件)。 MyISAM引擎把数据和索引分开成数据文件和索引文件两个文件，这叫做非聚集索引方式。Innodb 引擎把数据和索引放在同一个文件里了，这叫做聚集索引方式。 更详细一点的解释：聚集索引物理顺序与逻辑顺序一致，非聚集索引的物理顺序与逻辑顺序不一致。非聚集索引的叶子节点存key和对应地址，不存数据；聚集索引的叶子节点存key和对应的value，value内存地址是连续的 为什么MyISAM比InnoDB快？上面我们已经提到InnoDB使用聚集索引，MyISAM使用非聚集索引。两种引擎的数据组织方式不同。如图是两种引擎组织数据的方式查询时InnoDB需要通过主键索引树先拿到主键值后再去辅助索引树拿到整条记录（建表的时候InnoDB就会自动建立好主键索引树），而MyISAM拿到数据的索引即可直接以Offset形式直接在数据文件中定位数据。而且InnoDB因为支持ACID，还要检查MVCC多版本并发控制，而MyISAM不支持事务，也是其快的原因。还有MyISAM维护了一个保存整表行数的变量，count(*)很快。 其他数据库索引数据结构该部分会详细说明为什么Mysql用B+树索引而MongoDB用B树。1.MongoDB本身很少有范围搜索操作，做单一查询比较多2.Mysql关系型数据库，做范围检索的操作很多，如join，where x&gt;1等，B+树叶子节点有指针，遍历效率高 参考资料下面列出参考资料，我认为写得好的已加粗深入理解 Mysql 索引底层原理为什么Mongodb索引用B树，而Mysql用B+树?Mysql—索引失效磁盘IO概念及优化入门知识MVCC多版本并发控制Mysql索引Mysql索引必须了解的几个重要问题 其他细节1.ORDER BY与索引失效：当order by的字段出现在where条件中时，才会利用索引而不排序，更准确的说，order by中的字段在执行计划中利用了索引时，不用排序操作。这个结论不仅对order by有效，对其他需要排序的操作也有效。比如group by 、union 、distinct等。（出现在Order by 后的索引列都是用于排序的，不会用于查找，所以索引无效）2.innodb引擎的4大特性：插入缓冲（insert buffer),二次写(double write),自适应哈希索引(ahi),预读(read ahead)3.主键和唯一索引的区别：①唯一索引列允许空值，主键不允许空值 ②主键可以被其他表引用为外键，唯一索引不能 ③一个表只能一个主键但可有多个唯一索引","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"系统学习JVM","slug":"系统学习JVM","date":"2020-03-21T04:19:00.000Z","updated":"2020-06-01T10:32:27.837Z","comments":true,"path":"508b5c7/","link":"","permalink":"https://shmily-qjj.top/508b5c7/","excerpt":"","text":"系统学习JVMJava跨平台，一次编译到处运行，垃圾回收等特性离不开JVM，学习JVM的原理可以让我们在工作中更快速定位问题。写这篇的目的就是避免零零散散地学习JVM，那样效率很低，也方便以后回顾和复习。 字节码学习之前先要学会简单分析字节码。用户Java代码与JVM交互沟通的桥梁。代码编译为.class字节码给JVM运行。 $ javac Hello.java $ javap -c Hello.class # javap可查看字节码的操作数 $ javap -p -v Hello # -p打印私有字段和方法 -v尽量多打印一些信息 当在java代码中添加一些注释信息后，.class的MD5不一样了。因为javac可以指定输出一些额外内容到.class javac -g:lines 强制生成LineNumberTable | javac -g:vars 强制生成LocalVariableTable | javac -g 生成所有debug信息 当然如果使用IDEA，可以使用jclasslib Bytecode viewerb插件（插件商店搜索即可） JVM的程序运行是在栈上完成的，运行main方法自动分配一个栈帧，退出方法体时候再弹出相应栈帧。从javap得到的结果看，大多数字节码指令是不断操作栈帧。整个过程：Java 文件-&gt;编译器-&gt;字节码-&gt;JVM-&gt;机器码整个过程：Hello.java -&gt; Hello.class -&gt; Java类加载器(JVM中) -&gt; 执行引擎(JVM中) -&gt; 通过操作系统接口解释执行+JIT 如下有两段代码：我们可以通过字节码文件判断它们的执行结果 public class A{ # 第一段 static int a = 0; static { a = 1; b = 1; } static int b = 0; public static void main(String[] args) { System.out.println(a); System.out.println(b); } } //执行结果：1 0 //字节码如下： 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: iconst_0 13: putstatic #5 // Field b:I 16: return -------------------------------------------------------------------------------------------------- public class A{ # 第二段 static int a = 0; static { a = 1; b = 1; } static int b; public static void main(String[] args) { System.out.println(a); System.out.println(b); } } //执行结果：1 1 //字节码如下： 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: return 其他信息：stack=1, locals=0, args_size=0中stack表示该方法最大操作数栈深度为4，JVM根据这个分配栈帧中操作栈深度，locals变量存储了局部变量的存储空间，单位是Slot(槽)，args_size指方法参数个数其他字节码指令表可参照：https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html JVM定义JVM（JAVA虚拟机）是一个规范，定义了.class文件的结构，加载机制，数据存储，运行时栈等内容。JDK8以后Java是编译与解释混合执行模式。JDK8以后JVM的技术实现是HotSpot(包含一个解释器和两个编译器)。两个编译器：可以动态编译，含server模式和client模式。 client模式是一种轻量级编译器，也叫C1编译器，占用内存小，启动快，但是执行效率没有server模式高，默认状态下不进行动态编译，适用于桌面应用程序。 server模式是一种重量级编译器，也叫C2编译器，启动慢，占用内存大，执行效率高，默认是开启动态编译的，适合服务器应用。 -XX:RewriteFrequentPairs 用于开启动态编译。 -Xint:禁用JIT编译，UYZNGSUYZNGS即禁用两个编译器，纯解释执行。 -Xcomp:纯编译执行，如果方法无法编译，则回退到解释执行模式解释无法编译的代码。 内存管理 JVM内存区域如何划分？Java内存布局一直在调整，Java8开始彻底移除了持久代，使用MetaSpace(元空间)来代替。 =&gt; -XX:PermSize和-XX:MaxPermSize失效 Java的运行时数据区可以分成堆、元空间(含方法区)、虚拟机栈、本地方法栈和程序计数器 堆：存放绝大多数Java对象，是JVM中最大的一块内存，随着频繁创建对象，堆空间占用越来越大，需要不定期的GC。（JVM主要GC区域：堆和元空间）。是线程共享的。 对象是否被分配在堆中取决于对象的基本类型和Java类中存在的位置： 基本数据类型（byte,short,int,long,float,double,char）如果在方法体内声明则在栈上(栈帧的局部变量表)直接分配，其他情况在堆上分配。 int[]这样的数组类型不属于基本数据类型，在堆上分配。 栈：分虚拟机栈和本地方法栈。 虚拟机栈：Java中每个方法被调用时都会创建一个栈帧，执行完后再出栈，所有栈帧都出栈后线程结束。每一个方法对应一个栈帧，每一个线程对应一个栈。栈帧中包括：局部变量表，操作数，动态链接，返回地址，这些不是线程共享的。 本地方法栈：与虚拟机栈相似，但它主要包含Native对象。本地方法栈有一个叫returnAddress的数据类型。 元空间：存放类名与字段(类的元数据)，运行时常量池，JIT优化。 先对比一下JDK8和以前版本的方法区 Perm区(永久代)在JDK8废除，用元空间来取代。好处：元空间的出现解决了类和类加载器元数据过多导致的OOM问题，它是非堆区，使用操作系统内存，不会出现方法区内存溢出，省去了GC扫描压缩的开销，每个加载器有专门的存储空间；坏处：无限制使用操作系统内存会导致操作系统崩溃，所以一般要加-XX:MaxMetaspaceSize参数来控制大小。元空间不支持压缩，有内存碎片问题。 方法区：包含在元空间中。方法区存储：类信息、静态（static）变量，常量（final），编译后的代码等数据。是线程共享的。 元空间内存管理由元空间虚拟机完成。 程序计数器：[JVM中唯一不会OOM的区域]在多线程切换的情况下，Java通过程序计数器来记录字节码执行到什么地方，这样能保证切换回来时能够从原来的地方继续执行。（相当于字节码的行号指示器）。程序计数器实现了异常处理，跳转，循环分支的功能。因为每个线程都有其独立的程序计数器，所以是线程私有的。 JVM类加载机制类加载过程：加载-&gt;验证-&gt;准备-&gt;解析-&gt;初始化 大多数情况按这个流程加载。加载：将类的同名.class文件加载到方法区验证：检查.class是否合规。如果.class不合规，抛异常。如果任何.class都能加载就不安全了。准备：为一些类变量分配内存，并初始化为默认值。此时，实例对象还没有分配内存，所以这些动作是在方法区上进行的。 类加载的准备阶段会给类变量分配内存和初始化默认值。所以下面这段，我们不手动给a赋值也能编译通过。 public class test_java { static int a; public static void main(String[] args) { System.out.println(a); // output:0 } } 类变量有两个阶段可以被赋值，一是类加载准备阶段，二是初始化阶段。而局部变量只有一次初始化，如果没赋初值，不能使用，下面代码编译不通过。 public class test_java { public static void main(String[] args) { int a; System.out.println(a); } } 解析：保证引用的完整性。做了：类或接口解析，类方法解析，接口方法解析，字段解析。 这个阶段相关的报错信息： java.lang.NoSuchFieldError 根据继承关系从上往下没找到相关字段时报错 java.lang.IllegalAccessError 不具备访问权限时报错 java.lang.NoSuchMethodError 找不到相关方法时报错 初始化：初始化成员变量，这一步才开始执行字节码。 public class A { static{ System.out.println(1); } public A(){ System.out.println(&quot;A&quot;); } public static void main(String[] args) { A ab = new B(); ab = new B(); } } class B extends A{ static { System.out.println(&quot;2&quot;); } public B(){ System.out.println(&quot;B&quot;); } //执行结果: 1 2 A B A B 原因:初始化子类先调用父类无参构造，static在类加载的准备阶段执行一次，不重复执行。 //static只会执行一次，对应cint方法 //对象初始化调用构造方法，每次新建对象都会执行，对应init方法 如果你自己写一个java.lang包，改写了String类，编译后发现没起作用。JRE不能被轻易篡改，否则可能会有安全问题。这就是类加载机制在起作用。类加载机制流程： 双亲委派机制：当某个类加载器需要加载某个.class文件时，它首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，才会去真正加载这个类。比如Object类，毫无疑问会交给最上层的类加载器加载，保证只有一个被加载的Object类。如果没有双亲委派机制，会有多个Object类，很混乱。类加载器运行有先后顺序的，下面是类加载器的种类： BootstrapClassLoader（启动类加载器）：c++编写，加载java核心库 java.*,构造ExtClassLoader和AppClassLoader。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作 ExtentionClassLoader （标准扩展类加载器）：java编写，加载扩展库，如classpath中的jre(lib/ext下jar包和.class)，javax.*和java.ext.dirs指定位置中的类，开发者可以直接使用标准扩展类加载器。 AppClassLoader（系统类加载器）：java编写，加载程序所在的目录，classpath位置下其他所有jar和.class。我们写的代码最先尝试使用这个进行加载，再通过双亲委派机制递归委托上级类加载器。 CustomClassLoader（用户自定义类加载器）：java编写,用户自定义的类加载器,可加载指定路径的class文件。支持自定义扩展功能。 双亲委派机制作用：1、防止一个.class被重复加载，一个一个去上面问，加载过了就不加载了。保证数据安全。2、保证核心.class不被篡改，即使篡改也不会加载，即使加载也不会是同一个.class对象。（不同的类加载器加载同一个.class得到的是不同的对象）。保证.class执行没问题。 可以覆盖HashMap类的实现吗？可以，用到Java的endorsed技术，我们可以把自己的HashMap类打成jar放在-Djava.endorsed.dirs指定的目录，类名和包名应该与jdk原生的一致。这个目录下的jar会被优先加载，比rt.jar优先级更高。 哪些地方打破了Java的类加载机制?举例子：1.tomcat使用war包发布应用，由WebAppClassLoader类加载器优先加载，它加载自己目录的.class但不传递给父类加载器，但它可以通过SharedClassLoader实现共享和分离。2.Java的SPI机制，例子：Mysql的JDBC。使用JDBC Driver前使用Class.forName(“com.mysql.jdbc.driver)，但如果删除这行代码也能正确加载到驱动类，因为使用ServiceLoader来动态装载。 如何加载远程.class文件，怎么加密.class文件？通过实现一个新的自定义类加载器。 JVM的GCGC Roots：可达性分析法，是GC实现的一种方法(另一种是引用计数法)，GC Roots是一组活跃的引用，程序在接下来的运行中能直接或间接引用或能被引用的对象。从GC Roots不断向下追溯遍历，会产生Reference Chain引用链。GC Roots遍历过程是找出所有活对象，并把其余空间认定为无用，而不是找到死对象。如果一个对象连续两次遍历过程中跟GC Roots没有任何直接或间接引用，则会被GC掉。GC Roots包括： 活动线程相关的各种引用 类的静态变量的引用 JNI引用 GC Roots是引用不是对象引用级别（引用链的表现）： 强引用：[有用且必须]内存不足直到抛OOM，这种强引用的对象也不会被回收。 - 容易造成内存泄露(比如一个User类没有字段info，用HashMap&lt;User,String&gt;存，用完User但因为被HashMap使用而未能回收，就造成内存泄露) 软引用：[有用非必须]维护一些可有可无的对象，内存足够的时候不会被回收，内存不足会回收。如果回收了软引用对象后内存还不够则抛出OOM。 弱引用：[可能有用非必须]引用的对象相比软引用，要更加无用一些，生命周期更短。GC时无论内存是否充足都会回收弱引用关联的对象。 虚引用：[无用]形同虚设的引用，任何时候都可被回收。 //强引用 Shmily shmily = new Shmily(); //软引用 SoftReference&lt;Shmily&gt; softReference = new SoftReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = softReference.get(); //弱引用 WeakReference&lt;Shmily&gt; weakReference = new WeakReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = weakReference.get(); //虚引用 虚引用的使用必须和引用队列（Reference Queue）联合使用 ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(new Shmily(), referenceQueue); Shmily shmily = phantomReference.get(); //所有以上对象出了强引用之外，一旦被回收，get方法返回null。 //以上创建软引用，弱引用的对象softReference和weakReference还都属于强引用，用完也需要回收避免内存溢出，方法如下： ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(softReference, referenceQueue); 可能发生OOM的内存区域：除了程序计数器，都有可能。但主要是发生在堆上。OOM发生原因： 内存不足需要扩容。 错误的引用方式，没有及时切断GC Roots的引用，导致内存泄漏。(典型) 没有进行数据范围检查，比如全量查询了某个数据库。 无限制无节制使用MemoryOverHead JVM的垃圾回收算法GC的标记过程：从GC Roots遍历所有可达的活跃对象并标记。GC触发条件：1.老年代不足 2.调用了System.gc() 3.通过MinorGC进入老年代的对象大小总和大于老年代的大小（担保失败） 4.Eden区不够存放新创建的对象GC算法： 标记清除算法：标记-标记已用对象，清除-清除未被标记的对象。 缺点：产生内存碎片 场景：适合在收集频率低的老年代使用 复制算法：内存空间分等大两块，一块满了，未被标记的对象复制到另一块。 优点：解决了内存碎片问题，效率最高 缺点：会有一半的内存空间浪费 场景：适合收集频率高且追求收集效率的年轻代使用 标记整理算法：移动所有存活的对象，且按内存地址顺序依次排列，然后将末端内存全部回收。 优点：解决了内存碎片问题，同时解决标记复制算法的内存空间浪费问题 缺点：效率低于复制算法和标记清除算法 场景：适合在收集频率低的老年代使用 JVM采用分代收集算法，对不同的区域采用不用的收集算法。 GC种类： MinorGC 发生在年轻代的GC触发条件：Eden区不够存放新创建的对象 MajorGC 发生在老年代的GC 与FullGC区别是只清理老年代而不清理年轻代触发条件：① FullGC 全堆垃圾回收（如元空间引起的年轻代和老年代回收）触发条件：①调用System.gc ②老年代空间不足(可能无足够连续空间) ③担保机制失败(Eden大对象无法存入老年代，因为检测到老年代无足够连续内存空间) ④Minor GC后进入老年代的平均大小大于老年代可用内存 Mixed GC[G1收集器特有] 收集整个YoungGeneration和部分OldGeneration Java的大部分对象生命周期都不长，它们位于年轻代(Young Generation)，而生命周期较长的位于老年代(Old Generation)。 年轻代的GC：年轻代使用复制算法，因为年轻代大部分对象生命周期短，如果发生GC只会有少量对象存活，复制这部分对象是高效的。年轻代分为Eden:From Survivor:To Survivor = 8:1:1三个空间。对象首先在Eden区，如果Eden区满了就会触发MinorGC。单数次MinorGC:在MinorGC后，存活的对象进入Form Survivor区。双数次MinorGC，Eden和From Survivor区一起清理，存活对象被复制到To区，并清空From区。从上面可以得知每次GC都有一个Survivor区空闲，由于Eden:From Survivor:To Survivor = 8:1:1，年轻代GC复制算法只浪费了10%的内存空间，同时做到了高效，无碎片和节约空间。扩展：TLAB(Thread Local Allocation Buffer)，是JVM给每个线程单独开辟的区域，用来加速对象分配。在Eden区分多个TLAB，TLAB通常比较小，对象优先分配在TLAB上，对象较大才会在Eden区分配。TLAB是一种优化，类似于逃逸分析的对象在栈上分配的优化。老年代的GC：老年代一般使用标记整理和标记清除算法。因为老年代很多对象存活率高，占用较大，不方便复制。对象怎么进入老年代：1. 达到一定年龄每次发生MinorGC，对象年龄加1，达到阈值(最大值是15可通过‐XX:+MaxTenuringThreshold调)，进入老年代。2. 分配担保机制因为Survivor区只占年轻代10%的空间，发生MinorGC时无法保证每次Eden+其中一个Survivor存活的对象大小都小于另一个Survivor区空间，通过分配担保机制，另一个Survivor区放不下的对象直接进入老年代。JVM每次MinorGC前会检查老年代最大可用连续内存空间是否大于新生代对象的总空间，如果是的话确保MinorGC是安全的。3. 大对象直接进入老年代超过一定大小的对象直接进入老年代。(通过-XX:PretenureSizeThreshold设置，默认0表示都要先走年轻代)4. 动态年龄判定为了使内存分配更灵活，JVM不一定要求对象年龄达到MaxTenuringThreshold(15)才晋升为老年代，若Survivor区相同年龄对象总大小大于Survivor区空间的一半，则大于等于这个年龄的对象将会在MinorGC时移到老年代。JVM常见垃圾回收器：如果垃圾收集算法是JVM垃圾回收的方法论，那垃圾回收器就是上述算法的实现。 年轻代垃圾回收器 1. Serial垃圾回收器 单线程的垃圾回收器，垃圾回收时暂停一切用户线程，使用复制算法。 优点：简单轻量级，使用资源少。 场景：用于客户端应用，因为客户端应用不会频繁创建对象。 2. ParNew垃圾回收器 Serial回收器的多线程版本，多条GC线程并行回收，垃圾回收时仍暂停一切用户线程 优点：多CPU环境下收集效率高些，GC停顿时间缩短。 场景：多CPU场景下使用，ParNew适合交互多计算少的场景。 3. Parallel Scavenge垃圾回收器 多线程回收器 场景：多CPU下使用，追求CPU吞吐量，适用于交互少计算多的场景。 老年代垃圾回收器 1. Serial Old垃圾回收器 与年轻代的Serial垃圾回收器对应，也是单线程，使用标记-整理算法。 优点：简单轻量级，使用资源少。 场景：也适用于客户端应用 2. Parallel Old垃圾回收器 Parallel Scavenge垃圾回收器的老年代版本。 场景：多CPU下使用，追求CPU吞吐量，适用于交互少计算多的场景。 3.CMS垃圾回收器 以最短GC时间为目标，用户线程与GC线程可并发执行，垃圾回收过程用户不会感到明显卡顿。 长期来看G1、ZGC等更高级的垃圾回收器是趋势。 CMS垃圾回收器 全称：Mostly Concurrent Mark and Sweep Garbage Collector（主要并发­标记­清除­垃圾收集器） CMS在年轻代使用复制算法，在老年代使用标记-清除算法。它把耗时的GC操作通过多线程并发执行的。 优点：避免老年代GC出现长时间卡顿 缺点：对老年代的回收没整理阶段，产生内存碎片随时间推移增多时必须要FullGC才能清理。可能会导致大对象创建失败。 场景：不希望GC停顿时间长且CPU资源较充足 回收过程：1.初始标记阶段：只标记GC Roots直接关联的对象和年轻代中的引用，不向下追溯，缩短了标记时GC暂停时间。2.并发标记阶段，并发地追溯可达对象，持续时间较长但跟用户线程并行执行。3.并发预清理，这个过程会清理dirty状态的老年代对象。4.可选的预清理。5.最终标记，会GC暂停。6.并发清理，用户线程重新激活，删除不可达对象。 关于CMS的碎片整理问题：两个参数 UseCMSCompactAtFullCollection（默认开启）：FullGC时压缩，整理内存碎片，会造成较长时间停顿。 CMSFullGCsBeforeCompaction：每隔几次FullGC后执行一次带压缩的FullGC。 总结CMS中有哪些会造成STW(GC停顿)的操作： STW = stop the world. 初始标记阶段-较短停顿 最终标记阶段-较短停顿 老年代的回收-较长停顿 Full GC阶段-较长停顿 G1垃圾回收器 全称：Garbage First（尽可能多地收集垃圾以减少FullGC） 目前比较好的收集器，关注低延迟，用于替代CMS的功能更强大的新型收集器。 引入了分区概念，弱化了分带概念 优点：避免老年代GC出现长时间卡顿，同时与CMS相比解决了CMS产生碎片的缺陷。 缺点： 场景：不希望GC停顿时间长且CPU资源较充足 回收过程： 关于CMS的碎片整理问题：两个参数 总结G1中有哪些会造成STW(GC停顿)的操作： STW = stop the world. GC小技巧 GC日志查看 加-XX:+PrintGCDetails参数 查看GC日志，有关GC日志的解析后续我会单写一个博客。 使用Sun公司的gchisto，gcviewer离线分析工具 使用JDK自带的JConsole 使用jstat -gcutil pid命令 使用JvisualVM工具 查看当前Java版本垃圾回收信息 $ java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=266248768 -XX:MaxHeapSize=4259980288 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC java version &quot;1.8.0_191&quot; Java(TM) SE Runtime Environment (build 1.8.0_191-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) 设置应用的垃圾回收器： -XX:+UseSerialGC 年轻代和老年代都用串行收集器 -XX:+UseParNewGC 年轻代使用 ParNew，老年代使用 Serial Old [JDK9被抛弃] -XX:+UseParallelGC 年轻代使用 ParallerGC，老年代使用 Serial Old -XX:+UseParallelOldGC 新生代和老年代都使用并行收集器 -XX:+UseConcMarkSweepGC，表示年轻代使用 ParNew，老年代的用 CMS -XX:+UseG1GC 使用 G1垃圾回收器 -XX:+UseZGC 使用 ZGC 垃圾回收器 常量池分静态常量池和运行时常量池，静态常量池在 .class 中，运行时常量池在方法区中。字符串池在JDK 1.7 之后被分离到堆区。String str = new String(“Hello world”) 创建了 2 个对象，一个驻留在字符串池，一个分配在 Java 堆，str 指向堆上的实例。String.intern() 能在运行时向字符串池添加常量。为什么String为final：1.为了实现字符串池：创建字符串常量时，JVM会检测字符串常量池，如果已存在，直接返回常量池中的实例的引用，如果不存在就实例化并放入字符串常量池。因为String为Final类型，我们可以十分肯定字符串常量池不存在两个相同的字符串。2.为了线程安全：因为它不可变，本身就是线程安全的3.节约内存4.HashMap的key往往用String是因为String不可变，在被创建时HashCode就被缓存了不需要重新计算。 GC是怎么判断对象是被标记的？通过枚举根节点的方式，通过jvm提供的一种oopMap的数据结构，简单来说就是不要再通过去遍历内存里的东西，而是通过OOPMap的数据结构去记录该记录的信息,比如说它可以不用去遍历整个栈，而是扫描栈上面引用的信息并记录下来。总结:通过OOPMap把栈上代表引用的位置全部记录下来，避免全栈扫描，加快枚举根节点的速度，除此之外还有一个极为重要的作用，可以帮HotSpot实现准确式GC【这边的准确关键就是类型，可以根据给定位置的某块数据知道它的准确类型，HotSpot是通过oopMap外部记录下这些信息，存成映射表一样的东西】。 CMS收集器是否会扫描年轻代？会，在初始标记的时候会扫描新生代。虽然cms是老年代收集器，但是我们知道年轻代的对象是可以晋升为老年代的，为了空间分配担保，还是有必要去扫描年轻代。 小标题1小标题2原理（中标题） 字体斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本 字颜色大小 This is some text! This is some text! This is some text! 一些常用Java命令参考资料Java双亲委派机制及其作用拉勾网MetaSpace整体介绍深入理解JMM和GC","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://shmily-qjj.top/tags/Java/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"线程进程与锁","slug":"线程进程与锁","date":"2020-02-11T03:20:08.000Z","updated":"2020-05-04T12:53:02.530Z","comments":true,"path":"6f97dc89/","link":"","permalink":"https://shmily-qjj.top/6f97dc89/","excerpt":"","text":"线程进程与锁线程，进程与锁是一定要掌握的基础知识点，希望能通过写博客的方式加深印象，并且在以后能够随时补充和回看。 进程概念1.什么是进程进程是可并发执行的程序在某个数据集合上的一次计算活动，也是操作系统进行资源分配和调度的基本单位。 2.进程的三种基本状态运行态：当进程得到处理机，其执行程序正在处理机上运行时的状态称为运行状态。就绪态：当一个进程已经准备就绪，一旦得到CPU，就可立即运行，这时进程所处的状态称为就绪状态。阻塞态：若一个进程正等待着某一事件发生(如等待输入输出操作的完成)而暂时停止执行的状态称为等待状态。处于等待状态的进程不具备运行的条件，即使给它CPU，也无法执行。系统中有几个等待进程队列（按等待的事件组成相应的等待队列）。 进程的五种状态：新建-就绪-运行-阻塞-死亡就绪-&gt;运行:等待cpu调度运行-&gt;阻塞:放弃cpu时间片/IO请求运行-&gt;死亡:run运行完或抛异常 3.进程状态切换过程运行到等待：等待某事件的发生（如等待I/O完成）等待到就绪：事件已经发生（如I/O完成）运行到就绪：时间片到（例如，两节课时间到，下课）或出现更高优先级进程，当前进程被迫让出处理器。就绪到运行：当处理机空闭时，由调度（分派）程序从就绪进程队列中选择一个进程占用CPU。 4.并发与并行区别？并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。 线程概念1.什么是线程线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 2.线程与进程的关系 进程是系统资源分配的基本单位-线程是任务调度执行的基本单位 进程切换开销大-线程间切换开销小 进程间资源和地址空间独立-同一进程下线程共享资源和地址空间 进程崩溃不影响其他进程-线程崩溃整个进程都崩溃（多进程健壮性强） 3.线程间共享的资源 线程间共享堆和方法区 虚拟机栈、本地方法栈、程序计数器不共享 为什么程序计数器资源不共享：程序计数器不共享是为了线程切换后能恢复到正确的执行位置 为什么虚拟机栈和本地方法栈私有：栈帧用于存储局部变量表、操作数栈、常量池引用等信息，而本地方法栈则为虚拟机使用到的Native方法服务，两者相似。为了保证局部变量不被其他线程访问，所以不共享。 为什么堆和方法区共享：新创建的对象存放在堆中，类信息、常量、静态变量、即时编译器编译后的代码存放在方法区。 多线程不一定提高效率，只是让CPU利用率更高，如果频繁切换可能效率反而更低(见第7条解释)。 4.线程间通信方式 全局变量 线程上下文 共享内存 套接字Socket IPC通信如何实现线程间通讯：1.通过类变量直接将数据放到主存中 2.通过并发的数据结构来存储数据 3.使用volatile变量或者锁 4.调用atomic类 5.进程间通信方式 管道 有名管道 信号量 共享内存 消息队列 信号 套接字 6.守护线程？一种后台特殊进程，所有用户的线程都退出了，没有被守护的对象了，也不需要守护线程了，这时守护线程关闭。（JVM退出了它会关闭）比如GC线程就是守护线程。用户线程可以转为守护线程：thread.setDaemon(true); 7.线程切换开销中断处理，多任务处理，用户态切换等原因导致CPU从一个线程切换到另一个线程。线程上下文切换代价是高昂的，上下文切换的延迟由很多因素决定，平均要50-100ns，而CPU每核心每ns执行十几条指令，切换的过程就花费几百至几千条指令的执行时间。如果跨核上下文切换，代价更加高昂。 7.线程的几种状态？ 新建(NEW):线程被创建出来但还未启动就绪(RUNNABLE):调用start()后，线程准备就绪，等到CPU分配资源就可以运行阻塞(BLOCKED):线程处于活跃状态，等待Monitor监视器锁，等待线程同步锁等待(WAITING):等待另一个线程执行，如调用了wait(),就要等另一个线程notify()或notifyAll()才唤醒计时等待(TIMED_WAITING):调用了wait(timeout)或join(timeout)指定了超时时间终止(TERMINATED):线程执行完毕 终止并释放资源 总结： 线程池 为什么要用线程池？ 频繁创建和销毁线程费时低效而且浪费内存(线程死亡，相关对象变成垃圾)。线程池尽可能减少了对象创建和销毁的次数，让线程运行完不立即销毁，而是重复使用，从而提高效率。 ThreadPoolExecutor是线程池的核心类，构造参数的意义： corePoolSize：常驻核心线程数，如果为0且池中无线程执行时自动销毁线程池。如果大于0，线程执行完毕后不会销毁线程，而是进入缓存队列等待再次被运行。这个参数设置过小会频繁创建销毁线程，过大会浪费系统资源。 maximunPoolSize：线程池能创建最大的线程数量。如果常驻核心线程数和缓存队列都已经满了，新的任务进来就会创建新的线程来执行。但是数量不能超过maximunPoolSize，否侧会采取拒绝接受任务策略。 keepAliveTime：线程存活时间，未执行的线程空闲超过该时间则终止。多余线程会销毁直到线程数量达到corePoolSize。如果corePoolSize等于MaximumPoolSize，超过空闲时间也不会销毁任何线程。 unit：存活时间单位，和keepAliveTime配合使用。 workQueue：线程池执行的任务队列（缓存队）列，用来存放等待被执行的任务。 threadFactory：线程工厂，用来创建线程，一般有三种选择策略。（一般用默认即可） ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; handler：拒绝处理策略，线程数量大于最大线程数就会采用拒绝处理策略，四种策略为 ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：忽略最新任务 ThreadPoolExecutor.DiscardOldestPolicy：忽略最早的任务（最先加入队列的任务） ThreadPoolExecutor.CallerRunsPolicy：把任务交给当前线程执行。 自定义拒绝策略：新建RejectedExecutionHandler对象然后重写rejectedExecution方法 常见线程池newCachedThreadPool：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 适用大量耗时较少的线程任务 newFixedThreadPool：创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。 线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor：单线程串行执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。 单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newScheduleThreadPool：创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。 该线程池多用于执行延迟任务或者固定周期的任务。 常见线程池线程池执行流程： 协程协程是一种用户态的轻量级线程，协程的调度完全由用户控制，协程间切换只需要保存任务的上下文，没有内核开销。 锁分类1.公平锁和非公平锁：公平锁指多个线程按照申请锁的顺序排队来依次获取锁。非公平锁是没有顺序获取锁。(因为公平锁挂起和恢复存在一定开销，所以非公平锁性能好些)(非公平锁获取方式是随机抢占。公平锁和非公平锁就差在 !hasQueuedPredecessors() ，也就是前边没有排队者的话，我就可以获取锁了。tryAcquire方法。)2.可重入锁：又名递归锁，是指同一个线程在外层的方法获取到了锁，在进入内层方法会自动获取到锁。3.共享锁S和排它锁X：多个线程可以同时获取一个共享锁，一个共享锁可被多个线程拥有。排它锁也叫独占锁，同一时刻只能被统一线程占用，其他线程需要等待。4.互斥锁和读写锁：一次只能有一个线程拥有互斥锁，读写锁多个读者可同时读，写必须互斥。写优先于读，如果有写，读必须等待。5.乐观锁和悲观锁：乐观锁认为读取数据时其他线程不会对数据做修改，不加锁，更新数据时采用尝试更新不断重试的方式。悲观锁认为读取数据时其他线程会对数据做修改，会出问题，所以默认加锁。6.分段锁：提升并发程序性能的手段之一，粒度更小。将数据分成一段一段的存储（如ConcurrentHashMap的Segment），然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问，能够实现真正的并发访问。所以说，ConcurrentHashMap在并发情况下，不仅保证了线程安全，而且提高了性能。7.锁的状态：无锁、偏向锁、轻量级锁、重量级锁：偏向锁是减少无竞争且只有一个线程使用锁的情况下，使用轻量级锁产生的性能消耗。轻量级锁每次申请、释放锁都至少需要一次CAS，但偏向锁只有初始化时需要一次CAS。重量级锁，其他线程试图获取锁时，都会被阻塞，只有持有锁的线程释放锁之后才会唤醒这些线程，进行竞争。 无锁状态-&gt;偏向锁状态-&gt;轻量级锁状态-&gt;重量级锁状态 随竞争情况逐渐升级 不可逆 后续会在概念部分详细说 偏向锁：仅有一个线程进入临界区 轻量级锁：多个线程交替进入临界区 重量级锁：多个线程同时进入临界区 8.自旋锁：线程没获得锁时不会被挂起而是空循环，可以减少线程阻塞造成的线程切换的概率。首先，阻塞或唤醒Java线程需要操作系统切换CPU状态来完成，状态切换耗费CPU，可能切换CPU的时间比同步代码块中代码的执行时间都要长，为了很短的同步锁定时间而花费很长的线程切换时间是不值得的，如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。8.自适应自旋锁：自适应是值虚拟机会记录一个锁对象自旋时间和状态，如果之前自旋等待成功获得锁，这次自旋也很大概率成功，会允许持续更长时间的自选等待，后面的自旋会大概率拿到锁。如果一个对象的锁自旋等待很少能成功获取到锁，后续减少自旋次数甚至忽略自旋过程，直接阻塞，避免浪费CPU资源。（简而言之，自旋时间根据之前锁和线程状态动态变化，来减少线程阻塞时间）11.读写锁：对资源读取和写入的时候拆分为2部分处理，读的时候可以多线程一起读，写的时候必须同步地写。(如果已占用读锁，其他线程想写需要等读锁释放，其他线程想读可以读；如果已占用写锁，其他线程无论想读想写，都要等写锁释放) 概念1.从底层角度看，本质上只有一个关键字和两个接口：Synchronized和Lock接口以及ReadWriteLock接口（读写锁） 2.synchronized与reentrantLock简述 synchronized是一个隐式的重入锁，比较笨重，实现方式是锁主存和缓存一致性。现在的JDK版本已经做了很多优化synchronized的措施：自适应的自旋锁、锁粗化、锁消除、轻量级锁等 reentrantLock是一个显式的重入锁，比较灵活，可以扩展为分段锁，实现方式是AQS+双向链表（默认是NonFairSync非公平锁实现，而NonFairSync继承Sync，Sync继承AbstractQueuedSynchronizer，AQS维护volatile变量state作为同步状态）。一个线程可多次获取锁，每次获取都会计数count++，解锁时count--直到变0. ReentrantLock采用的是独占锁。Semaphore，CountDownLatch等采用的是共享锁，即有多个线程可以同时获取锁。 3.synchronized和Lock区别： 1、synchronize是java内置关键字，而Lock是一个类。（通过javap看字节码，发现有monitorenter和monitorexit命令，分别对应进入monitor加锁和释放） 2、synchronize可以作用于变量、方法、代码块，而Lock是显式地指定开始和结束位置。 3、synchronize不需要手动解锁，当线程抛出异常的时候，会自动释放锁；而Lock则需要手动释放，所以lock.unlock()需要放在finally中去执行。 4、性能方面，如果竞争不激烈的时候，synchronize和Lock的性能差不多，如果竞争激烈的时候，Lock的效率会比synchronize高。 5、Lock可以知道是否已经获得锁，synchronize不能知道。Lock扩展了一些其他功能如让等待的锁中断、知道是否获得锁等功能，Lock 可以提高效率。 6、synchronize是悲观锁的实现，而Lock则是乐观锁的实现，采用的CAS的尝试机制。 4.synchronized和ReenTrantLock区别： 1、ReenTrantLock可以中断锁的等待，提供了一些高级功能。 2、ReenTrantLock默认非公平锁，可以设为公平锁，synchronized不行 3、ReenTrantLock可以绑定多个锁条件。 4、synchronized是JVM关键字，ReentrantLock是Java API 5、ReentrantLock只能修饰代码块，synchronized既可以修饰方法也可以修饰代码块 6、ReentrantLock需要手动加锁和释放锁，synchronized自动释放 7、ReentrantLock可以知道是否成功获得了锁，synchronized不能 5.锁膨胀：无锁、偏向锁、轻量级锁、重量级锁及升级过程无锁：每个对象都有一把看不见的锁（内部锁 也叫 Monitor锁） 特点：不断尝试修改资源，失败的线程重试直到成功。偏向锁：特点：一段同步代码一直被同一个线程访问，锁总是被同一线程多次获得，降低锁的获取代价。只有一个线程执行同步代码块时能提高性能。 当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID（锁标志位）。 在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。 引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。 撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 轻量级锁：当前对象持有偏向锁时被另外的线程访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。重量级锁：当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。 特点：等待锁的线程都会进入阻塞状态。 6.锁消除： 先说“逃逸分析技术”，该技术在编译期使用，分析对象的动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中，称为方法逃逸。 该技术将确定不会发生逃逸的对象放入栈内存而非堆(故不是所有对象都在堆中)。 为了减少锁的请求和释放操作，“逃逸分析技术”在编译期分析出那些本来不存在竞争却加了锁的代码，让他们的锁失效，从而达到减少锁的请求和释放的目的。 而锁消除就是，发现不存在多线程并发抢占问题的时候，编译后去掉该锁。 7.锁偏向： 先说一下Java对象头，它包括Mark Words、Klass Words两部分，可能还包括数组的长度（如果对象是个数组）。 Klass Word里面存的是一个地址，占32位或64位，是一个指向当前对象所属于的类的地址，可以通过这个地址获取到它的元数据信息。 Mark Word！重点，这里面主要包含对象的Hashcode、年龄分代、锁标志位等，大小为32位或64位。锁状态不同时MarkWord里内容会不同。 锁偏向：当第一个线程请求时，会判断锁的对象头里的ThreadId字段的值，如果为空，则让该线程持有偏向锁，并将ThreadId的值置为当前线程ID。当前线程再次进入时，如果线程ID与ThreadId的值相等，则该线程就不会再重复获取锁了。因为锁的请求与释放是要消耗系统资源的。 如果有其他线程也来请求该锁，则偏向锁就会撤销，然后升级为轻量级锁。如果锁的竞争十分激烈，则轻量级锁又会升级为重量级锁。 对象头： 8.锁粗化：在编译期间将相邻的同步代码块合并成一个大同步块。这样做可以减少反复申请和释放同一个锁对象导致的系统开销。当一个循环中存在加锁操作时，可以将加锁操作提到循环外面执行，一次加锁代替多次加锁，提升性能。 9.volatile：是Java关键字，功能是保证被修饰的元素： 任何进程读取时都会清空本进程持有的共享变量值，而强制从主存获取(每次访问变量都刷新，每次都能得到最新变量) 任何进程写完毕时都强制将共享变量写回主存 防止指令重排(指令重排作用是JVM单线程程序在不影响运行结果的条件下的优化。多线程指令重排会出问题。) 保证内存可见性(一个线程对共享变量改动，要让其他线程立刻知道) 并不保证操作原子性它是怎么避免指令重排的呢？通过内存屏障内存屏障是通过JVM生成内存屏障指令，在读写操作前后添加内存屏障(解决L1,L2,L3高速缓存与主存速度不一致导致的缓存一致性问题)volatile读操作的后面插入一个LoadLoad屏障。volatile写操作的后面插入一个StoreLoad屏障。它的内存读写过程：LoadStoreLoadLoadStoreStoreStoreLoad注：StoreLoad就是触发后续指令中的线程缓存回写到内存; 而LoadLoad会触发线程重新从主存里面读数据进行处理。 应用：volatile应用于单例模式，防止因指令重排导致单例模式返回一个初始化到一半的对象 public class Singleton { /** * 双重校验 保证线程安全，提高执行效率，节约内存空间 */ private static volatile Singleton instance; //防止JVM指令重排 private Singleton(){} public static Singleton getInstance(){ /** * 如果instance不加volatile，JVM指令重排会先分配地址再初始化（此时这个地址存在但没值）， * 所以这里判断不为null为true时有可能对象还未完成初始化，单例可能返回了一个未初始化完的对象。 */ if(instance == null){ synchronized (Singleton.class){ if(instance == null){ instance = new Singleton(); } } } return instance; } } 10.CAS：全称Compare-And-Swap，它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子的。CAS是CPU的原子指令，底层是汇编语言，Unsafe类中的compareAndSwapInt，是一个本地方法，该方法的实现位于unsafe.cpp。CAS缺点 循环时间长CPU开销大 只能保证一个共享变量的原子操作 会引发ABA问题(当变量从A修改为B再修改回A时，变量值等于期望值A，但是无法判断是否修改，CAS操作在ABA修改后依然成功。比如说一个线程one从内存位置V中取出A，这个时候另一个线程two也从内存位置V中取出A，并且线程two进行了一些操作将值变成了B，然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作时发现内存中仍然是A，然后线程one操作成功。这个线程one的操作可能出问题。) 11.AQS：AbstractQueuedSynchronizer，维护一个volatile int state（代表共享资源状态）和一个FIFO线程等待队列。 死锁1.什么是死锁两个或多个线程互相持有对方需要的资源，导致一直等待状态，互相等待对方释放资源，如果没有主动释放资源，就会死锁。 2.死锁产生条件 1、存在循环等待 2、存在资源竞争 3、已经获得的资源不会被剥夺 4、请求与保持，一个线程因请求资源被阻塞时，拥有资源的线程的状态不会改变。 3.避免死锁 产生死锁条件中任意一条不满足，就不会产生死锁 4.死锁定位修复 执行程序时，程序没停止，也不继续运行，则是死锁 通过jps获取端口号，再jstack工具可以看到 其他关于AQS和CAS详细：https://www.jianshu.com/p/2a48778871a9锁状态升级详解：http://www.jetchen.cn/synchronized-status/Java并发-volatile内存可见性和指令重排：https://blog.csdn.net/jiyiqinlovexx/article/details/50989328","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"线程、进程","slug":"线程、进程","permalink":"https://shmily-qjj.top/tags/%E7%BA%BF%E7%A8%8B%E3%80%81%E8%BF%9B%E7%A8%8B/"},{"name":"概念","slug":"概念","permalink":"https://shmily-qjj.top/tags/%E6%A6%82%E5%BF%B5/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Apache Zeppelin初探","slug":"Apache Zeppelin初探","date":"2020-02-11T02:16:00.000Z","updated":"2020-04-12T14:15:47.997Z","comments":true,"path":"174820fd/","link":"","permalink":"https://shmily-qjj.top/174820fd/","excerpt":"","text":"Apache Zeppelin什么是ZeppelinApache Zeppelin是一个高性能，高可用，高可靠的分布式Key-Value存储与可视化平台，它是集数据摄取，数据分析，数据可视化与协作于一身的notebook形式的基于Web的工具，支持多种解释器(Interpreter),能广泛支持多种大数据查询引擎和计算引擎(如Spark，Flink，Presto，Kylin…)，多种存储系统(如JDBC数据源，HBase，Elasticsearch，Hive，Neo4j，Alluxio，Ignite…),以及多种脚本语言(如python,scala,R,shell…)和markdown。 Apache Zeppelin支持的部分组件： Zeppelin优势 为数据分析与可视化提供便利：在Zeppelin中以笔记本（notebook）的形式组织和管理交互式数据探索任务，一个笔记本（note）可以包括多个段（paragraph）。段是进行数据分析的最小单位，即在段中可以完成数据分析代码的编写以及结果的可视化查看 为多人协作提供便利：可以共享你的notebook，使他人也能看到你的数据分析笔记和结果 提供权限管理：可以管理notebook的权限以及执行者是否对已有数据有修改权限 支持多种查询计算引擎：兼容多种主流大数据查询，计算引擎，使得数据分析更加方便，数据分析人员可以对底层无感知 为临时获取某些数据提供便利：有需要临时获取一些数据的需求，通过配置Interpreter即可 配置与部署简单：已完全支持的组件只需简单填写解释器参数即可使用，支持安装第三方解释器 支持简单任务调度：Linux Crontab调度器功能 Zeppelin适用场景 多个部门需要在大数据平台取数据做分析的场景 需要多种查询引擎做数据分析的场景 需要对多种数据源进行数据可视化的场景 需要多人协作的场景 数据平台与数据分析分离，对数据分析人员无感知的场景 Zeppelin详细解释器Interpreters（重要）Zeppelin Interpreter是一个插件，允许将支持的语言/数据处理后端插入Zeppelin。通过简单的配置即可将语言/数据处理查询后端接入Zeppelin。Zeppelin解释器Zeppelin-Spark解释器 NotebookZeppelin的工作簿(Notebook)支持分为多个段，每段支持绑定多个不同的解释器，支持做单独处理和执行不同的操作，结果会一直被保留，可以选择让其他人浏览或者修改。Notebook提供给数据分析人员的前端工作环境，方便数据分析和数据可视化。 Interpreter Group解释器组：默认情况下，每个解释器属于一个解释器组，一个解释器组可能包含多个解释器同一InterpreterGroup中的Interpreter可以相互引用例如Spark解释器组包括Spark支持，PSpark，SparkSql和其他依赖项同一解释器组中的Zeppelin程序在同一JVM运行解释器组是开启、停止解释器运行的基本单位。(同时开启，停止) Interpreter binding mode解释器绑定模式：可选’shared’, ‘scoped’, ‘isolated’ 其一shared：共享模式，绑定解释器的每个Notebook共享单个解释器实例(方便不同Notebook间共享变量，但资源利用率低)scoped：作用域模式，在相同解释器程序中创建新的解释器实例(每个Notebook拥有自己的回话，资源利用率略高，不能直接共享变量)isolated：隔离模式，每个Notebook创建新的解释器程序(笔记本之间互不影响，不能直接共享变量) 比如shared模式下，每个Notebook都可以使用SparkInterpreter但是只有一个SparkContext 如果isolated模式，每个Notebook都可以使用SparkInterpreter但每个Notebook有单独的SparkContext 解释器绑定模式-官方详细介绍 Interpreter生命周期Zeppelin 0.8.0以后支持LifecycleManager来控制解释器生命周期(之前是关闭UI界面后生命周期结束)NullLifecycleManager不做操作，要像以前一样自行控制生命周期TimeoutLifecycleManager(默认生命周期管理)默认超过1小时关闭解释器，可以更改 Generic ConfInterpreterZeppelin解释器配置由所有用户和Notebook共享，如果想使用其他的设置，需要创建新的解释器，能实现但不方便，ConfInterpreter可以提供对解释器设置的更细粒度的控制和更大的灵活性。ConfInterpreter是可以被任何解释器使用的通用解释器，输入格式应为属性文件格式。它用于为任何解释器进行自定义设置。用户需要将ConfInterpreter放在Notebook的第一段如上图%spark.conf独立设置了该Notebook中的Spark解释器 Interpreter进程恢复0.8.0版本前，关闭Zeppelin会同时关闭所有正在运行的解释器程序，但是我们可能只是想维护Zeppelin服务器而不想关闭解释器程序，Interpreter进程恢复就派上用场了。0.8.0版本后，设置zeppelin.recovery.storage.class属性的值默认org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage不开启进程恢复设置为org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorage开启进程恢复，关闭Zeppelin不会关闭解释器程序如果开启了进程恢复，关闭了Zeppelin，又想再关闭解释器程序，则执行bin/stop-interpreter.sh 官方文档官方Docs 常见问题及错误排除 Interpreter * is not found**：检查是否已经配置了该解释器，如果配置了，检查该解释器是否已被点亮(右上角设置图标点为蓝色并保存) 详细深入了解: Apache Zeppelin官网","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zeppelin","slug":"Zeppelin","permalink":"https://shmily-qjj.top/tags/Zeppelin/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"分享我的技术调研流程","slug":"分享我的技术调研流程","date":"2020-01-03T13:10:00.000Z","updated":"2020-04-12T14:15:48.000Z","comments":true,"path":"4b21953d/","link":"","permalink":"https://shmily-qjj.top/4b21953d/","excerpt":"","text":"技术调研流程分享为何制定流程？ 明确调研需求，提高调研文档质量，规范调研流程，保证调研产出。 我是如何制定调研流程的？ 先总结一版自己的调研流程，再查阅资料以及查看阿里等大厂的调研报告，结合部门目前的实际情况来明确部门的调研流程。 技术调研流程整个调研流程分四个阶段 第一阶段：需求分析 分析目前/未来可能出现的瓶颈点 明确调研目标和方向（为了实现新需求？为了优化瓶颈点？） 引入新工具后的结果衡量（效率提升、成本降低等，如何衡量） 结构化思考新工具引入的目标和衡量标准：场景（适用场景、知识要求）、效率（性能、效果预测）、成本（容量、硬件资源、维护成本）、稳定性（故障分析工具、监控完善度）等 第二阶段：准备阶段 理解需求 结合现状评估可行性和收益 第三阶段：调研阶段 简单调研 短时间内粗略了解所调研技术的应用场景和部署环境，进一步评估和权衡可行性和收益 经过权衡后发现值得调研，发送邮件至直接上级并抄送部门Leader (标题：申请调研xxx 内容：简述xxx值得详细调研的理由) 协商决定是否批准，若批准，则开始进入详细调研阶段 详细调研 包括但不限于： 先设计调研方法与调研过程 预估调研时间，并在北森设定Deadline，根据调研报告的要求按时完成调研报告 了解相关技术在其他公司的应用及收益 原理及核心技术调研 总结适合我们的场景及解决方案 调研过程遇到的问题与解决方案 未解决的问题/收集需求可随时讨论，有必要的话可以开讨论会 设计落地方案 第四阶段：反馈与落地 调研反馈 必须产出一份调研报告 选择反馈形式：分享会、文档、邮件、群通知（如果是分享会，则要有完善的PPT，会前共享出来） 技术落地 根据自己设计的落地方案得出详细的部署文档和使用文档 配合运维部署 后续阶段：落地后如何跟进 出现问题及时跟进解决，并把问题与解决方案更新到使用文档中，如果影响较大，要在更新完使用文档后发群通知。 相关的新人文档/Wiki更新 文档要求所有调研文档统一保存在gitlab部门文档中的技术调研文档目录 部署文档要求 这部分为了方便让运维人员傻瓜式部署，并可以把简单的运维工作交给运维。 尽量打包好主从节点的分发包（提前编译好） 例： xxx-master.zip xxx-worker.zip 或整理conf配置文件包 xxx-master-conf.zip xxx-worker-conf.zip 尽量采用傻瓜式命令 例:sudo -uhdfs tar -zxvf /opt/alluxio-2.1.0-bin.tar.gz -C /opt/ sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh master sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh worker chmod 777 /opt/alluxio-2.1.0/logs/user ... 尽量写出部署过程可能的报错及解决方案 常用维护方案总结 使用文档要求 这部分目的是方便大家使用新技术新组件。 格式包括但不限于：场景1：示例代码/操作 场景2：示例代码/操作 场景3：示例代码/操作 ... 常见错误及解决 遇到问题请联系：调研人 调研报告要求 这部分的目的是调研时可能有遗漏的点，可以从这个列表里做参考。 开头写清 标题 + 调研人 + 调研时间 可以参考但不限于这些点： xx是什么 xx的优缺点 xx的应用场景 xx的功能/特性 xx的原理与架构简述 相似技术横向对比 初步评估带来的收益 遇到的问题Q&amp;A xx的兼容性（支持什么不支持什么） xx技术的核心点 xx的性能与扩展性（测试结果） xx的部署难度 如何部署与简单实践 应用该技术带来的工作量和学习成本 总结 注意事项 关注缺点的优先级高于关注优点的优先级（优点再多，也可能因为一个缺点而不能被应用） 明确场景，及时沟通需求，明确需求细节 多搜集信息，不急于出结果（搜集足够的信息才能做出比较准确的判断） 要从可行性，稳定性，可维护性，工作量和学习成本等几个重要方面考虑 合理安排时间，自己规定了Deadline，就要及时交付反馈","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"技术调研","slug":"技术调研","permalink":"https://shmily-qjj.top/tags/%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Alluxio-基于内存的虚拟分布式存储系统","slug":"Alluxio-基于内存的虚拟分布式存储系统","date":"2020-01-01T14:16:00.000Z","updated":"2020-06-25T03:00:56.536Z","comments":true,"path":"44511/","link":"","permalink":"https://shmily-qjj.top/44511/","excerpt":"","text":"什么是AlluxioAlluxio 是世界上第一个虚拟的分布式存储系统，它为计算框架和存储系统构建了桥梁，使计算框架能够通过一个公共接口连接到多个独立的存储系统,使计算与存储隔离。 Alluxio 是内存为中心的架构，以内存速度统一了数据访问速度，使得数据的访问速度能比现有方案快几个数量级,为大数据软件栈带来了显著的性能提升在大数据生态系统中，Alluxio 位于数据驱动框架或应用（如 Apache Spark、Presto、Tensorflow、Apache HBase、Apache Hive 或 Apache Flink）和各种持久化存储系统（如 Amazon S3、Google Cloud Storage、OpenStack Swift、GlusterFS、HDFS、IBM Cleversafe、EMC ECS、Ceph、NFS 和 Alibaba OSS）之间,Alluxio 统一了存储在这些不同存储系统中的数据,为其上层数据框架提供统一的客户端API和全局命名空间 Alluxio最新动态:为了方便大家可以持续跟进Alluxio发展动态，这里给出两条跟进Alluxio最新发展和动态的途径:Alluxio官方文档Alluxio知乎专栏 Alluxio优势 内存速度 I/O:Alluxio 能够用于分布式共享缓存服务，这样与 Alluxio 通信的计算应用程序可以透明地缓存频繁访问的数据（尤其是从远程位置）,以提供近似于内存级 I/O 吞吐率，同时提升稳定性。 简化云存储和对象存储接入:与传统文件系统相比,云存储系统和对象存储系统使用不同的语义,这些语义对性能的影响也不同于传统文件系统。常见的文件系统操作（如列出目录和重命名）通常会导致显著的性能开销。当访问云存储中的数据时，应用程序没有节点级数据本地性或跨应用程序缓存。将 Alluxio 与云存储或对象存储一起部署可以缓解这些问题,因为这样将从Alluxio中检索读取数据,而不是从底层云存储或对象存储中检索读取。 简化数据管理:Alluxio 提供对多数据源的单点访问,便捷地管理远程的存储系统,并向上层提供统一的命名空间。除了连接不同类型的数据源之外,Alluxio 还允许用户同时连接到不同版本的同一存储系统,如多个版本的HDFS,并且无需复杂的系统配置和管理，提高了数据访问灵活性。 应用程序部署简易:Alluxio 管理应用程序和文件或对象存储之间的通信，将应用程序的数据访问请求转换为底层存储接口的请求。Alluxio 与 Hadoop 兼容,现有的数据分析应用程序,如Spark和MapReduce程序,无需更改任何代码就能在Alluxio上运行。 分层存储特性:综合使用了内存、SSD和磁盘多种存储资源。通过Alluxio提供的LRU、LFU等缓存策略可以保证热数据一直保留在内存中，冷数据则被持久化到level 2甚至level 3的存储设备上 方便迁移可插拔:Alluxio提供多种易用的API方便将整个系统迁移到Alluxio Alluxio的特征:对Alluxio的优势和特征进行了概括 点击可进入官网介绍超大规模工作负载:支持超大规模工作负载并具有HA高可用性灵活的API :计算框架可使用HDFS、S3、Java、RESTful或POSIX为基础的API来访问Alluxio智能数据缓存和分层 : 使用包括内存在内的本地存储，来充当分布式缓存,很大程度上改善I/O性能，且缓存对用户透明存储系统接口 : 通过一系列接口集成HDFS，S3，Azure Blob Store，Google Cloud Store等存储系统统一全局命名空间 : 多个存储系统安装到一个统一的名称空间中，不需要创建永久数据副本，方便管理多数据源安全性 : 通过内置审核、基于角色的访问控制、LDAP、活动目录和加密通信，提供数据保护监控和管理 : 提供了用户友好的Web界面和命令行工具，允许用户监控和管理集群分层次的本地性 : 将更多的读写安排在本地,实现成本和性能的优化 Alluxio的应用场景Alluxio 的落地非常依赖场景，否则优化效果并不明显（无法发挥内存读取的优势）1.计算应用需要反复访问远程云端或机房的数据（存储计算分离）2.混合云,计算与存储分离,异构的数据存储带来的系统耦合（Alluxio提供统一命名空间，统一访问接口）3.多个独立的大数据应用（比如不同的Spark Job）需要高速有效的共享数据（数据并发访问）4.计算框架所在机器内存占用较高,GC频繁,或者任务失败率较高,Alluxio通过数据的OffHeap来减少GC开销5.有明显热表/热数据，相同数据被单应用多次访问6.需要加速人工智能云上分析（如TensorFlow本地训练，可通过FUSE挂载Alluxio FS到本地） 我也做了很多Allxuio的性能测试工作,效果都不是很理想,有幸与Alluxio PMC范斌和李浩源交流了测试结果不如人意的原因,大佬是这么说的:”如果HDFS本身已经和Spark和Hive共置了，那么这个场景并不算Alluxio的目标场景。计算和存储分离的情况下才会有明显效果，否则通常是HDFS已经成为瓶颈时才会有帮助。“还有,如果HDFS部署在计算框架本地,作业的输入数据可能会存在于系统的高速缓存区,则Alluxio对数据加速也并不明显。所以:应用场景很关键,新技术产生时,一定要了解其应用场景和原理并经过考虑之后再做一些性能测试之类的后续工作!官方介绍的Alluxio应用场景 Alluxio原理如图，一个完整的Alluxio集群部署在逻辑上包括master、worker、client及底层存储(UFS)。master和worker进程通常由集群管理员维护和管理，它们通过RPC通信相互协作，从而构成了Alluxio服务端。而应用程序则通过Alluxio Client来和Alluxio服务交互，读写数据或操作文件、目录。 Alluxio核心组件Alluxio使用了单Master和多Worker的架构,Master和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件,Client通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。Alluxio用户一般只与Alluxio的Client组件进行交互。 Master: 负责管理整个集群的全局元数据并响应Client对文件系统的请求。在Alluxio文件系统内部，每一个文件被划分为一个或多个数据块(block)，并以数据块为单位存储在Worker中。Master节点负责管理文件系统的元数据(如文件系统的inode树、文件到数据块的映射)、数据块的元数据(如block到Worker的位置映射)，以及Worker元数据(如集群当中每个Worker的状态)。所有Worker定期向Master发送心跳消息汇报自己状态，以维持参与服务的资格。Master通常不主动与其他组件通信，只通过RPC服务被动响应请求，同时Master还负责实时记录文件系统的日志(Journal)，以保证集群重启之后可以准确恢复文件系统的状态。Master分为Primary Master和Secondary Master，Secondary Master需要将文件系统日志写入持久化存储，从而实现在多Master（HA模式下）间共享日志，实现Master主从切换时可以恢复Master的状态信息。Alluxio集群中可以有多个Secondary Master，每个Secondary Master定期压缩文件系统日志并生成Checkpoint以便快速恢复，并在切换成Primary Master时读取之前Primary Master写入的日志。Secondary Master不处理任何Alluxio组件的任何请求。 Worker: Alluxio Master只负责响应Client对文件系统元数据的操作，而具体文件数据传输的任务由Worker负责，如图，每个Worker负责管理分配给Alluxio的本地存储资源(如RAM,SSD,HDD),记录所有被管理的数据块的元数据，并根据Client对数据块的读写请求做出响应。Worker会把新的数据存储在本地存储，并响应未来的Client读请求，Client未命中本地资源时也可能从底层持久化存储系统中读数据并缓存至Worker本地。Worker代替Client在持久化存储上操作数据有两个好处:1.底层读取的数据可直接存储在Worker中，可立即供其他Client使用 2.Alluxio Worker的存在让Client不依赖底层存储的连接器，更加轻量化。Alluxio采取可配置的缓存策略，Worker空间满了的时候添加新数据块需要替换已有数据块，缓存策略来决定保留哪些数据块。 Client: 允许分析和AI/ML应用程序与Alluxio连接和交互，它发起与Master的通信，执行元数据操作，并从Worker读取和写入存储在Alluxio中的数据。它提供了Java的本机文件系统API，支持多种客户端语言包括REST，Go，Python等，而且还兼容HDFS和Amazon S3的API。可以把Client理解为一个库，它实现了文件系统的接口，根据用户请求调用Alluxio服务，客户端被编译为alluxio-2.0.1-client.jar文件，它应当位于JVM类路径上，才能正常运行。当Client和Worker在同一节点时，客户端对本地缓存数据的读写请求可以绕过RPC接口，使本地文件系统可以直接访问Worker所管理的数据，这种情况被称为短路写，速度比较快，如果该节点没有Worker在运行，则Client的读写需要通过网络访问其他节点上的Worker，速度受网络宽带的限制。 Alluxio读写场景与参数 Alluxio读场景与性能分析: 命中本地Worker Client向Master检索存储该数据的Worker位置 如果本地存有该数据，则”短路读”,避免网络传输 短路读提供内存级别访问速度，是Alluxio最高读性能的方式 命中远程Worker Client请求的数据不在本地Worker则Client将从远程Worker读取数据 远程Worker将数据返回本地Worker并写一个本地副本，请求频繁的数据会有更多副本，从而实现热度优化计算的本地性，也可选NO_CACHE读取方式禁用本地副本写入 远程缓存命中，读取速度受网络速度限制 未命中Worker Alluxio任何一个Worker没有缓存所需数据，则Client把请求委托给本地Worker从底层存储系统(UFS)读取，缓存未命中的情况下延迟较高 Alluxio 1.7前Worker从底层读取完整数据块缓存下来并返回给Client，1.7版本后支持异步缓存，Client读取，Worker缓存，不需要等待缓存完成即可返回结果 指定NO_CACHE读取方式则禁用本地缓存 Alluxio写场景与性能分析: 仅写缓存 写入类型通过alluxio.user.file.writetype.default来设置，MUST_CACHE仅写本地缓存而不写入UFS 如果”短路写”可用，则直接写本地Worker避免网络传输，性能最高3.如果无本地Worker，即”短路写”不可用，数据写入远端Worker，写速度受限于网络IO 数据没有持久化，机器崩溃或需要释放数据用于较新的写入时，数据可能丢失 同步写缓存和持久化存储 alluxio.user.file.writetype.default=CACHE_THROUGH，同步写入Worker和UFS 速度比仅写缓存的方式慢很多，需要数据持久化时使用 仅写持久化存储 alluxio.user.file.writetype.default=THROUGH，只将数据写入UFS，不会创建Alluxio缓存中的副本 输入数据重要但不立刻使用的情况下使用该方式 异步写持久化存储(目前2.0.1为实验性) alluxio.user.file.writetype.default=ASYNC_THROUGH 可以以内存的速度写入Alluxio Worker，并异步完成持久化 实验性功能-如果异步持久化到底层存储前机器崩溃，数据丢失，异步写机制要求文件所有块都在同一个Worker中 Alluxio读写参数总结 写参数: alluxio.user.file.writetype.default CACHE_THROUGH:数据被同步写入AlluxioWorker和底层存储 MUST_CACHE:数据被同步写入AlluxioWorker,不写底层存储 THROUGH:数据只写底层存储,不写入AlluxioWorker ASYNC_THROUGH:数据同步写入AlluxioWorker并异步写底层存储(速度快) 读参数: alluxio.user.file.readtype.default CACHE_PROMOTE:数据在Worker上,则被移动到Worker的最高层,否则创建副本到本地Worker CACHE:数据不在本地Worker中时直接创建副本到本地Worker NO_CACHE:仅读数据,不写副本到Worker 是否缓存全部数据块: alluxio.user.file.cache.partially.read.block (v1.7以前,V1.7以后采取异步缓存策略) false读多少缓存多少,一个数据块只有完全被读取时，才能被缓存 true读部分缓存全部,没有完全读取的数据块也会被全部存到Alluxio内 Worker写文件数据块的数据分布策略: alluxio.user.block.write.location.policy.class LocalFirstPolicy (alluxio.client.block.policy.LocalFirstPolicy) 默认值,首先返回本地主机，如果本地worker没有足够的块容量，它从活动worker列表中随机选择一名worker。 MostAvailableFirstPolicy (alluxio.client.block.policy.MostAvailableFirstPolicy) 返回具有最多可用字节的worker。 RoundRobinPolicy (alluxio.client.block.policy.RoundRobinPolicy) 以循环方式选择下一个worker，跳过没有足够容量的worker。 SpecificHostPolicy (alluxio.client.block.policy.SpecificHostPolicy) 返回具有指定主机名的worker。此策略不能设置为默认策略。 目前有六种策略,详见配置项列表Alluxio的分层存储概念: Alluxio workers节点使用包括内存在内的本地存储来充当分布式缓冲缓存区,可以很大程度上改善I/O性能。每个Alluxio节点管理的存储数量和类型由用户配置,Alluxio还支持层次化存储,让数据存储获得类似于L1/L2 cpu缓存的优化。单层存储设置(推荐): 默认使用两个参数alluxio.worker.memory.size=16GB + alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk来设置Alluxio Worker的缓存大小 也可以单层多个存储介质并指定每个介质可用空间大小alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd + alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB alluxio.worker.memory.size和alluxio.worker.tieredstore.level0.dirs.quota的区别-&gt;ramdisk的大小默认由前者决定,后者可以决定除内存外的其他介质如ssd和hdd的大小 多层存储设置: 多层存储的配置-使用两层存储MEM和HDD alluxio.worker.tieredstore.levels=2 # 最大存储级数 在Alluxio中配置了两级存储 alluxio.worker.tieredstore.level0.alias=MEM # alluxio.worker.tieredstore.level0.alias=MEM 配置了首层(顶层)是内存存储层 alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # 设置了ramdisk的配额是100GB alluxio.worker.tieredstore.level0.dirs.quota=100GB alluxio.worker.tieredstore.level0.watermark.high.ratio=0.9 # 回收策略的高水位 alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 alluxio.worker.tieredstore.level1.alias=HDD # 配置了第二层是硬盘层 alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3 # 定义了第二层3个文件路径各自的配额 alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB alluxio.worker.tieredstore.level1.watermark.high.ratio=0.9 alluxio.worker.tieredstore.level1.watermark.low.ratio=0.7 写数据默认写入顶层存储,也可以指定写数据的默认层级 alluxio.user.file.write.tier.default 默认0最顶层,1表示第二层,-1倒数第一层 Alluxio收到写请求,直接把数据写入有足够缓存的层,如果缓存全满,则置换掉底层的一个Block. Alluxio缓存回收策略缓存回收: Alluxio中的数据是动态变化的,存储空间不足时会为新数据腾出空间 异步缓存回收与同步缓存回收 alluxio.worker.tieredstore.reserver.enabled=true (默认异步回收) 在读写缓存工作负载较高的情况下异步回收可以提升性能 alluxio.worker.tieredstore.reserver.enabled=false (同步回收) 请求所用空间比Worker上请求空间更多时,同步回收可以最大化Alluxio空间利用率,同步回收建议使用小数据块配置(64-128MB)来降低回收延迟 缓存回收中空间预留器的水位(阈值) Worker存储利用率达到高水位时,基于回收策略回收Worker缓存直到达到配置的低水位 高水位: alluxio.worker.tieredstore.level0.watermark.high.ratio=0.95 (默认95%) 低水位: alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 (默认70%) 比如配置了32GB(MEM)+100GB(SSD)=132GB的Worker内存,当内存达到高水位132x0.95=125.4GB时开始回收缓存,直到到达低水位132x0.7=92.4GB时停止回收缓存 自定义回收策略 alluxio.worker.allocator.class=alluxio.worker.block.allocator.MaxFreeAllocator (Alluxio中新数据块分配策略的类名) alluxio.worker.evictor.class=alluxio.worker.block.evictor.LRUEvictor (当存储层空间用尽时块回收策略的类名) 贪心回收策略: 回收任意数据块直到释放出所需空间 LRU回收策略: 回收最近最少使用数据块直到释放出所需空间 部分LRU回收策略: 在最大剩余空间的目录回收最近最少使用数据块 LRFU回收策略: 基于权重分配的最近最少使用和最不经常使用策略回收数据块,如果权重完全偏向最近最少使用,则LRFU变为LRU Alluxio异步缓存策略 Alluxio v1.7以后支持异步缓存 异步缓存是将Alluxio的缓存开销由客户端转移到Worker上,第一次读数据时,在不设置读属性为NO_CACHE的情况下Client只负责从底层存储读数据,然后缓存任务由Worker来执行,对Client读性能没有影响,也不需要像V1.7版本前那样设置alluxio.user.file.cache.partially.read.block来决定缓存部分或全部数据,而且Worker内部也在Client读取底层存储系统的数据方面做了优化,设置读属性为CACHE的情况下: Client顺序读完整数据块时Worker顺便缓存完整数据块 Client只读部分数据或非顺序读数据时Worker不会读取时顺便缓存,等客户端读取完以后再向Worker系欸点发送异步缓存命令,Worker节点再从底层存储中获取完整的块 异步缓存使得第一次从Alluxio读取和直接从底层存储读取花费相同时间,且数据异步缓存到Alluxio中,提高集群整体性能 异步缓存参数调整 Worker在异步缓存的同时也响应Client读取请求,可通过设置Worker端的线程池大小来加快异步缓存的速度 alluxio.worker.network.netty.async.cache.manager.threads.max 指定Worker线程池大小,该属性默认为8,表示最多同时用八核从其他Worker或底层存储读数据并缓存,提高此值可以加快后台异步缓存的速度,但会增加CPU使用率 Alluxio元数据 Alluxio元数据的存储在Alluxio新的2.x版本中，对元数据存储做了优化，使其能应对数以亿级的元数据存储。首先，文件系统是INode-Tree组成的，即文件目录树，Alluxio Master管理多个底层存储系统的元数据，每个文件目录都是INode-Tree的节点，在Java对象中，可能一个目录信息本身占用空间不大，但映射在JavaHeap内存中，算上附加信息，每个文件大概要有1KB左右的元数据，如果有十亿个文件和路径，则要有约1TB的堆内存来存储元数据，完全是不现实的。所以，为了方便管理元数据，减小因为元数据过多对Master性能造成的影响，Alluxio的元数据通过RocksDB键值数据库来管理元数据，Master会Cache常用数据的元数据，而大部分元数据则存在RocksDB中，这样大大减小了Master Heap的压力，降低OOM可能性，使Alluxio可以同时管理多个存储系统的元数据。通过RocksDB的行锁，也可以方便高并发的操作Alluxio元数据。高可用过程中，INode-Tree是进程中的资源，不共享，如果ActiveMaster挂掉，StandByMaster节点可以从Journal持久日志（位于持久化存储中如HDFS）恢复状态。这样会依赖持久存储（如HDFS）的健康状况，如果持久存储服务宕机，Journal日志也不能写，Alluxio高可用服务就会受到影响。所以，Alluxio通过Raft算法保证元数据的完整性，即使宕机，也不会丢失已经提交的元数据。 Alluxio元数据一致性 Alluxio读取磁层存储系统的元数据,包括文件名,文件大小,创建者,组别,目录结构等 如果绕过Alluxio修改底层存储系统的目录结构,Alluxio会同步更新alluxio.user.file.metadata.sync.interval=-1 Alluxio不主动同步底层存储元数据alluxio.user.file.metadata.sync.interval=正整数 正整数指定了时间窗口,该时间窗口内不触发元数据同步alluxio.user.file.metadata.sync.interval=0 时间窗口为0,每次读取都触发元数据同步时间窗口越大,同步元数据频率越低,Alluxio Master性能受影响越小 Alluxio不加载具体数据,只加载元数据,若要加载文件数据,可以通过load命令或FileStream API 在Alluxio中创建文件或文件夹时可以指定是否持久化alluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH mkdir /xxxalluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH touch /xxx/xx Alluxio RPCAlluxio 1.x中 Master RPC using Thrift（元数据操作） Workers RPC using Netty（数据操作）而新的Alluxio 2.x中 使用gRPC保证高吞吐，方便代码维护 Alluxio的Metrics度量指标信息可以让用户深入了解集群上运行的任务,是监控和调试的宝贵资源。Alluxio的度量指标信息被分配到各种相关Alluxio组件的实例中。每个实例中，用户可以配置一组度量指标槽，来决定报告哪些度量指标信息。现支持Master进程,Worker进程和Client进程的度量指标 。 度量指标的sink参数为alluxio.metrics.sink.xxxConsoleSink: 输出控制台的度量值。CsvSink: 每隔一段时间将度量指标信息导出到CSV文件中。JmxSink: 查看JMX控制台中注册的度量信息。GraphiteSink: 给Graphite服务器发送度量信息。MetricsServlet: 添加Web UI中的servlet，作为JSON数据来为度量指标数据服务。 可选度量的配置 Master的Metrics 配置方法 master.* 例如:master.CapacityTotal常规信息CapacityTotal: 文件系统总容量（以字节为单位）。CapacityUsed: 文件系统中已使用的容量（以字节为单位）。CapacityFree: 文件系统中未使用的容量（以字节为单位）。PathsTotal: 文件系统中文件和目录的数目。UnderFsCapacityTotal: 底层文件系统总容量（以字节为单位）。UnderFsCapacityUsed: 底层文件系统中已使用的容量（以字节为单位）。UnderFsCapacityFree: 底层文件系统中未使用的容量（以字节为单位）。Workers: Worker的数目。逻辑操作DirectoriesCreated: 创建的目录数目。FileBlockInfosGot: 被检索的文件块数目。FileInfosGot: 被检索的文件数目。FilesCompleted: 完成的文件数目。FilesCreated: 创建的文件数目。FilesFreed: 释放掉的文件数目。FilesPersisted: 持久化的文件数目。FilesPinned: 被固定的文件数目。NewBlocksGot: 获得的新数据块数目。PathsDeleted: 删除的文件和目录数目。PathsMounted: 挂载的路径数目。PathsRenamed: 重命名的文件和目录数目。PathsUnmounted: 未被挂载的路径数目。RPC调用CompleteFileOps: CompleteFile操作的数目。CreateDirectoryOps: CreateDirectory操作的数目。CreateFileOps: CreateFile操作的数目。DeletePathOps: DeletePath操作的数目。FreeFileOps: FreeFile操作的数目。GetFileBlockInfoOps: GetFileBlockInfo操作的数目。GetFileInfoOps: GetFileInfo操作的数目。GetNewBlockOps: GetNewBlock操作的数目。MountOps: Mount操作的数目。RenamePathOps: RenamePath操作的数目。SetStateOps: SetState操作的数目。UnmountOps: Unmount操作的数目。 Worker的Metrics 配置方法 192_168_1_1.* 例如:192_168_1_1.CapacityTotal常规信息CapacityTotal: 该Worker的总容量（以字节为单位）。CapacityUsed: 该Worker已使用的容量（以字节为单位）。CapacityFree: 该Worker未使用的容量（以字节为单位）。逻辑操作BlocksAccessed: 访问的数据块数目。BlocksCached: 被缓存的数据块数目。BlocksCanceled: 被取消的数据块数目。BlocksDeleted: 被删除的数据块数目。BlocksEvicted: 被替换的数据块数目。BlocksPromoted: 被提升到内存的数据块数目。BytesReadAlluxio: 通过该worker从Alluxio存储读取的数据量，单位为byte。其中不包括UFS读。BytesWrittenAlluxio: 通过该worker写到Alluxio存储的数据量，单位为byte。其中不包括UTF写。BytesReadUfs-UFS:${UFS}: 通过该worker从指定UFS读取的数据量，单位为byte。BytesWrittenUfs-UFS:${UFS}: 通过该worker写到指定UFS的数据量，单位为byte。 Client的Metrics 配置方法 client.* 例如:clien.BytesReadRemote常规信息NettyConnectionOpen: 当前Netty网络连接的数目。逻辑操作BytesReadRemote: 远程读取的字节数目。BytesWrittenRemote: 远程写入的字节数目。BytesReadUfs: 从ufs中读取的字节数目。BytesWrittenUfs: 写入ufs的字节数目。 配置示例 vim metrics.properties # List of available sinks and their properties. alluxio.metrics.sink.ConsoleSink alluxio.metrics.sink.CsvSink alluxio.metrics.sink.JmxSink alluxio.metrics.sink.MetricsServlet alluxio.metrics.sink.PrometheusMetricsServlet alluxio.metrics.sink.GraphiteSink master.GetFileBlockInfoOps master.GetNewBlockOps master.FreeFileOps 192_168_1_101.BytesReadAlluxio 192_168_1_101.BytesWrittenAlluxio 192_168_1_101.BlocksAccessed 192_168_1_101.BlocksCached 192_168_1_101.BlocksCanceled 192_168_1_101.BlocksDeleted 192_168_1_101.BlocksEvicted 192_168_1_101.BlocksPromoted 192_168_1_102.BytesReadAlluxio 192_168_1_102.BytesWrittenAlluxio 192_168_1_102.BlocksAccessed 192_168_1_102.BlocksCached 192_168_1_102.BlocksCanceled 192_168_1_102.BlocksDeleted 192_168_1_102.BlocksEvicted 192_168_1_102.BlocksPromoted 192_168_1_103.BytesReadAlluxio 192_168_1_103.BytesWrittenAlluxio 192_168_1_103.BlocksAccessed 192_168_1_103.BlocksCached 192_168_1_103.BlocksCanceled 192_168_1_103.BlocksDeleted 192_168_1_103.BlocksEvicted 192_168_1_103.BlocksPromoted 然后访问 http://192.168.1.101:19999/metrics/json/ 可得到监控信息喜欢看源码的小伙伴可以戳这里哟-&gt;Alluxio源码入口 Alluixo审计日志Alluxio提供审计日志来方便管理员可以追踪用户对元数据的访问操作。开启审计日志： 讲JVM参数alluxio.master.audit.logging.enabled设为true审计日志包含如下条目： key desc succeeded 如果命令成功运行，值为true。在命令成功运行前，该命令必须是被允许的。 allowed 如果命令是被允许的，值为true。即使一条命令是被允许的它也可能运行失败。 ugi 用户组信息，包括用户名，主要组，认证类型。 ip 客户端IP地址。 cmd 用户运行的命令。 src 源文件或目录地址。 dst 目标文件或目录的地址。如果不适用，值为空。 perm user:group:mask，如果不适用值为空。 Alluxio安装和部署准备工作1.下载Alluxio压缩包并上传到NN所在集群2.解压并进入安装目录 tar -zxvf alluxio-2.0.1-bin.tar.gz -C /opt/module/ mv /opt/module/alluxio-2.0.1 /opt/module/alluxio cd /opt/module/alluxio cp conf/alluxio-site.properties.template conf/alluxio-site.properties cp conf/alluxio-env.sh.template conf/alluxio-env.sh 常规集群参数配置常规非高可用集群配置，针对1.x和2.x版本通用conf/alluxio-env.sh vim conf/alluxio-env.sh ALLUXIO_HOME=/opt/module/alluxio-2.0.1 ALLUXIO_LOGS_DIR=/opt/module/alluxio-2.1.0/logs ALLUXIO_MASTER_HOSTNAME=hadoop101 ALLUXIO_RAM_FOLDER=/mnt/ramdisk ALLUXIO_UNDERFS_ADDRESS=hdfs://hadoop101:9000/alluxio ALLUXIO_WORKER_MEMORY_SIZE=512MB JAVA_HOME=/opt/module/jdk1.8.0_161 # 设置ALLUXIO_MASTER_JAVA_OPTS作用于master JVM # 设置ALLUXIO_WORKER_JAVA_OPTS作用于worker JVM # 以及ALLUXIO_JAVA_OPTS同时作用于master以及worker JVM # 增加worker JVM GC事件的logging, 输出写至worker节点的logs/worker.out文件中 ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # 设置master JVM的的heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; conf/alluxio-site.properties vim conf/alluxio-site.properties # Common properties alluxio.master.hostname=hadoop101 alluxio.master.mount.table.root.ufs=hdfs://192.168.1.101:9000/alluxio alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk vim conf/masters hadoop101 vim conf/workers hadoop102 hadoop103 scp -r /opt/module/alluxio/ root@hadoop102:/opt/module/ scp -r /opt/module/alluxio/ root@hadoop103:/opt/module/ # 打开Alluxio服务 alluxio format alluxio-start.sh master alluxio-start.sh workers NoMount 或直接 alluxio-start.sh all 访问Master节点的WEB UI: hadoop101:19999 访问Worker节点的WEB UI: hadoop102:30000 #测试部署是否成功 bin/alluxio runTests # 如果出现Passed the test则说明部署成功 bin/alluxio-stop.sh all # 关闭集群 出现类似以下界面即为部署成功此时可以通过命令alluxio fsdamin report来查看集群状态 高可用集群参数配置高可用(HA)通过支持同时运行多个master来保证服务的高可用性，多个master中有一个master被选为primary master作为所有worker和client的通信首选，其余master为备选状态(StandBy)，它们通过和primary master共享日志来维护同样的文件系统元数据，并在primary master失效时迅速接替其工作(master主从切换过程中，客户端可能会出现短暂的延迟或瞬态错误)搭建高可用集群前的准备:①确保Zookeeper服务已经运行②一个单独安装的可靠的共享日志存储系统(可用HDFS或S3等系统)③这个配置针对Alluxio 2.x版本，不适用于1.x版本④需要事先创建好ramdisk挂载目录 注意去掉中文注释 否则会报错 在所有机器上配置env.sh vim alluxio-env.sh ALLUXIO_HOME=/opt/alluxio ALLUXIO_LOGS_DIR=/opt/alluxio/logs ALLUXIO_RAM_FOLDER=/mnt/ramdisk JAVA_HOME=/opt/module/jdk1.8.0_161 # 设置ALLUXIO_MASTER_JAVA_OPTS作用于master JVM # 设置ALLUXIO_WORKER_JAVA_OPTS作用于worker JVM # 以及ALLUXIO_JAVA_OPTS同时作用于master以及worker JVM # 增加worker JVM GC事件的logging, 输出写至worker节点的logs/worker.out文件中 ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # 设置master JVM的的heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; 在101机器上配置Master和Worker vim alluxio-site.properties # 192.168.1.101 Master Worker # Common properties alluxio.master.hostname=192.168.1.101 # 要写其他机器能识别的地址而非localhost等 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # 如果底层HDFS存储为高可用，则要写hdfs配置文件地址 alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # 指向高可用或非高可用的HDFS地址（可以是根目录，也可以是某个文件夹） # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 # Zookeeper地址中间逗号隔开 alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal # 回滚日志的地址，写入可靠的分布式HDFS alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* # 可以模拟很多用户来实现权限控制 alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在102机器上配置Master和Worker vim alluxio-site.properties # 192.168.1.102 Master Worker # Common properties alluxio.master.hostname=192.168.1.102 # 要写其他机器能识别的地址而非localhost等 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在103机器上配置Worker vim alluxio-site.properties # 192.168.1.103 Worker # Common properties # Worker不需要写alluxio.master.hostname参数和alluxio.master.journal.folder参数 alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* 在所有机器上指定Master和Worker节点 vim masters 192.168.1.101 192.168.1.102 vim workers 192.168.1.101 192.168.1.102 192.168.1.103 # 测试部署是否成功 alluxio format alluxio-start.sh all SudoMount alluxio fsadmin report alluxio runTests # 如果出现Passed the test则说明部署成功 # 测试高可用模式的自动故障处理: (假设此时hadoop101位primary master) ssh hadoop101 jps | grep AlluxioMaster kill -9 &lt;AlluxioMaster PID&gt; alluxio fs leader # 显示新的primary Master(可能需要等待一小段时间选举) 部署说明 Alluxio可以像CM一样，部署在同一网络中的节点上且不需要机器间免密登陆。免密登陆只是为了方便使用start-all.sh脚本一键启动。非免密登陆的集群可以使用Ansible自动化运维工具对每个节点执行启动和挂载等操作(在每个Master上使用部署Alluxio的用户分别执行alluxio-start.sh master,然后如果使用非root用户启动Alluxio服务，则要在每个worker的root用户执行alluxio-mount.sh Mount local ,然后用部署Alluxio的用户执行alluxio-start.sh worker,并在所有节点alluxio-start.sh job_master,alluxio-start.sh job_worker即可)，作用等同于start-all.sh脚本，不会对Alluxio服务的运行造成影响。 Mount和SudoMount需要在root权限下执行，因为只有root用户有权限创建和访问RamFS，启动Alluxio的用户要有这个RamFS的读写执行权限，Alluxio的RAM FLODER（ramdisk）可以理解为是在普通HDD磁盘目录上挂载的一个RamFS文件系统，RamFS是把系统的RAM作为存储，且RamFS不会使用swap交换内存分区，Linux会把RamFS视为一个磁盘文件目录。 查看RamFS的方法： mount | grep -E “(tmpfs|ramfs)” 这里的tmpFS也是基于内存的存储系统，但它会使用到Swap分区，使读写效率降低，Alluxio也可以使用tmpFS作为缓存。 了解更多:ramfs和tmpfs的区别 Alluxio的”/“目录权限由启动Mater和Worker的用户决定，并与UFS中对应的文件夹权限一致，可以修改Alluxio根目录权限，Alluxio创建文件和文件夹的用户和组与Linux用户合组一致，并且与持久化到HDFS的文件的用户和组一致。 Mount|SudoMount|Umount|SudoUmount说一下这四个参数，Mount和SudoMount是挂载RamFS，后者带sudo权限，Umount和SudoUmount是卸载RamFS，后者带sudo权限。Mount和SudoMount会格式化已存在的RamFS。5.关于用户模拟的一些理解和使用很重要参考这篇文章：User Impersonation相关配置问题分析与解决 Alluxio部署前，要决定用哪个用户启动Alluxio，如果底层存储是HDFS，建议使用启动NameNode进程的用户来启动Alluxio Master和Workers,保证HDFS权限映射：Alluxio On HDFS Mount参数一般只在Worker节点使用 可以在HDFS建立一个777权限的文件路径作为Alluxio的底层存储 job_master和job_worker官网没做介绍，但在当前版本这两个组件必须启动，否则会影响persist功能以及一些其他功能(我目前只知道persist会Time Out) 配置这块踩了好多坑，终于，Alluxio基本服务部署完毕,一些关于优化和细节的参数在Alluxio原理部分中涉及到,也可查阅Alluxio配置参数大全 Alluxio2.1.0版本官方介绍说使用ASYNC_THROUGH进行写入时防止数据丢失，所以我这里设置了ASYNC_THROUGH异步写磁盘，既能保证写入速度，又能将文件持久化之前配置Alluxio高可用，一直不稳定，心跳中断，Master和Worker掉线问题频发，Alluxio2.1版本官方说修复了各种心跳中断问题,当然Alluxio的高可用要求底层的Journal日志存储系统的稳定性很高，如果底层Journal存储系统不稳定（比如HDFS No More Good DataNode的情况），就会导致Master崩溃。 Alluxio常用命令Alluxio命令速查表包括缓存载入,驻留,释放,数据生存时间等重要命令Alluxio常用Shell命令速查表: #文件基本操作 可以在执行命令时指定参数 方法: alluxio fs -D...指定参数 copyFromLocal .... alluxio fs cat &lt;path/file&gt; # 打开文件 alluxio fs ls [-d|-f|-p|-R|-h|--sort=option|-r] &lt;path&gt; #查看目录 alluxio fs copyFromLocal [--thread &lt;num&gt;] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;remoteDst&gt; # 从本地上传文件到Alluxio alluxio fs copyToLocal [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;localDst&gt; #从Alluxio载文件到本地 alluxio fs count &lt;path&gt; # 统计Alluxio目录的文件数,文件夹数和总大小 alluxio fs du [-h|-s|--memory] &lt;path&gt; # 文件大小 alluxio fs cp [-R] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;dst&gt; #复制文件 alluxio fs mv &lt;src&gt; &lt;dst&gt; # 移动文件 alluxio fs rm [-R] [-U] [--alluxioOnly] &lt;path&gt; # 删除文件 alluxio fs mkdir &lt;path1&gt; [path2] ... [pathn] # 创建文件夹 alluxio fs touch &lt;path&gt; #创建一个空文件 alluxio fs setTtl [--action delete|free] &lt;path&gt; &lt;time to live&gt; # 设置一个文件的TTL时间 alluxio fs unsetTtl &lt;path&gt; # 删除文件的TTL值 alluxio fs checksum &lt;Alluxio path&gt; # 得到文件的MD5值 alluxio fs stat &lt;path&gt; # 显示文件路径信息 alluxio fs tail &lt;path&gt; # 显示文件最后1KB的内容 alluxio fs location &lt;path&gt; # 输出包含某个文件数据的主机,使用location命令可以调试数据局部性 alluxio fs help &lt;command&gt; # 查看命令介绍和用法 alluxio fs distributedMv &lt;src&gt; &lt;dst&gt; # 并行移动文件或目录 alluxio fs distributedCp &lt;src&gt; &lt;dst&gt; # 并行复制文件或目录 alluxio fs distributedLoad [--replication &lt;num&gt;] &lt;path&gt; # 在alluxio空间中加载文件或目录，使其驻留在内存中 #与底层存储的交互操作 alluxio fs load [--local] &lt;path&gt; # load命令可为数据分析编排数据,加快数据分析的效率,load命令将底层文件系统中的数据载入到Alluxio中,如果运行该命令的机器上正在运行一个Alluxio worker,那么数据将移动到该worker上,否则数据会被随机移动到一个worker上。 如果该文件已经存在在Alluxio中,设置了--local选项,并且有本地worker,则数据将移动到该worker上。否则该命令不进行任何操作。如果该命令的目标是一个文件夹,那么其子文件和子文件夹会被递归载入。 alluxio fs persist [-p|--parallelism &lt;#&gt;] [-t|--timeout &lt;milliseconds&gt;] [-w|--wait &lt;milliseconds&gt;] &lt;path&gt; [&lt;path&gt; ...] # 持久化Alluxio中的数据到底层存储 alluxio fs checkConsistency [-r] [-t|--threads &lt;threads&gt;] &lt;Alluxio path&gt; # 检查Alluxio与底层存储系统的元数据一致性(确定文件在底层存储还是在under storage system.) alluxio fs free -f &lt;&gt; # 已经持久化到底层存储,但内存中还保留着的文件可以通过free从内存中释放,未被持久化的文件不能被free alluxio fs mount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; &lt;ufsURI&gt;] # 将底层文件系统的&quot;ufsURI&quot;路径挂载到Alluxio命名空间中的&quot;alluxioPath&quot;路径下，&quot;path&quot;路径事先不能存在并由该命令生成。 没有任何数据或者元数据从底层文件系统加载。当挂载完成后，对该挂载路径下的操作会同时作用于底层文件系统的挂载点。monut命令可以挂载linux服务器上的某个文件夹alluxio fs mount /demo file:///tmp/alluxio-demo alluxio fs unmount &lt;alluxioPath&gt; # 取消挂载 alluxio fs updateMount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; # 保留元数据的同时更改挂载点设置 alluxio fs pin &lt;path&gt; media1 media2 media3 ... 如果管理员对作业运行流程十分清楚，那么可以使用pin命令手动提高性能。pin命令对Alluxio中的文件或文件夹进行标记。该命令只针对元数据进行操作，不会导致任何数据被加载到Alluxio中。如果一个文件在Alluxio中被标记了，该文件的任何数据块都不会从Alluxio worker中被剔除。如果存在过多的被锁定的文件，Alluxio worker将会剩余少量存储空间，从而导致无法对其他文件进行缓存。 alluxio fs unpin &lt;path&gt; # 将Alluxio中的文件或文件夹解除标记。该命令仅作用于元数据，不会剔除或者删除任何数据块。一旦文件被解除锁定，Alluxio worker可以剔除该文件的数据块。 alluxio fs startSync &lt;path&gt; # 启动指定路径的自动同步进程 alluxio fs stopSync &lt;path&gt; # 关闭指定路径的自动同步进程 alluxio fs setReplication [-R] [--max &lt;num&gt; | --min &lt;num&gt;] &lt;path&gt; # 设置给定路径或文件的最大/最小副本数 (-1表示不限制最大副本数) -R递归 #权限相关操作及管理员命令 alluxio fs chgrp [-R] &lt;group&gt; &lt;path&gt; # 换组 alluxio fs chmod [-R] &lt;mode&gt; &lt;path&gt; # 更改读写执行等权限 alluxio fs chown [-R] &lt;owner&gt;[:&lt;group&gt;] &lt;path&gt; # 所有者 alluxio fsadmin backup [directory] [--local] # 备份Alluxio的元数据到备份目录(默认目录由alluxio.master.backup.directory决定) alluxio fsadmin doctor [category] # 显示错误和警告 alluxio fsadmin report [category] [category args] # 报告运行集群的信息 alluxio fsadmin ufs --mode &lt;noAccess/readOnly/readWrite&gt; &quot;ufsPath&quot; # 更新挂载的底层存储系统的属性 alluxio formatMaster 初始化Master元数据 alluxio formatWorker 初始化Worker数据，Worker数据会被清空 alluxio getConf [key] 查看各个组件的参数和配置 key:[--master / --source] alluxio runJournalCrashTest 测试Alluxio 高可用日志系统（会停止服务一段时间） alluxio runUfsTests --path &lt;ufs path&gt; alluxio validateConf 使修改的配置生效 alluxio validateEnv &lt;args&gt; 使运行环境生效 alluxio copyDir &lt;PATH&gt; 类似于xsync脚本，可以向各个节点分发文件 #集群相关信息 alluxio fs masterInfo # 获得master节点的信息 alluxio fs leader # 打印当前Alluxio的leader master节点主机名。 alluxio fs getCapacityBytes # 获取Alluxio总容量 alluxio fs getSyncPathList # 获取同步路径列表 alluxio fs getUsedBytes # 获取已用空间大小 alluxio fs getfacl &lt;path&gt; # 显示访问控制列表(ACLs) alluxio fs setfacl [-d] [-R] [--set | -m | -x &lt;acl_entries&gt; &lt;path&gt;] | [-b | -k &lt;path&gt;] # 设置访问控制列表(ACLs) 上面的命令不能帮到你? 那就戳这里:Alluxio命令使用示例管理员命令使用示例 Alluxio WEB UI介绍及使用Alluxio master提供了Web界面以便用户管理Alluxio master Web界面的默认端口是19999:访问 http://MASTER IP:19999 即可查看Alluxio worker Web界面的默认端口是30000:访问 http://WORKER IP:30000 即可查看WEB UI官方介绍很明确-&gt;戳这里:Alluxio Web UI Alluxio与计算框架整合 Alluxio+Hive频繁使用的表存在Alluxio上，可通过内存读文件获得更高的吞吐量和更低的延迟 准备工作:cd /opt/module/hive vim conf/hive-env.sh export HADOOP_HOME=/opt/module/hadoop-2.7.2 # 添加 export HIVE_AUX_JARS_PATH=$ALLUXIO_HOME/client:$HIVE_AUX_JARS_PATH 四种情况: 创建一个Hive表并指定其存储在Alluxio bin/hive create table alluxio_test( id int, name string, color string ) row format delimited fields terminated by &#39;\\t&#39; LOCATION &quot;alluxio://hadoop101:19998/user/hive/warehouse/alluxio_test&quot;; # 查看表位置 describe extended alluxio_test; 已存在HDFS的内部表 bin/hive describe extended table_name; # 查看Hive表存储位置 alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; msck repair table table_name; # 确定alluxio对应位置存在表数据后修复Hive表元数据 第一次访问alluxio中的文件默认会被认为访问hdfs的文件，一旦数据被缓存在Alluxio中，之后的查询数据都会从Alluxio读取。 已存在HDFS的外部表 bin/hive describe extended table_name; # 将表数据改为Alluxio存储 alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; # 还原表数据到HDFS alter table table_name set location &quot;hdfs://hadoop101:9000/user/hive/warehouse/table_name&quot; describe extended table_name; Hive使用Alluxio作为默认存储系统 vim conf/hive-site.xml # 添加以下属性 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;alluxio://hadoop101:19998&lt;/value&gt; &lt;description&gt;Hive Use Alluxio As Default FileSystem&lt;/description&gt; &lt;/property&gt; # 对Hive指定的Alluxio配置属性，将它们添加到每个结点的Hadoop配置目录下core-site.xml中。例如，将alluxio.user.file.writetype.default 属性由默认的MUST_CACHE修改成CACHE_THROUGH: &lt;property&gt; &lt;name&gt;alluxio.user.file.writetype.default&lt;/name&gt; &lt;value&gt;CACHE_THROUGH&lt;/value&gt; &lt;/property&gt; # Alluxio中为Hive创建目录 alluxio fs mkdir /tmp alluxio fs mkdir /user/hive/warehouse alluxio fs chmod 775 /tmp alluxio fs chmod 775 /user/hive/warehouse # 检查Hive与Alluxio的集成情况 integration/checker/bin/alluxio-checker.sh -h # 查看该命令帮助 integration/checker/bin/alluxio-checker.sh hive -hiveurl [HIVE_URL] 注:CM集群设置Hive连接Alluxio Client的方式: 排坑: 安全认证问题: alluxio-site.properties中添加要模拟的用户: alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* Alluxio+SparkSpark可以在进行简单配置后直接使用Alluxio作为数据访问层，Spark应用程序可以通过Alluxio透明地访问许多不同类型的持久化存储服务（例如，AWS S3 bucket、Azure Object Store buckets、远程部署的 HDFS 等）的数据，也可以透明地访问同一类型持久化存储服务不同实例中的数据。为了加快I/O性能，用户可以主动获取数据到Alluxio中或将数据透明地缓存到Alluxio中，尤其是在Spark部署位置与数据相距较远时特别有效。此外，通过将计算和物理存储解耦，Alluxio 能够有助于简化系统架构。当底层持久化存储中真实数据的路径对 Spark 隐藏时，对底层存储的更改可以独立于应用程序逻辑；同时Alluxio作为邻近计算的缓存，仍然可以给计算框架提供类似 Spark 数据本地性的特性。 配置参数配置（spark-defaults.conf中添加）spark.driver.extraClassPath /&lt;PATH_TO_ALLUXIO&gt;/client/alluxio-2.0.1-client.jarspark.executor.extraClassPath /&lt;PATH_TO_ALLUXIO&gt;/client/alluxio-2.0.1-client.jar或者Jar包拷贝cp client/alluxio-2.0.1-client.jar $SPARK_HOME/jars/如果高可用的Alluxio,还需在spark-default中指定: spark.driver.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true spark.executor.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true 或者配置Hadoop文件core-site.xml如下 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.address&lt;/name&gt; &lt;value&gt;zkHost1:2181,zkHost2:2181,zkHost3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 自定义Spark作业中Alluxio的属性：spark-submit…. –driver-java-options “-Dalluxio.user.file.writetype.default=CACHE_THROUGH” 而不是–conf val s = sc.textFile(&quot;alluxio://192.168.1.101:19998/LICENSE&quot;) val double = s.map(line =&gt; line + line) double.saveAsTextFile(&quot;alluxio://192.168.1.101:19998/out&quot;) df = spark.table(&quot;select ...&quot;) df.format.parquet(&quot;alluxio://xxxxx&quot;) 官方Alluxio+Spark配置设置 检查配置是否正确在$ALLUXIO_HOME运行 integration/checker/bin/alluxio-checker.sh spark spark://sparkMaster:7077 使用 存储 RDD 到 Alluxio 内存中就是将 RDD 作为文件保存到 Alluxio 中: saveAsTextFile：将 RDD 作为文本文件写入，其中每个元素都是文件中的一行 saveAsObjectFile：通过对每个元素使用 Java 序列化，将 RDD 写到一个文件中 // as text file rdd.saveAsTextFile(&quot;alluxio://localhost:19998/rdd1&quot;) rdd = sc.textFile(&quot;alluxio://localhost:19998/rdd1&quot;) // as object file rdd.saveAsObjectFile(&quot;alluxio://localhost:19998/rdd2&quot;) rdd = sc.objectFile(&quot;alluxio://localhost:19998/rdd2&quot;) 缓存 Dataframe 到 Alluxio 中(将 DataFrame 作为文件保存到 Alluxio 中): df.write.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) df = sqlContext.read.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) Alluxio对Shuffle的提升目前三种方案:一是基于Alluxio-Fuse客户端,无需修改源码,直接挂载Shuffle目录,但Alluxio-Fuse目前的性能不是很好二是重写Spark Shuffle Service底层源码实现基于Alluxio Client的Shuffle三是可以Splash Shuffle Manager插件,我的另一篇文章有讲到 -&gt; QCon总结-Splash Shuffle Manager当然也可以选择等Spark3.0的Remote Shuffle Service Alluxio+HadoopMR运行HadoopMR程序: bin/hadoop jar ../libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount -Dalluxio.user.file.writetype.default=CACHE_THROUGH -libjars /opt/module/alluxio/client/alluxio-2.0.1-client.jar \\&lt;INPUT FILES&gt; &lt;OUTPUT DIRECTORY&gt; Alluxio+Presto后续更新… 性能测试使用官方提供的沙箱申请官方测试沙箱Sandbox：ALLUXIO SANDBOX申请成功后，按照邮件的指引操作，注意，bin/sandbox setup &amp;的过程中千万不要Ctrl+C中止,部署完成状态如下图： 运行基准测试（TPC-DS），耐心等待后的测试结果：已安装TPC-DS基准套件，用于运行性能测试。Spark已安装为TPC-DS用来将其作业发送到的计算框架。TPC-DS的比例因子为100，这与26GB的数据集大小相关。由索引单独标识的基准按不同的使用方案分组，并且将结果报告为每个方案的汇总。其中 w/o是without，即只是用S3为直接底层存储的情况；w/是with，即使用了Alluxio作为中间件下的性能从图中测试结果可以看出,当计算数据存储在公有云虚拟机实例中时，Alluxio作为存储与计算框架的中间件，能够有1.5-3倍左右的性能提升受到各方面限制，以上测试结果并非Alluxio的最佳预期。其他人的试过程 自测Spark Sql做测试时候多次重复作业输入数据位于OS的高速缓冲区,Alluxio没有加速效果甚至变慢我的测试环境是三台机器,每台101GB内存,16核,同台机器部署CM Hadoop,Spark,Hive,AlluxioWorker,AlluxioClientAlluxio读参数CACHE_PROMOTE,写参数CACHE_THROUGH 测试方法 测试操作 运行时间(HDFS) 运行时间(Alluxio) 表结构 SparkSQL select count(1) from table; 4s 6s 13.5GB 17字段 SparkSQL select count(1) from table; 5s 6s 13.5GB 17字段 SparkSQL select count(1) from table; 6s 8s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 80s 80s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 77s 52s 13.5GB 17字段 SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 60s 73s 13.5GB 17字段 SparkSQL select count(1) from test.table group by language; 11.5s 11.5s 13.5GB 17字段 Spark Persist df.write.parquet(Path) 3.0min 4.0min 13.5GB 17字段 Spark Persist spark.read.parquet(Path).count() 4s 5s 13.5GB 17字段 Spark Persist spark.read.parquet(Path).count() 6s 6s 13.5GB 17字段 后来又做了Spark Dataframe的Persist到MEMORY_ONLY和Persist到Alluxio,效果也不是很好,究其原因,我认为是我的HDFS DataNode已经和计算框架Spark部署在一起了,而且磁盘IO没有瓶颈,所以这不符合Alluxio的应用场景,从而没有令人满意的效果.至于HDFS更快的原因,我想是Spark要读取的数据很可能已经存在OS的高速缓冲区Alluxio还是要用对场景才行. Alluxio FUSE什么是Alluxio FUSEAlluxio-FUSE可以在一台Unix机器上的本地文件系统中挂载一个Alluxio分布式文件系统。通过使用该特性，一些标准的命令行工具（例如ls、 cat以及echo）可以直接访问Alluxio分布式文件系统中的数据。此外更重要的是用不同语言实现的应用程序如C, C++, Python, Ruby, Perl, Java都可以通过标准的POSIX接口(例如open, write, read)来读写Alluxio，而不需要任何Alluxio的客户端整合与设置。 Alluxio FUSE局限性 文件只能顺序地一次写入,不能修改和覆盖,如果要修改就要删除原文件再创建 不支持soft-link和hard-link(即ln) alluxio.security.group.mapping.class选项设置为ShellBasedUnixGroupsMapping的值时,用户与分组信息才与Unix系统的用户分组对应 与直接使用Alluxio客户端相比，使用挂载文件系统的性能会相对较差 Alluxio FUSE使用 挂载 挂载alluxio_path到本地mount_point,mount_point必须是本地文件系统中的一个空文件夹，并且启动Alluxio-FUSE进程的用户拥有该挂载点及对其的读写权限。可以多次调用该命令来将Alluxio挂载到不同的本地目录下。所有的Alluxio-FUSE会共享$ALLUXIO_HOME\\logs\\fuse.log这个日志文件。 integration/fuse/bin/alluxio-fuse mount mount_point [alluxio_path] 卸载 integration/fuse/bin/alluxio-fuse umount mount_point 检查挂载点运行信息 integration/fuse/bin/alluxio-fuse stat 注意事项要使用启动master和worker的用户来挂载fuse，比如使用hdfs用户启动的Alluxio，则要用hdfs来挂载，可以正常使用，如果使用root用户挂载，目录信息会乱码且无法正常使用。hdfs用户下成功mount后，切换到root用户也会看到挂载点信息乱码。Alluxio相关服务未启动，挂载点信息也会乱码。Alluxio默认只能写本地worker，如果明确知道要写入的文件大小的范围，可以使用ASYNC_THROUGH并加大worker的缓存大小，或者配置多级缓存使worker的缓存空间大于写入文件的大小，才能防止被置换，从而提高效率如果不确定写的文件大小的范围，就不要使用ASYNC_THROUGH这个参数，因为如果本地Worker缓存空间不够就会写入失败，这时，为了保险起见可以使用写参数CACHE_THROUGH边缓存边写或写参数THROUGH只写底层存储，来防止写入文件失败。当然，还有一种比较好的方案，写参数设为ASYNC_THROUGH配合更大的Worker缓存来提高效率，同时设置alluxio.user.file.write.location.policy.class=alluxio.client.file.policy.RoundRobinPolicy参数来保证写入不会失败。如果只写一次，可以及时free掉无用的缓存，减少后面写数据时发生的缓存置换。 Alluxio 客户端APIJava APIAlluxio提供了两种不同的文件系统API：Alluxio API和与Hadoop兼容的API,Alluxio API提供了更多功能，而Hadoop兼容API为用户提供了使用Alluxio的灵活性，无需修改使用Hadoop API编写的现有代码.Maven项目依赖设置 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.my.alluxio&lt;/groupId&gt; &lt;artifactId&gt;AlluxioTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- alluxio-fs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.alluxio&lt;/groupId&gt; &lt;artifactId&gt;alluxio-core-client-fs&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hdfs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;7&lt;/source&gt; &lt;target&gt;7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打jar插件 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--Jar包运行时的主类--&gt; &lt;mainClass&gt;IOTestUtil&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Java读写文件API import alluxio.AlluxioURI; import alluxio.client.file.FileInStream; import alluxio.client.file.FileOutStream; import alluxio.exception.AlluxioException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; /** * HDFS &amp; Allxuio IO读取文件测试工具 IO接口 文件API */ public class IOTestUtil { public static void main(String[] args) throws IOException, AlluxioException { String filePath = args[0]; HDFSUtil h = new HDFSUtil(&quot;hdfs://192.168.1.101:8020&quot;); h.readFile(filePath); AlluxioUtil a = new AlluxioUtil(); a.readFile(filePath); System.out.println(&quot;读文件测试 Finished&quot;); System.out.println(&quot;------------------------&quot;); if (args.length != 1){ String fileToWritePath = args[1]; a.writeFile(fileToWritePath); System.out.println(&quot;写文件测试 Finished&quot;); } } } class HDFSUtil{ private Configuration conf = new Configuration(); public HDFSUtil(String HDFSURL){ conf.set(&quot;fs.defaultFS&quot;,HDFSURL); System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;hdfs&quot;); } public void readFile(String path) throws IOException { FileSystem fs = FileSystem.get(conf); fs.getFileStatus(new Path(path)); FSDataInputStream in = fs.open(new Path(path)); try{ long hdfsStartTime=System.currentTimeMillis(); in = fs.open(new Path(path)); byte[] buffer = new byte[1024]; int byteRead = 0; while ((byteRead = in.read(buffer)) != -1) { System.out.write(buffer, 0, byteRead); //输出字符流 } long hdfsEndTime=System.currentTimeMillis(); System.out.println(&quot;HDFS读取运行时间:&quot;+(hdfsEndTime-hdfsStartTime)+&quot; ms&quot;); }catch (Exception e){ e.printStackTrace(); } finally { in.close(); } } } class AlluxioUtil{ private static final alluxio.client.file.FileSystem fs = alluxio.client.file.FileSystem.Factory.get(); public AlluxioUtil(){} public FileInStream readFile(String AlluxioPath) throws IOException, AlluxioException { AlluxioURI path = new AlluxioURI(AlluxioPath); //封装Alluxio 文件路径的path FileInStream in = fs.openFile(path); try{ long startTime=System.currentTimeMillis(); in = fs.openFile(path); // 调用文件输入流FileInStream实例的read()方法读数据 byte[] buffer = new byte[1024]; int byteRead = 0; // 读入多个字节到字节数组中，byteRead为一次读入的字节数 while ((byteRead = in.read(buffer)) != -1) { System.out.write(buffer, 0, byteRead); //输出字符流 } long endTime=System.currentTimeMillis(); System.out.println(&quot;Alluxio读取运行时间:&quot;+(endTime-startTime)+&quot; ms&quot;); }catch (IOException | AlluxioException e){ e.printStackTrace(); }finally { in.close(); } in.close(); //关闭文件并释放锁 return in; } public void writeFile(String AlluxioPath) throws IOException, AlluxioException { AlluxioURI path = new AlluxioURI(AlluxioPath); // 文件夹路径 FileOutStream out = null; try { out = fs.createFile(path); //创建文件并得到文件输入流 out.write(&quot;qjj1234567&quot;.getBytes()); // 调用文件输出流FileOutStream实例的write()方法写入数据 }catch (IOException | AlluxioException e){ e.printStackTrace(); }finally { out.close(); // 关闭和释放文件 } } } Python APIAlluxio的Python库基于REST API实现的CentOS6和Windows的环境下安装alluxio的python库失败，最终在CentOS7 Python2.7.5的环境下成功执行了pip install alluxio if __name__ == &#39;__main__&#39;: print(&quot;后续用到API再更新&quot;) pass Q&amp;A 加速不明显? Alluxio通过使用分布式的内存存储以及分层存储,和时间或空间的本地化来实现性能加速。如果数据集没有任何本地化, 性能加速效果并不明显。 速度反而更慢了? 测试时尽量多观察集群的CPU占用率,Yarn内存分配和网络IO等多种因素,可能瓶颈不在读取数据的IO上。 确保要读取的数据缓存在Alluxio中,才能加速加速数据的读取。 一定要明确应用场景,Alluxio的设计主要是针对计算与存储分离的场景。在数据远端读取且网络延迟和吞吐量存在瓶颈的情况下,Alluxio的加速效果会很明显,但如果HDFS和Spark等计算框架已经共存在一台机器(计算和存储未分离),Alluxio的加速效果并不明显,甚至可能出现更慢的情况。 多次重复作业输入数据位于OS的高速缓冲区,Alluxio没有加速效果甚至变慢。 内存爆炸，副本过多内存占用过大？ 两种方案：关闭被动缓存alluxio.user.file.passive.cache.enabled=false关闭被动缓存对于不需要数据本地性但希望更大的Alluxio存储容量的工作负载是有益的，或者通过命令alluxio fs setReplication -R –max 5 限制某个目录的文件最大副本数 一些官方的Q&amp;A Alluxio官方问题与答案 总结 对新技术的调研，最重要的是了解它的应用场景，只有场景对了，效果才会很明显 一定要多看官方文档，虽然Alluxio文档不是很详细，但也有帮助，要自己找细节 对自己遇到的难以解决的问题要积极与社区沟通和讨论 自己遇到的问题可能别人也遇到了，有可能是版本的BUG，或许已经有人提交Issue了，一定多留意 新的稳定版发行，一定要了解它的新特性以及修复了哪些漏洞","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Alluxio","slug":"Alluxio","permalink":"https://shmily-qjj.top/tags/Alluxio/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"我的2019年度总结博客","slug":"我的2019年度总结博客","date":"2020-01-01T00:08:08.000Z","updated":"2021-04-05T05:03:42.531Z","comments":true,"path":"2019Summary/","link":"","permalink":"https://shmily-qjj.top/2019Summary/","excerpt":"","text":"我的2019年度总结2019对我来说是很有意义的一年，离开学校步入社会，找到了人生的第一份工作，第一次建立了个人博客，技术也是突飞猛进，还结交了许多新伙(da)伴(lao),头发也少了几根哈哈哈哈….感谢2019，感谢她带给我这么多，一向高冷的她刚刚竟然用不舍的语气跟我说：“去吧，2020在等着你呢，她更需要你！我会望着你的背影渐渐远去，期待着2020使你更优秀，我也会默默替你高兴的！我也舍不得你，时光不等人，我们只好就此别过了。”还有几个小时就是2020年了，我还真有点舍不得她(2019),可她终究还是要离开我，所以我把对她(2019)的怀念，以及对她(2020)的憧憬都记录在这篇博客里吧！(吃瓜群众:tui，你个渣男!!!) 回顾2019主持人：欢迎今晚的嘉宾，佳境同学！2019即将结束，2020将如期而至，说说你现在的感受？我：2019年，我收获了好多，离开学校，步入社会，这才感觉自己真正长大了，肩上的责任开始重了，生活节奏也完全不一样了，我觉得步入社会的感觉挺好，没有想象中的那么可怕，工作虽不轻松，但能充实自己，身边也都是一些有趣的同事，很聊得来，每天都挺嗨皮的！感谢2019给予我的这些！主持人：说说2019你都经历了什么吧！我：三月四月，天天泡在曲府(学校老校区东区的607自习室，我的府上)，效率不高，但也能学学习，同学经常来光顾，一起学习，挺欢乐的。还有可嘉和健哥我们一起去吃饭，去学校南区看小姑娘，哈哈。 五月，劳动节假期去了五常玩，吃到了纯正的五常大米，爬了山，坐着小电驴放着乡村爱情主题曲，玩的很尽兴！吃过了东区最后一顿早餐，上过了东区最后一节课，我们离开了生活三年的老校区，虽然老校区很破，但生活了这么久，也有感情了，离开还挺感慨的，就像现在我要离开2019一样！不过来了新校区，也算是体验到了大学生活该有的样子，跟骏儿他们坐在操场看晚会，还跟俄罗斯小姐姐合照，hin开心的一段时光！ 六月，端午回家，第一次和爸妈去歌厅，唱到《时间都去哪了》，瞬间泪崩，感叹岁月不饶人。六月的英语六级考试虽然没过没，但却收获了一枚女朋友，还趁着年轻玩了一场异地恋奔现。没能最终走到一起，但也感谢她的出现使我成长，她是个善良的好姑娘，我在这里也祝她幸福，每天开心吧！ 七月，那段时间坚持早起跑步，瘦了，瘦的挺快的，从一个小胖墩变得不那么胖了！那段时间我努力地复习着学过的知识，同时在哈行开发部门实习，什么也不让我们实习生碰，所以我想早点离开，于是紧张地准备秋招，总觉得心里有块石头一直悬着似的，因为还没面试。 八月辞掉了那边，来上海这边工作，由于我的专业限制，只有在北上广深才有发展，离家那么远，还是毅然决然地来了，趁着年轻，就是要闯荡一番！一开始对魔都很陌生，以为是一座冷冰冰的城市，现在好多了。 九月，第一次参加部门团建，吃阳澄湖大闸蟹+打卡苏州园林，玩的好开心，跟同事也熟悉了！ 十月，网恋奔现，去天津玩了几天，跟她各种游荡，去了世纪钟广场，喝了玫瑰酒，看了《中国机长》，去了猫咖撸猫，还去了“分手轮”天津之眼，最终还是没能逃过它的诅咒哈哈！劝广大情侣们去天津之眼要慎重啦！中旬，沾Leader的光，有机会参加QCon全球软件开发大会，收获了很多！ 十一月到现在，写代码，改代码，修BUG，测试，调研，调优… 主持人：那2019年对你来说还真是有意义的一年啊！那你觉得2019年你最大的变化是什么呢？我：假期变少了算吗。。(主持人一脸无奈：不算！！！)好叭…身体上，我比以前瘦了，哈哈，减肥成功不易！心理上呢。。。我觉得我更加独立了，更加成熟了，上进心和求知欲都UP UP!主持人：嗯，你一定可以在保持好身材的同时，保持上进行和求知欲的！2019年，有没有发现自己的不足？我：有的！上班才发现原来工作中有好多提高效率的方法我不会，有个编码前思考过多的毛病，还有有时候想问题会想当然，而且一些原理相关的东西掌握不熟还有一些关键问题会拿捏不准，算法基础也比较薄弱，要学要补的东西很多！干就完了！有优秀的老哥们带着，不怕了！主持人：那生活方面和做事方面呢？我：我觉得还好，以前我是内向型的，但是跟身边人熟悉了就会开朗许多了！奥，对了，有个坏习惯，一定要改，那就是熬夜，希望在2020我能逐渐改掉熬夜的坏毛病，有规律的生活。还有要多锻炼身体，毕竟身体是革命的本钱嘛！主持人：娱乐方面呢？记得你喜欢弹吉他？我：诶，半年没弹了，生疏了，不过会捡起来的。最近只靠打游戏来娱乐了，在王者峡谷里他们都夸我是我国服元歌。。。主持人：2019年的音乐方面作品少了很多，原来时间花在了玩游戏上！我：不不，2019是很忙的一年，没有时间做后期…不过，游戏确实也玩了，玩了一直很喜欢的一款《地铁离去》，里面主角阿尔乔姆弹的吉他很动听，喏，我还自己扒了谱子《Metro Exodus》。现在一想，确实比去年少了很多作品，在云村只发了有一首翻弹《少年的梦》，和两首翻唱《懂了就懂了》、《浮生未歇》。其实还有两首已经录好了，还没来得及混音，哈哈！主持人：此时此刻，你有什么想对朋友和亲人说的？我：对我的朋友们说：朋友们等我回去嗨！啤酒踩着箱子喝，一天三顿小烧烤给我安排上！然后对爸妈说，过年等我回去过个团圆年！我会注意身体少熬夜的！等我发达了带你们旅游去！我不在身边，你们可要注意身体呀！主持人：你的朋友们听了很开心吧，你的父母，也会为你感动，为你骄傲，希望你在2020年，无论事业还是生活，都能更上一层楼！我：谢谢您！主持人：那在节目的最后，你还有什么要给大家分享的吗？我：嗯，今天准备了几张照片，把2019最美好的回忆分享给大家！ 展望2020挥手告别2019，2020我来啦！2020我们一起加油！ 小目标 代码能力提高，深入理解设计模式 框架底层原理掌握，开始逐步向源码层面深入 提高对大数据技术栈的整体认识，先有深度，后有广度 工作效率提高，熟悉快捷键 养成好的写文档习惯，只要值得深入学习的东西都尽量写博客 吉他能弹得更好吧，网易云作品的质量提高 改掉熬夜坏习惯 交几个好朋友啥的 凑九条，希望能久久铭记我的2020小目标 小期待 期待早睡早起健康的自己 期待求知进取爱拼的自己 期待天天向上进步的自己 期待争分夺秒高效的自己 期待悠然自得快乐的自己 期待……..一夜暴富哈哈哈 小愿望第一个愿望就是家人能健健康康，自己也养成健康的生活习惯第二个愿望是在大数据方面有更深入的学习和实践第三个愿望是结交好朋友，认识新朋友最后一个愿望就是要正在看我博客的你每天都开心呀 小想法真该早睡了，我想早睡，我好想早睡呀！可是一到晚上就贼精神。。。有没有小伙伴可以互相监督的呀！可以在下方评论区给我留言吼！最后，祝大家在2020年，身体倍儿棒，吃嘛嘛香，学业有成，生意兴旺…还有一夜暴富昂，最重要的是暴富了可别忘了我昂，哈哈哈哈。","categories":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"个人总结","slug":"个人总结","permalink":"https://shmily-qjj.top/tags/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"},{"name":"2019","slug":"2019","permalink":"https://shmily-qjj.top/tags/2019/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"数据库事务ACID理解","slug":"数据库事务ACID理解","date":"2019-12-28T12:22:00.000Z","updated":"2020-04-12T14:15:48.001Z","comments":true,"path":"1f7eb1b3/","link":"","permalink":"https://shmily-qjj.top/1f7eb1b3/","excerpt":"","text":"Intro对于事务ACID，知道大概的意思，但总觉得对这个概念还有点模糊，所以写一篇博客加深一下印象。 数据库的事务一个事务中可能有多个操作，当所有操作都成功了的情况下这个事务才会被提交，如果其中一个操作失败，整个事务都将回滚(Rollback)到事务开始前的状态，好像这个事务从未执行过。简单来说就是:要么什么都不做，要么做全套（All or Nothing） ACIDACID是指数据库事务正确执行的四个基本特征的缩写通过上图可以大概了解ACID的基本特征，下面做详细介绍 原子性（Atomicity）事务中包含的操作集合，要么全部操作执行完成，要么全部都不执行。即当事务执行过程中，发生了某些异常情况，如系统崩溃、执行出错，则需要对已执行的操作进行回滚，清除所有执行痕迹。例子：A向B转账100，这个事务包括两步(A失去100，B得到100)，原子性保证这两步都成功或者都失败。 一致性（Consistency）事务执行前和事务执行后，数据库的完整性约束不被破坏。即事务的执行是从一个有效状态转移到另一个有效状态。例子：A向B转账100，这个事务包括两步(A失去100，B得到100)，A和B所在的表收入和支出存在外键约束，若A支出增加而B收入未增加，则违反了一致性约束。 隔离性（Isolation）数据库允许多个并发事务同时对数据进行读写和修改的能力，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。多个事务并发执行时，彼此之间不应该存在相互影响。隔离程度不是绝对的，每个数据库都提供有自己的隔离级别，每个数据库的默认隔离级别也不尽相同。例子：A向B转账100，交易还未完成时，B查询不到100元入账。 持久性（Durability）事务正常执行完毕后，对数据库的修改是永久性的，即便系统故障也不会丢失。即事务的修改操作已经记录到了存储介质中。例子：A向B转账100，A永久失去了100元而B永久得到100元，不能赖账。 总结ACID 原子性：事务操作的整体性。 一致性：事务操作下数据的正确性。 隔离性：事务并发操作下数据的正确性。 持久性：事务对数据修改的可靠性。 事务隔离级别上面说过“每个数据库都提供有自己的隔离级别，每个数据库的默认隔离级别也不尽相同”，事物隔离级别分为四种，下面一一介绍。首先简述共享锁（S）和排它锁（X），方便后续理解：多个共享锁(S)可以同时获取，但是排它锁(X)会阻塞其它所有锁 未提交读(Read Uncommitted)指一个事务读取到了另外一个事务未提交的数据。即事务的修改阶段未加排他锁，对其他事务可见。例如事务T1可能读取到只是事务T2中某一步的修改状态，即存在脏读的现象。脏读：事务读取到的数据可能是不正确、不合理或者处于非法状态的数据，例如在事务T1读取后，事务T2可能又对数据做了修改，或者事务T2中某些操作违反了一致性约束，做了回滚操作，该情况下事务T1读取到的数据称之为脏数据，该行为称之为脏读。 提交读(Read Committed)一个事务过程中只能读取到其他事务对数据的提交后修改，即事务的修改阶段加了排它锁，直到事务结束才释放，执行读命令那一刻加了共享锁，读完即释放，以此维持事务修改阶段对其他事务的不可见。例如事务T2读取到的只能是事务T2提交完成后的状态。该隔离级别避免了脏读现象，但正是由于事务T1可能读取到的是事务T2修改完成后的数据，以致出现了不可重复读现象。不可重复读：对于同一个事务的前后两次读取操作，读取到的内容不同。例如在事务T1读取操作后，事务T2可能对数据做了修改，事务T2修改完成提交后，事务T1又做了读取操作，因为内容已被修改，导致读取到的内容与上一次不同，即存在不可重复读现象。 可重复读(Repeatable Reads)一个事务过程中不允许其他事务对数据进行修改。即事务的读取过程加了共享锁，事务的修改过程加了排它锁，并一直维持锁定状态直到事务结束。因为事务的读取或修改都需要维持整个阶段的锁定状态，所以避免了脏读和不可重复读现象。但是因为只对现有的记录上进行了锁定，并未维持间隙锁/范围锁，导致某些数据记录的插入未受阻拦（结果多了一行），即存在幻读现象。幻读：事务中前后相同的查询语句，返回的结果集不同。例如在事务T1查询表记录后，事务T2向表中增加了一条记录，当事务T1再次执行相同的查询时，返回的结果集可能不同，即存在幻读现象。 可串行化(Serializable)一个事务过程中不允许其他事务对指定范围数据进行修改。即事务过程中若指定了操作集合的范围，在可重复读的锁基础上增加了对操作集合的范围锁，通过增加范围锁避免了幻读现象。 四种隔离级别设置: 级别 说明 Serializable 可避免脏读、不可重复读、虚读情况的发生 Repeatable read 可避免脏读、不可重复读情况的发生 Read committed 可避免脏读情况发生 Read uncommitted 最低级别，以上情况均无法保证 锁的使用是为了在并发环境中保持每个业务流处理结果的正确性，这样的概念在计算机领域中很普遍，但是都必须要基于一个前提，或者称之为约定：在执行操作前，首先尝试去获取锁，获取成功则可以执行，若获取失败，则不执行或等待重复获取。因为无论任何类型的操作，有没有锁都不影响程序本身的执行流程，但只有遵从这个约定才能体现出其价值。就像红绿灯并不影响车辆本身的行驶能力，只有声明所有个体皆遵守相同的规则，所以一切才变得有序。在数据库的并发环境下，隔离程度越高，也就意味着并发程度越低，所以各个数据库中一般设置的都是一个折中的隔离级别。 基于Mysql测试隔离级别 SELECT @@global.tx_isolation; # 查看全局事物隔离级别 SELECT @@session.tx_isolation; # 查看会话事物隔离级别 SELECT @@tx_isolation; # 查看当前事务隔离级别 SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; # 可避免脏读、不可重复读、虚读情况的发生 SET SESSION TRANSACTION ISOLATION LEVEL read committed; # 可避免脏读情况发生 SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; # 可避免脏读、不可重复读情况的发生 SET SESSION TRANSACTION ISOLATION LEVEL serializable; # 可避免脏读、不可重复读、幻读情况的发生 start transaction; --建表 drop table AMOUNT; CREATE TABLE `AMOUNT` ( `id` varchar(10) NULL, `money` numeric NULL ) ; --插入数据 insert into amount(id,money) values(&#39;A&#39;, 800); insert into amount(id,money) values(&#39;B&#39;, 200); insert into amount(id,money) values(&#39;C&#39;, 1000); --测试可重复读，插入数据 insert into amount(id,money) values(&#39;D&#39;, 1000); --设置事务 SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; SELECT @@tx_isolation; --开启事务 start transaction; --脏读演示，读到其他事务未提交的数据 --案列1，事务一：A向B转200，事务二：查看B金额变化，事务一回滚事务 update amount set money = money - 200 where id = &#39;A&#39;; update amount set money = money + 200 where id = &#39;B&#39;; --不可重复读演示，读到了其他事务提交的数据 --案列2，事务一：B向A转200，事务二：B向C转200转100 SET SESSION TRANSACTION ISOLATION LEVEL read committed; --开启事务 start transaction; --两个事务都查一下数据(转账之前需要，查一下金额是否够满足转账) select * from amount; --事务一：B向A转200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --事务二：B向C转200转100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --从事务二的角度来看，读到了事务一提交事务的数据，导致金额出现负数 --幻读演示 --案列3，事务一：B向A转200，事务二：B向C转200转100 SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; --开启事务 start transaction; --两个事务都查一下数据(转账之前需要，查一下金额是否够满足转账) select * from amount; --事务一：B向A转200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --事务二：B向C转200转100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --从事务二的角度来看，读到了事务一提交事务的数据，导致金额出现负数 参考资料事务的ACID事务ACID理解事务ACID属性与隔离级别","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Hive3.x初探","slug":"Hive3.x新特性","date":"2019-12-27T07:18:25.000Z","updated":"2021-02-03T03:00:57.189Z","comments":true,"path":"7fbbfd34/","link":"","permalink":"https://shmily-qjj.top/7fbbfd34/","excerpt":"","text":"Hive3.x新特性新特性简述 执行引擎变更为**TEZ**,不使用MR 成熟的ACID大数据事务支持 LLAP用于妙极，毫秒级查询访问 基于Apache Ranger的统一权限管理 默认开启HDFS ACLs Beeline代替Hive Cli，降低启动开销 不再支持内嵌Metastore Spark Catalog不与Hive Catalog集成，但可以互相访问 批处理使用TEZ，实时查询使用LLAP Hive3支持联邦查询 架构原理 TEZ执行引擎 Apache TEZ**是一个针对Hadoop数据处理应用程序的分布式计算框架，基于Yarn且支持DAG作业的开源计算框架。Tez产生的主要原因是绕开MapReduce所施加的限制，逐步取代MR，提供更高的性能和灵活性。Apache TEZ的核心思想是将Map和Reduce拆分成若干子过程，即Map被拆分成Input、Processor、Sort、Merge和Output， Reduce被拆分成Input、Shuffle、Sort、Merge、Processor和Output等，分解后可以灵活组合成一个大的DAG作业。Apache TEZ兼容MR任务，不需要代码层面的改动。Apache TEZ提供了较低级别的抽象，为了增强Hive/Pig的底层实现，而不是最终面向用户的。Hive3的TEZ+内存查询结合**的性能据说是Hive2的50倍(也有文章说是100倍，这个数字是不是很熟悉，它到底能不能与Spark内存计算速度媲美呢)。 上图是Hive On MR和Hive On Tez执行任务流程对比图，解释： Hive On MR Hive On Tez 计算需要多个MR任务而且中间结果都要落盘 只有一个作业，只写一次HDFS 没有资源重用 资源复用 处理完释放资源 Applications Manager资源池启动若干Container，处理完不释放直接分配给未运行任务 Map:Reduce = 1:1 不再是一个Map只对应一个Reduce 在磁盘处理数据集 小的数据集完全在内存中处理以及内存Shuffle 新的HiveQL执行流程Hive编译查询-&gt;Tez执行查询-&gt;Yarn分配资源-&gt;Hive根据表类型更新HDFS或Hive仓库中的数据-&gt;Hive通过JDBC连接返回查询结果 LLAPLLAP(Live Long and Process)实时长期处理，是Hive3的一种查询模式，由一个守护进程和一个基于DAG的框架组成，LLAP不是执行引擎(MR/Tez),它用来保证Hive的可伸缩性和多功能性，增强现有的执行引擎。LLAP的守护进程长期存在且与DataNode直接交互，缓存，预读取，某些查询处理和访问控制功能包含在这个守护程序中用于直接处理小的查询，而计算与IO较大的繁重任务会提交Yarn执行。守护程序不是必须的，没有它Hive仍能正常工作。对LLAP节点的请求都包含元数据信息和数据位置，所以LLAP节点无状态。可以使用Hive on Tez use LLAP来加速OLAP场景(OnLine Analytical Processing联机分析处理)LLAP为了避免JVM内存设置的限制，使用堆外内存缓存数据以及处理GROUP BY/JOIN等操作，而守护程序仅使用少量内存。Hive3支持两种查询模式Container和LLAP 如图LLAP执行示例，TEZ作为执行引擎，初始阶段数据被推到LLAP，LLAP直接与DataNode交互。而在Reduce阶段，大的Shuffle数据在不同的Container容器中进行，多个查询和应用能同时访问LLAP。 更成熟的ACID支持Hive的UPDATE一直是大数据仓库的一个问题，虽然在Hive3.x之前也支持UPDATE操作，但是性能很差，还需要进行分桶。Hive3.x支持全新的更成熟的ACID。Hive3默认对内部表支持事务和ACID特性。默认情况下启用ACID不会导致性能或操作过载。 物化视图重写和自动查询缓存多个查询可能需要用到相同的中间表，可以通过预先计算和将中间表缓存到视图中来避免重复计算。查询优化器会自动利用预先计算的缓存来提高性能。例如加速仪表盘中的join数据查询速度。 元数据映射表Hive会从JDBC数据源创建两个数据库：information_schema和sys。所有Metastore表都映射到表空间，并在sys中可用。information_schema数据显示系统的状态。 支持基于成本优化的智能下推查询一个数据源时如果读取全部数据后再进行分析是很高成本的，通过JDBC获取过多数据导致资源浪费以及性能不佳，Hive依靠其storage handler接口和Apache Calcite支持的基于成本的优化器（CBO）实现了对其他系统的智能下推。特别是，Calcite提供与查询的逻辑表示中的运算符子集匹配的规则，然后生成在外部系统中等效的表示以执行更多操作。Hive在其查询计划器中将计算推送到外部系统，并且依靠Calcite生成外部系统支持的查询语言。storage handler的实现负责将生成的查询发送到外部系统，检索其结果，并将传入的数据转换为Hive内部表示，以便在需要时进一步处理。这不仅限于SQL系统：例如，Apache Hive也可以联邦Apache Druid或Apache Kafka进行查询，正如我们在最近的博文中所描述的，Druid可以非常高效的处理时序数据的汇总和过滤。因此，当对存储在Druid中的数据源执行查询时，Hive可以将过滤和聚合推送给Druid，生成并发送JSON查询到引擎暴露的REST API。另一方面，如果是查询Kafka上的数据，Hive可以在分区或offset上推送过滤器，从而根据条件读取topic中的数据。 Hive 3.0其他特性1、连接Kafka Topic，简化了对Kafka数据的查询2、执行查询所需的少量守护进程简化了监视和调试3、工作负载管理(会话资源限制)：用户会话数，服务器会话数，每个服务器每个用户会话数等限制，防止资源争用导致资源不足4、会话状态，内部数据结构，密码等驻留在客户端而不是服务器上5、黑名单可以限制内存配置以防止HiveServer不稳定，可以使用不同的白名单和黑名单配置多个HiveServer实例，以建立不同级别的稳定性 联邦查询支持Oracle、MySQL、Kafka、Druid、HDFS、PostgreSQL等多个数据源的联邦查询，可以对多个数据源统一访问。联邦查询的优势：1.单个SQL方言和API2.集中统一的权限控制和审计跟踪（Hive支持表、行、列的访问控制）3.统一治理4.能够合并来自多个数据源的数据 优缺点 优点：性能，安全性，对ACID事物的支持，对任务资源调度的优化。 缺点：目前最新的CDH6.3还不兼容Hive3，自己安装坑点多；目前相关文献较少，排错难。 实践https://link.zhihu.com/?target=https%3A//hortonworks.com/tutorial/interactive-sql-on-hadoop-with-hive-llap/https://link.zhihu.com/?target=https%3A//dzone.com/articles/3x-faster-interactive-query-with-apache-hive-llaphttps://link.zhihu.com/?target=https%3A//community.hortonworks.com/articles/149486/llap-sizing-and-setup.html 参考资料Hive3新特性Apache Tez 了解Hive 3.x 功能介绍使用Apache Hive3实现跨数据库的联邦查询","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hive","slug":"Hive","permalink":"https://shmily-qjj.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"人声混音总结","slug":"人声混音总结","date":"2019-12-21T07:06:06.000Z","updated":"2020-04-12T14:15:48.000Z","comments":true,"path":"d2c4813b/","link":"","permalink":"https://shmily-qjj.top/d2c4813b/","excerpt":"","text":"人声混音总结前言2019年因为比较忙，出的音乐作品并不多，我记得只是翻弹了一首岸部真明的指弹曲《少年的梦》和翻唱了一首自己很喜欢的古风歌曲《浮生未歇》…2018年比较活跃，记得2018年的暑假买了录音笔，开始录吉他，录翻唱，对录音笔爱不释手，我的设备很简陋，只有一个ZOOM H1N小型录音笔和一台音质不好的笔记本电脑（这个电脑对我的混音造成了很大影响，用它听起来不错的音色，放在手机上挺就是另一个样子…），但是通过自己摸索，发现效果还是可以的。但现在当我又面对一条一条的音轨，眼花缭乱的效果器时，我有点不知所措的感觉，的却，将近大半年时间没混音，听感和对效果器的掌握已经忘掉太多，所以决定写这篇博客，督促自己学习混音知识，做到精益求精。通过深入混音技术，我相信我的混音作品能够变得更好！网易云歌手页：佳境Shmily 基本步骤也是我人声部分的插件效果器顺序，当然也不绝对 录音（一切始于源头，如果录的干声有问题，后期再强也无法修复，所以有瑕疵的地方要反复录制） 降噪干声的空白部分的噪音缓解，多多少少对干声音质有损可以使用比较强大的iZotope RX7，也可以使用Waves家的X-Noise和Z-Noise 修音（使用WavesTune修复音准，轻微跑调是基本可以无损修复的） 人声动态调整人声的动态很大，这里可以为每段音频调整输出的动态，有Waves的也可以使用Vocal Rider来解决人声音量不均匀的问题Vocal Rider可以侦测音量什么时候该高，什么时候该低，并自动进行调整，Vocal Rider不是压缩器，不会对声音音色产生影响用法：①调节“目标”选一个理想的默认音量 ②调节range自动音量调节范围，也就是动态大小 均衡器这步应该是对人声音色影响最大的，可以使用多个均衡器串联 齿音缓解齿音不能被完全消除，应该在保证人声音色无损的前提下尽可能减少齿音使用RDeEsser或DeEsser 人声齿音6khz左右 压限压缩和限制，防止信号过载，防止人声忽大忽小，防止人声动态过大不同压缩器有不同的音染特色，能使音色更优美压缩器种类很多，我常用CLA-76 C1等Waves家的压缩器 限制器一般也是使用Waves家的L3LL Ultra Stereo、L1、L2等 饱和度与失真很小的饱和度与失真能让人声更厚实，添加泛音让人声更突出可用NLS Channel的失真部分，也有推荐Scheps 73，不过我的Waves版本没有这个插件 混响和延迟增加空间感，混响和延迟对单声道可以增加深度感，对双声道可以增加宽度感混响的时间短，产生的空间小，时间长则产生的空间也较长混响使用Waves RVerb延迟使用H-Delay，用来强调某些词 激励器对一些音轨进行音量增益，干湿度增益等操作，为了方便，使用Waves中的OneKnob系列插件，一个旋钮解决问题！ 母带母带处理包括总体激励、均衡、混响、音量提升等，推荐使用Ozone 8。我目前对母带处理还不是很了解。 知识点 均衡器EQ 均衡器EQ注意事项网上有推荐FabFilter Pro-Q这个插件，不过我用的Waves的一套 EQ黄金定律： 0、减窄增宽，在用减法eq时要用较高的Q值，用加法EQ时采用较低的Q值。 1、如果声音浑浊，请衰减250hz附近的频段 2、如果声音听起来有喇叭音，请衰减800hz附近的频段 3、当你试图让声音听起来更好，请考虑用衰减 4、当你试图让声音听起来与众不同，请考虑用提升 5、不要放大原先没有的声音 减法均衡器F6 Dynamic EQ Waves Q10100hz以下切掉 加法均衡器 常用效果器的使用","categories":[{"name":"音乐","slug":"音乐","permalink":"https://shmily-qjj.top/categories/%E9%9F%B3%E4%B9%90/"}],"tags":[{"name":"人声混音","slug":"人声混音","permalink":"https://shmily-qjj.top/tags/%E4%BA%BA%E5%A3%B0%E6%B7%B7%E9%9F%B3/"}],"keywords":[{"name":"音乐","slug":"音乐","permalink":"https://shmily-qjj.top/categories/%E9%9F%B3%E4%B9%90/"}]},{"title":"程序猿是怎么表白的","slug":"程序猿是怎么表白的","date":"2019-12-11T13:10:21.000Z","updated":"2020-08-29T03:24:43.627Z","comments":true,"path":"d1c9241f/","link":"","permalink":"https://shmily-qjj.top/d1c9241f/","excerpt":"","text":"程序猿是怎么表白的Hi Dear,要看看我的代码吗？不看怎么知道我爱你！程序猿是怎么表白的？当然也离不开敲代码啦！就缺个懂代码的女朋友了。。。 零.一行代码输出一个心print(&#39;\\n&#39;.join([&#39;&#39;.join([(&#39;Love&#39;[(x-y) % len(&#39;Love&#39;)] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 &lt;= 0 else &#39; &#39;) for x in range(-30, 30)]) for y in range(30, -30, -1)])) python -c &quot;print(&#39;\\033[35m&#39;+&#39;\\n&#39;.join([&#39;&#39;.join([(&#39;Love&#39;[(x-y) % len(&#39;Love&#39;)] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 &lt;= 0 else &#39; &#39;) for x in range(-30, 30)]) for y in range(30, -30, -1)])+&#39;\\033[0m&#39;)&quot; 一.我的世界只有太阳、月亮和你/** * I love three things in this world.Sun, moon and you. Sun for morning, moon for night, and you forever. */ class LoveThreeThings extends Me &#123; const loveFirstThings = &#39;Sun&#39;; const loveSecondThings = &#39;Moon&#39;; const loveThirdThings = &#39;You&#39;; public function MyLove() &#123; return &#39;I Love&#39; . self::loveThirdThings . &#39;forever. Never change!&#39;; &#125; &#125; 二.百年好合while(&#39;ILoveyou&#39;): for IBeWithYou in range(0,50*365): time.sleep(60*60*24) //程序能一直执行，执行完50年，若我们还有50年，余生继续。 //从前的日色变得很慢 //车马邮件都慢 //一生只够爱一人 三.谁都不能掌控全世界，但你至少可以掌控我，这是我的温柔 world.controlledBy(NoOne) withMyGentle() &#123; you.control(me).equals(true) &#125; //谁都不能掌控全世界，但你至少可以掌控我，这是我的温柔 四.若爱，请深爱 if(love ==1) &#123; while(1) &#123; love_depth ++; &#125; &#125; 五.将我手上的温度全部给予你，换取你幸福的脸庞if(you.hand==cold&amp;&amp;weather==winter): //如果冬天里你的手是冰冷的 giveyoulove(myhand.temp,yourhand.temp); //将我手上的温度全部给予你 return you.happyface; //换取你幸福的脸庞 六.我一直在找你，当我找到你，也就找到了整个世界while (i.findYou()) &#123; if (i.get() == you) &#123; System.out.print(&quot;Hello,Word!&quot;); &#125; &#125; 七.如果彼此相爱，那就白头偕老if(you.love(me) &amp;&amp; I.love(you))&#123; // 如果彼此相爱 this.liveToOld();// 那就白头偕老 &#125;else&#123;// 否则 this.bestWishesToYou(); // 祝你一切安好 &#125; 八.陪伴是最长情的告白StringBuilder love = newStringBuilder(&quot;&quot;); for(;;)｛ love.append(Math.random()&gt;0.5?1:0); ｝ 九.自从遇见你，就不停地想你public void missing_you(meet_you) int time = 0 for（time=meet_you;;time++）｛ missing_you(); ｝ 十.你若不来，我便不弃public Me waitYou()&#123; if(!appear)&#123; this.waitYou(); &#125;else&#123; this.waitYou(); &#125; &#125; 十一.听说你要走，站在雨里，任凭身体被水珠撕裂成一个个没有意义的字母public void hearYouLeave()&#123; String body = myself.toString(); body.split(&quot;.&quot;); &#125; 十二.每一世，我都会在这等你！就算容颜变迁，就算时光流转public Me findYou()&#123; for(int age = 0;age &lt;= 120; age++)&#123; try&#123; Thread.sleep(60); if(this.waitMyLove() != null)&#123; return this.waitMyLove(); &#125;cache(InterruptedException e)&#123; System.out.println(&quot;Time is nothing...&quot;); &#125; &#125; return null; &#125; 十三.待更新待更新... 参考资料：如何用你的专业来表白？","categories":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"表白","slug":"表白","permalink":"https://shmily-qjj.top/tags/%E8%A1%A8%E7%99%BD/"},{"name":"程序猿","slug":"程序猿","permalink":"https://shmily-qjj.top/tags/%E7%A8%8B%E5%BA%8F%E7%8C%BF/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"Mysql Event Scheduler","slug":"Mysql Event Scheduler","date":"2019-11-15T13:25:04.000Z","updated":"2020-04-12T14:15:47.999Z","comments":true,"path":"3c26421b/","link":"","permalink":"https://shmily-qjj.top/3c26421b/","excerpt":"","text":"Mysql事件调度器工作的时候遇到一张表需要每天Truncate，就想到了Mysql的Event Scheduler，但是又忘了它的语法了，所以这里来复习一下。 什么是Event Scheduler事件调度器，可以作为定时调度器，类似于Crontab，可以取代部分操作系统任务调度器的定时任务工作。Mysql在5.1版本后新增了事件调度器，它可以支持秒级调度，很实用方便。时间调度器也可以看作是一个触发器，是针对某个表进行操作的，时间调度器执行采用了单独一个线程，可通过SHOW PROCESSLIST命令查看 Event Scheduler语法CREATE [DEFINER = { user | CURRENT_USER }] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT &#39;string&#39;] DO event_body; schedule: AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...] interval: quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND} 语法说明： DEFINER：指定可执行该定时器的MySQL账号，user的格式是’user_name’@’host_name’，CURRENT_USER或CURRENT_USER()，单引号是需要在语句中输入的。如果不指定，默认是DEFINER = CURRENT_USER。 event_name：事件名称，最大64个字符，不区分大小写，MyEvent和myevent是一样的，命名规则和其他MySQL对象是一样的。 ON SCHEDULE schedule：ON SCHEDULE指定事件何时执行，执行的频率和执行的时间段，有AT和EVERY两种形式。 [ON COMPLETION [NOT] PRESERVE]：可选，preserve是保持的意思，这里是说这个定时器第一次执行完成以后是否还需要保持，如果是NOT PRESERVE，该定时器只执行一次，完成后自动删除事件；没有NOT，该定时器会多次执行，可以理解为这个定时器是持久性的。默认是NOT PRESERVE。 [ENABLE | DISABLE | DISABLE ON SLAVE]：可选，是否启用该事件，ENABLE-启用，DISABLE-禁用，可使用alter event语句修改该状态。DISABLE ON SLAVE是指在主备复制的数据库服务器中，在备机上也创建该定时器，但是不执行。 COMMENT: 注释，必须用单引号括住。 DO event_body：事件要执行的SQL语句，可以是一个SQL，也可以是使用BEGIN和END的复合语句，和存储过程相同。 ON SCHEDULE时间类型两种时间类型AT timestamp和Every interval AT timestamp用于只执行一次的事件。执行的时间由timestamp指定，timestamp必须包含完整的日期和时间，即年月日时分秒都要有。可以使用DATETIME或TIMESTAMP类型，或者可以转换成时间的值，例如“2018-01-21 00:00:00”。如果指定是时间是过去的时间，该事件不会执行，并生成警告。 mysql&gt; create table test(id int,name varchar(255)); Query OK, 0 rows affected (0.01 sec) mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:30:59 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:30:59&#39; DO show tables; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; show warnings\\G; *************************** 1. row *************************** Level: Note Code: 1588 Message: Event execution time is in the past and ON COMPLETION NOT PRESERVE is set. The event was dropped immediately after creation. 1 row in set (0.00 sec) ERROR: No query specified mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:34:59&#39; DO show tables; Query OK, 0 rows affected (0.01 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:37:59&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) 时间过后我发现我的test表里仍然没数据 mysql&gt; show variables like &quot;event_scheduler&quot;; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | event_scheduler | OFF | +-----------------+-------+ 1 row in set (0.00 sec) 原因是我没开启event_schedulervim /etc/my.cnf 在[mysqld]这一栏下添加event_scheduler = ON来永久启用event_scheduler重启mysql服务systemctl restart mysqld.service mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:40:37 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:41:37&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test; +------+------+ | id | name | +------+------+ | 1 | qjj | +------+------+ 1 row in set (0.00 sec) mysql&gt; show events; Empty set (0.00 sec) # 一小时后执行 命令示例 mysql&gt; CREATE EVENT update_test ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOUR DO UPDATE test SET id = 2; Query OK, 0 rows affected (0.00 sec) 上述结果说明:必须先开启event_scheduler之后event才会生效，AT timestamp的方式只会在指定时间点执行一次，然后这个event就会被销毁，如果指定的时间是过去的是时间点，则这个event会有警告，且不执行也不保留event。 Every interval让事件定期执行，每多久执行一次ON SCHEDULE后面时间写法的几个栗子：EVERY 6 WEEK 每六周EVERY 20 second 每20秒EVERY 3 MONTH STARTS CURRENT_TIMESTAMP + INTERVAL 1 WEEK 一周以后开始，每隔三个月EVERY 2 WEEK STARTS CURRENT_TIMESTAMP + INTERVAL ‘6:15’ HOUR_MINUTE 6小时15分钟以后开始，每隔两周执行EVERY 1 DAY STARTS CURRENT_TIMESTAMP + INTERVAL 5 MINUTE ENDS CURRENT_TIMESTAMP + INTERVAL 2 WEEK 5分钟以后开始，每隔一天执行，两周后结束 举个栗子 mysql&gt; create event daily_truncate_test -&gt; ON SCHEDULE -&gt; EVERY 1 DAY -&gt; COMMENT &#39;每天执行一次清空test表数据&#39; -&gt; DO -&gt; truncate test; Query OK, 0 rows affected (0.00 sec) mysql&gt; mysql&gt; show events; +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | Db | Name | Definer | Time zone | Type | Execute at | Interval value | Interval field | Starts | Ends | Status | Originator | character_set_client | collation_connection | Database Collation | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | test | daily_truncate_test | root@CDH066 | SYSTEM | RECURRING | NULL | 1 | DAY | 2019-11-16 12:10:36 | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | | test | update_test | root@CDH066 | SYSTEM | ONE TIME | 2019-11-16 12:58:52 | NULL | NULL | NULL | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ 2 rows in set (0.00 sec) mysql&gt; SHOW PROCESSLIST; +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | 1 | event_scheduler | localhost | NULL | Daemon | 721 | Waiting for next activation | NULL | | 7 | root | CDH066:34902 | test | Query | 0 | starting | SHOW PROCESSLIST | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ 2 rows in set (0.00 sec) # 示例2 指定每天具体时间点的event事件 CREATE EVENT truncate_with_time ON SCHEDULE EVERY 1 day STARTS date_add(concat(current_date(), &#39; 00:00:00&#39;), interval 0 second) ON COMPLETION PRESERVE ENABLE COMMENT DO TRUNCATE test; 操作和查看事件show events; # 查看事件及其状态 ALTER EVENT daily_truncate_test DISABLE; # 禁用指定事件 ALTER EVENT daily_truncate_test ENABLE; # 启用指定事件 ALTER EVENT daily_truncate_test RENAME TO daily_truncate; # 重命名事件 ALTER EVENT test.daily_truncate_test RENAME TO qjj_test.daily_truncate_test; # 事件是数据库层面的，可以把事件从一个数据库移动到另一个数据库(另一个数据库要有对应的表) DROP EVENT daily_truncate; # 删除事件 总结Mysql作为最热门的关系型数据库之一，有很多东西值得我们去探索，好记性不如烂笔头，写了博客，对事件调度器的理解更加深刻了。","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://shmily-qjj.top/tags/Mysql/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"浅谈group by与distinct去重","slug":"浅谈group by与distinct去重","date":"2019-11-13T12:45:37.000Z","updated":"2020-05-04T12:53:02.527Z","comments":true,"path":"96009187/","link":"","permalink":"https://shmily-qjj.top/96009187/","excerpt":"","text":"前言今天带我的老哥让我改一下报警模块，把一些联系方式等信息存在Mysql里，方便以后管理和维护，很简单的东西，为了减少Mysql并发压力，我想每次报警只查询一次数据库，但子查询返回的结果有重复记录，于是我写了个类似SELECT col1,col2,col3,col4 FROM (select…) GROUP BY col1;的语句来去重，可以正常执行不报错且达到了目的就直接使用了，也没深究。直到老哥看了代码说找我说老弟呀你这个逻辑，有点问题呀。。。 按理说，GROUP BY都是要与聚合函数搭配使用的，所以确实是逻辑有问题，写代码规范很重要，规范的同时还要弄清楚原理，于是就有了这篇博客，详细说一下使用GROUP BY和DISTINCT去重… DISTINCT去重DISTINCT可多字段去重，每个字段的值都完全相同的情况下使用DISTINCT去重DISTINCT也可以单个字段值去重 select(name),id from table; mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel from person_info where wxid in (&#39;SCALA&#39;); +-------+ | tel | +-------+ | 10010 | +-------+ 1 row in set (0.00 sec) mysql&gt; select distinct name from person_info where wxid in (&#39;SCALA&#39;); +--------+ | name | +--------+ | scala | | scala1 | +--------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +-------+-------+ | tel | wxid | +-------+-------+ | 10010 | SCALA | +-------+-------+ 1 row in set (0.00 sec) 通过上面的实验可以看出，DISTINCT的去重效果是 如果取出的所有字段的值都完全相同则可以去重，如果取出的字段不完全相同，就无法去重。 GROUP BYGROUP BY与聚合函数连用，主要用于分组聚合，但它也可以用来去重，与DISTINCT相反，它支持多个字段值不完全相同的情况下去重，但会舍弃一些值。假如A,B,C三个字段，A和B两个字段在多条记录中值都相同，但C不同，使用GROUP BY去重后只会得到一条记录，C的值只保留一个，其余记录C字段不同的值舍弃。 mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) # GROUP BY的正确用法 分组聚合 mysql&gt; select count(name),tel,wxid from person_info group by tel; +-------------+-------+--------+ | count(name) | tel | wxid | +-------------+-------+--------+ | 1 | 10000 | SBT | | 3 | 10010 | SCALA | | 1 | 10086 | PYTHON | | 1 | 110 | QJJ | | 1 | 114 | JAVA | | 1 | 119 | MAVEN | | 1 | 120 | JJQ | +-------------+-------+--------+ 7 rows in set (0.00 sec) # 报警模块不希望重复报警，所以只想获取一个电话号码 下面的语句逻辑有问题，不符合GROUP BY的使用规范，但是能执行 mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) # 改成符合使用规范的 mysql&gt; select max(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | max(name) | tel | wxid | +-----------+-------+-------+ | scala2 | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) mysql&gt; select min(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | min(name) | tel | wxid | +-----------+-------+-------+ | scala | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) # mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,name; +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) 从上面实验可以得出的结论:如果只需要tel和wxid两个字段，无所谓name的字段值，就可以用GROUP BY的方式去重，但是也要尽量写得规范。如果需要name字段的值，就不能用GROUP BY来去重了。 问题情景重现报警有两种方式，一种是传人名，还有一种是传组名，人与组是多对多关系联系方式信息存为两张表，person_info和group_info,大致如下name 人名，tel是电话，wxid是微信，groupname是组名 mysql&gt; show tables; +----------------+ | Tables_in_test | +----------------+ | group_info | | persion_info | +----------------+ 2 rows in set (0.00 sec) mysql&gt; desc person_info; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | tel | varchar(255) | NO | | NULL | | | wxid | varchar(255) | NO | | NULL | | +-------+--------------+------+-----+---------+-------+ 3 rows in set (0.00 sec) mysql&gt; desc group_info; +-----------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | groupname | varchar(255) | NO | | NULL | | +-----------+--------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) mysql&gt; select * from person_info; +--------+-------+--------+ | name | tel | wxid | +--------+-------+--------+ | java | 114 | JAVA | | jjq | 120 | JJQ | | maven | 119 | MAVEN | | python | 10086 | PYTHON | | qjj | 110 | QJJ | | sbt | 10000 | SBT | | scala | 10010 | SCALA | +--------+-------+--------+ 7 rows in set (0.00 sec) mysql&gt; select * from group_info; +--------+-------------+ | name | groupname | +--------+-------------+ | java | languages | | jjq | person | | maven | build-tools | | python | languages | | qjj | person | | sbt | build-tools | | scala | languages | +--------+-------------+ 7 rows in set (0.00 sec) 报警接口传进来的参数可能是多个人名或者组名的组合列表，我想通过一次查询获取到所有报警人信息，于是我先写了内部子查询: SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;); # 结果: +------+------+------+-----------+ | name | tel | wxid | groupname | +------+------+------+-----------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | | jjq | 120 | JJQ | groupname | | qjj | 110 | QJJ | groupname | +------+------+------+-----------+ 4 rows in set (0.00 sec) 因为有重复的人名和重复的联系方式会重复报警，所以为了避免重复报警，我又加了外面的一层: SELECT name,tel,wxid,max(groupname) FROM (SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;)) a GROUP BY name; # 结果: +------+------+------+----------------+ | name | tel | wxid | max(groupname) | +------+------+------+----------------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | +------+------+------+----------------+ 2 rows in set (0.01 sec) 准确地拿到了name,tel,wxid，但是groupname字段呢，到底是哪个被舍弃了？不同情况下不一定。如果我们只要name,tel,wxid这三个字段，在groupname字段上加个max()好了，这样逻辑也说得通，也比较规范，如果要求精确拿到groupname字段的值，就不能使用group by去重。 关于效率DISTINCT和GROUP BY同时适用的场景下，不能说一定是谁的效率更高DISTINCT就是字段值对比的方式，要遍历整个表。GROUP BY类似于先建索引再查索引。小表DISTINCT去重效率高，大表GROUP BY去重效率高 总结 认清DISTINCT和GROUP BY的去重场景 某些场景下DISTINCT与GROUP BY去重同时适用，但DISTINCT效率更高 代码要规范且符合逻辑 要对SQL每个语法的使用场景有明确的认识 多总结问题","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://shmily-qjj.top/tags/SQL/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"王者荣耀-元歌教学","slug":"王者荣耀-元歌教学","date":"2019-11-03T15:05:06.000Z","updated":"2020-04-12T14:15:48.002Z","comments":true,"path":"26e6fc90/","link":"","permalink":"https://shmily-qjj.top/26e6fc90/","excerpt":"","text":"一波元歌攻略分享有些英雄，看似很秀，看似很难，其实只要理解了它的每个步骤，就会发现它并不难。今天我为大家带来一波干货满满的元歌攻略分享！元歌，是一个低风险，高回报的强势英雄，虽然经历过天美无情的剥削，但依然能在峡谷里绽放光芒！很多玩家一看到元歌，就想起那个口诀“1433223”，但如果只会记口诀，那就Out啦！其实很简单，我们只要明确它每一步都是干什么的，就更透彻地理解元歌，就能运用自如！如果一直靠记口诀玩，那将毫无意义！下面我们开始吧！什么？没有积极性？要不看一下秀操作视频==&gt;链接 出装解析 傀儡冷却较长，推荐冷却鞋 主要针对脆皮，推荐暗影战斧 秒杀型刺客，自带斩杀效果，配合破军，能秒杀任何脆皮 伤害足够秒杀敌方，为了确保万无一失，推荐名刀 对面战坦出了肉装，可能也有脆皮出了肉装，这时推荐碎星锤 伤害完全足够，但元歌基础血量太低，为了不被秒杀以及续航能力，推荐霸者重装 召唤师技能，推荐闪现 口诀解析不得不说“1433223”是个很好的例子，我来解析一下每一步都做了什么吧！ 1技能释放傀儡，傀儡弹出的路径上会对英雄造成伤害和半秒击飞(眩晕)。 傀儡交换的路径上都有伤害和不到半秒的击飞。 4技能拉动傀儡和真身一段距离。当然，路径上也有伤害，但没控制。 3技能这两个三技能是关键，要将傀儡靠近敌方英雄，三技能第一段是减少敌方英雄500移速持续两秒，3秒内可释放第二段。多么强大的减速！ 第二个3技能三技能第二段是元歌的核心控制技能。用傀儡贴近敌方英雄并释放三技能第二段，控制敌方约2.5s。多么强大的控制！ 2技能在三技能二段释放后的三秒内释放二技能，将真身拉到敌人身边。 第二个2技能【即真身的2技能】此时敌人还在被控制的状态，如同待宰的羔羊，这时释放第二个二技能，直接使敌方英雄残血。 3技能【即真身的3技能】此时敌人还在被控制的状态，如同待宰的羔羊，三技能是由两条线组成，每条线都有伤害，交叉点命中英雄会对其造成已损生命16%的额外伤害，和持续两秒的50%减速并安全退出战场一段距离，同时得到一个护盾和瞬间加速效果。如果是脆皮或者残血就直接带走了，如果经济高出1000，这一套操作下来将秒杀一切！ 真身4技能自己按一下就知道，解控+位移+免伤 如何练习根据上面的口诀解析，我们已经大致知道元歌的玩法，下面就是入门的练习方法。 一开始还不熟悉，肯定会很懵，所以，先尽可能慢地按出1-3-3-2-2-3，越慢越好，千万不要急于求成，体会每一步做了什么，然后再逐渐加快速度。 玩上几把后就会对1-3-3-2-2-3这套操作恍然大悟，虽然可能按不熟悉，但也理解了每一步的作用。 之后，我们加入4技能，1-4-3-3-2-2-3，就会发现4技能的目的就是让傀儡接近敌方英雄，来提高控住敌方英雄的概率。 再玩上几把或十几把后，你就会对4技能的距离有一个初步的把握，口诀是死的，人是活的，聪明的你还会发现，对方在你首次按3技能减速的时候位移了怎么办，这时，4技能的作用就大了，你会想到1-3-4-3-2-2-3，即先减速，对方位移，4技能让傀儡贴近，3技能控制，2技能将真身拉过来，2技能打伤害，3技能收割+退出战场。 怎么样，行云流水的操作！这时你就会体会到，理解每个步骤的作用的重要性。相信后面你肯定会更加灵活！ 你会发现，傀儡除了可以探视野，吓唬人，骗技能之外，还可以蹲在草丛里守株待兔，敌方英雄进草丛后，一套3-3-2-2-3，就能直接带走，是不是像极了小妲己！ 再多打些场次，你就会发现，手速不够呀！别灰心，多打就能找到感觉，保持紧张，手速自然也提上来了！ 久经峡谷，因为你深刻理解了元歌的每一步操作，所以你自然有10000种方式逃跑，比如4-1-4-2-1-3-2，比如1-2-1，再比如3-4-1-2-1等等…没有人能轻易搞死你，想死都难… 进阶恭喜你入门了！想必也发现了元歌这个英雄的强大了。下面我还要介绍一些独门秘籍！ 15度角Surprise亲自截图给你萌看！ 元歌的三技能是交叉的，交叉点很近，但线的攻击范围很远 如果想打正前方的敌人，则手动施法像偏离15度角的方向释放3技能 敌方残血会被3技能的线刮到，虽然不如交叉点疼，但后期也有至少1000+的伤害，足以收割 这个技巧较难，需要多去感受和观察 越塔Surprise注意：手速不够快请忽略此条！ 众所周知，元歌傀儡能抗2-3次防御塔攻击。 2次防御塔攻击大约2秒。 2秒内要做什么：1-4-3-3-1(释放傀儡，靠近敌方，减速瞬间控住敌方，傀儡要没血了赶快收回傀儡) 因为收回傀儡也有击飞0.5秒，0.5秒之内做什么：3-2或2-3 恭喜完成了越塔秒杀 残血收割机看到残血，利用傀儡的冲撞和收回直接收割。具体操作：1-4-3-1(-2-4)括号里是可选操作。 1技能释放傀儡，4技能冲撞对方造成少量伤害，对方可能还没死 3技能第一段伤害很高，直接收割 1技能收回傀儡 如果残血没死可选补个2技能，如果有追兵，可选4技能逃跑 持续输出其实这不算啥技巧，就是一定要多参团。元歌后期2，3技能距离长，冷却短，跟在团后面持续消耗，收割，配合15度角，谁也顶不住！输出最高非你莫属！ End感谢你看完我的元歌攻略分享，有什么心得体会，欢迎在下方评论区留言！B站元歌视频：链接加油，下一个国服元歌，就是你！","categories":[{"name":"其他","slug":"其他","permalink":"https://shmily-qjj.top/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"王者荣耀","slug":"王者荣耀","permalink":"https://shmily-qjj.top/tags/%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"https://shmily-qjj.top/categories/%E5%85%B6%E4%BB%96/"}]},{"title":"Sqoop学习笔记","slug":"Sqoop学习笔记","date":"2019-11-03T05:15:27.000Z","updated":"2021-07-17T14:39:14.714Z","comments":true,"path":"26078/","link":"","permalink":"https://shmily-qjj.top/26078/","excerpt":"","text":"什么是SqoopSqoop是一款开源工具，用于Hadoop(Hive)与mysql等传统数据库间进行数据传递，可以将关系型数据库mysql,Oracle等中的数据导入HDFS中，也可以把HDFS中的数据导入到关系型数据库中。Sqoop2与Sqoop1完全不兼容，一般生产环境使用Sqoop1，这里主要说Sqoop1 Sqoop原理Sqoop原理很简单，就是将导入导出的命令翻译成MapReduce程序，Sqoop的操作主要目的（工作）是对MR程序的inputformat和outputformat进行定制.下图是Sqoop原理架构图图上意思很明确，这里不多赘述。戳**官方文档**了解更多 Sqoop安装部署去官网下载Sqoop的二进制包 tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt/module/ cd /opt/module/ mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop vim /etc/profile export SQOOP_HOME=/opt/module/sqoop export PATH=$PATH:$SQOOP_HOME/bin source /etc/profile cd sqoop/conf cp sqoop-env-template.sh sqoop-env.sh vim sqoop-env.sh 文件末尾加入如下配置： #Set the path for where zookeper config dir is #export ZOOCFGDIR= export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2 export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2 export HIVE_HOME=/opt/module/hive export HBASE_HOME=/opt/module/hbase export ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.13 export ZOOCFGDIR=/opt/module/zookeeper-3.4.13/conf # SQOOP写哪个集群，用哪个Yarn，用以下Hive、Hadoop客户端配置指定到sqoop-env.sh export HADOOP_CONF_DIR=/etc/hadoop/cluster_client_conf/hadoop-conf/ export HIVE_CONF_DIR=/etc/hadoop/cluster_client_conf/hive-conf/ 拷贝mysql驱动到Sqoop的lib目录下 cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/ 运行sqoop help命令 部署完成，测试： 查看Mysql表 sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password 000000 如果一切正常 - 则安装没问题了 Sqoop操作Sqoop的导入和导出导入：数据从RDBMS到HDFS的过程，数据源是RDBMS，目标是HDFS导出：数据从HDFS到RDBMS的过程，数据源是HDFS，目标是RDBMS 导数据到HDFS准备数据： 创建数据库和表 create database test; create table sqoop_test(id int primary key not null auto_increment,name varchar(255),sex varchar(255)); insert into test.sqoop_test(name,sex) values(&#39;qjj&#39;,&#39;male&#39;); insert into test.sqoop_test(name,sex) values(&#39;abc&#39;,&#39;female&#39;); use test; select * from sqoop_test; 导入HDFS方式分为全部导入/查询导入/导入指定列/筛选导入/增量更新 全部导入 –num-mappers 1 设置一个map，输出文件个数也为1–null-string 指定字段为空时用什么代替 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --null-string &quot;-&quot; \\ --target-dir /user/sqoop/out \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 查询导入不通过**- -table来指定，而是通过写- -query**来指定$CONDITIONS是必须加的，为了在多个Map的情况下，可以传递参数，以保证导出数据的顺序不变。 __where $CONDITIONS__必备``` shellsqoop import \\ –connect jdbc:mysql://localhost:3306/test –username root –password 000000 –target-dir /user/sqoop/out –num-mappers 1 –fields-terminated-by “\\t” –query ‘select name,sex from sqoop_test where id &lt;= 1 and $CONDITIONS;’ (这里如果用双引号，则$CONDITIONS需要转义) 3. 导入指定列 --delete-target-dir -&gt; 如果HDFS目录已经存在则删除 --columns指定多个列 ``` shell sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --columns id,sex \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 筛选导入关键字筛选/字段筛选 –where “条件”而且–where与–columns可以同时使用，但不能与–query同时使用 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --where &quot;id=1&quot; \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; 增量更新表更新时重新导入浪费时间和资源增量更新三个重要参数 –incremental append 指定增量导入–check-column col_name 以一个列作为增量导入的标准，这个列变化才会触发增量导入–last-value 指定上次导入的参考列的最后一个值（比如check-column为id，上次导入的id值为4，则增量导入要指定last-value为4） Sqoop官方用户文档: Sqoop User Guide 导数据到Hivemysql数据导入Hive过程分两步： Mysql先导入到HDFS 已被导出到HDFS的数据移动到hive仓库 hive-import指定target为hivetarget hive表会自动创建 sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --hive-import \\ --fields-terminated-by &quot;\\t&quot; \\ --hive-overwrite \\ --hive-table sqoop_hive Sqoop官方参考: Importing Data Into Hive 导数据到HBase–columns指定source表中哪几列–column-family指定列族名称–split-by按指定列名字段做切分注意：需要手动创建HBase目标表（以前1.0老版本HBase自动创建） sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --columns &quot;id,name,sex&quot; \\ --column-family &quot;info&quot; \\ --hbase-create-table \\ --hbase-row-key &quot;id&quot; \\ --hbase-table &quot;hbase_company&quot; \\ --split-by id Sqoop官方参考: Importing Data Into HBase Sqoop数据导出Hive/HDFS导出数据到RDBMS过程是将表数据每行都编程字符串，然后插入mysql，所以必须指定–input-fields-terminated-by来把字段切割开注意，RDBMS作为target表，需要手动创建target表–export-dir指定了数据仓库中表数据位置 sqoop export \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --export-dir /user/hive/warehouse/sqoop_hive \\ --input-fields-terminated-by &quot;\\t&quot; 脚本操作Sqoop公司一般会使用调度工具定期执行脚本，比如获取前一天的数据要在凌晨一两点进行抽取数据，这就需要定时任务，所以为了方便定时任务，Sqoop参数也要写在脚本里，类似于hive -f hql_file touch hdfs_to_mysql_job vim hdfs_to_mysql_job 内容如下: export --connect jdbc:mysql://localhost:3306/test --username root --password 000000 --table sqoop_test --num-mappers 1 --export-dir /user/hive/warehouse/sqoop_hive --input-fields-terminated-by &quot;\\t&quot; 执行该任务: sqoop --options-file hdfs_to_mysql_job Sqoop参数中文参考文档Sqoop参数中文文档，里面还包括了参数的实现类类名，供参考和深入学习，点击链接下载:Sqoop参数-PDF版 总结 强行结束MR任务后，不急着再启MR任务，MRAppMaster任务需要kill掉再运行新任务 多看官方文档，里面很详细 要根据实际使用场景学习官方文档中重要的常用的部分 Sqoop毕竟是基于MapReduce的，而MR的运算速度已经不能满足我们的需求，所以导数据和抽取数据的流程完全可以用Spark来代替Sqoop，Spark2.4版本后稳定性和效率都有提升，且能兼容多种数据源，能完成99%的Sqoop任务，当然有一些追求稳定而非速度的抽取数据的任务仍然可以使用Sqoop","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Sqoop工具","slug":"Sqoop工具","permalink":"https://shmily-qjj.top/tags/Sqoop%E5%B7%A5%E5%85%B7/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"CentOS7安装CDH6安装与排坑","slug":"CentOS7安装CDH6全程记录","date":"2019-10-29T02:50:40.000Z","updated":"2021-12-13T15:17:16.754Z","comments":true,"path":"38328/","link":"","permalink":"https://shmily-qjj.top/38328/","excerpt":"","text":"前言一开始搭集群时，都是装Apache原生的Hadoop，Spark包，一个一个装，一个一个配，好麻烦，而且通过命令或REST监控还很不直观，直到我遇到了Cloudera Manager，这东西简直就是神器。Cloudera Manager（简称CM），是Cloudera开发的一款大数据集群部署神器，而且它具有集群自动化安装、中心化管理、集群监控、报警等功能，通过它，可以轻松一键部署，大大方便了运维，也极大的提高集群管理的效率。一开始因为CentOS8出来了，想尝鲜，发现ClouderaManager没有el8的版本，所以暂时还不能用CentOS8来安装CM，请千万不要尝试使用CentOS8。 CM的主要功能： 管理：对集群进行管理，如添加、删除节点等操作 监控：监控集群的健康情况，对设置的各种指标和系统运行情况进行全面监控 诊断：对集群出现的问题进行诊断，会针对集群问题给出建议的方案 集成：对hadoop生态的多种组件和框架进行整合，减少部署时间和工作量 兼容：与各个生态圈的兼容性强 总结一下就是：方便搭建和运维，提供全面监控 CDH架构CDH的组件： Agent：在每台机器上安装，该代理程序负责启动和停止服务和角色的过程，拆包配置，触发装置和监控主机。 Management Service：负责执行各种监控，警报和报告功能角色等服务。 Database：存储配置和监视信息。通常情况下，多个逻辑数据库在一个或多个数据库服务器上运行。例如，Cloudera的管理服务器和监控角色使用不同的逻辑数据库。 Cloudera Repository：软件由Cloudera管理分布存储库。 Clients：是用于与服务器进行交互的接口 CDH中都有哪些服务? 组件名称 用途 Zookeeper Apache ZooKeeper 是用于维护和同步配置数据的集中服务。 HDFS HDFS是 Hadoop 应用程序使用的主要存储系统。 yarn Apache Hadoop MapReduce 2.0 (MRv2) 或 YARN 是支持 MapReduce 应用程序的数据计算框架。依赖HDFS服务。 HBase 支持随机读/写访问的Hadoop数据库(HBase是一个分布式、面向列的开源数据库，) Hive 在大数据集合上的类SQL查询和表。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 impala Impala是一个新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。 solr Solr是一个分布式服务，用于编制存储在 HDFS 中的数据的索引并搜索这些数据。 spark Spark是强大的开源并行计算引擎，基于内存计算，速度更快；接口丰富，易于开发；集成SQL、Streaming、GraphX、MLlib，提供一栈式解决方案。 flume 高可靠、可配置的数据流集合。 storm Storm是一个分布式的、容错的实时计算系统。 kafka Kafka是一种高吞吐量的分布式发布订阅消息系统。 Hue 可视化Hadoop应用的用户接口框架和SDK。。 Sqoop 以高度可扩展的方式跨关系数据库和HDFS移动数据 oozie Oozie是一种框架，是用于hadoop平台的作业调度服务。 Avro 数据序列化：丰富的数据结构，快速/紧凑的二进制格式和RPC。 Crunch Java库，可以更轻松地编写，测试和运行MR管道。 DataFu 用于进行大规模分析的有用统计UDF库。 Mahout 用于群集，分类和协作过滤的库。 Parquet 在Hadoop中提供压缩，高效的列式数据表示。 Pig 提供使用高级语言批量分析大型数据集的框架。 MapReduce 强大的并行数据处理框架。 Pig 数据流语言和编译器 Sqoop 利用集成到Hadoop的数据库和数据仓库 Sentry 为Hadoop用户提供精细支持，基于角色的访问控制。 Kudu 完成Hadoop的存储层，以实现对快速数据的快速分析。 安装部署在虚拟机环境上部署Cloudera Manager，可能达不到预期的效果，但是基本的功能可以实现。我的电脑内存16GB勉强可以使用，如果电脑16GB以上的可以考虑折腾CDH，16GB以下的想都不要想… 环境物理机i7-6700hq 16GB内存 1T HDD虚拟机四台 8个逻辑核心 内存分配分别是 5GB 3GB 2GB 2GB （可以说是榨干了物理机性能）建议如果没有i7-8th及以上CPU或没有32G+的电脑，就不要尝试了。还是直接装Apache版的好些。VMWare 15SecureCRT 8.1.4FileZilla 3.40.0CentOS 7以上是旧配置，后面再有更新均使用新的配置：物理机i7-9750h 64GB内存 2T HDD+2T SSD虚拟机四台 12个逻辑核心 内存分配分别是 20GB 14GB 14GB 10GBHyper-V虚拟机SecureCRT 8.5.3FileZilla 3.40.0CentOS 7能够同时运行所有服务。 一.基础配置下载CentOS7:CentOS7 Minimal下载 虚拟机配置 使用VMWare采用NAT格式网卡,按如下配置虚拟网卡设置（编辑-虚拟网络编辑器）点击NAT设置:点击DHCP设置:以后我们的虚拟机都使用NAT网卡安装CentOS7文件-&gt;新建虚拟机-&gt;选择自定义(高级)-&gt;下一步-&gt;下一步-&gt;稍后安装操作系统-&gt;选择Linux/CentOS7 64位-&gt;下一步-&gt;虚拟机名称CDH066-&gt;下一步-&gt;根据自己电脑设置核心数-&gt;下一步-&gt;虚拟机内存5120MB-&gt;网络类型选NAT-&gt;下一步…-&gt;磁盘分配80GB-&gt;下一步-&gt;下一步-&gt;自定义硬件-&gt;选择CentOS7的安装镜像,如图:关闭-&gt;完成-&gt;开启此虚拟机开始安装安装Minimal版的CentOS，感觉很清爽！但是后续需要自己手动装一些依赖包，不过这样也好，可以避免安装过多无用的依赖。时区选择ShangHai。 此步骤时指定root密码123456安装时指定一个管理员用户shmily 密码123456 使用Hyper-V（推荐）在Hyper-V管理器中的虚拟交换机管理器新建内部网络，然后如果要指定IP，需要去电脑的网络设置IPV4，然后设置把Wifi网络共享给这个网卡。IPV4：192.168.x.1 (x均替换为你喜欢的值 1-254)网关255.255.255.255.0然后配置虚拟机ifcfg-eth0时IPADDR=192.168.x.101NETMASK=255.255.255.0DNS1=192.168.x.1DNS2=192.168.x.2……..我的设置如图： 安装完成后Reboot，按步骤进行如下配置 rm -rf * vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=cdh066 vi /etc/sysconfig/network-scripts/ifcfg-ens33 修改以下几项的值 BOOTPROTO=static ONBOOT=yes NM_CONTROLLED=yes IPADDR=192.168.1.66 GATEWAY=192.168.1.2 DNS1=192.168.1.2 vi /etc/sudoers 添加以下，必要的话可以加其他用户权限控制策略，这里我对root和shmily两个用户赋权 root ALL=(ALL) ALL 下面添加： shmily ALL=(ALL) ALL systemctl start NetworkManger systemctl enable NetworkManger service NetworkManager status systemctl status firewalld.service # 查看防火墙状态 systemctl stop firewalld.service # 关闭防火墙 systemctl disable firewalld.service # 关闭防火墙开机启动 systemctl is-enabled firewalld.service # 查看防火墙是否开机启动 # 关闭selinux vi /etc/selinux/config 配置文件中的 SELINUX=disabled # 开启SSH服务，用于使用SecureCRT连接 # 检查ssh服务是否开启（CentOS7默认开启） ps -e | grep sshd # 修改Hostname vi /etc/hostname localhost.localdomain改为cdh066 sudo hostnamectl set-hostname CDH067 vi /etc/hosts # 添加如下记录 192.168.1.66 cdh066 192.168.1.67 cdh067 192.168.1.68 cdh068 192.168.1.69 cdh069 reboot # 重启机器CDH066 # 检查22端口是否开启 （CentOS87默认开启） yum install net-tools netstat -an | grep 22 SecureCRT连接测试SSHSecureCRT创建New Session -&gt; SSH2 -&gt; Hostname是CDH066 username是root发现还是会提示Hostname lookup failed: host not found需要修改Windows的C:\\Windows\\System32\\drivers\\etc\\hosts添加如下并保存192.168.1.66 cdh066192.168.1.67 cdh067192.168.1.68 cdh068192.168.1.69 cdh069 重新用SecureCRT连接出现如下图:Accept &amp; Save,输入密码并勾选Save password完成FileZilla也能连接了: 检查一下网络:ping 8.8.8.8能ping通即可进行下一步，如果ping不通，需要仔细检查网络配置文件: 安装python:CentOS7 Minimal默认带Python2.7.5版本，已经满足需求，为了开发方便，还是安装个ipython吧 yum -y install epel-release yum install python-pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests # 安装必要的库可以指定源 以安装requests库为例 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ipython # 安装ipython 完成后，命令行执行python即可运行python2.7.5，命令行执行ipython即可使用ipython 安装一些必要的常用命令[必要]yum install bind-utilsyum -q install /usr/bin/iostatyum install vim wget iotop lsofyum install -y gityum install dstat (全面的系统监控工具-推荐)yum install nload 安装一些CDH所需的必要依赖[必要] yum -y install chkconfig bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs redhat-lsb httpd httpd-tools unzip ntp systemctl start httpd.service # 启动httpd服务 systemctl enable httpd.service # 设置httpd开机启动 yum -y install httpd createrepo # createrepo是安装CDH6集群必备 vim /etc/rc.local 添加 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled chmod +x /etc/rc.d/rc.local 安装JDK1.8[千万不要自行更换版本]去Oracle官网下载1.8版本8u181的安装包JDK 1.8历史版本下载如果安装最新版本，后续CDH安装服务会无法启动，遇到各种问题。要明确CDH6.3支持的JDK版本： 这里注意，JDK目录一定是/usr/java/jdk_1.8.x_xx，这样CM服务才能检测到JDK，否则服务无法启动 mkdir /opt/software # 通过FileZilla上传到CDH066节点的 &lt;u&gt;/opt/software&lt;/u&gt;目录下 cd /opt/software mkdir /usr/java/ tar -zxvf jdk-8u181-linux-x64.tar.gz -C /usr/java/ cd .. vim /etc/profile 添加 #JAVA_HOME export JAVA_HOME=/usr/java/jdk1.8.0_181 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar source /etc/profile java -version 优化服务器配置 # swappiness echo 10 &gt; /proc/sys/vm/swappiness # 关闭透明大页面压缩 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled echo &quot;vm.swappiness=0&quot; &gt;&gt; /etc/sysctl.conf 还有一些其他的监控命令Linux监控命令汇总 Mysql安装(CDH必备)首先查看Cloudera Manager官网要求的Mysql版本：Database Requirements参考CDH6.x兼容的版本，我们选择Mysql5.7版本注意:mysql-server依赖mysql-clientmysql-client依赖mysql-community-libsmysql-community-libs依赖mysql-community-common所以安装Server会默认安装其全部依赖在线安装： rpm -qa|grep mariadb rpm -e --nodeps mariadb-libs-5.5.64-1.el7.x86_64 # 卸载MariaDB 虽然CDH6支持了MariaDB，但还是推荐Mysql wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum -y install mysql-community-server 离线安装：去Mysql-Archives 下载对应版本的rpm包 # 安装依赖包 rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm # 若安装失败 删除mariadb包 rpm -qa | grep mariadb rpm -ve mariadb # 安装libs rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm # 安装客户端 rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm # 安装服务端 rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm # 启动 systemctl status mysqld # 开机自启 systemctl enable mysqld # 查找临时密码 grep &#39;temporary password&#39; /var/log/mysqld.log # 设置密码强度 set global validate_password_policy=LOW; set global validate_password_length=6; 如图安装完成，接着我们对其进行一些配置 systemctl start mysqld.service systemctl enable mysqld.service # 设置开机启动 systemctl status mysqld.service # 查看mysql运行状态 grep &#39;temporary password&#39; /var/log/mysqld.log # 找到root初始密码，我的是cWgrI9:14%=_ mysql -uroot -p # 登陆mysql # 提示Enter Password cWgrI9:14%=_ set global validate_password_policy=LOW; # 没有这项会提示Your password does not satisfy the current policy requirements 如果不是生产环境需要修改密码安全策略等级为LOW set global validate_password_length=6; # 最低密码长度，因为测试所以设为了6 生产环境则不需要修改 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; # 修改数据库密码为123456 CREATE USER &#39;mysql&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # root登陆然后创建用户及其密码（用户名mysql为例） GRANT ALL ON mysql.* TO &#39;mysql&#39;@&#39;%&#39;; # 赋予mysql用户所有权限 flush privileges; # 刷新配置 status; # 通过这个命令发现Mysql目前不是UTF-8字符集 配置utf-8字符集vim /etc/my.cnf 添加如下配置注意顺序，client一定在mysqld属性的上方 [client] default-character-set=utf8 [mysqld] init_connect=&#39;SET collation_connection = utf8_unicode_ci&#39; init_connect=&#39;SET NAMES utf8&#39; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake 根据CDH官方推荐的Mysql参数配置,继续添加如下参数:如果生产环境，需要根据集群配置的实际情况来设定 [mysqld] transaction-isolation = READ-COMMITTED symbolic-links = 0 key_buffer_size = 32M max_allowed_packet = 32M thread_stack = 256K thread_cache_size = 64 query_cache_limit = 8M query_cache_size = 64M query_cache_type = 1 max_connections = 550 expire_logs_days = 10 max_binlog_size = 100M log_bin=/var/lib/mysql/mysql_binary_log server_id=1 binlog_format = mixed read_buffer_size = 2M read_rnd_buffer_size = 16M sort_buffer_size = 8M join_buffer_size = 8M # InnoDB settings innodb_file_per_table = 1 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 64M innodb_buffer_pool_size = 128M innodb_thread_concurrency = 8 innodb_flush_method = O_DIRECT innodb_log_file_size = 512M sql_mode=STRICT_ALL_TABLES 重启Mysql服务systemctl restart mysqld.service 登录Mysql并查看是否修改成功mysql -hlocalhost -P3306 -uroot -p123456show variables like “%character%”;show variables like “%collation%”;如图即为配置成功 创建CM的数据库并增加数据库所属用户的远程登陆权限： CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;rman&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;sentry&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;nav&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;navms&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON scm.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; set global validate_password_policy=LOW; set global validate_password_length=6; GRANT ALL ON root.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # 让root用户可以在cdh066节点上登录 FLUSH PRIVILEGES; 关于如何查看和修改用户的远程登录权限：select user,host from mysql.user;host字段为%的则是允许远程登录的用户，是localhost的只能本地登录所以想给远程某台机器开通远程访问某个用户的权限： update mysql.user set host=’CDH066’ where user=’root’;或者想给某个用户所有局域网内机器的访问权限： update mysql.user set host=’%’ where user=’root’;然后重启服务或者刷新配置就可以通过mysql -hCDH066 -uroot -p123456来登录了远程其他节点可以通过制定-h来访问非root用户的mysql Mysql JDBC库配置：右键 链接另存为 进行下载**下载mysql-connector-java-5.1.47-bin.jar**，将mysql-connector-java-5.1.47-bin.jar文件上传到CDH066节点上的/usr/share/java/目录下并重命名为mysql-connector-java.jar（如果/usr/share/java/目录不存在，需要手动创建） 系统文件描述符限制修改vi /etc/security/limits.conf * soft nofile 32728 * hard nofile 1029345 * soft nproc 65536 * hard nproc unlimited * soft memlock unlimited * hard memlock unlimited 好玩的screenfetch(可选，用来娱乐…) cd /usr/local/src git clone https://github.com/KittyKatt/screenFetch.git cp screenFetch/screenfetch-dev /usr/local/bin/screenfetch chmod 777 /usr/local/bin/screenfetch 更多安全与防火墙配置参考安全与防火墙配置有关linux用户和组的详细文章:Linux用户和组 开启ntpd时间同步：参考：ntp本地服务器搭建1.创建本地NTP时间服务器 vim /etc/ntp.conf 注释掉： #restrict default nomodify notrap nopeer noquery #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst 添加： restrict default nomodify restrict 192.168.1.0 mask 255.255.255.0 nomodify 显式的指出时间服务器所涉及的ip范围 server 127.127.1.0 fudge 127.127.1.0 stratum 10 2.配置NTP客户端（其他节点） 注释掉： #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst 添加： server cdh101 指明本地ntp服务器地址 3.分别启动ntpd服务systemctl status ntpdsystemctl start ntpdsystemctl enable ntpdntpdate -u cdh101 手动同步一次ntpq –p 查看ntpd服务状态 二.克隆虚拟机克隆CDH所需的另外三台虚拟机右键CDH066这台已关闭的虚拟机，右键-&gt;管理-&gt;克隆选择虚拟机中当前状态 下一步选择创建完整克隆 下一步虚拟机名称 CDH067 完成同样方法克隆 CDH068 CDH069克隆完成对机器进行设置:CDH067 3GB内存CDH068 2GB内存CDH069 2GB内存 开启CDH067机器确保 /ect/hosts里已经添加了其他机器的ip和hostvim /etc/sysconfig/network-scripts/ifcfg-ens33删除UUID和HWADDRIPADDR重新分配为192.168.1.67修改后如图 vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=CDH067 vi /etc/hostnameCDH066改为CDH067sudo hostnamectl set-hostname CDH067 重启 reboot 同样方式修改CDH068,CDH069 检查： ping8.8.8.8能通 使用SecureCRT可以正常连接到机器 ifconfig 或 ip addr命令查看ip地址成功改过来了则配置成功 三.配置免密登录[可选 非必须]在四台机器分别操作：ssh-keygen 并连续敲三下回车 在66机器上ssh-copy-id cdh066 建立 cdh066自身免密ssh-copy-id cdh067 建立 cdh066 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh066 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh066 -&gt; cdh069单向免密 在67机器上ssh-copy-id cdh066 建立 cdh067 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh066自身免密ssh-copy-id cdh068 建立 cdh067 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh067 -&gt; cdh069单向免密 在68机器上ssh-copy-id cdh066 建立 cdh068 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh068 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh068自身免密ssh-copy-id cdh069 建立 cdh068 -&gt; cdh069单向免密 在69机器上ssh-copy-id cdh066 建立 cdh069 -&gt; cdh066单向免密ssh-copy-id cdh067 建立 cdh069 -&gt; cdh067单向免密ssh-copy-id cdh068 建立 cdh069 -&gt; cdh068单向免密ssh-copy-id cdh069 建立 cdh069自身免密 测试都能免密登录:至此免密登录配置完成 四.CDH6安装下面的是下载地址，因为我之前手动安装了JDK1.8，所以可以不下载oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm这个包，其余的包全部下载下来CDH6.3.1下载地址还需要一个asc文件，下载地址：allkeys.asc,右键另存为即可在cdh066节点上进行操作mkdir /opt/software/cloudera-repos将下载的所有文件通过FileZilla上传到/opt/software/cloudera-repos目录，目录结构如下:├── allkeys.asc├── cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm├── cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm├── cloudera-manager-server-db-2-6.3.1-1466458.el7.x86_64.rpm├── enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpm└── cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm CDH066节点执行如下命令 目的是建立本地存储库 搭建本地源 为了节省空间，也可以只在一台机器上搭建源 cd /opt/software/cloudera-repos createrepo . # 将cloudera-repos目录移动到httpd的html目录下 制作本地源 cd .. mv cloudera-repos /var/www/html/ cd /etc/yum.repos.d touch cloudera-manager.repo vim cloudera-manager.repo 添加如下 baseurl地址对应自己的主机host 如 cdh067节点:http://cdh067/cloudera-repos/ [cloudera-manager] name=Cloudera Manager 6.3.1 baseurl=http://cdh066/cloudera-repos/ gpgcheck=0 enabled=1 autorefresh=0 type=rpm-md yum clean all yum makecache 制作本地源后http://cdh066/cloudera-repos/这个链接可以访问到源的文件我们搭建的本地源，后面会用到 安装Cloudera Manager组件: # 在CDH066节点运行 yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server --skip-broken --nogpgcheck 下载parcel包，：Index of cdh6/6.3.1/parcels/该链接目前需要有License的Cloudera帐号才可以下载了。 cd /opt/cloudera/parcel-repo sha1sum CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel | awk &#39;&#123; print $1 &#125;&#39; &gt; CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha chown -R cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo/* 初始化数据库该步骤很重要，可以在第一次启动ClouderaManager前检测数据库连接是否有问题，是否会影响到CMServer初始化。通过该脚本输出的日志可以定位到错误原因，并修改mysql中不合理的配置文件，修改系统环境配置错误的地方。 # （scm_prepare_database.sh 库类型 scm库名称 scm库连接的用户名 密码 -h服务端所在地址） /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm 123456 -hHOST systemctl start cloudera-scm-server.service # 启动CM服务systemctl status cloudera-scm-server.service # 查看启动状态 等待几分钟后访问http://cdh066:7180，默认帐号密码都是admin 这里选择免费版本 下面就是群集安装的步骤：主机名称填写cdh066,cdh067,cdh068,cdh069，然后点击搜索搜索这里搜到了67，68，69节点，但是66节点是灰色的，安装时，66节点不会被安装Agent，意味着后续安装的组件只能部署在67，68，69节点上运行，不过没有关系，可以在添加组件的步骤之前新开个页面将cdh066也加进去。 这步使用我们搭建的本地源 http://cdh066/cloudera-repos/ 如下设置若继续按钮仍为灰色，可以点击更多选项，将所有外部源的链接全部删掉，增加本地parcel源地址，保存更改。 这步不要勾选 填入root用户的密码 这步耐心等待，不要手动刷新 这步勾选最后一项企业安装时，最好先Inspect Hosts，验证节点是否有配置不当的地方，避免影响稳定性。针对Inspect Hosts的结果，可以一条一条优化集群配置。 开始安装服务 如图，选自定义服务根据集群环境和需求选择合适的服务和搭配。 填上之前建的数据库，选的服务不同要求也不同 最后部署成功，启动服务：因为我虚拟机搭建，物理机本身配置就很差，有内存不足和请求延迟高的问题，所以，虽然服务都能正常打开，跑一两个小的计算任务也还能勉强承受，但CDH都会报警告，大多都是提示分配内存低了，请求延迟高了，内存不足等信息。后续文章更新内容采用新电脑64GB内存i7-9750h物理机环境，能正常运行CDH服务： ClockOffset的报警：集群全红，提示ClockOffset 未检测到ntpd服务。这个时候就需要配置NTP时间同步服务，重新参考之前NTP时钟同步的配置。 企业级部署时，需要在安装好各角色后，依次设置里面每个角色(包括CMServer)的dir、path等路径，将一些日志、dump路径等放在数据盘。 五.Flink集成集成官方Flink-1.9.0到CDH管理下载相应的csd文件和parcels文件到本地：csd下载地址parcels下载地址下载后得到如下： FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jar FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.sha FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel manifest.json 将FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jar放入/opt/cloudera/csd中将FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel和FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.sha放入/opt/cloudera/parcel-repo中然后重启Cloudera Manager Server服务：sudo systemctl restart cloudera-scm-server重启完成后进入页面，主机-&gt;Parcel-&gt;检查新Parcel-&gt;找到Flink-&gt;分配完成分配后开始添加服务： Flink1.9.0版本比较老，对Hive的兼容不是很友好，可以参考：https://blog.csdn.net/qq_31454379/article/details/110440037 安装Flink官方1.12版本 六.功能扩展自定义告警脚本 七.坑点总结 如果遇到HDFS无法启动的问题，可能是因为**/dfs/nn/,/dfs/dn/,/dfs/snn/这些目录和里面的文件权限不够，请检查每个节点的这几个目录，保证nn,dn,snn文件夹权限为drwx—— 3 hdfs hadoop，即hdfs用户hadoop组，里面的current文件夹的权限为drwxr-xr-x 3 hdfs hdfs**。 提示Error: JAVA_HOME is not set and Java could not be found 先确保JDK安装路径在/usr/java/jdkxxxxx，再确定JAVA版本是当前CDH支持的JAVA版本，过高过低都不会兼容，就报这个错误。 The number of live datanodes 2 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. 不多说，关闭安全模式 hdfs dfsadmin -safemode leave 注意，需要sudo到hdfs用户操作 如果sudo到hdfs失败，就vim /etc/passwd 将hdfs用户对应的/sbin/nologin改成/bin/bash 即可sudo到hdfs 启动ClouderaManagerServer报错 INFO main:com.cloudera.server.cmf.bootstrap.EntityManagerFactoryBean: MYSQL database engine and table mapping: &#123;InnoDB=[AUDITS, COMMANDS, CONFIGS, SCHEMA_VERSION]&#125; 2020-03-25 16:27:22,021 WARN main:com.cloudera.server.cmf.bootstrap.EntityManagerFactoryBean: Failed to determine prior version. This is expected if you are starting Cloudera Manager for the first time. Please also ignore any error messages about missing tables. Moving ahead assuming no upgrade: org.hibernate.exception.SQLGrammarException: could not extract ResultSet 2020-03-25 16:27:22,031 INFO main:com.cloudera.enterprise.dbutil.DbUtil: Schema version table already exists. 2020-03-25 16:27:22,032 INFO main:com.cloudera.enterprise.dbutil.DbUtil: DB Schema version 1. 2020-03-25 16:27:22,039 WARN main:org.springframework.context.support.GenericApplicationContext: Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;com.cloudera.server.cmf.TrialState&#39;: Cannot resolve reference to bean &#39;entityManagerFactoryBean&#39; while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;entityManagerFactoryBean&#39;: FactoryBean threw exception on object creation; nested exception is java.lang.RuntimeException: Unable to obtain CM release version. 2020-03-25 16:27:22,040 ERROR main:com.cloudera.server.cmf.Main: Server failed. 使用/opt/cloudera/cm/schema/scm_prepare_database.sh工具初始化scm数据库时报错：when @@GLOBAL.ENFORCE_GTID_CONSISTENCY = 1原因是数据库启用了gtid_mode，my.cnf有如下配置 gtid_mode = on enforce_gtid_consistency = 1 binlog_gtid_simple_recovery = 1 解决：①注释掉如上配置，重启mysql server，②删掉scm库并重建，③重启cloudera-scm-server 即可 向CM添加host时，只有某几台主机可以被识别，其他主机，能显示出来但是灰色的，提示无法解析主机名称解决：检查/etc/hosts 配置是否正确 HDFS高可用进入HDFS角色，操作-&gt;启用High Availability-&gt;选择两个NN、三个JN节点，下一步（注意需要格式化NN，重启HDFS相关所有正在运行的依赖服务，重新部署客户端配置）-&gt;完成进入HUE 将“Web界面角色”改为httpfs关闭Hive，备份HiveMetastore数据库的数据（以防万一），操作-&gt;更新Hive Metastore NameNodes-&gt;重启Hive服务 升级Python版本为3.8cd /opt/software wget https://www.python.org/ftp/python/3.8.5/Python-3.8.5.tgz tar -zxvf Python-3.8.5.tgz xsync或scp -r Python-3.8.5拷贝到其他节点，并对所有节点如下操作 cd Python-3.8.5 yum update -y yum groupinstall -y &#39;Development Tools&#39; yum install -y gcc openssl-devel bzip2-devel libffi-devel ./configure prefix=/usr/local/python3 make &amp;&amp; make install ls -la /usr/bin/python* vim /usr/bin/yum #!/usr/bin/python 改为 #!/usr/bin/python2 vim /usr/libexec/urlgrabber-ext-down #!/usr/bin/python 改为 #!/usr/bin/python2 mv /usr/bin/python /usr/bin/python_bak ln -s /usr/local/python3/bin/python3.8 /usr/bin/python ln -s /usr/local/python3/bin/python3.8 /usr/bin/python3 mv /usr/bin/pip /usr/bin/pip_bak ln -s /usr/local/python3/bin/pip3.8 /usr/bin/pip3 python -V &amp;&amp; pip3 -V ---------------后续操作---------------- /usr/local/python3/bin/python3.8 -m pip install --upgrade pip -i https://pypi.doubanio.com/simple pip3 install pyspark -i https://pypi.doubanio.com/simple spark-env增加export PYSPARK_PYTHON=/usr/local/python3/bin/python3.8 pip3 install koalas -i https://pypi.doubanio.com/simple HiveMetastoreServer异常解决去HMS机器找/var/log/hive看到如下Error日志：Failed to sync requested HMS notifications up to the event ID xxxxx查看sentry 异常CounterWait源码发现传递的id比 currentid 大导致一直等待超时，超时时间为200s。CDH可以启用Sentry同步ACL权限，启动后HDFS、Sentry、HMS三者间权限同步的消息处理，突然大批量的目录权限消息需要处理，后台线程处理不过来，消息积压就会报Failed to sync requested HMS notifications up to the event ID: xxxxx，该错误不会导致HMS不可用但会导致响应速度很慢。解决： sentry_hms_notification_id表插入最大的ID，重启Sentry忽略掉之前积压的消息 设置Sentry参数sentry.notification.sync.timeout.ms（默认200s）参数调小超时时间，减小等待时间，积压不多的话可以让它自行消费处理掉 CDH添加外部HDFS集群的nameservice现在添加对外部HDFS集群nameservice-test的支持。在配置项hdfs-site.xml 的 HDFS 客户端高级配置代码段（安全阀）中添加配置 &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;nameservice-dev,nameservice-test&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.nameservice-test&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameservice-test.nn1&lt;/name&gt; &lt;value&gt;test1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameservice-test.nn2&lt;/name&gt; &lt;value&gt;test2:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.nameservice-test&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; impala-shell脚本报错升级Python版本后，Impala-shell报语法错误原python命令是python2.7，现python命令是python3.8，而/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/bin/impala-shell脚本没考虑环境变量，直接用python命令执行，故出现语法兼容性问题。解决：vim /opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/bin/impala-shell将最后一行exec python…替换为exec python2.7 PYTHONPATH=&quot;$&#123;EGG_PATH&#125;$&#123;SHELL_HOME&#125;/gen-py:$&#123;SHELL_HOME&#125;/lib:$&#123;PYTHONPATH&#125;&quot; \\ exec python2.7 $&#123;SHELL_HOME&#125;/impala_shell.py &quot;$@&quot; beeline连接Impala1.拷贝ImpalaJDBC41.jar到/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/hive/auxlib2.beeline -d “com.cloudera.impala.jdbc41.Driver” -u “jdbc:impala://one_impalad_ip:21050”3.如果报warn：Error: [Simba]JDBC Unsupported transaction isolation level: 4. (state=HY000,code=11975) 则加beeline参数–isolation=default CDH升级JDK版本主机-&gt; 选择一台 -&gt; 配置 -&gt; Java 主目录 -&gt; 填上新的jdk路径每台节点设置/etc/profile中JAVA_HOME和PATH为新jdk路径（或者修改cloudera-scm-server\\cloudera-scm-agent启动脚本上添加export JAVA_HOME=xxx）停止服务重启CMServer重启各节点CM Agent在页面重启CMService启动服务 部署与CDH6组件版本兼容的开源版Spark 2.xCDH不是自带Spark吗，为什么要使用开源版？因为CDH Spark阉割了很多Spark的功能，不支持spark-sql入口和spark-thriftserver入口，还有一些其他功能，可以参考:使用Spark2.3.x：下载或编译获得spark2.3.2-bin-hadoop2.6官方tarball可以直接兼容当前CDH个版本。其源码中hive依赖版本为1.2.1.spark2，hadoop版本为2.6.5，作为兼容hive、hadoop的客户端。使用Spark2.4.x: 下载或编译获得spark2.4.8-bin-hadoop2.7官方tarball，不可直接兼容，需要替换jar包： echo &quot;export SPARK_HOME=/opt/spark/spark-2.4.8-bin-hadoop2.7&quot; &gt;&gt; /etc/profile;source /etc/profile rm -f $SPARK_HOME:/jars/hadoop-yarn* cp /opt/cloudera/parcels/CDH/jars/hadoop-yarn* $SPARK_HOME:/jars/ cp /opt/cloudera/parcels/CDH/jars/hive-shims-scheduler-2.1.1-cdh6.3.2.jar $SPARK_HOME:/jars/ spark-env.sh JAVA_HOME=/usr/local/jdk1.8.0_181/ HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_DIST_CLASSPATH=$(/opt/cloudera/parcels/CDH/bin/hadoop classpath) export SPARK_LOCAL_DIRS=/data/spark_tmp_data spark-defaults.conf spark.kryoserializer.buffer.max=512m spark.serializer=org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled=false spark.eventLog.dir=file:///data/spark_tmp_data spark.driver.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.executor.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.yarn.am.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_181/ spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_181/ spark.local.dir=/data/spark_tmp_data ln -s /etc/hadoop/conf/core-site.xml $SPARK_HOME:/conf/core-site.xmlln -s /etc/hadoop/conf/hdfs-site.xml $SPARK_HOME:/conf/hdfs-site.xmlln -s /etc/hadoop/conf/mapred-site.xml $SPARK_HOME:/conf/mapred-site.xmlln -s /etc/hadoop/conf/yarn-site.xml $SPARK_HOME:/conf/yarn-site.xmlln -s /etc/hive/conf/hive-site.xml $SPARK_HOME:/conf/hive-site.xml如果在Kerberos集群启用ThriftServer，在hive-site.xml添加或修改如下参数 &lt;property&gt; &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 以下三项填写HMS所在服务器的地址和HiveServer2服务的keytab，HS2服务的keytab文件到HS2节点找最新/var/run/cloudera-scm-agent/process/ 里面的hive.keytab --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/cdh02@SMYOA.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/cdh02@SMYOA.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/hadoop/bigdata/kerberos/keytab/hiveserver2_cdh02.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;NONE&lt;/value&gt; &lt;/property&gt; 提交任务： cd $SPARK_HOME bin/spark-sql --master yarn sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10002 --queue thrift --master yarn --executor-memory 8g --executor-cores 5 --num-executors 20 [启用Kerberos] sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10002 --queue thrift --master yarn --executor-memory 8g --executor-cores 5 --num-executors 20 --hiveconf hive.server2.authentication.kerberos.keytab /hadoop/bigdata/kerberos/keytab/hiveserver2_cdh02.keytab 升级与CDH6组件版本兼容的开源版Spark 3.x编译https://blog.csdn.net/Young2018/article/details/108856622部署https://www.pianshen.com/article/46531976066/ Cloudera Manager使用 CDH卸载https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247484118&amp;idx=1&amp;sn=e7109978013a286fe1f172f99459c45a&amp;chksm=ec2ad2dfdb5d5bc96fcaeb6ce2ed42a025e1d95ab251372fb4769c29434f0d84fb66bbfec1d2&amp;scene=21#wechat_redirect","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"CDH6+CentOS7","slug":"CDH6-CentOS7","permalink":"https://shmily-qjj.top/tags/CDH6-CentOS7/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"记一次参加QCon全球软件开发大会","slug":"记一次参加QCon全球软件开发大会","date":"2019-10-21T11:59:50.000Z","updated":"2020-12-04T14:48:20.533Z","comments":true,"path":"39595/","link":"","permalink":"https://shmily-qjj.top/39595/","excerpt":"","text":"QCon全球软件开发大会全球软件开发大会是为团队领导者、架构师、项目经理和高级软件开发人员量身打造的企业软件开发大会，其所覆盖的主题内容与InfoQ网站相同，关注架构与设计、真实案例分析等等。秉承”促进软件开发领域知识与创新的传播”原则，QCon各项议题专为中高端技术人员设计，内容源于实践并面向社区。演讲嘉宾依据各重点和热点话题，分享技术趋势和最佳实践。**了解更多请戳👉 QCon官网 ** 犹豫去不去QCon是为团队领导者、架构师、项目经理和高级软件开发人员量身打造的企业软件开发大会，如果你们公司买了票给机会参加，那就要珍惜呀！看看还是很有好处的，选自己感兴趣的方向场次参加，可以扩宽思路，你会发现我们要解决的问题其实还有更多解决方案！哪怕有听不懂的地方，记下来回去查都很受益！ 分享我所听因为我只参加了18号下午场的QCon，所以也只听了不到四个分享会,但我会把我觉得很有用的技术或者思路分享出来！ Splash Shuffle Manager 关于Spark的ShuffleShuffle简而言之:下一个Stage向上一个Stage要数据这个过程，就称之为 Shuffle.学过Spark的童鞋都知道大多数Spark作业的运行时间主要浪费在Shuflle过程中,因为该过程包含了大量的本地磁盘IO,网络IO和序列化过程.而看过Spark源码的童鞋应该都知道Spark的ShuffleManager,虽然Spark2.x已经摒弃了HashShuffleManager,但是如果过大的表遇到”去重”,”聚合”,”排序”,”重分区”或”集合”操作等shuffle算子时还是会有大量文件落盘,而本地磁盘的性能会严重拖慢Spark计算的整体速度. 而且Shuffle发生的机器如果发生故障还会导致Stage重算,性能和稳定性都大大降低有些大规模的计算是Shuffle调优不能解决的 Splash介绍关于以上问题,我们可以通过更改Shuffle Manager的源码来实现自定义Shuffle的溢写文件存储位置,但是,改源码辣么难……咋办……Splash-支持指定Shuffle过程溢写文件的存储位置 可以指定Shuffle文件存储到高可靠的分布式存储中 ShuffleFile接口代替本地文件访问 可以使用不同的网络传输和后端存储协议来实现随机读取和写入 Splash Shuffle Manager优点 使Executor变为无状态 使添加删除节点更有灵活性,宕机无需重复计算整个shuffle文件 Shuffle文件提交符合原子性,未提交的文件可以轻松清理 随机存储和计算的分离,提供Shuffle存储介质的更多选择 Splash Shuffle Manager位于Executor上,降低部署难度 Shuffle Performance Tool可以检验存储介质性能 Splash结构和原理如图蓝色框为Splash实现类,橙色框是Spark定义的接口,绿色框是基本数据结构使用Splash后的Shuffle过程:ShuffleManager是入口,ShuffleWriter在map stage写shuffle数据,用SplashSorter或SplashUnsafeSorter将数据保存在内存中,内存不足时则会将数据溢写到TmpShuffleFile,等所有数据计算完成,SplashSorter或SplashUnsafeSorter,合并内存文件和溢写文件,SplashAggregator负责数据聚合,使用SplashAppendOnlyMap数据结构,内存不够时持久化到shuffle数据存储系统;ShuffleReader从shuffle数据存储系统收集reduce stage需要的数据,SplashShuffleBlockResolver用来查找随即数据,是无状态的. 我认为我认为Splash的设计比较符合Spark计算与存储分离的理念,所以Splash的思路是好的,但展望Spark3.0的新特性,也发现了其实Spark3.0也有一个新特性叫”Remote Shuffle Service”,Remote Shuffle Service的基本想法是，如果Map Task能将Shuffle数据写到独立的Shuffle服务，然后Reduce Task从这个Shuffle服务读Shuffle数据，这样计算节点就不再需要为Shuffle任务保留本地磁盘空间了。这个理念与Splash这个项目的理念很相近,所以我们可以尝试和部署Splash也可以期待Spark3.0的新特性! 最后附上:Splash项目的Git地址 英特尔持久内存Intel Optane DC Persistent Memory,专为数据中心使用而设计的新的内存和存储技术特性就是有媲美DRAM的性能(较DRAM略差)和有SSD一般的容量大小(目前单条512GB),一定程度消除吞吐量瓶颈对大数据计算还是有一定加成的，这里我就不做广告啦，毕竟没有广告费哈哈！ 其他 什么DBDK?这是我听到的名词,搜了一下觉得还挺厉害…不能怪我孤陋寡闻.DBDK出的时候我连HelloWorld都还不会…数据平面开发套件,可以极大提高数据处理性能和吞吐量，提高数据平面应用程序的工作效率。DPDK使用了轮询(polling)而不是中断来处理数据包。在收到数据包时，经DPDK重载的网卡驱动不会通过中断通知CPU，而是直接将数据包存入内存，交付应用层软件通过DPDK提供的接口来直接处理，这样节省了大量的CPU中断时间和内存拷贝时间。 什么推荐中台?推荐中台是个很新的名词吧…我没怎么接触推荐这块,听的要睡着了…嗯,这场分享就是爱奇艺推荐中台…上链接吧:爱奇艺推荐中台：从搭建到上线仅10天，效率提升超30% 什么数据中台?分三部分:数据仓库,大数据中间件,数据资产管理主要元素:累计接入应用数,服务调用,数仓核心表…主要作用:解决数据管控问题,知道谁用.用在哪数据中台能力: 数据资产管理,数据质量管理,数据模型管理,构建标签体系,数据应用规划及实现上链接吧:什么是数据中台,关于数据中台最好的解读 写在最后QCon让我学到了很多东西,扩展了思路,鼓励我在学习新技术的道路上奋勇向前!感觉对新技术更加感兴趣了,如果下次还有机会参加那该多好呀!话说是不是我下次再去就能听懂那些大佬说的东西了吧…做个简单的总结吧 对新技术要仔细调研，琢磨存在的问题 对于陌生的技术要先明白它是做什么的，对扩宽思路有很大帮助 编程不仅是写代码，更要考虑性能，安全性和稳定性 一个新架构的出现必然有其优势，仔细思考它带来的影响 我顺便在会场周边玩了一圈,贴几张自认为不是直男拍的照片:","categories":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"QCon全球软件开发大会","slug":"QCon全球软件开发大会","permalink":"https://shmily-qjj.top/tags/QCon%E5%85%A8%E7%90%83%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E5%A4%A7%E4%BC%9A/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"随想类博客-待更新","slug":"随想类模板","date":"2019-09-21T14:16:00.000Z","updated":"2020-04-12T14:15:48.005Z","comments":true,"path":"14419/","link":"","permalink":"https://shmily-qjj.top/14419/","excerpt":"","text":"今天的话题是…. 内容……..注意结尾两个空格 中标题0中标题1中标题2字颜色大小 This is some text!This is some text!This is some text! 更多内容: Writing 我认为（中标题）xxxxx 小标题小标题0 字体斜体文本斜体文本粗体文本粗体文本粗斜体文本粗斜体文本带下划线文本 脚注 列表无序列表用* + -三种符号表示 列表嵌套 有序列表第一项： 第一项嵌套的第一个元素 第一项嵌套的第二个元素 有序列表第二项： 第二项嵌套的第一个元素 第二项嵌套的第二个元素 最多第三层嵌套 最多第三层嵌套 最多第三层嵌套 更多内容: Generating 总结","categories":[{"name":"随想","slug":"随想","permalink":"https://shmily-qjj.top/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"标签1","slug":"标签1","permalink":"https://shmily-qjj.top/tags/%E6%A0%87%E7%AD%BE1/"},{"name":"标签2","slug":"标签2","permalink":"https://shmily-qjj.top/tags/%E6%A0%87%E7%AD%BE2/"}],"keywords":[{"name":"随想","slug":"随想","permalink":"https://shmily-qjj.top/categories/%E9%9A%8F%E6%83%B3/"}]}]}