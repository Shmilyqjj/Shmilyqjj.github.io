{"meta":{"title":"ä½³å¢ƒçš„å°æœ¬æœ¬","subtitle":"ä½³å¢ƒShmilyçš„ä¸ªäººç½‘ç«™","description":"ä½³å¢ƒShmilyçš„ä¸ªäººç½‘ç«™","author":"ä½³å¢ƒShmily","url":"https://shmily-qjj.top"},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2022-12-11T05:35:07.927Z","comments":false,"path":"about/index.html","permalink":"https://shmily-qjj.top/about/index.html","excerpt":"","text":"[ä½³å¢ƒShmily] ä¸&nbsp; ä½³å¢ƒ&nbsp; ï¼ˆ Shmily ï¼‰ å¯¹è¯ä¸­... bot_ui_ini()","keywords":"å…³äº"},{"title":"çœ‹å‰§","date":"2019-09-21T13:32:48.000Z","updated":"2022-12-11T05:35:07.928Z","comments":true,"path":"bangumi/index.html","permalink":"https://shmily-qjj.top/bangumi/index.html","excerpt":"","text":"","keywords":"å¥½å‰§å®‰åˆ©"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2022-12-11T05:35:07.930Z","comments":false,"path":"donate/index.html","permalink":"https://shmily-qjj.top/donate/index.html","excerpt":"","text":"","keywords":"æ„Ÿè°¢å¤§ä½¬çš„æ‰“èµï¼Œæˆ‘ä¼šåŠªåŠ›æ›´åšæ»´ï¼"},{"title":"album","date":"2019-09-21T13:47:59.000Z","updated":"2022-12-11T05:35:07.928Z","comments":true,"path":"album/index.html","permalink":"https://shmily-qjj.top/album/index.html","excerpt":"","text":"è¯¥æ¨¡å—æ­£åœ¨å¼€å‘balabala","keywords":"ç›¸å†Œæ¨¡å—æ­£åœ¨å¼€å‘"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2022-12-11T05:35:07.929Z","comments":true,"path":"comment/index.html","permalink":"https://shmily-qjj.top/comment/index.html","excerpt":"","text":"å¿µä¸¤å¥è¯— å™åˆ«æ¢¦ã€æ‰¬å·ä¸€è§‰ã€‚ ã€å®‹ä»£ã€‘å´æ–‡è‹±ã€Šå¤œæ¸¸å®«Â·äººå»è¥¿æ¥¼é›æ³ã€‹","keywords":"ç•™è¨€æ¿"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2022-12-11T05:35:07.929Z","comments":false,"path":"client/index.html","permalink":"https://shmily-qjj.top/client/index.html","excerpt":"","text":"ç›´æ¥ä¸‹è½½ or æ‰«ç ä¸‹è½½ï¼š","keywords":"Androidå®¢æˆ·ç«¯"},{"title":"lab","date":"2019-09-21T13:47:59.000Z","updated":"2022-12-11T05:35:07.930Z","comments":false,"path":"lab/index.html","permalink":"https://shmily-qjj.top/lab/index.html","excerpt":"","text":"è¯¥æ¨¡å—æ­£åœ¨å¼€å‘balabala","keywords":"Labå®éªŒå®¤æ¨¡å—æ­£åœ¨å¼€å‘"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2021-06-11T13:15:49.489Z","comments":true,"path":"tags/index.html","permalink":"https://shmily-qjj.top/tags/index.html","excerpt":"","text":""},{"title":"rss","date":"2019-09-21T15:59:59.000Z","updated":"2021-06-11T13:15:49.488Z","comments":true,"path":"rss/index.html","permalink":"https://shmily-qjj.top/rss/index.html","excerpt":"","text":""},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2022-12-11T05:35:07.932Z","comments":false,"path":"music/index.html","permalink":"https://shmily-qjj.top/music/index.html","excerpt":"","text":"æ¬¢è¿å…‰é¡¾ä½³å¢ƒçš„éŸ³ä¹å°çªï¼è¿™é‡Œæœ‰æˆ‘çš„åŸåˆ›åŠç¿»å”±ä½œå“ï¼Œæ¬¢è¿æ”¶å¬å‘¦ï¼æˆ³è¿™é‡Œï¼šShmily_ä½³å¢ƒçŒœä½ æƒ³å¬ï¼šå¹æ¢¦åˆ°è¥¿æ´² coveræ‹æ‹æ•…äººéš¾ç¥ç§˜å›­ä¹‹æ­Œ æŒ‡å¼¹ç‰ˆé£ä¹‹è¯— coveræŠ¼å°¾æ¡‘Cries In The Drizzleæˆ‘å¿ƒæ°¸æ’ æŒ‡å¼¹ç‰ˆæ‹¥æŠ± coverå¾ç§‰é¾™ æ‰€æœ‰ä½œå“ï¼š","keywords":"ä½³å¢ƒéŸ³ä¹å°çª"},{"title":"links","date":"2019-11-04T06:11:06.000Z","updated":"2022-12-11T05:35:07.931Z","comments":true,"path":"links/index.html","permalink":"https://shmily-qjj.top/links/index.html","excerpt":"","text":"","keywords":"åšå‹åœˆ"},{"title":"æŠ€æœ¯åˆ†äº«","date":"2019-09-21T14:53:25.000Z","updated":"2021-06-11T13:15:49.489Z","comments":false,"path":"tech/index.html","permalink":"https://shmily-qjj.top/tech/index.html","excerpt":"","text":"å¼€å‘ä¸­","keywords":"æŠ€æœ¯åˆ†äº«æ¨¡å—æ­£åœ¨è®¾è®¡å’Œå¼€å‘"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2022-12-11T05:35:07.932Z","comments":false,"path":"video/index.html","permalink":"https://shmily-qjj.top/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/Resources/img/bangumi/linglong.jpg', title: 'aaa', status: 'å·²è¿½å®Œ', progress: 100, jp: 'åºŸåœŸé£æ ¼çš„å›½äº§åŠ¨æ¼«å·…å³°ä¹‹ä½œ', time: 'æ”¾é€æ—¶é—´: 2019-07-13', desc: ' ä¸ä¹…çš„æœªæ¥ï¼Œäººç±»çš„ä¸–ç•Œæ—©å·²æ‹¥æŒ¤ä¸å ªï¼Œè¿ˆå‘æ˜Ÿæ²³ã€å¯»æ‰¾æ–°å®¶å›­çš„è¡ŒåŠ¨è¿«åœ¨çœ‰æ·ã€‚æ­£å½“ä¸€åˆ‡æœ‰æ¡ä¸ç´Šçš„æ¨è¿›ä¹‹æ—¶ï¼Œæœˆç›¸å¼‚åŠ¨ï¼Œè„šä¸‹çš„å¤§åœ°çˆ†å‘äº†é•¿è¾¾æ•°åå¹´ã€å‰§çƒˆçš„åœ°è´¨å˜åŒ–ï¼Œäººç±»åœ¨è¿™åœºæµ©åŠ«ä¸­æ‰€å‰©æ— å‡ ã€‚å½“å¤©åœ°é€æ¸æ¢å¤å¹³é™ï¼Œäººä»¬ä»åºŸå¢Ÿå’Œæ·±æ¸Šä¸­é‡æ–°è¸ä¸Šäº†è¿™ç‰‡ç†Ÿæ‚‰è€Œåˆé™Œç”Ÿçš„å¤§åœ°ã€‚ä¹ æƒ¯äº†ä¸»å®°ä¸€åˆ‡çš„æˆ‘ä»¬æ˜¯å¦è¿˜æ˜¯è¿™ä¸ªä¸–ç•Œçš„ä¸»äººï¼Ÿ' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: 'æœèŠ±å¤•èª“â€”â€”äºç¦»åˆ«ä¹‹æœæŸèµ·çº¦å®šä¹‹èŠ±', status: 'å·²è¿½å®Œ', progress: 100, jp: 'ã•ã‚ˆãªã‚‰ã®æœã«ç´„æŸã®èŠ±ã‚’ã‹ã–ã‚ã†', time: '2018-02-24 SUN.', desc: ' ä½åœ¨è¿œç¦»å°˜åš£çš„åœŸåœ°ï¼Œä¸€è¾¹å°†æ¯å¤©çš„äº‹æƒ…ç¼–ç»‡æˆåä¸ºå¸Œæ¯”æ¬§çš„å¸ƒï¼Œä¸€è¾¹é™é™ç”Ÿæ´»çš„ä¼Šæ¬§å¤«äººæ°‘ã€‚åœ¨15å²å·¦å³å¤–è¡¨å°±åœæ­¢æˆé•¿ï¼Œæ‹¥æœ‰æ•°ç™¾å¹´å¯¿å‘½çš„ä»–ä»¬ï¼Œè¢«ç§°ä¸ºâ€œç¦»åˆ«çš„ä¸€æ—â€ï¼Œå¹¶è¢«è§†ä¸ºæ´»ç€çš„ä¼ è¯´ã€‚æ²¡æœ‰åŒäº²çš„ä¼Šæ¬§å¤«å°‘å¥³ç›å¥‡äºšï¼Œè¿‡ç€è¢«ä¼™ä¼´åŒ…å›´çš„å¹³ç¨³æ—¥å­ï¼Œå´æ€»æ„Ÿè§‰â€œå­¤èº«ä¸€äººâ€ã€‚ä»–ä»¬çš„è¿™ç§æ—¥å¸¸ï¼Œä¸€ç¬é—´å°±å´©æºƒæ¶ˆå¤±ã€‚è¿½æ±‚ä¼Šæ¬§å¤«çš„é•¿å¯¿ä¹‹è¡€ï¼Œæ¢…è¨è’‚å†›ä¹˜åç€åä¸ºé›·çº³ç‰¹çš„å¤ä»£å…½å‘åŠ¨äº†è¿›æ”»ã€‚åœ¨ç»æœ›ä¸æ··ä¹±ä¹‹ä¸­ï¼Œä¼Šæ¬§å¤«çš„ç¬¬ä¸€ç¾å¥³è•¾è‰äºšè¢«æ¢…è¨è’‚å¸¦èµ°ï¼Œè€Œç›å¥‡äºšæš—æ‹çš„å°‘å¹´å…‹é‡Œå§†ä¹Ÿå¤±è¸ªäº†ã€‚ç›å¥‡äºšè™½ç„¶æ€»ç®—é€ƒè„±äº†ï¼Œå´å¤±å»äº†ä¼™ä¼´å’Œå½’å»ä¹‹åœ°â€¦â€¦ã€‚' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} å½±è§†å®‰åˆ© åˆ†äº«å¥½ç”µå½±å’Œå¥½å‰§ window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} æ”¾é€æ—¶é—´: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"bå "}],"posts":[{"title":"Hiveåˆ—çº§è¡€ç¼˜ä¸å…ƒæ•°æ®æ”¶é›†","slug":"Hiveåˆ—çº§è¡€ç¼˜ä¸å…ƒæ•°æ®æ”¶é›†","date":"2023-08-02T04:16:00.000Z","updated":"2023-08-02T14:42:26.750Z","comments":true,"path":"3d59c888/","link":"","permalink":"https://shmily-qjj.top/3d59c888/","excerpt":"","text":"Hiveåˆ—çº§è¡€ç¼˜ä¸å…ƒæ•°æ®æ”¶é›†èƒŒæ™¯å¤§æ•°æ®ç¦»çº¿è®¡ç®—é‡Œä¸å¼€Hive,è€Œæ•°æ®å­—å…¸ä¸æ•°æ®è¡€ç¼˜åœ¨æ•°æ®æ²»ç†ä¸­æ˜¯æœ€åŸºæœ¬å’Œå¿…è¦çš„ã€‚æ•°æ®å­—å…¸å’Œæ•°æ®è¡€ç¼˜å¯ä»¥å¤§å¹…æå‡æ•°æ®åˆ†ææ•ˆç‡ï¼Œæ–¹ä¾¿åˆ†æäººå‘˜å¿«é€Ÿäº†è§£ä¸šåŠ¡ã€‚æœ¬æ–‡ä¸»è¦åŸºäºHive 3.xå®ç°åˆ—çº§è¡€ç¼˜ä¸å…ƒæ•°æ®æ”¶é›†ã€‚ åˆ—çº§è¡€ç¼˜æ”¶é›†æ€è·¯ï¼šä½¿ç”¨Hive Hookæœºåˆ¶é›¶ä¾µå…¥å®ç°åˆ—çº§è¡€ç¼˜çš„æ”¶é›†ã€‚å‚è€ƒHiveæºç ä¸­org.apache.hadoop.hive.ql.hooks.LineageLoggerç±»ï¼Œå¯¹å…¶è¿›è¡Œæ”¹é€ ï¼Œå³å¯å®ç°åˆ—çº§åˆ«è¡€ç¼˜å…³ç³»çš„æ”¶é›†ã€‚ Hookå®ç°ä»£ç ï¼šColumnLevelLineageHook.java Hookéƒ¨ç½²ï¼šæ‰“jaråŒ…ï¼Œæ”¾å…¥$HIVE_HOME/auxlib/ä¸‹ï¼Œä¿®æ”¹hive-site.xmlå¦‚ä¸‹ &lt;property&gt; &lt;name&gt;hive.exec.post.hooks&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.hooks.LineageLogger,top.qjj.shmily.lineage.ColumnLevelLineageHook&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;column.lineage.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; æ³¨æ„äº‹é¡¹ï¼šhive.exec.post.hooksé…ç½®é¡¹éœ€å¸¦æœ‰HiveåŸç”ŸHookç±»org.apache.hadoop.hive.ql.hooks.LineageLoggerï¼Œå¦åˆ™æ— æ³•è·å–åˆ°åˆ—çº§åˆ«çš„è¡€ç¼˜å…³ç³»ï¼ŒåŸå› è§org/apache/hadoop/hive/ql/optimizer/Optimizer.java:78åªæœ‰æŒ‡å®šäº†org.apache.hadoop.hive.ql.hooks.LineageLoggerï¼Œæ‰ä¼šæ·»åŠ Generatoræ¥ç”Ÿæˆåˆ—çº§è¡€ç¼˜ï¼Œå¦åˆ™é»˜è®¤é’ˆå¯¹å…¶ä»–Hookä¸æ”¶é›†åˆ—çº§åˆ«è¡€ç¼˜ã€‚ å…ƒæ•°æ®å®æ—¶æ”¶é›†æ€è·¯ï¼šä½¿ç”¨Hive Metastore Listeneré›¶ä¾µå…¥å®ç°å®æ—¶æ”¶é›†Hiveå…ƒæ•°æ®å˜æ›´ï¼Œå‚è€ƒæºç ä¸­org.apache.hive.hcatalog.listener.DbNotificationListeneræˆ–org.apache.hive.hcatalog.listener.NotificationListenerç±»å®ç°ã€‚ Listenerå®ç°ä»£ç ï¼šMetadataListener.java Listeneréƒ¨ç½²ï¼šæ‰“jaråŒ…ï¼Œæ”¾å…¥$HIVE_HOME/libä¸‹ï¼Œä¿®æ”¹hive-site.xmlå¦‚ä¸‹ &lt;property&gt; &lt;name&gt;hive.metastore.event.listeners&lt;/name&gt; &lt;value&gt;top.qjj.shmily.metadata.MetadataListener&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;metadata.listener.service.name&lt;/name&gt; &lt;value&gt;hive_local&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;metadata.listener.meta.server.rest&lt;/name&gt; &lt;value&gt;http://host:port/metadata/receive&lt;/value&gt; &lt;/property&gt; æ³¨æ„äº‹é¡¹ï¼š metadata.listener.service.nameå¯ä»¥æŒ‡å®šä½ çš„é›†ç¾¤åç§°ï¼Œç”¨äºåŒºåˆ†å¤šé›†ç¾¤ï¼Œå½±å“è¾“å‡ºç»“æœä¸­database\\tableçš„fqn(å”¯ä¸€æ ‡è¯†) metadata.listener.meta.server.restæŒ‡å®šå…ƒæ•°æ®ä¸ŠæŠ¥åœ°å€ å¯ä»¥å¼€å¯lastAccessTimeå±æ€§ç»Ÿè®¡ï¼Œé€šè¿‡å¦‚ä¸‹HiveåŸç”Ÿå‚æ•°&lt;property&gt; &lt;name&gt;hive.security.authorization.sqlstd.confwhitelist.append&lt;/name&gt; &lt;!-- &lt;value&gt;hive\\.exec\\.pre\\.hooks&lt;/value&gt; --&gt; &lt;value&gt;hive\\.*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.pre.hooks&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec&lt;/value&gt; &lt;/property&gt; å¯ä»¥å¼€å¯è¡¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå‚æ•°ä¸ºhive.stats.autogather=trueï¼Œä¼šé¢å¤–æ”¶é›†åˆ°å¦‚ä¸‹ä¿¡æ¯last_modified_by xxxxx last_modified_time 1690965029 numFiles 11 numRows 10 rawDataSize 34 totalSize 2433 transient_lastDdlTime 1690965096 ä¸å»ºè®®åœ¨å…¨å±€å‚æ•°è®¾ç½®è¯¥å‚æ•°ï¼Œä¼šå½±å“å†™å…¥æ€§èƒ½ï¼Œä½†å¯ä»¥é’ˆå¯¹éƒ¨åˆ†è¡¨å•ç‹¬è®¾ç½® ALTER TABLE table_name SET TBLPROPERTIES (&#39;hive.stats.autogather&#39;=&#39;true&#39;);","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"æ•°ä»“","slug":"æ•°ä»“","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E4%BB%93/"},{"name":"æ•°æ®æ²»ç†","slug":"æ•°æ®æ²»ç†","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Icebergæ•°æ®æ¹–æ¢ç´¢ä¸å®è·µ","slug":"Icebergæ•°æ®æ¹–æ¢ç´¢ä¸å®è·µ","date":"2022-10-31T02:10:00.000Z","updated":"2023-05-24T17:00:20.462Z","comments":true,"path":"38dd005e/","link":"","permalink":"https://shmily-qjj.top/38dd005e/","excerpt":"","text":"Icebergæ•°æ®æ¹–æ¢ç´¢ä¸å®è·µæ¦‚å¿µå¼•å…¥â€“æ•°æ®æ¹– æ•°æ®æ¹–æ˜¯ä¸€ç§å­˜å‚¨æ•°æ®çš„æ–¹å¼,ç”¨äºç»„ç»‡ä¸åŒæ•°æ®ç»“æ„.æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ä¼ä¸šæ•°æ®æ¶æ„æ–¹æ³•,ç‰©ç†å®ç°ä¸Šåˆ™æ˜¯åŸºäºæ•°æ®å­˜å‚¨å¹³å°(ä¾‹å¦‚Hadoop,OSS,S3ç­‰å­˜å‚¨ç³»ç»Ÿ),é›†ä¸­å­˜å‚¨ä¼ä¸šå†…æµ·é‡çš„ã€å¤šæ¥æº,å¤šç§ç±»çš„æ•°æ®,å¹¶æ”¯æŒå¯¹æ•°æ®è¿›è¡Œå¿«é€ŸåŠ å·¥å’Œåˆ†æ. æ•°æ®æ¹–çš„ä¸»è¦æ€æƒ³æ˜¯å¯¹ä¼ä¸šä¸­çš„æ‰€æœ‰æ•°æ®è¿›è¡Œç»Ÿä¸€å­˜å‚¨,ä»åŸå§‹æ•°æ®è½¬æ¢ä¸ºç”¨äºæŠ¥å‘Šã€å¯è§†åŒ–ã€åˆ†æå’Œæœºå™¨å­¦ä¹ ç­‰å„ç§ä»»åŠ¡çš„ç›®æ ‡æ•°æ®. æ•°æ®æ¹–ä¸­çš„æ•°æ®åŒ…æ‹¬ç»“æ„åŒ–æ•°æ®ï¼ˆå…³ç³»æ•°æ®åº“æ•°æ®ï¼‰,åŠç»“æ„åŒ–æ•°æ®ï¼ˆCSVã€XMLã€JSONç­‰ï¼‰,éç»“æ„åŒ–æ•°æ®ï¼ˆç”µå­é‚®ä»¶,æ–‡æ¡£,PDFï¼‰å’ŒäºŒè¿›åˆ¶æ•°æ®ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰,ä»è€Œå½¢æˆä¸€ä¸ªå®¹çº³æ‰€æœ‰å½¢å¼æ•°æ®çš„é›†ä¸­å¼æ•°æ®å­˜å‚¨. Icebergç®€ä»‹Icebergæ˜¯ä¸€ç§é«˜æ€§èƒ½çš„TableFormat(è¡¨æ ¼å¼),å®šä¹‰äº†æ•°æ®ã€å…ƒæ•°æ®çš„ç»„ç»‡æ–¹å¼,æ”¯æŒåœ¨Sparkã€Trinoã€Flinkã€HiveåŠImpalaç­‰è®¡ç®—å¼•æ“ä¸­ä½¿ç”¨. Icebergç‰¹æ€§ çœŸæ­£çš„æµæ‰¹ä¸€ä½“: ä¸Šæ¸¸å†™å…¥æ•°æ®åä¸‹æ¸¸ç«‹å³å¯æŸ¥,æ»¡è¶³å®æ—¶åœºæ™¯;Icebergæä¾›äº†æµæ‰¹è¯»å–å’Œæµæ‰¹å†™å…¥æ¥å£,ç”¨æˆ·å¯ä»¥åœ¨åŒä¸€ä¸ªæµç¨‹åŒæ—¶å¤„ç†æµæ‰¹æ•°æ®,ä½¿å¾—æµæ‰¹å¤„ç†å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å­˜å‚¨æ¨¡å‹,ç®€åŒ–äº†ETLæ€è·¯. æ”¯æŒå¼‚æ„è®¡ç®—å’Œå­˜å‚¨å¼•æ“: å­˜å‚¨ä¸Šæ”¯æŒå¸¸è§å­˜å‚¨å¦‚HDFSä»¥åŠå„ç§å¯¹è±¡å­˜å‚¨(ä¸ä¸åº•å±‚å­˜å‚¨å¼ºç»‘å®š);è®¡ç®—ä¸Šæ”¯æŒFlink,Spark,Presto,Hiveç­‰å¸¸è§è®¡ç®—å¼•æ“. Schema Evolution(æ¨¡å¼æ¼”åŒ–): æ”¯æŒæ— å‰¯ä½œç”¨åœ°å¢(ADD)åˆ (Drop)æ”¹(Update)åˆ—,æ”¹å˜åˆ—é¡ºåº(Reorder)ä»¥åŠé‡å‘½ååˆ—(Rename),ä¸”ä»£ä»·å¾ˆä½(åªæ¶‰åŠå…ƒæ•°æ®æ“ä½œ,ä¸å­˜åœ¨æ•°æ®é‡æ–°è¯»å†™æ“ä½œ)(Icebergä½¿ç”¨å”¯ä¸€IDå®šä½åˆ—,æ–°å¢åˆ—ä¼šåˆ†é…æ–°çš„ID,æ‰€ä»¥åˆ—ä¸ä¼šé”™ä½) Partition Evolution(åˆ†åŒºæ¼”åŒ–): åœ¨å·²æœ‰çš„è¡¨ä¸Šæ”¹å˜åˆ†åŒºç­–ç•¥æ—¶,ä¹‹å‰çš„åˆ†åŒºæ•°æ®ä¸ä¼šå˜ä¸”ä¾ç„¶é‡‡ç”¨è€çš„åˆ†åŒºç­–ç•¥,æ–°æ•°æ®ä¼šé‡‡ç”¨æ–°çš„åˆ†åŒºç­–ç•¥.åœ¨Icebergå…ƒæ•°æ®é‡Œ,ä¸¤ä¸ªåˆ†åŒºç­–ç•¥ç›¸äº’ç‹¬ç«‹.æ¯”å¦‚ä»¥å‰æœ‰ä¸ªå¤©åˆ†åŒºè¡¨,ç°åœ¨ä¸šåŠ¡éœ€è¦å°æ—¶åˆ†åŒº,æŒ‰Hiveæ•°ä»“çš„å¤„ç†æ–¹å¼éœ€è¦é‡æ–°å»ºè¡¨,ä½†Icebergè¡¨ç›´æ¥åœ¨åŸè¡¨ä¸Šæ›´æ”¹åˆ†åŒºå¸ƒå±€å³å¯. æ”¯æŒéšè—åˆ†åŒº: Icebergçš„åˆ†åŒºä¿¡æ¯ä¸éœ€è¦äººå·¥ç»´æŠ¤,å¯ä»¥è¢«éšè—èµ·æ¥.ä¸HiveæŒ‡å®šåˆ†åŒºå­—æ®µçš„æ–¹å¼ä¸åŒ,Icebergçš„åˆ†åŒºå­—æ®µ(åˆ†åŒºç­–ç•¥)æ”¯æŒé€šè¿‡æŸå­—æ®µè®¡ç®—å‡ºæ¥,åœ¨å»ºè¡¨æˆ–è€…ä¿®æ”¹åˆ†åŒºç­–ç•¥ä¹‹å, æ–°çš„æ•°æ®ä¼šè‡ªåŠ¨è®¡ç®—æ‰€å±äºçš„åˆ†åŒº,æŸ¥è¯¢æ—¶Icebergä¼šè‡ªåŠ¨è¿‡æ»¤ä¸éœ€è¦æ‰«æçš„æ•°æ®,é¿å…äº†å› ç”¨æˆ·SQLæœªæŒ‡å®šåˆ†åŒºè¿‡æ»¤æ¡ä»¶è€Œå¯¼è‡´çš„æ€§èƒ½é—®é¢˜,è®©ç”¨æˆ·æ›´ä¸“æ³¨ä¸šåŠ¡é€»è¾‘è€Œæ— éœ€è€ƒè™‘åˆ†åŒºå­—æ®µè¿‡æ»¤é—®é¢˜.(Icebergåˆ†åŒºä¿¡æ¯å’Œæ•°æ®å­˜å‚¨ç›®å½•æ˜¯ç›¸äº’ç‹¬ç«‹å¼€çš„,ä½¿å¾—Icebergè¡¨åˆ†åŒºå¯ä»¥è¢«ä¿®æ”¹,è€Œä¸”ä¸æ¶‰åŠæ•°æ®è¿ç§»;åˆ†åŒºä¿¡æ¯ä¸å­˜åœ¨HMS,å‡è½»äº†HMSçš„å‹åŠ›) åˆ†åŒºæ¼”åŒ–å’Œéšè—åˆ†åŒºä½¿å¾—ä¸šåŠ¡å¯ä»¥æ–¹ä¾¿åœ°è°ƒæ•´åˆ†åŒºç­–ç•¥. Time Travel: å¯ä»¥æŸ¥è¯¢å†å²æŸä¸€æ—¶é—´ç‚¹snapshotçš„æ•°æ®,æ”¯æŒå›æ»šåˆ°å†å²snapshot. æ”¯æŒäº‹åŠ¡(ACID): Icebergæä¾›äº†è¾¹è¯»è¾¹å†™çš„èƒ½åŠ›,ä¸Šæ¸¸æ•°æ®å†™å…¥å³å¯è§,é€šè¿‡äº‹åŠ¡,ä¿è¯äº†ä¸‹æ¸¸ç»„ä»¶åªèƒ½æ¶ˆè´¹å·²ç»commitçš„æ•°æ®,æ— æ³•è¯»åˆ°æœªæäº¤çš„æ•°æ®.æ”¯æŒæ·»åŠ åˆ é™¤æ›´æ–°æ•°æ®. æ”¯æŒåŸºäºä¹è§‚é”çš„å¹¶å‘å†™: IcebergåŸºäºä¹è§‚é”æä¾›äº†å¤šä¸ªç¨‹åºå¹¶å‘å†™å…¥çš„èƒ½åŠ›å¹¶ä¸”ä¿è¯æ•°æ®çº¿æ€§ä¸€è‡´.(ä¹è§‚åˆ›å»ºmetadataæ–‡ä»¶,æäº¤æ›´æ–°ä¼šè§¦å‘metadataåŸå­äº¤æ¢,å®Œæˆæäº¤) æ–‡ä»¶çº§æ•°æ®å‰ªè£: Icebergé€šè¿‡å…ƒæ•°æ®æ¥å¯¹æŸ¥è¯¢è¿›è¡Œé«˜æ•ˆè¿‡æ»¤,Icebergçš„å…ƒæ•°æ®é‡Œé¢æä¾›äº†æ¯ä¸ªæ•°æ®æ–‡ä»¶çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯, æ¯”å¦‚æœ€å¤§å€¼, æœ€å°å€¼, Countè®¡æ•°ç­‰ç­‰. å› æ­¤, æŸ¥è¯¢SQLçš„è¿‡æ»¤æ¡ä»¶é™¤äº†å¸¸è§„çš„åˆ†åŒº, åˆ—è¿‡æ»¤, ç”šè‡³å¯ä»¥ä¸‹æ¨åˆ°æ–‡ä»¶çº§åˆ«, å¤§å¤§åŠ å¿«äº†æŸ¥è¯¢æ•ˆç‡. æ”¯æŒå¤šç§åº•å±‚å­˜å‚¨æ ¼å¼å¦‚Parquetã€Avroä»¥åŠORCç­‰. æ”¯æŒUpsertèƒ½åŠ›,ä¸”æ›´æ–°å³å¯è§,ä½†ä¸èƒ½è¿‡äºé¢‘ç¹,è‹¥Upsertè¿‡äºé¢‘ç¹,åˆ™éœ€è¦é¢‘ç¹æ•°æ®åˆå¹¶ IcebergåŸç†Icebergå…ƒæ•°æ®1.DataFilesæ•°æ®æ–‡ä»¶ å­˜æ”¾çœŸå®æ•°æ®æ–‡ä»¶,ç”±ä¸€ä¸ªæˆ–å¤šä¸ªManifestFileè·Ÿè¸ª 2.MetadataFileæ–‡ä»¶ â€œ*.metadata.jsonâ€æ–‡ä»¶,Icebergè¡¨æŸæ—¶åˆ»çš„çŠ¶æ€,é‡Œé¢è®°å½•äº†è¡¨Schema,åˆ†åŒºé…ç½®,è¡¨å‚æ•°,snapshotè®°å½•ä»¥åŠè¿™ä¸ªæ—¶åˆ»æ¶‰åŠåˆ°çš„æ‰€æœ‰çš„ManifestList. 3.ManifestListæ¸…å•åˆ—è¡¨ â€œsnap-*.avroâ€æ–‡ä»¶,å­˜å‚¨äº†æ„å»ºå¿«ç…§çš„æ‰€æœ‰ManifestFileåˆ—è¡¨,æ¯ä¸ªManifestFileåœ¨é‡Œé¢å ä¸€è¡Œ,æ¯è¡Œå­˜å‚¨äº†ManifestFileè·¯å¾„,åˆ†åŒºèŒƒå›´,å¢åˆ æ–‡ä»¶ä¿¡æ¯,æ¥ä¸ºæŸ¥è¯¢æ—¶æä¾›è¿‡æ»¤èƒ½åŠ›,æé«˜æ€§èƒ½.ä¸€ä¸ªå¿«ç…§å¯¹åº”ä¸€ä¸ªManifestListæ–‡ä»¶. 4.ManifestFileæ¸…å•æ–‡ä»¶ ésnapå¼€å¤´çš„avroæ ¼å¼æ–‡ä»¶,åŒ…å«äº†DataFilesåˆ—è¡¨,æ¯è¡ŒåŒ…å«ä¸€ä¸ªæ•°æ®æ–‡ä»¶çš„è¯¦ç»†æè¿°(çŠ¶æ€,è·¯å¾„,åˆ†åŒºä¿¡æ¯,åˆ—çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯,æœ€å¤§å€¼æœ€å°å€¼ç©ºå€¼ä¸ªæ•°,æ–‡ä»¶å¤§å°,è¡Œæ•°ç­‰),ä¸ºæŸ¥è¯¢æ—¶æä¾›è¿‡æ»¤èƒ½åŠ›,æé«˜æ€§èƒ½. HadoopCatalogä¸HiveCatalogè¡¨çš„ç›®å½•ç»“æ„å·®å¼‚:1.HadoopCatalogè¡¨MetadataFileå‘½åä¸º*.metadata.json,ä¸HiveCatalogè¡¨ManifestListå‘½åè§„èŒƒä¸åŒ2.HadoopCatalogè¡¨é€šè¿‡version-hint.textè®°å½•æœ€æ–°å¿«ç…§ID,HiveCatalogé€šè¿‡HiveMetaStoreè®°å½•æœ€æ–°metadata_location.3.HadoopCatalogä¸HiveCatalogè¡¨å…ƒæ•°æ®ä¸äº’é€š,æ— æ³•äº’ç›¸è½¬æ¢ HadoopCatalogè¡¨å…ƒæ•°æ®è§£æ # æŸ¥çœ‹avroæ–‡ä»¶å†…å®¹ wget https://repo1.maven.org/maven2/org/apache/avro/avro-tools/1.11.1/avro-tools-1.11.1.jar java -jar avro-tools-1.11.1.jar tojson xxx.avro MetadataFileæ–‡ä»¶ v3.metadata.json &#123; &quot;format-version&quot; : 1, &quot;table-uuid&quot; : &quot;eeffbc08-9156-4a6f-8380-6138c6b67889&quot;, &quot;location&quot; : &quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table&quot;, &quot;last-updated-ms&quot; : 1667357677633, &quot;last-column-id&quot; : 4, &quot;schema&quot; : &#123; &quot;type&quot; : &quot;struct&quot;, &quot;schema-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;id&quot; : 1, &quot;name&quot; : &quot;id&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;long&quot; &#125;, &#123; &quot;id&quot; : 2, &quot;name&quot; : &quot;name&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125;, &#123; &quot;id&quot; : 3, &quot;name&quot; : &quot;age&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;int&quot; &#125;, &#123; &quot;id&quot; : 4, &quot;name&quot; : &quot;dt&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125; ] &#125;, &quot;current-schema-id&quot; : 0, &quot;schemas&quot; : [ &#123; &quot;type&quot; : &quot;struct&quot;, &quot;schema-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;id&quot; : 1, &quot;name&quot; : &quot;id&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;long&quot; &#125;, &#123; &quot;id&quot; : 2, &quot;name&quot; : &quot;name&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125;, &#123; &quot;id&quot; : 3, &quot;name&quot; : &quot;age&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;int&quot; &#125;, &#123; &quot;id&quot; : 4, &quot;name&quot; : &quot;dt&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125; ] &#125; ], &quot;partition-spec&quot; : [ &#123; &quot;name&quot; : &quot;dt&quot;, &quot;transform&quot; : &quot;identity&quot;, &quot;source-id&quot; : 4, &quot;field-id&quot; : 1000 &#125; ], &quot;default-spec-id&quot; : 0, &quot;partition-specs&quot; : [ &#123; &quot;spec-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;name&quot; : &quot;dt&quot;, &quot;transform&quot; : &quot;identity&quot;, &quot;source-id&quot; : 4, &quot;field-id&quot; : 1000 &#125; ] &#125; ], &quot;last-partition-id&quot; : 1000, &quot;default-sort-order-id&quot; : 0, &quot;sort-orders&quot; : [ &#123; &quot;order-id&quot; : 0, &quot;fields&quot; : [ ] &#125; ], &quot;properties&quot; : &#123; &quot;EXTERNAL&quot; : &quot;TRUE&quot;, &quot;write.metadata.previous-versions-max&quot; : &quot;5&quot;, &quot;bucketing_version&quot; : &quot;2&quot;, &quot;write.metadata.delete-after-commit.enabled&quot; : &quot;true&quot;, &quot;write.distribution-mode&quot; : &quot;hash&quot;, &quot;storage_handler&quot; : &quot;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&quot; &#125;, &quot;current-snapshot-id&quot; : 1244418053907939374, &quot;refs&quot; : &#123; &quot;main&quot; : &#123; &quot;snapshot-id&quot; : 1244418053907939374, &quot;type&quot; : &quot;branch&quot; &#125; &#125;, &quot;snapshots&quot; : [ &#123; &quot;snapshot-id&quot; : 7688152750730458585, &quot;timestamp-ms&quot; : 1667357628763, &quot;summary&quot; : &#123; &quot;operation&quot; : &quot;append&quot;, &quot;added-data-files&quot; : &quot;1&quot;, &quot;added-records&quot; : &quot;2&quot;, &quot;added-files-size&quot; : &quot;1272&quot;, &quot;changed-partition-count&quot; : &quot;1&quot;, &quot;total-records&quot; : &quot;2&quot;, &quot;total-files-size&quot; : &quot;1272&quot;, &quot;total-data-files&quot; : &quot;1&quot;, &quot;total-delete-files&quot; : &quot;0&quot;, &quot;total-position-deletes&quot; : &quot;0&quot;, &quot;total-equality-deletes&quot; : &quot;0&quot; &#125;, &quot;manifest-list&quot; : &quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/metadata/snap-7688152750730458585-1-f0e6c6ca-51a7-42e6-b412-4036e27c7d98.avro&quot;, &quot;schema-id&quot; : 0 &#125;, &#123; &quot;snapshot-id&quot; : 1244418053907939374, &quot;parent-snapshot-id&quot; : 7688152750730458585, &quot;timestamp-ms&quot; : 1667357677633, &quot;summary&quot; : &#123; &quot;operation&quot; : &quot;append&quot;, &quot;added-data-files&quot; : &quot;1&quot;, &quot;added-records&quot; : &quot;1&quot;, &quot;added-files-size&quot; : &quot;1166&quot;, &quot;changed-partition-count&quot; : &quot;1&quot;, &quot;total-records&quot; : &quot;3&quot;, &quot;total-files-size&quot; : &quot;2438&quot;, &quot;total-data-files&quot; : &quot;2&quot;, &quot;total-delete-files&quot; : &quot;0&quot;, &quot;total-position-deletes&quot; : &quot;0&quot;, &quot;total-equality-deletes&quot; : &quot;0&quot; &#125;, &quot;manifest-list&quot; : &quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/metadata/snap-1244418053907939374-1-44671db7-02ca-47c1-a229-c7f62d8aa12f.avro&quot;, &quot;schema-id&quot; : 0 &#125; ], &quot;snapshot-log&quot; : [ &#123; &quot;timestamp-ms&quot; : 1667357628763, &quot;snapshot-id&quot; : 7688152750730458585 &#125;, &#123; &quot;timestamp-ms&quot; : 1667357677633, &quot;snapshot-id&quot; : 1244418053907939374 &#125; ], &quot;metadata-log&quot; : [ &#123; &quot;timestamp-ms&quot; : 1667357528190, &quot;metadata-file&quot; : &quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/metadata/v1.metadata.json&quot; &#125;, &#123; &quot;timestamp-ms&quot; : 1667357628763, &quot;metadata-file&quot; : &quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/metadata/v2.metadata.json&quot; &#125; ] &#125; ManifestListæ¸…å•åˆ—è¡¨æ–‡ä»¶ snap-7688152750730458585-1-f0e6c6ca-51a7-42e6-b412-4036e27c7d98.avro &#123;&quot;manifest_path&quot;:&quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/metadata/f0e6c6ca-51a7-42e6-b412-4036e27c7d98-m0.avro&quot;,&quot;manifest_length&quot;:6189,&quot;partition_spec_id&quot;:0,&quot;added_snapshot_id&quot;:&#123;&quot;long&quot;:7688152750730458585&#125;,&quot;added_data_files_count&quot;:&#123;&quot;int&quot;:1&#125;,&quot;existing_data_files_count&quot;:&#123;&quot;int&quot;:0&#125;,&quot;deleted_data_files_count&quot;:&#123;&quot;int&quot;:0&#125;,&quot;partitions&quot;:&#123;&quot;array&quot;:[&#123;&quot;contains_null&quot;:false,&quot;contains_nan&quot;:&#123;&quot;boolean&quot;:false&#125;,&quot;lower_bound&quot;:&#123;&quot;bytes&quot;:&quot;20221011&quot;&#125;,&quot;upper_bound&quot;:&#123;&quot;bytes&quot;:&quot;20221011&quot;&#125;&#125;]&#125;,&quot;added_rows_count&quot;:&#123;&quot;long&quot;:2&#125;,&quot;existing_rows_count&quot;:&#123;&quot;long&quot;:0&#125;,&quot;deleted_rows_count&quot;:&#123;&quot;long&quot;:0&#125;&#125; ManifestFileæ¸…å•æ–‡ä»¶ f0e6c6ca-51a7-42e6-b412-4036e27c7d98-m0.avro &#123;&quot;status&quot;:1,&quot;snapshot_id&quot;:&#123;&quot;long&quot;:7688152750730458585&#125;,&quot;data_file&quot;:&#123;&quot;file_path&quot;:&quot;hdfs://shmily:8020/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table/data/dt=20221011/00000-0-hive_20221102105315_5bb17fc0-3092-4bed-8839-253f19117b6d-job_1667357081446_0001-00001.parquet&quot;,&quot;file_format&quot;:&quot;PARQUET&quot;,&quot;partition&quot;:&#123;&quot;dt&quot;:&#123;&quot;string&quot;:&quot;20221011&quot;&#125;&#125;,&quot;record_count&quot;:2,&quot;file_size_in_bytes&quot;:1272,&quot;block_size_in_bytes&quot;:67108864,&quot;column_sizes&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:55&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:59&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:93&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:101&#125;]&#125;,&quot;value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:2&#125;]&#125;,&quot;null_value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:0&#125;]&#125;,&quot;nan_value_counts&quot;:&#123;&quot;array&quot;:[]&#125;,&quot;lower_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;abc&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221011&quot;&#125;]&#125;,&quot;upper_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0002\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;qjj&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221011&quot;&#125;]&#125;,&quot;key_metadata&quot;:null,&quot;split_offsets&quot;:&#123;&quot;array&quot;:[4]&#125;,&quot;sort_order_id&quot;:&#123;&quot;int&quot;:0&#125;&#125;&#125; HiveCatalogè¡¨å…ƒæ•°æ®è§£æMetadataFileæ–‡ä»¶ 00001-66c5832f-9d6d-4674-9a52-2aa6b8e29991.metadata.json &#123; &quot;format-version&quot; : 1, &quot;table-uuid&quot; : &quot;5397c8ee-2b24-4eea-83ae-55e024ccd2c0&quot;, &quot;location&quot; : &quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table&quot;, &quot;last-updated-ms&quot; : 1665470339331, &quot;last-column-id&quot; : 4, &quot;schema&quot; : &#123; &quot;type&quot; : &quot;struct&quot;, &quot;schema-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;id&quot; : 1, &quot;name&quot; : &quot;id&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;long&quot; &#125;, &#123; &quot;id&quot; : 2, &quot;name&quot; : &quot;name&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125;, &#123; &quot;id&quot; : 3, &quot;name&quot; : &quot;age&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;int&quot; &#125;, &#123; &quot;id&quot; : 4, &quot;name&quot; : &quot;dt&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125; ] &#125;, &quot;current-schema-id&quot; : 0, &quot;schemas&quot; : [ &#123; &quot;type&quot; : &quot;struct&quot;, &quot;schema-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;id&quot; : 1, &quot;name&quot; : &quot;id&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;long&quot; &#125;, &#123; &quot;id&quot; : 2, &quot;name&quot; : &quot;name&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125;, &#123; &quot;id&quot; : 3, &quot;name&quot; : &quot;age&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;int&quot; &#125;, &#123; &quot;id&quot; : 4, &quot;name&quot; : &quot;dt&quot;, &quot;required&quot; : false, &quot;type&quot; : &quot;string&quot; &#125; ] &#125; ], &quot;partition-spec&quot; : [ &#123; &quot;name&quot; : &quot;dt&quot;, &quot;transform&quot; : &quot;identity&quot;, &quot;source-id&quot; : 4, &quot;field-id&quot; : 1000 &#125; ], &quot;default-spec-id&quot; : 0, &quot;partition-specs&quot; : [ &#123; &quot;spec-id&quot; : 0, &quot;fields&quot; : [ &#123; &quot;name&quot; : &quot;dt&quot;, &quot;transform&quot; : &quot;identity&quot;, &quot;source-id&quot; : 4, &quot;field-id&quot; : 1000 &#125; ] &#125; ], &quot;last-partition-id&quot; : 1000, &quot;default-sort-order-id&quot; : 0, &quot;sort-orders&quot; : [ &#123; &quot;order-id&quot; : 0, &quot;fields&quot; : [ ] &#125; ], &quot;properties&quot; : &#123; &quot;engine.hive.enabled&quot; : &quot;true&quot;, &quot;write.metadata.previous-versions-max&quot; : &quot;5&quot;, &quot;bucketing_version&quot; : &quot;2&quot;, &quot;write.metadata.delete-after-commit.enabled&quot; : &quot;true&quot;, &quot;write.distribution-mode&quot; : &quot;hash&quot;, &quot;storage_handler&quot; : &quot;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&quot; &#125;, &quot;current-snapshot-id&quot; : 6283861985931247372, &quot;refs&quot; : &#123; &quot;main&quot; : &#123; &quot;snapshot-id&quot; : 6283861985931247372, &quot;type&quot; : &quot;branch&quot; &#125; &#125;, &quot;snapshots&quot; : [ &#123; &quot;snapshot-id&quot; : 6283861985931247372, &quot;timestamp-ms&quot; : 1665470339331, &quot;summary&quot; : &#123; &quot;operation&quot; : &quot;append&quot;, &quot;added-data-files&quot; : &quot;2&quot;, &quot;added-records&quot; : &quot;3&quot;, &quot;added-files-size&quot; : &quot;2438&quot;, &quot;changed-partition-count&quot; : &quot;2&quot;, &quot;total-records&quot; : &quot;3&quot;, &quot;total-files-size&quot; : &quot;2438&quot;, &quot;total-data-files&quot; : &quot;2&quot;, &quot;total-delete-files&quot; : &quot;0&quot;, &quot;total-position-deletes&quot; : &quot;0&quot;, &quot;total-equality-deletes&quot; : &quot;0&quot; &#125;, &quot;manifest-list&quot; : &quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table/metadata/snap-6283861985931247372-1-825f6beb-3be7-485c-b338-8dec6068be94.avro&quot;, &quot;schema-id&quot; : 0 &#125; ], &quot;snapshot-log&quot; : [ &#123; &quot;timestamp-ms&quot; : 1665470339331, &quot;snapshot-id&quot; : 6283861985931247372 &#125; ], &quot;metadata-log&quot; : [ &#123; &quot;timestamp-ms&quot; : 1665470256700, &quot;metadata-file&quot; : &quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table/metadata/00000-d4b0dc94-f59f-4968-950c-31d22c2aab0d.metadata.json&quot; &#125; ] &#125; ManifestListæ¸…å•åˆ—è¡¨æ–‡ä»¶ snap-6283861985931247372-1-825f6beb-3be7-485c-b338-8dec6068be94.avro &#123;&quot;manifest_path&quot;:&quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table/metadata/825f6beb-3be7-485c-b338-8dec6068be94-m0.avro&quot;,&quot;manifest_length&quot;:6234,&quot;partition_spec_id&quot;:0,&quot;added_snapshot_id&quot;:&#123;&quot;long&quot;:6283861985931247372&#125;,&quot;added_data_files_count&quot;:&#123;&quot;int&quot;:2&#125;,&quot;existing_data_files_count&quot;:&#123;&quot;int&quot;:0&#125;,&quot;deleted_data_files_count&quot;:&#123;&quot;int&quot;:0&#125;,&quot;partitions&quot;:&#123;&quot;array&quot;:[&#123;&quot;contains_null&quot;:false,&quot;contains_nan&quot;:&#123;&quot;boolean&quot;:false&#125;,&quot;lower_bound&quot;:&#123;&quot;bytes&quot;:&quot;20221010&quot;&#125;,&quot;upper_bound&quot;:&#123;&quot;bytes&quot;:&quot;20221011&quot;&#125;&#125;]&#125;,&quot;added_rows_count&quot;:&#123;&quot;long&quot;:3&#125;,&quot;existing_rows_count&quot;:&#123;&quot;long&quot;:0&#125;,&quot;deleted_rows_count&quot;:&#123;&quot;long&quot;:0&#125;&#125; ManifestFileæ¸…å•æ–‡ä»¶ 825f6beb-3be7-485c-b338-8dec6068be94-m0.avro &#123;&quot;status&quot;:1,&quot;snapshot_id&quot;:&#123;&quot;long&quot;:6283861985931247372&#125;,&quot;data_file&quot;:&#123;&quot;file_path&quot;:&quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table/data/dt=20221011/00000-0-shmily_20221011143858_89f7e99f-7227-4b19-9a44-b6807cf3b718-job_local1764035342_0002-00001.parquet&quot;,&quot;file_format&quot;:&quot;PARQUET&quot;,&quot;partition&quot;:&#123;&quot;dt&quot;:&#123;&quot;string&quot;:&quot;20221011&quot;&#125;&#125;,&quot;record_count&quot;:2,&quot;file_size_in_bytes&quot;:1272,&quot;block_size_in_bytes&quot;:67108864,&quot;column_sizes&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:55&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:59&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:93&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:101&#125;]&#125;,&quot;value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:2&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:2&#125;]&#125;,&quot;null_value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:0&#125;]&#125;,&quot;nan_value_counts&quot;:&#123;&quot;array&quot;:[]&#125;,&quot;lower_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;abc&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221011&quot;&#125;]&#125;,&quot;upper_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0002\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;qjj&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221011&quot;&#125;]&#125;,&quot;key_metadata&quot;:null,&quot;split_offsets&quot;:&#123;&quot;array&quot;:[4]&#125;,&quot;sort_order_id&quot;:&#123;&quot;int&quot;:0&#125;&#125;&#125; &#123;&quot;status&quot;:1,&quot;snapshot_id&quot;:&#123;&quot;long&quot;:6283861985931247372&#125;,&quot;data_file&quot;:&#123;&quot;file_path&quot;:&quot;hdfs://shmily:8020/user/hive/warehouse/iceberg_db.db/hive_iceberg_partitioned_table/data/dt=20221010/00000-0-shmily_20221011143858_89f7e99f-7227-4b19-9a44-b6807cf3b718-job_local1764035342_0002-00002.parquet&quot;,&quot;file_format&quot;:&quot;PARQUET&quot;,&quot;partition&quot;:&#123;&quot;dt&quot;:&#123;&quot;string&quot;:&quot;20221010&quot;&#125;&#125;,&quot;record_count&quot;:1,&quot;file_size_in_bytes&quot;:1166,&quot;block_size_in_bytes&quot;:67108864,&quot;column_sizes&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:51&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:53&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:51&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:59&#125;]&#125;,&quot;value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:1&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:1&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:1&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:1&#125;]&#125;,&quot;null_value_counts&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:0&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:0&#125;]&#125;,&quot;nan_value_counts&quot;:&#123;&quot;array&quot;:[]&#125;,&quot;lower_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0003\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;abc&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221010&quot;&#125;]&#125;,&quot;upper_bounds&quot;:&#123;&quot;array&quot;:[&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;\\u0003\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:2,&quot;value&quot;:&quot;abc&quot;&#125;,&#123;&quot;key&quot;:3,&quot;value&quot;:&quot;\\u0018\\u0000\\u0000\\u0000&quot;&#125;,&#123;&quot;key&quot;:4,&quot;value&quot;:&quot;20221010&quot;&#125;]&#125;,&quot;key_metadata&quot;:null,&quot;split_offsets&quot;:&#123;&quot;array&quot;:[4]&#125;,&quot;sort_order_id&quot;:&#123;&quot;int&quot;:0&#125;&#125;&#125; Icebergè¡¨ç±»å‹å½“Icebergæ·»åŠ äº†æ–°ç‰¹æ€§ä½†è¯¥æ–°ç‰¹æ€§ç ´åäº†å‘å‰å…¼å®¹æ€§æ—¶,è¡¨çš„versionä¼šå¢åŠ ,ä»¥ä¿è¯æ—§çš„è¡¨ç‰ˆæœ¬ä»ç„¶å¯ä»¥å…¼å®¹.Icebergå½“å‰æœ‰V1å’ŒV2ä¸¤ç§è¡¨ç±»å‹,å»ºè¡¨æ—¶ç”±property-versionæŒ‡å®š.Version 1: Analytic Data Tables ğŸ”— åŸºäºä¸å¯å˜æ–‡ä»¶æ ¼å¼ç®¡ç†çš„å¤§å‹åˆ†æè¡¨V1è¡¨å¯ä»¥æŒ‰åˆ†åŒºåˆ é™¤æ•°æ®(å¦‚åœ¨Trinoä¸­delete from iceberg_table where ds=â€™2022120114â€™;),åˆ é™¤å¹¶ä¸ä¼šçœŸæ­£åˆ é™¤æ•°æ®,è€Œæ˜¯commitæ–°çš„å…ƒæ•°æ®æ–°çš„å¿«ç…§,åªè¦æ—§å¿«ç…§æœªè¿‡æœŸ,ä»ç„¶å¯ä»¥å›æ»šåˆ°åˆ é™¤å‰çš„çŠ¶æ€,ä½†ä¸€æ—¦å¿«ç…§è¿‡æœŸ,æ•°æ®æ–‡ä»¶ä¼šè¢«åˆ é™¤æ— æ³•è¿˜åŸ;V1è¡¨ä¸æ”¯æŒè¡Œçº§åˆ é™¤(ä¼šæŠ¥é”™failed: Iceberg table updates require at least format version 2) Version 2: Row-level Deletes ğŸ”— è¾ƒVersion 1æ·»åŠ äº†è¡Œçº§æ›´æ–°\\åˆ é™¤èƒ½åŠ›;æ·»åŠ äº†Delete filesä»¥å¯¹ç°æœ‰æ•°æ®æ–‡ä»¶ä¸­åˆ é™¤çš„è¡Œè¿›è¡Œç¼–ç ã€‚Version2å¯å®ç°åˆ é™¤æˆ–æ›¿æ¢ä¸å¯å˜æ•°æ®æ–‡ä»¶ä¸­çš„å•ä¸ªè¡Œï¼Œè€Œæ— éœ€é‡å†™æ–‡ä»¶ã€‚ Icebergè¡¨æ•°æ®ç±»å‹ æ•°æ®ç±»å‹ ä»‹ç» è¦æ±‚ int 32ä½æœ‰ç¬¦å·æ•´å½¢ å¯è½¬ä¸ºlong long 64ä½æœ‰ç¬¦å·æ•´å½¢ float å•ç²¾åº¦æµ®ç‚¹å‹ å¯è½¬ä¸ºdouble double åŒç²¾åº¦æµ®ç‚¹å‹ decimal(P,S) å›ºå®šå°æ•°ç‚¹ç±»å‹æ•°å€¼ ç²¾åº¦P,å†³å®šæ€»ä½æ•°;æ¯”ä¾‹S,å†³å®šå°æ•°ä½æ•°;På¿…é¡»å°äºç­‰äº38 date æ—¥æœŸ,ä¸å«æ—¶é—´å’Œæ—¶åŒº time æ—¶é—´,ä¸å«æ—¥æœŸå’Œæ—¶åŒº ä»¥å¾®å¦™å­˜å‚¨ timestamp ä¸å«æ—¶åŒºçš„æ—¶é—´æˆ³ ä»¥å¾®å¦™å­˜å‚¨ timestamptz å«æ—¶åŒºçš„æ—¶é—´æˆ³ ä»¥å¾®å¦™å­˜å‚¨ string å­—ç¬¦ä¸²,ä»»æ„é•¿åº¦ Encoded with UTF-8 fixed(L) å›ºå®šé•¿åº¦ä¸ºLçš„å­—èŠ‚æ•°ç»„ binary ä»»æ„é•¿åº¦å­—èŠ‚æ•°ç»„ struct&lt;â€¦&gt; ä»»æ„æ•°æ®ç±»å‹ç»„æˆçš„ç»“æ„ä½“ list ä»»æ„æ•°æ®ç±»å‹ç»„æˆçš„List map&lt;K,V&gt; ä»»æ„æ•°æ®ç±»å‹ç»„æˆçš„é”®å€¼å¯¹ è¡Œå­˜å‚¨ç±»å‹,å­˜å‚¨å’Œæ£€ç´¢æ—¶æ‰«ææ•°æ®é‡è¾ƒå¤§ Icebergé›†æˆIcebergä¸Hiveé›†æˆæ·»åŠ iceberg-hive-runtime-0.14.1.jar,libfb303-0.9.3.jarä¸¤ä¸ªjaråˆ°$HIVE_HOME/auxlibä¸‹æ·»åŠ iceberg.engine.hive.enabled=trueå‚æ•°åˆ°hive-site.xml Hiveåˆ›å»ºIcebergè¡¨(Hiveæ“ä½œIcebergæ”¯æŒå¤šç§Catalogï¼Œæ”¯æŒHadoopã€Hive(é»˜è®¤)ã€location_based_tableã€Customå‡ ç§ç®¡ç†æ–¹å¼,å…¶ä¸­å‰ä¸‰ç§æ˜¯å¼€ç®±å³ç”¨çš„) 1.HiveCatalogç±»å‹(è¡¨å…ƒæ•°æ®ä¿¡æ¯ä½¿ç”¨HiveMetaStoreæ¥ç®¡ç†ï¼Œä¾èµ–Hive): -- ä¸è®¾ç½®Catalogç±»å‹æ—¶é»˜è®¤ä¼šä½¿ç”¨HiveCatalogç±»å‹çš„Icebergè¡¨ -- ç¤ºä¾‹1 éåˆ†åŒºè¡¨ CREATE TABLE iceberg_db.hive_iceberg_table ( id BIGINT, name STRING ) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; LOCATION &#39;/user/hive/warehouse/iceberg_db.db/hive_iceberg_table&#39; TBLPROPERTIES ( &#39;write.distribution-mode&#39;=&#39;hash&#39;, -- æ•°æ®å†™å…¥å‚æ•°,è®¾ç½®ä¸ºhashè¡¨ç¤ºæŒ‰keyå“ˆå¸Œ,æ¯ä¸€ä¸ªPartitionæ•°æ®æœ€å¤šç”±ä¸€ä¸ªTaskæ¥å†™å…¥ï¼Œå‡å°‘å°æ–‡ä»¶ &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, -- (æ¯æ¬¡æäº¤åæ˜¯å¦åˆ é™¤æ—§å…ƒæ•°æ®æ–‡ä»¶) è‡ªåŠ¨æ¸…ç†æ—§å…ƒæ•°æ® metadata.json ä¸èƒ½æ¸…ç†manifestå’Œsnapshotçš„avroæ–‡ä»¶ &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39; -- ä¿ç•™çš„metadata.jsonæ•°é‡ ); -- ç¤ºä¾‹2 åˆ†åŒºè¡¨ CREATE TABLE iceberg_db.hive_iceberg_partitioned_table ( id BIGINT, name STRING, age int ) partitioned by (dt string) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; TBLPROPERTIES ( &#39;write.distribution-mode&#39;=&#39;hash&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39;, &#39;format-version&#39;=&#39;2&#39;, &#39;engine.hive.enabled&#39;=&#39;true&#39;, &#39;write.target-file-size-bytes&#39;=&#39;268435456&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.parquet.compression-codec&#39;=&#39;zstd&#39;, &#39;write.parquet.compression-level&#39;=&#39;10&#39;, &#39;write.avro.compression-codec&#39;=&#39;zstd&#39;, &#39;write.avro.compression-level&#39;=&#39;10&#39; ); -- ç¤ºä¾‹3 æ‰‹åŠ¨æŒ‡å®šcatalogåç§°,æŒ‡å®šcatalogç±»å‹ä¸ºHiveCatalogç±»å‹å¹¶å»ºè¡¨: set iceberg.catalog.&lt;catalog_name&gt;.type=hive; -- è®¾ç½®catalogç±»å‹ CREATE TABLE iceberg_db.hive_iceberg_partitioned_table ( id BIGINT, name STRING, age int ) partitioned by (dt string) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; TBLPROPERTIES ( &#39;iceberg.catalog&#39;=&#39;&lt;catalog_name&gt;&#39;, &#39;write.distribution-mode&#39;=&#39;hash&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39; ); HiveCatalogè¡¨åœ¨HMSä¸­ä¿å­˜äº†å¾ˆå¤šTable Parametersä¿¡æ¯,å¦‚current-schema,current-snapshot-xx,default-partition-spec,metadata_location,previous_metadata_location,snapshot-countç­‰ä¿¡æ¯. HiveCatalogè¡¨åœ¨Hiveä¸‹å­˜åœ¨çš„é—®é¢˜: åœ¨Kerberosè®¤è¯çš„HMSç¯å¢ƒä¸‹,Hiveå®¢æˆ·ç«¯å¯ä»¥å»ºè¡¨å’ŒæŸ¥è¯¢,ä½†æ— æ³•inssertæ•°æ®;å¯ä»¥ä½¿ç”¨beeline+hiveserver2è¿›è¡ŒIcebergè¡¨çš„insertæ“ä½œ.é€‚ç”¨åœºæ™¯: HiveCatalogåœ¨å…¼å®¹æ€§æ–¹é¢æœ‰å¤©ç„¶çš„ä¼˜åŠ¿,å‡ ä¹å¤§éƒ¨åˆ†å¸¸è§è®¡ç®—å¼•æ“éƒ½æ”¯æŒHiveCatalog,è€Œå…¶ä»–ç±»å‹çš„Catalogåˆ™æœ‰ä¸è¢«è®¡ç®—å¼•æ“æ”¯æŒçš„å¯èƒ½.å°¤å…¶æ˜¯å¦‚æœä½¿ç”¨Icebergè‡ªå®šä¹‰Catalog,åˆ™éœ€è¦ä¸ºæ¯ä¸ªè¯•ç”¨Icebergçš„å¼•æ“åšä¸€å®šçš„å¼€å‘å·¥ä½œä»¥å…¼å®¹è‡ªå®šä¹‰Catalog. 2.HadoopCatalogç±»å‹(å…ƒæ•°æ®ä¿¡æ¯ä½¿ç”¨åº•å±‚å¤–éƒ¨å­˜å‚¨æ¥ç®¡ç†) set iceberg.catalog.&lt;catalog_name&gt;.type=hadoop; -- å¿…é¡»æ¯æ¬¡è®¾ç½®catalogç±»å‹ set iceberg.catalog.&lt;catalog_name&gt;.warehouse=hdfs://nameservice/user/iceberg/warehouse; -- å¿…é¡»æ¯æ¬¡è®¾ç½®warehouseå­˜å‚¨è·¯å¾„ create external table iceberg_db.hadoop_iceberg_partitioned_table ( id BIGINT, name STRING, age int ) partitioned by (dt string) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; LOCATION &#39;hdfs://nameservice/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_partitioned_table&#39; -- è·¯å¾„å¿…é¡»æ˜¯$&#123;iceberg.catalog.&lt;catalog_name&gt;.warehous&#125;/$&#123;db_name&#125;/$&#123;table_name&#125; tblproperties ( &#39;iceberg.catalog&#39;=&#39;&lt;catalog_name&gt;&#39;, &#39;write.distribution-mode&#39;=&#39;hash&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39; ); 3.LocationBasedTable(å¤–éƒ¨å­˜å‚¨ä¸­å·²ç»å­˜åœ¨HadoopCatalogç±»å‹Icebergè¡¨çš„æ•°æ®,å°†å…¶æ˜ å°„åˆ°Hiveè¡¨)HDFSå·²ç»å­˜åœ¨äº†Icebergæ ¼å¼è¡¨çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®štblproperties(â€˜iceberg.catalogâ€™=â€™location_based_tableâ€™)å’ŒLOCATION,å®ƒä¼šå»æŒ‡å®šçš„LOCATIONè·¯å¾„ä¸‹åŠ è½½icebergè¡¨æ•°æ®.å‰æLOCATIONä¸‹å·²ç»å­˜åœ¨Icebergæ ¼å¼è¡¨æ•°æ®äº†.å»ºè¡¨æ—¶ä¸éœ€è¦åŠ PARTITION BY,åªéœ€è¦åŠ å­—æ®µå³å¯. create external table iceberg_db.location_iceberg_partitioned_table ( id BIGINT, name STRING, age INT, dt STRING ) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; LOCATION &#39;hdfs://nameservice/user/iceberg/warehouse/iceberg_db/location_iceberg_partitioned_table&#39; tblproperties (&#39;iceberg.catalog&#39;=&#39;location_based_table&#39;); æˆ– create table iceberg_db.location_iceberg_partitioned_table ( id BIGINT, name STRING, age INT, dt STRING ) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; LOCATION &#39;hdfs://nameservice/user/iceberg/warehouse/iceberg_db/location_iceberg_partitioned_table&#39; tblproperties (&#39;iceberg.catalog&#39;=&#39;location_based_table&#39;); æ¨èåœºæ™¯: å¤–éƒ¨è®¡ç®—å¼•æ“å‡æ”¯æŒHadoopCatalogç±»å‹Icebergè¡¨çš„æƒ…å†µä¸‹,æ¯”å¦‚Flinkã€Sparkç­‰å¼•æ“å†™å…¥çš„æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§æ–¹å¼åˆ›å»ºHiveè¡¨æ¥æ‰“é€šHive.ä¸æ¨èåœºæ™¯: éœ€è¦ä½¿ç”¨Trinoåˆ†æè¯¥è¡¨.(å› ä¸ºTrinoå½“å‰ä¸æ”¯æŒHadoopCatalogç±»å‹Icebergè¡¨)æ³¨æ„: å¤–éƒ¨å­˜å‚¨ä¸Šçš„Icebergè¡¨,Catalogå¿…é¡»æ˜¯HadoopCatalogç±»å‹çš„ï¼Œå¦åˆ™æ— æ³•è¯»å–æ•°æ®ã€‚å¦‚æœæ˜¯å…¶ä»–Catalogç±»å‹,è¡¨åˆ›å»ºæ—¶ä¼šæŠ¥é”™File does not exist: /table_pathâ€¦/metadata/version-hint.textï¼Œè¡¨èƒ½åˆ›å»ºæˆåŠŸï¼Œä½†æŸ¥è¯¢ç»“æœä¸ºç©ºã€‚ 4.CustomCatalogè‡ªå®šä¹‰Catalog,é€šè¿‡Icebergæä¾›çš„APIå®šåˆ¶Catalog,ä½¿Icebergèƒ½æ›´åŠ çµæ´»åœ°ä½¿ç”¨å„ç±»å…ƒæ•°æ®ç®¡ç†æ–¹æ¡ˆ.é€‚ç”¨åœºæ™¯: éœ€è¦ä¸Hive Hadoopç­‰è§£è€¦çš„åœºæ™¯,ä»¥åŠéœ€è¦çµæ´»ç®¡ç†å…ƒæ•°æ®çš„åœºæ™¯.åœ¨å…ƒæ•°æ®ç®¡ç†å’Œå…¼å®¹è®¡ç®—å¼•æ“æ–¹é¢éœ€è¦ä¸€å®šçš„å¼€å‘å·¥ä½œé‡. Icebergä¸Flinké›†æˆFlink 1.14åˆ™ä¸‹è½½iceberg-flink-runtime-1.14-0.14.1.jar æ”¾å…¥$FLINK_HOME/libç›®å½•ä¸‹ Flink DataStreamAPIé›†æˆIcebergå†™äº†å‡ ä¸ªæ¡ˆä¾‹:Kafkaæ•°æ®é€šè¿‡Flink Datastream APIå†™å…¥Iceberg:KafkaSinkHadoopCatalogIcebergTableKafkaSinkHiveCatalogIcebergTableé€šè¿‡Flink Datastream APIè¯»å–Iceberg:HadoopCatalogIcebergTableSourceHiveCatalogIcebergTableSource Flink SQLé›†æˆIceberg æ‰“é€šKafka-&gt;Flink SQL-&gt;HadoopCatalogç±»å‹Icebergè¡¨-&gt;Hive -- å¯åŠ¨flinké›†ç¾¤ï¼šcd $FLINK_HOME ; bin/start-cluster.sh -- å¯åŠ¨FlinkSQL Consoleï¼šbin/sql-client.sh embedded shell set execution.checkpointing.interval=10sec; -- å¿…é¡»è®¾ç½®checkpoint é checkpointæäº¤æ›´æ–°æ•°æ®åˆ°Iceberg SET execution.runtime-mode = streaming; -- æµå¼å†™ CREATE TABLE t_kafka_source ( id BIGINT, name STRING, age INT, dt STRING ) WITH ( &#39;connector&#39; = &#39;kafka&#39;, &#39;topic&#39; = &#39;flink_topic1&#39;, &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, &#39;properties.bootstrap.servers&#39; = &#39;cdh101:9092,cdh102:9092,cdh103:9092,cdh104:9092&#39;, &#39;properties.group.id&#39; = &#39;test&#39;, &#39;format&#39; = &#39;csv&#39; ); -- 1.å†™å…¥Icebergè¡¨[HadoopCatalogç±»å‹] CREATE CATALOG hadoop_iceberg_catalog WITH ( &#39;type&#39;=&#39;iceberg&#39;, -- åˆ›å»ºHadoopCatalogç±»å‹Icebergè¡¨åœ¨FlinkSQLä¸­çš„Catalog &#39;catalog-type&#39;=&#39;hadoop&#39;, &#39;warehouse&#39;=&#39;hdfs://nameservice/user/iceberg/warehouse&#39;, &#39;property-version&#39;=&#39;1&#39; ); CREATE TABLE if not exists `hadoop_iceberg_catalog`.`iceberg_db`.`hadoop_iceberg_table_flink_sql` ( id BIGINT, name STRING, age INT, dt STRING ) PARTITIONED BY (dt) WITH(&#39;type&#39;=&#39;ICEBERG&#39;, &#39;engine.hive.enabled&#39;=&#39;true&#39;, -- æ”¯æŒhiveæŸ¥è¯¢(å®æµ‹å‘ç°ä¸åŠ ä¹Ÿæ²¡å½±å“) &#39;read.split.target-size&#39;=&#39;1073741824&#39;, -- å‡å°‘splitæ•°æå‡æŸ¥è¯¢æ•ˆç‡ &#39;write.target-file-size-bytes&#39;=&#39;134217728&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;9&#39;, &#39;write.distribution-mode&#39;=&#39;hash&#39;); insert into hadoop_iceberg_catalog.iceberg_db.hadoop_iceberg_table_flink_sql select id,name,age,dt from t_kafka_source; -- 2.FlinkSQLæ‰¹å¼æŸ¥è¯¢ SET execution.runtime-mode = batch; select id,name,age,dt from `hadoop_iceberg_catalog`.`iceberg_db`.`hadoop_iceberg_table_flink_sql`; -- 3.FlinkSQLæµå¼æŸ¥è¯¢ ---- æ³¨:ä¸æŒ‡å®šstart-snapshot-idåˆ™ä¼šé€æ¸å›æº¯å…¨é‡æ•°æ® ---- æŒ‡å®šäº†start-snapshot-idå,ä¼šä»è¯¥snapshotçš„æ•°æ®å¼€å§‹æ¶ˆè´¹ ---- é‡å¯Flinkåº”ç”¨æ—¶,è‹¥ä¸æŒ‡å®šä¸Šæ¬¡å…³é—­æ—¶çš„checkpointæˆ–savepoint,åˆ™æ¯æ¬¡é‡å¯Flinkåº”ç”¨éƒ½ä¼šä»start-snapshot-idæŒ‡å®šsnapshotå¼€å§‹æ¶ˆè´¹,å¯¼è‡´é‡å¤æ¶ˆè´¹å†å²æ•°æ® ---- é‡å¯Flinkåº”ç”¨æ—¶,è‹¥æŒ‡å®šäº†ä¸Šæ¬¡å…³é—­æ—¶çš„checkpointæˆ–savepoint,åˆ™ä¼šä»ä¸Šæ¬¡æ¶ˆè´¹çš„ä½ç‚¹ç»§ç»­æ¶ˆè´¹ select id,name,age,dt from `hadoop_iceberg_catalog`.`iceberg_db`.`hadoop_iceberg_table_flink_sql` /*+ OPTIONS(&#39;streaming&#39;=&#39;true&#39;, &#39;monitor-interval&#39;=&#39;5s&#39;, &#39;start-snapshot-id&#39;=&#39;3821550127947089987&#39;)*/ ; -- 4.åœ¨Hiveä¸­åˆ›å»ºIcebergæ˜ å°„è¡¨[åªé’ˆå¯¹HadoopCatalogç±»å‹è¡¨] create external table iceberg_db.hadoop_iceberg_table_flink_sql ( id BIGINT, name STRING, age INT, dt STRING ) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39; LOCATION &#39;hdfs://nameservice/user/iceberg/warehouse/iceberg_db/hadoop_iceberg_table_flink_sql&#39; tblproperties (&#39;iceberg.catalog&#39;=&#39;location_based_table&#39;); -- 5.HiveSQLæŸ¥è¯¢(èƒ½æŸ¥åˆ°å®æ—¶æœ€æ–°æ•°æ®) select * from iceberg_db.hadoop_iceberg_table_flink_sql; -- 6.FlinkSQL Upsertæ›´æ–° åŸºäºç´¯ç§¯çª—å£ç»Ÿè®¡é€»è¾‘ (æ³¨æ„upsertæ¬¡æ•°è¿‡å¤šä¼šå¯¼è‡´æŸ¥è¯¢æ€§èƒ½å¾ˆå·®,å¦‚æœé¢‘ç¹upsert,åˆ™éœ€è¦é¢‘ç¹åšcompactæ¥ä¿è¯æŸ¥è¯¢æ€§èƒ½,å¯æ ¹æ®éœ€è¦è®¾ç½®åªä¿ç•™1-3ä¸ªsnapshot) CREATE TABLE if not exists `hive_iceberg_catalog`.`iceberg_db`.`summary_iceberg_table` ( actionid STRING, userid STRING, `success_cnt` bigint, `failed_cnt` bigint, window_start TIMESTAMP(3) NOT NULL, window_end TIMESTAMP(3) NOT NULL, ds STRING, PRIMARY KEY(`actionid`,`userid`,`ds`) NOT ENFORCED -- å¿…é¡»è®¾ç½®ä¸»é”® æ ¹æ®ä¸»é”®upsert ) PARTITIONED BY (ds) WITH(&#39;type&#39;=&#39;ICEBERG&#39;, &#39;format-version&#39;=&#39;2&#39;, -- å¿…é¡»æ˜¯v2è¡¨ &#39;write.upsert.enabled&#39;=&#39;true&#39;, -- æŒ‡å®šè¯¥å‚æ•° ä½¿è¡¨å¯upsert &#39;engine.hive.enabled&#39;=&#39;true&#39;, &#39;read.split.target-size&#39;=&#39;536870912&#39;, &#39;write.target-file-size-bytes&#39;=&#39;268435456&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.parquet.compression-codec&#39;=&#39;zstd&#39;, &#39;write.parquet.compression-level&#39;=&#39;10&#39;, &#39;write.avro.compression-codec&#39;=&#39;zstd&#39;, &#39;write.avro.compression-level&#39;=&#39;10&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39;, &#39;write.distribution-mode&#39;=&#39;hash&#39;); INSERT INTO `hive_iceberg_catalog`.`iceberg_db`.`summary_iceberg_table` /*+ OPTIONS(&#39;upsert-enabled&#39;=&#39;true&#39;) */ -- éœ€è¦æŒ‡å®š&#39;upsert-enabled&#39;=&#39;true&#39; SELECT actionid, userid, sum(cast(if(`error` = &#39;ok&#39;, 1, 0) as BIGINT)) AS success_cnt, sum(cast(if(`error` &lt;&gt; &#39;ok&#39;, 1, 0) as BIGINT)) AS failed_cnt, window_start, window_end, DATE_FORMAT(window_start, &#39;yyyyMMdd&#39;) AS ds FROM TABLE( CUMULATE( TABLE kafka_table, DESCRIPTOR(event_time), INTERVAL &#39;1&#39; MINUTES, INTERVAL &#39;1&#39; DAY ) ) GROUP BY window_start, window_end, actionid, userid; -- 7. FlinkSQL Upsertæ›´æ–° åŸºäºç´¯ç§¯çª—å£è®¡ç®—TopNé€»è¾‘ (æ³¨æ„upsertæ¬¡æ•°è¿‡å¤šä¼šå¯¼è‡´æŸ¥è¯¢æ€§èƒ½å¾ˆå·®,å¦‚æœé¢‘ç¹upsert,åˆ™éœ€è¦é¢‘ç¹åšcompactæ¥ä¿è¯æŸ¥è¯¢æ€§èƒ½,å¯æ ¹æ®éœ€è¦è®¾ç½®åªä¿ç•™1-3ä¸ªsnapshot) CREATE TABLE if not exists `hive_iceberg_catalog`.`iceberg_db`.`top_iceberg_table` ( actionid STRING, success_cnt bigint, failed_cnt bigint, ranking_num bigint, window_start TIMESTAMP(3) NOT NULL, window_end TIMESTAMP(3) NOT NULL, ds STRING, PRIMARY KEY(`actionid`,`ds`) NOT ENFORCED -- å¿…é¡»è®¾ç½®ä¸»é”® æ ¹æ®ä¸»é”®upsert ) PARTITIONED BY (ds) WITH(&#39;type&#39;=&#39;ICEBERG&#39;, &#39;format-version&#39;=&#39;2&#39;, -- å¿…é¡»æ˜¯v2è¡¨ &#39;write.upsert.enabled&#39;=&#39;true&#39;, -- æŒ‡å®šè¯¥å‚æ•° ä½¿è¡¨å¯upsert &#39;engine.hive.enabled&#39;=&#39;true&#39;, &#39;read.split.target-size&#39;=&#39;536870912&#39;, &#39;write.target-file-size-bytes&#39;=&#39;268435456&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.parquet.compression-codec&#39;=&#39;zstd&#39;, &#39;write.parquet.compression-level&#39;=&#39;10&#39;, &#39;write.avro.compression-codec&#39;=&#39;zstd&#39;, &#39;write.avro.compression-level&#39;=&#39;10&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39;, &#39;write.distribution-mode&#39;=&#39;hash&#39;); INSERT INTO `hive_iceberg_catalog`.`iceberg_db`.`user_experience_topn_action_report` /*+ OPTIONS(&#39;upsert-enabled&#39;=&#39;true&#39;) */ -- éœ€è¦æŒ‡å®š&#39;upsert-enabled&#39;=&#39;true&#39; SELECT * from ( SELECT actionid, success_cnt, failed_cnt, ROW_NUMBER() OVER ( PARTITION BY window_start, window_end ORDER BY failed_cnt asc ) AS rn, window_start, window_end, ds from ( select actionid, sum(cast(if(`error` = &#39;ok&#39;, 1, 0) as BIGINT)) AS success_cnt, sum(cast(if(`error` &lt;&gt; &#39;ok&#39;, 1, 0) as BIGINT)) AS failed_cnt, window_start, window_end, DATE_FORMAT(window_end, &#39;yyyyMMdd&#39;) AS ds FROM TABLE( CUMULATE( TABLE kafka_shoulei_odl_odl_xlpan_server_log, DESCRIPTOR(event_time), INTERVAL &#39;1&#39; MINUTES, INTERVAL &#39;1&#39; DAY ) ) GROUP BY window_start, window_end, actionid ) t_inner ) t_outer where rn &lt;= 100; æ‰“é€šKafka-&gt;Flink SQL-&gt;HiveCatalogç±»å‹Icebergè¡¨-&gt;Hive/Trino -- å†™å…¥Icebergè¡¨[HiveCatalogç±»å‹] -- å¯åŠ¨flinké›†ç¾¤ï¼šcd $FLINK_HOME ; bin/start-cluster.sh -- å¯åŠ¨FlinkSQL Consoleï¼šbin/sql-client.sh embedded -j iceberg-flink-runtime-1.13-0.14.0.jar -j /opt/cloudera/parcels/CDH/jars/hive-metastore-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libthrift-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-common-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/hive-serde-2.1.1-cdh6.3.1.jar -j /opt/cloudera/parcels/CDH/jars/libfb303-0.9.3.jar -j /opt/cloudera/parcels/CDH/jars/hive-shims-common-2.1.1-cdh6.3.1.jar shell set execution.checkpointing.interval=10sec; -- å¿…é¡»è®¾ç½®checkpoint é checkpointæäº¤æ›´æ–°æ•°æ®åˆ°Iceberg SET execution.runtime-mode = streaming; -- æµå¼å†™ CREATE TABLE t_kafka_source ( id BIGINT, name STRING, age INT, dt STRING ) WITH ( &#39;connector&#39; = &#39;kafka&#39;, &#39;topic&#39; = &#39;flink_topic1&#39;, &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, &#39;properties.bootstrap.servers&#39; = &#39;cdh101:9092,cdh102:9092,cdh103:9092,cdh104:9092&#39;, &#39;properties.group.id&#39; = &#39;test&#39;, &#39;format&#39; = &#39;csv&#39; ); CREATE CATALOG hive_iceberg_catalog WITH ( &#39;type&#39;=&#39;iceberg&#39;, &#39;catalog-type&#39;=&#39;hive&#39;, &#39;uri&#39;=&#39;thrift://cdh101:9083,thrift://cdh103:9083&#39;, &#39;clients&#39;=&#39;5&#39;, &#39;property-version&#39;=&#39;1&#39;, &#39;warehouse&#39;=&#39;hdfs://nameservice/user/iceberg/warehouse&#39;, &#39;hive-conf-dir&#39;=&#39;/etc/ecm/hive-conf&#39; -- å¦‚æœhiveæ˜¯kerberosè®¤è¯çš„,å¿…é¡»è¦åŠ hive-conf-dirå‚æ•°,ékerberosé›†ç¾¤å¯å¿½ç•¥ ); CREATE TABLE if not exists `hive_iceberg_catalog`.`iceberg_db`.`hive_iceberg_table_flink_sql` ( id BIGINT, name STRING, age INT, dt STRING ) PARTITIONED BY (dt) WITH(&#39;type&#39;=&#39;ICEBERG&#39;, &#39;engine.hive.enabled&#39;=&#39;true&#39;, &#39;read.split.target-size&#39;=&#39;1073741824&#39;, &#39;write.target-file-size-bytes&#39;=&#39;536870912&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.parquet.compression-codec&#39;=&#39;zstd&#39;, &#39;write.parquet.compression-level&#39;=&#39;10&#39;, &#39;write.avro.compression-codec&#39;=&#39;zstd&#39;, &#39;write.avro.compression-level&#39;=&#39;10&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;5&#39;, &#39;write.distribution-mode&#39;=&#39;none&#39;); insert into hive_iceberg_catalog.iceberg_db.hive_iceberg_table_flink_sql select id,name,age,dt from t_kafka_source; å†™å…¥HiveCatalogIcebergè¡¨åï¼Œåœ¨Hiveå¯ä»¥ç›´æ¥çœ‹åˆ°å¹¶æŸ¥è¯¢è¡¨iceberg_db.hive_iceberg_table_flink_sql.ä¹Ÿå¯ä»¥å…ˆåœ¨hiveåˆ›å»ºè¡¨,å†Flinkå†™å…¥,å‡æ­£å¸¸.Trinoä¸­ä¹Ÿå¯ä»¥ç›´æ¥çœ‹åˆ°å¹¶æŸ¥è¯¢è¯¥è¡¨.3. StreamParké›†æˆIceberg(åŸºäºHiveCatalog)StreamParkæ˜¯åŸºäºFlink SQLçš„æµå¼è®¡ç®—å¹³å°.åœ¨StreamParkä¸Šå¯ä»¥å¾ˆæ–¹ä¾¿åœ°å¼€å‘å®æ—¶æ“ä½œIcebergçš„Flinkä»»åŠ¡.ç¯å¢ƒ: Hadoop 3.2.1 + Hive 3.1.2 + Iceberg 0.14.1 + Flink 1.14.5 + StreamPark 1.2.4 + OSSFlinkSQLç¼–å†™: CREATE CATALOG hive_iceberg_catalog WITH ( &#39;type&#39;=&#39;iceberg&#39;, &#39;catalog-type&#39;=&#39;hive&#39;, &#39;uri&#39;=&#39;thrift://thrift-host:9083&#39;, &#39;clients&#39;=&#39;5&#39;, &#39;property-version&#39;=&#39;1&#39;, &#39;warehouse&#39;=&#39;oss://bucket_name/data/iceberg/warehouse&#39;, &#39;hive-conf-dir&#39;=&#39;/etc/ecm/hive-conf&#39; ); -- Kafka source table CREATE TABLE t_kafka_source ( id BIGINT, name STRING, age INT, dt STRING ) WITH ( &#39;connector&#39; = &#39;kafka&#39;, &#39;topic&#39; = &#39;t_qjj_flink_test&#39;, &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;, &#39;properties.bootstrap.servers&#39; = &#39;broker1:9092,broker2:9092,broker3:9092&#39;, &#39;properties.group.id&#39; = &#39;test&#39;, &#39;format&#39; = &#39;csv&#39; ); -- Iceberg target table CREATE TABLE IF NOT EXISTS `hive_iceberg_catalog`.`iceberg_db`.`hive_krb_iceberg_table_flink_sql` ( id BIGINT, name STRING, age INT, dt STRING ) PARTITIONED BY (dt) WITH(&#39;type&#39;=&#39;ICEBERG&#39;, &#39;engine.hive.enabled&#39;=&#39;true&#39;, &#39;read.split.target-size&#39;=&#39;1073741824&#39;, &#39;write.target-file-size-bytes&#39;=&#39;536870912&#39;, &#39;write.format.default&#39;=&#39;parquet&#39;, &#39;write.parquet.compression-codec&#39;=&#39;zstd&#39;, &#39;write.parquet.compression-level&#39;=&#39;10&#39;, &#39;write.avro.compression-codec&#39;=&#39;zstd&#39;, &#39;write.avro.compression-level&#39;=&#39;10&#39;, &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39;, &#39;write.metadata.previous-versions-max&#39;=&#39;10&#39;, &#39;write.distribution-mode&#39;=&#39;none&#39;); -- Insert data insert into hive_iceberg_catalog.iceberg_db.hive_krb_iceberg_table_flink_sql select id,name,age,dt from t_kafka_source; ä¾èµ–jar: &lt;dependency&gt; &lt;groupId&gt;org.apache.iceberg&lt;/groupId&gt; &lt;artifactId&gt;iceberg-flink-runtime-1.14&lt;/artifactId&gt; &lt;version&gt;0.14.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libthrift&lt;/artifactId&gt; &lt;version&gt;0.9.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt; &lt;artifactId&gt;libfb303&lt;/artifactId&gt; &lt;version&gt;0.9.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-serde&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive.shims&lt;/groupId&gt; &lt;artifactId&gt;hive-shims-common&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt; &lt;version&gt;1.14.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-cli&lt;/groupId&gt; &lt;artifactId&gt;commons-cli&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; å¯èƒ½å‡ºç°çš„å¼‚å¸¸: Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder; at org.apache.flink.runtime.entrypoint.parser.CommandLineOptions.&lt;clinit&gt;(CommandLineOptions.java:27) åŸå› : streamxåœ¨ä¸‹è½½hiveä¾èµ–æ—¶,ä¸‹è½½äº†å®ƒçš„å­ä¾èµ–,ä¸”hiveä½¿ç”¨çš„commons-cliä¸streamxä½¿ç”¨çš„commons-cliç‰ˆæœ¬ä¸ä¸€è‡´,å¯¼è‡´jarå†²çª.è§£å†³: æ¯æ¬¡buildåæ‰‹åŠ¨åˆ é™¤hdfs dfs -rm -f hdfs://ns/streamx/workspace/é¡¹ç›®ID/lib/commons-cli-1.2.jar Icebergä¸Trinoé›†æˆTrinoæ•´åˆIcebergéœ€è¦é…ç½®$TRINO_HOME/etc/catalog/iceberg.propertieså†…å®¹å¦‚ä¸‹: connector.name=iceberg iceberg.file-format=PARQUET hive.metastore.service.principal=hive/metastore-server-ip@realm-name hive.metastore.authentication.type=KERBEROS hive.metastore.uri=thrift://metastore-server-ip:9083,metastore-server-ip-bk:9083 hive.metastore.client.principal=principal-in-hive-keytab hive.metastore.client.keytab=/path/to/hive.keytab hive.config.resources=/etc/ecm/hadoop-conf/core-site.xml, /etc/ecm/hadoop-conf/hdfs-site.xml iceberg.compression-codec=SNAPPY è‹¥éœ€è¦å…¶æ”¯æŒå¤–éƒ¨å­˜å‚¨ä¾‹å¦‚oss,åˆ™éœ€è¦å°†jindo-core-4.3.0.jarå’Œjindo-sdk-4.3.0.jarä¸¤ä¸ªjaræ‹·è´åˆ°$TRINO_HOME/plugin/iceberg/å’Œ$TRINO_HOME/plugin/hive/ä»¥å…¼å®¹å¤–éƒ¨å­˜å‚¨. Trinoå½“å‰ä»…æ”¯æŒHiveCatalogç±»å‹çš„Icebergè¡¨,ä¸æ”¯æŒHadoopCatalogç±»å‹Icebergè¡¨.å¦‚æœæŸ¥è¯¢çš„æ˜¯HadoopCatalog,location_based_table,Customeç±»å‹çš„Icebergè¡¨ä¼šæŠ¥é”™:Table is missing [metadata_location] property: iceberg_db.iceberg_table Trinoæ“ä½œIcebergè¡¨å¸¸ç”¨æ“ä½œ: 1.æŸ¥çœ‹æœ‰å“ªäº›åˆ†åŒº select * from &quot;iceberg_table$partitions&quot;; 2.æŸ¥çœ‹æœ‰å“ªäº›å¿«ç…§ select * from &quot;iceberg_table$snapshots&quot;; SELECT snapshot_id,committed_at FROM &quot;iceberg_table$snapshots&quot; ORDER BY committed_at; 3.è¡¨ä¼˜åŒ– ä¹‹ å¿«ç…§è¿‡æœŸ ALTER TABLE iceberg_table EXECUTE expire_snapshots(retention_threshold =&gt; &#39;7d&#39;) 4.è¡¨ä¼˜åŒ– ä¹‹ æ–‡ä»¶åˆå¹¶ ALTER TABLE iceberg_table EXECUTE optimize [é»˜è®¤åˆå¹¶å°äºfile_size_thresholdçš„æ•°æ®æ–‡ä»¶,file_size_thresholdé»˜è®¤100MB] ALTER TABLE iceberg_table EXECUTE optimize(file_size_threshold =&gt; &#39;256MB&#39;) ALTER TABLE iceberg_table EXECUTE optimize WHERE partition_key = 1 [æŒ‰åˆ†åŒºä¼˜åŒ–] 5.è¡¨ä¼˜åŒ– ä¹‹ æ¸…ç†å­¤ç«‹æ— æ•ˆçš„æ–‡ä»¶ ALTER TABLE iceberg_table EXECUTE remove_orphan_files(retention_threshold =&gt; &#39;7d&#39;) 6.å‡çº§è¡¨çš„ç‰ˆæœ¬å¦‚V1å‡çº§åˆ°V2 ALTER TABLE iceberg_table SET PROPERTIES format_version = 2; 7.V2è¡¨æ ¹æ®æ¡ä»¶è¿›è¡Œè¡Œçº§åˆ é™¤æ“ä½œ (V1è¡¨ä¸æ”¯æŒè¡Œçº§åˆ é™¤,åªæ”¯æŒåˆ†åŒºæ¡ä»¶åˆ é™¤) delete from iceberg_table where ds=&#39;2022120102&#39; and eventid = &#39;event_1&#39;; 8.ä¿®æ”¹åˆ†åŒºä¹‹æ·»åŠ ä¸€ä¸ªåˆ†åŒºå­—æ®µ ALTER TABLE iceberg_table SET PROPERTIES partitioning = ARRAY[&lt;existing partition columns&gt;, &#39;my_new_partition_column&#39;]; 9.ä¿®æ”¹è¡¨å’Œå­—æ®µæ³¨é‡Š åœ¨Trinoä¿®æ”¹ååŒæ ·ä¼šåœ¨Hiveç”Ÿæ•ˆ COMMENT ON TABLE iceberg_table IS &#39;Table comment&#39;; COMMENT ON COLUMN iceberg_table.name IS &#39;Column comment&#39;; 10.TimeTravelæŸ¥è¯¢ ä¸´æ—¶æŸ¥è¯¢å†å²æŸä¸ªå¿«ç…§çš„æ•°æ® SELECT * FROM iceberg.iceberg_db.iceberg_table FOR VERSION AS OF 8954597067493422955; SELECT * FROM iceberg.iceberg_db.iceberg_table FOR TIMESTAMP AS OF TIMESTAMP &#39;2022-12-02 09:59:29.803 Europe/Vienna&#39;; 11.å›æ»šå½“å‰è¡¨çŠ¶æ€åˆ°æŸä¸ªå†å²å¿«ç…§çš„çŠ¶æ€ CALL iceberg.system.rollback_to_snapshot(&#39;iceberg_db&#39;, &#39;iceberg_table&#39;, 8954597067493422955); 12.æŸ¥çœ‹è¡¨çš„æ–‡ä»¶å’Œæ–‡ä»¶ä¿®æ”¹æ—¶é—´ select &quot;$path&quot;, &quot;$file_modified_time&quot; from iceberg_table; 13.æŸ¥è¯¢æ•°æ® select * from iceberg_table limit 10; select * from &quot;iceberg_table$data&quot; limit 10; [ç­‰ä»·äºä¸Šé¢çš„SQL] 14.æŸ¥çœ‹è¡¨é…ç½®å‚æ•° select * from &quot;iceberg_table$properties&quot;; 15.æŸ¥çœ‹è¡¨å…ƒæ•°æ®æ›´æ”¹å†å²è®°å½• select * from &quot;iceberg_table$history&quot;; 16.åˆ—å‡ºè¡¨æ¶‰åŠåˆ°çš„manifest fileåˆ—è¡¨ select * from &quot;iceberg_table$manifests&quot;; 17.åˆ—å‡ºè¡¨åœ¨å½“å‰å¿«ç…§(å½“å‰çŠ¶æ€)ä¸‹å¼•ç”¨çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶ select * from &quot;iceberg_table$files&quot;; 18.åˆ›å»ºTrinoç‰©åŒ–è§†å›¾ åªæ”¯æŒTrinoä¸­æŸ¥è¯¢ CREATE OR REPLACE MATERIALIZED VIEW iceberg_view COMMENT &#39;materializedView&#39; WITH ( format = &#39;ORC&#39;, partitioning = ARRAY[&#39;ds&#39;] ) as select appid,ds from iceberg_table; CREATE MATERIALIZED VIEW IF NOT EXISTS iceberg_view COMMENT &#39;materializedView&#39; WITH ( format = &#39;ORC&#39;, partitioning = ARRAY[&#39;ds&#39;] ) as select appid,ds from iceberg_table; REFRESH MATERIALIZED VIEW iceberg_view; [åº•å±‚è¡¨æ•°æ®å˜åŒ–å¯¼è‡´ç‰©åŒ–è§†å›¾ä¸åº•å±‚è¡¨æ•°æ®ä¸ä¸€è‡´æ—¶,ä½¿ç”¨è¯¥å‘½ä»¤æ›´æ–°ç‰©åŒ–è§†å›¾] 19.å¦‚æœæŸ¥è¯¢å¾ˆå¤æ‚å¹¶ä¸”åŒ…æ‹¬è¿æ¥å¤§å‹æ•°æ®é›†ï¼Œåˆ™åœ¨è¡¨ä¸Šè¿è¡ŒANALYZEå¯ä»¥é€šè¿‡æ”¶é›†æœ‰å…³æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯æ¥æé«˜æŸ¥è¯¢æ€§èƒ½ SET SESSION iceberg.experimental_extended_statistics_enabled = true; ANALYZE iceberg_table; ANALYZE iceberg_table WITH (columns = ARRAY[&#39;col_1&#39;, &#39;col_2&#39;]); ALTER TABLE iceberg_table EXECUTE drop_extended_stats; [å¦‚æœéœ€è¦é‡æ–°åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯,åˆ™å†é‡æ–°åˆ†æå‰å…ˆæ¸…é™¤ä¹‹å‰ç»Ÿè®¡çš„ä¿¡æ¯] 20.åˆ›å»ºè¡¨(æœ¬è´¨ä¹Ÿæ˜¯åˆ›å»ºHiveCatalogè¡¨,ä¸å»ºè®®åœ¨Trinoå»ºIcebergè¡¨,å› ä¸ºHiveå¼•æ“æ— æ³•æ”¯æŒ) CREATE TABLE iceberg_oss_table ( c1 integer, c2 date, c3 double ) WITH ( format = &#39;PARQUET&#39;, partitioning = ARRAY[&#39;c1&#39;, &#39;c2&#39;], location = &#39;oss://bucket-name/user/iceberg/warehouse/iceberg_oss_table&#39; ); Icebergä¸Sparké›†æˆä¸‹è½½iceberg-spark-runtime-3.3_2.12-1.0.0.jaråˆ°$SPARK_HOME/jarsè·¯å¾„ç¼–è¾‘$SPARK_HOME/conf/spark-defaults.confæ·»åŠ å¦‚ä¸‹å†…å®¹ spark.sql.catalog.spark_catalog org.apache.iceberg.spark.SparkSessionCatalog spark.sql.catalog.spark_catalog.type hive Sparkåˆ›å»ºçš„Icebergè¡¨æ‰“é€šHive -- Sparkåˆ›å»ºIcebergè¡¨ CREATE TABLE spark_catalog.default.qjj_iceberg_test ( id bigint COMMENT &#39;unique id&#39;, data string ) USING iceberg LOCATION &#39;oss://bucket-name/user/hive/warehouse/qjj_iceberg_test&#39;; -- Hiveä¸­å…¼å®¹Sparkåˆ›å»ºçš„Icebergè¡¨éœ€è¦åšçš„æ“ä½œ ALTER TABLE qjj_iceberg_test SET FILEFORMAT INPUTFORMAT &quot;org.apache.iceberg.mr.hive.HiveIcebergInputFormat&quot; OUTPUTFORMAT &quot;org.apache.iceberg.mr.hive.HiveIcebergOutputFormat&quot; SERDE &quot;org.apache.iceberg.mr.hive.HiveIcebergSerDe&quot;; alter table qjj_iceberg_test set TBLPROPERTIES (&#39;storage_handler&#39;=&#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39;); æ³¨: Hiveåˆ›å»ºçš„Icebergè¡¨å¯ä»¥ç›´æ¥è¢«Sparkè¯»å– Icebergè¡¨ç®¡ç†ç»´æŠ¤æ•°æ®å®æ—¶å†™å…¥Icebergè¡¨ä¼šé¢‘ç¹å‘ç”ŸCommitæ“ä½œ,äº§ç”Ÿå¤§é‡å…ƒæ•°æ®æ–‡ä»¶å’Œæ•°æ®æ–‡ä»¶,æ–‡ä»¶æ•°è†¨èƒ€å’Œå°æ–‡ä»¶é—®é¢˜ä¼šä½¿å…¶æ€§èƒ½ä¸‹é™,ç”šè‡³å½±å“åº•å±‚å­˜å‚¨ç³»ç»Ÿç¨³å®šæ€§.ç›®å‰Icebergè¡¨å¹¶ä¸èƒ½åƒHudiä¸€æ ·è‡ªåŠ¨å¤„ç†å°æ–‡ä»¶é—®é¢˜,éœ€è¦ä¸€å®šçš„æ‰‹åŠ¨ç»´æŠ¤å·¥ä½œ.ä»¥ä¸‹æ˜¯å‘ç”Ÿ31æ¬¡commitå,Icebergè¡¨ç›®å½•æ ‘å½¢ç»“æ„. hadoop_iceberg_partitioned_table â”œâ”€â”€ data â”‚ â”œâ”€â”€ dt=20221010 â”‚ â”‚ â””â”€â”€ 00000-0-hive_20221102105407_0605b24c-e823-4244-a994-83887ea7e430-job_1667357081446_0002-00001.parquet â”‚ â”œâ”€â”€ dt=20221011 â”‚ â”‚ â””â”€â”€ 00000-0-hive_20221102105315_5bb17fc0-3092-4bed-8839-253f19117b6d-job_1667357081446_0001-00001.parquet â”‚ â””â”€â”€ dt=20221104 â”‚ â”œâ”€â”€ 00000-0-shmily_20221104104344_a66cc954-b33c-48df-812c-cc59d609ec59-job_1667529535736_0001-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104104714_7bc45bcf-b8dc-4730-b9b5-d0c92ae46d5d-job_1667529535736_0002-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104104805_4a490286-3499-43fb-affb-4317e07128d1-job_1667529535736_0003-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104104851_71127db1-1170-4a38-a3d4-83e296fd3330-job_1667529535736_0004-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104105901_1d66566e-af6c-4311-b123-86cfd18e0102-job_1667529535736_0005-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110033_e6e579d7-b08f-48b1-9277-b51b318dec7c-job_1667529535736_0006-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110100_345a4caa-bb75-4ea4-be5c-51a2420c428d-job_1667529535736_0007-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110124_bbb8626e-3d06-48a4-be6a-3ef061be4122-job_1667529535736_0008-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110152_13d0e1a5-6630-4dc2-ae24-ee104cbca3b5-job_1667529535736_0009-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110158_4283a83a-2662-4a89-8311-e8f89fe64603-job_1667529535736_0010-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110218_66253aad-58ca-4121-b454-8383e1ae7aae-job_1667529535736_0011-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110245_c041e352-dacd-4f5e-9fb4-ca321b4b3468-job_1667529535736_0012-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110311_29653ebe-6426-4163-8ace-7b1d305c9253-job_1667529535736_0013-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110336_35708806-e3a6-4746-851a-e8d306812810-job_1667529535736_0014-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110400_d1949e60-6123-49ff-8a85-0281651cf0b2-job_1667529535736_0015-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110425_e0cdc030-7d88-4b5f-8956-c95ffd71e698-job_1667529535736_0016-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110448_e144f00c-60b7-4935-a749-d3d88eba828a-job_1667529535736_0017-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110513_01b3285c-a3c8-490c-bc3c-5dbd696824e8-job_1667529535736_0018-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110538_bbe889dd-8615-4019-b6cc-636d1503dc8c-job_1667529535736_0019-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110602_09da4a19-c2ac-46af-af6c-5d97a3fbdcd9-job_1667529535736_0020-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110627_b1544cff-a0c2-4310-a06a-cd6103e40e9d-job_1667529535736_0021-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110651_a7e21f79-764d-4e62-8174-109dc4f1e7e2-job_1667529535736_0022-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110716_a657cb8b-45be-4484-9a21-bc2814e0c6b1-job_1667529535736_0023-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110742_f0e29e2e-4225-4e42-9699-524a19dacf44-job_1667529535736_0024-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110805_1827ea66-b863-4ecb-adc2-285470309490-job_1667529535736_0025-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110831_1019e38e-4845-4792-83fb-39822e497983-job_1667529535736_0026-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110919_4f1e3e96-1d37-40f3-aa2e-2a0139170381-job_1667529535736_0027-00001.parquet â”‚ â”œâ”€â”€ 00000-0-shmily_20221104110943_9ff35be2-cec0-4389-843e-395c7c6ec428-job_1667529535736_0028-00001.parquet â”‚ â””â”€â”€ 00000-0-shmily_20221104111008_7a8ca8e3-d4c6-4aa1-81b6-c79d6ba7dd4f-job_1667529535736_0029-00001.parquet â”œâ”€â”€ metadata â”‚ â”œâ”€â”€ 03710058-d552-4dc1-b9cb-9340729e8f5e-m0.avro â”‚ â”œâ”€â”€ 09fd33b8-c9ce-4f7c-b871-ca31f096e3b1-m0.avro â”‚ â”œâ”€â”€ 12169e7b-13cb-4393-8158-1c9effe14e8f-m0.avro â”‚ â”œâ”€â”€ 2e834cae-756b-4498-a82a-2418db4b1092-m0.avro â”‚ â”œâ”€â”€ 3486a62e-1d74-49bd-bad3-c61187fac97f-m0.avro â”‚ â”œâ”€â”€ 3a9351b4-388b-44d9-8243-4a11189d81b2-m0.avro â”‚ â”œâ”€â”€ 3d0a560f-06f4-4402-a388-0b3cc7e25598-m0.avro â”‚ â”œâ”€â”€ 40862af2-6d95-40b8-a979-2360ea3b7175-m0.avro â”‚ â”œâ”€â”€ 44671db7-02ca-47c1-a229-c7f62d8aa12f-m0.avro â”‚ â”œâ”€â”€ 551c586b-9c4d-4ca4-a0ef-d46c30fb01f8-m0.avro â”‚ â”œâ”€â”€ 5baf0fec-3247-48f3-84f6-2f6402e866c7-m0.avro â”‚ â”œâ”€â”€ 639416fc-47c0-452e-a1d9-f17864cf008f-m0.avro â”‚ â”œâ”€â”€ 63ab2797-6a07-4886-9c27-43765bc31851-m0.avro â”‚ â”œâ”€â”€ 74ca6f5e-4eab-45d9-b0b1-04ba48e53971-m0.avro â”‚ â”œâ”€â”€ 8a4ce917-5986-4a95-9573-62103a116559-m0.avro â”‚ â”œâ”€â”€ 90bcc77d-5516-4c3e-96c8-242713920b1b-m0.avro â”‚ â”œâ”€â”€ 9f87073d-4cbc-46e9-b4dd-42fc28c86726-m0.avro â”‚ â”œâ”€â”€ a4c74672-5ace-4a79-aca9-677926532794-m0.avro â”‚ â”œâ”€â”€ b0e01ba5-bbe9-4ce4-ad9e-f07ab774a041-m0.avro â”‚ â”œâ”€â”€ b1a2cd1f-e30a-4c45-9119-9ba0e185cc58-m0.avro â”‚ â”œâ”€â”€ b209efb3-aab3-4bc2-a821-0557a0cda8d3-m0.avro â”‚ â”œâ”€â”€ b7c5b752-49d4-4840-8990-fb4a84e0f71d-m0.avro â”‚ â”œâ”€â”€ b9ba125d-bd76-483d-94f7-f6a9b664f633-m0.avro â”‚ â”œâ”€â”€ bcc5bf7b-7f01-4969-9f4c-cc9c1c920029-m0.avro â”‚ â”œâ”€â”€ d5b51efd-32b4-4948-9cc8-f2422919f1d7-m0.avro â”‚ â”œâ”€â”€ d6492cb2-7012-4668-9af9-c25cbe4df95a-m0.avro â”‚ â”œâ”€â”€ e511d02d-ecd8-4a3f-b8b7-45ad864026dc-m0.avro â”‚ â”œâ”€â”€ e652600f-4167-4f59-92cc-45faf15b03b1-m0.avro â”‚ â”œâ”€â”€ f0e6c6ca-51a7-42e6-b412-4036e27c7d98-m0.avro â”‚ â”œâ”€â”€ f3646fc2-7e64-4395-8adc-cd6a75413d37-m0.avro â”‚ â”œâ”€â”€ f91de7e0-2bf3-4804-bb28-64b61ebc588f-m0.avro â”‚ â”œâ”€â”€ snap-1244418053907939374-1-44671db7-02ca-47c1-a229-c7f62d8aa12f.avro â”‚ â”œâ”€â”€ snap-1477308230043616149-1-f91de7e0-2bf3-4804-bb28-64b61ebc588f.avro â”‚ â”œâ”€â”€ snap-1490572932134542813-1-5baf0fec-3247-48f3-84f6-2f6402e866c7.avro â”‚ â”œâ”€â”€ snap-1778869542790618047-1-12169e7b-13cb-4393-8158-1c9effe14e8f.avro â”‚ â”œâ”€â”€ snap-2054318792294634903-1-f3646fc2-7e64-4395-8adc-cd6a75413d37.avro â”‚ â”œâ”€â”€ snap-2520326235035414997-1-b7c5b752-49d4-4840-8990-fb4a84e0f71d.avro â”‚ â”œâ”€â”€ snap-3185789235788477057-1-e652600f-4167-4f59-92cc-45faf15b03b1.avro â”‚ â”œâ”€â”€ snap-3406584701390941146-1-b209efb3-aab3-4bc2-a821-0557a0cda8d3.avro â”‚ â”œâ”€â”€ snap-3684994728472824032-1-9f87073d-4cbc-46e9-b4dd-42fc28c86726.avro â”‚ â”œâ”€â”€ snap-3706799770416474623-1-a4c74672-5ace-4a79-aca9-677926532794.avro â”‚ â”œâ”€â”€ snap-3951591399252751391-1-3a9351b4-388b-44d9-8243-4a11189d81b2.avro â”‚ â”œâ”€â”€ snap-4081427338556096982-1-d5b51efd-32b4-4948-9cc8-f2422919f1d7.avro â”‚ â”œâ”€â”€ snap-4367759472594176887-1-b0e01ba5-bbe9-4ce4-ad9e-f07ab774a041.avro â”‚ â”œâ”€â”€ snap-4477640749996566080-1-63ab2797-6a07-4886-9c27-43765bc31851.avro â”‚ â”œâ”€â”€ snap-4792262885242972970-1-551c586b-9c4d-4ca4-a0ef-d46c30fb01f8.avro â”‚ â”œâ”€â”€ snap-501818490576080743-1-3486a62e-1d74-49bd-bad3-c61187fac97f.avro â”‚ â”œâ”€â”€ snap-558299450529529123-1-bcc5bf7b-7f01-4969-9f4c-cc9c1c920029.avro â”‚ â”œâ”€â”€ snap-6000755959745218957-1-09fd33b8-c9ce-4f7c-b871-ca31f096e3b1.avro â”‚ â”œâ”€â”€ snap-6590633258547705279-1-639416fc-47c0-452e-a1d9-f17864cf008f.avro â”‚ â”œâ”€â”€ snap-70006429373167712-1-d6492cb2-7012-4668-9af9-c25cbe4df95a.avro â”‚ â”œâ”€â”€ snap-7258286604987289050-1-03710058-d552-4dc1-b9cb-9340729e8f5e.avro â”‚ â”œâ”€â”€ snap-7353150060042609479-1-e511d02d-ecd8-4a3f-b8b7-45ad864026dc.avro â”‚ â”œâ”€â”€ snap-7512257803790292671-1-b9ba125d-bd76-483d-94f7-f6a9b664f633.avro â”‚ â”œâ”€â”€ snap-7520911403174383355-1-90bcc77d-5516-4c3e-96c8-242713920b1b.avro â”‚ â”œâ”€â”€ snap-7612339408675772086-1-40862af2-6d95-40b8-a979-2360ea3b7175.avro â”‚ â”œâ”€â”€ snap-7688152750730458585-1-f0e6c6ca-51a7-42e6-b412-4036e27c7d98.avro â”‚ â”œâ”€â”€ snap-8654338094020315416-1-8a4ce917-5986-4a95-9573-62103a116559.avro â”‚ â”œâ”€â”€ snap-8685114841540976719-1-b1a2cd1f-e30a-4c45-9119-9ba0e185cc58.avro â”‚ â”œâ”€â”€ snap-8693851636236625016-1-2e834cae-756b-4498-a82a-2418db4b1092.avro â”‚ â”œâ”€â”€ snap-8855760427151465849-1-3d0a560f-06f4-4402-a388-0b3cc7e25598.avro â”‚ â”œâ”€â”€ snap-9102081850556452524-1-74ca6f5e-4eab-45d9-b0b1-04ba48e53971.avro â”‚ â”œâ”€â”€ v1.metadata.json â”‚ â”œâ”€â”€ v2.metadata.json â”‚ â”œâ”€â”€ v3.metadata.json â”‚ â”œâ”€â”€ v4.metadata.json â”‚ â”œâ”€â”€ v5.metadata.json â”‚ â”œâ”€â”€ v6.metadata.json â”‚ â”œâ”€â”€ v7.metadata.json â”‚ â”œâ”€â”€ v8.metadata.json â”‚ â”œâ”€â”€ v9.metadata.json â”‚ â”œâ”€â”€ v10.metadata.json â”‚ â”œâ”€â”€ v11.metadata.json â”‚ â”œâ”€â”€ v12.metadata.json â”‚ â”œâ”€â”€ v13.metadata.json â”‚ â”œâ”€â”€ v14.metadata.json â”‚ â”œâ”€â”€ v15.metadata.json â”‚ â”œâ”€â”€ v16.metadata.json â”‚ â”œâ”€â”€ v17.metadata.json â”‚ â”œâ”€â”€ v18.metadata.json â”‚ â”œâ”€â”€ v19.metadata.json â”‚ â”œâ”€â”€ v20.metadata.json â”‚ â”œâ”€â”€ v21.metadata.json â”‚ â”œâ”€â”€ v22.metadata.json â”‚ â”œâ”€â”€ v23.metadata.json â”‚ â”œâ”€â”€ v24.metadata.json â”‚ â”œâ”€â”€ v25.metadata.json â”‚ â”œâ”€â”€ v26.metadata.json â”‚ â”œâ”€â”€ v27.metadata.json â”‚ â”œâ”€â”€ v28.metadata.json â”‚ â”œâ”€â”€ v29.metadata.json â”‚ â”œâ”€â”€ v30.metadata.json â”‚ â”œâ”€â”€ v31.metadata.json â”‚ â”œâ”€â”€ v32.metadata.json â”‚ â””â”€â”€ version-hint.text â””â”€â”€ temp å…¶ä¸­æœ‰32ä¸ªMetadataFileæ–‡ä»¶(metadata.json),31ä¸ªManifestListæ–‡ä»¶(snap-*.avro),31ä¸ªManifestFileæ–‡ä»¶(xx-m0.avro)ä»¥åŠ31ä¸ªDataFile(xx.parquet)æ–‡ä»¶.næ¬¡commitä¼šå¸¦æ¥3n+1ä¸ªæ–‡ä»¶è½ç›˜.æ‰§è¡Œæ¸…ç†(åˆå¹¶æ•°æ®æ–‡ä»¶-&gt;æ¸…ç†è¿‡æœŸå¿«ç…§-&gt;é‡å†™ManifestFile-&gt;æ¸…ç†å­¤ç«‹æ–‡ä»¶)å,å°æ–‡ä»¶æ•°é‡å¤šçš„é—®é¢˜ä¼šæœ‰æ˜æ˜¾æ”¹å–„,ç»“æœå¦‚ä¸‹: hadoop_iceberg_partitioned_table_after â”œâ”€â”€ data â”‚ â”œâ”€â”€ dt=20221010 â”‚ â”‚ â”œâ”€â”€ 00000-0-hive_20221102105407_0605b24c-e823-4244-a994-83887ea7e430-job_1667357081446_0002-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-1-5ea18300-180b-465d-8310-bbbf422e15b8-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-1-78fcc067-f967-465f-be58-f5beff8561dd-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-2-be5c1e4a-254f-4503-9ff5-f8817a9e92f7-00001.parquet â”‚ â”‚ â””â”€â”€ 00000-613-8bdfbded-0300-425a-8201-031920536100-00001.parquet â”‚ â”œâ”€â”€ dt=20221011 â”‚ â”‚ â”œâ”€â”€ 00000-0-d75b9b2f-4794-42e2-94fe-f6ae22ccd7d9-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-0-hive_20221102105315_5bb17fc0-3092-4bed-8839-253f19117b6d-job_1667357081446_0001-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-2-1402730f-cc62-4daf-a47e-a8aecaa545c8-00001.parquet â”‚ â”‚ â”œâ”€â”€ 00000-2-73f4cb74-1ab1-494d-ba4d-242b070bb82d-00001.parquet â”‚ â”‚ â””â”€â”€ 00000-611-6edbc04e-0919-4ae4-be30-89a8b91478e6-00001.parquet â”‚ â””â”€â”€ dt=20221104 â”‚ â”œâ”€â”€ 00000-0-11b9e51d-1ebd-496c-ae07-9e480c92c35e-00001.parquet â”‚ â”œâ”€â”€ 00000-0-274072c6-cefa-4395-ab31-016eacd19f08-00001.parquet â”‚ â”œâ”€â”€ 00000-0-ad26a63a-9b36-4c7d-9cb9-109aafae96fc-00001.parquet â”‚ â”œâ”€â”€ 00000-1-bd95039f-65c4-4b19-938e-185d615e3e0d-00001.parquet â”‚ â””â”€â”€ 00000-612-73dd262d-7377-42f7-87e8-024447dc6fd6-00001.parquet â”œâ”€â”€ metadata â”‚ â”œâ”€â”€ 6d24ddd9-be10-42f3-a5d6-4551ee5a8bf0-m0.avro â”‚ â”œâ”€â”€ 79077b45-29e8-4d19-89a0-aef243b6a4ca-m0.avro â”‚ â”œâ”€â”€ 79077b45-29e8-4d19-89a0-aef243b6a4ca-m1.avro â”‚ â”œâ”€â”€ 7f9cc85d-0de4-4c4c-bde7-54d4f4f78447-m0.avro â”‚ â”œâ”€â”€ ae99e2ab-fcc5-44b0-bd94-f2d73eab22f3-m0.avro â”‚ â”œâ”€â”€ ae99e2ab-fcc5-44b0-bd94-f2d73eab22f3-m1.avro â”‚ â”œâ”€â”€ d2eeb7b2-9607-4c5b-bdc5-6cfe2c81ed94-m0.avro â”‚ â”œâ”€â”€ e52f34b0-ff45-4207-87a0-95b1897da11a-m0.avro â”‚ â”œâ”€â”€ e52f34b0-ff45-4207-87a0-95b1897da11a-m1.avro â”‚ â”œâ”€â”€ e715d26c-152c-4ff3-9533-7860d920503d-m0.avro â”‚ â”œâ”€â”€ e715d26c-152c-4ff3-9533-7860d920503d-m1.avro â”‚ â”œâ”€â”€ snap-2296367325872747730-1-e715d26c-152c-4ff3-9533-7860d920503d.avro â”‚ â”œâ”€â”€ snap-3114464783889165727-1-6d24ddd9-be10-42f3-a5d6-4551ee5a8bf0.avro â”‚ â”œâ”€â”€ snap-4397206702551297792-1-ae99e2ab-fcc5-44b0-bd94-f2d73eab22f3.avro â”‚ â”œâ”€â”€ snap-5015314203544980905-1-7f9cc85d-0de4-4c4c-bde7-54d4f4f78447.avro â”‚ â”œâ”€â”€ snap-761586103173871579-1-79077b45-29e8-4d19-89a0-aef243b6a4ca.avro â”‚ â”œâ”€â”€ snap-8390029841836840556-1-d2eeb7b2-9607-4c5b-bdc5-6cfe2c81ed94.avro â”‚ â”œâ”€â”€ snap-8895954507409072148-1-e52f34b0-ff45-4207-87a0-95b1897da11a.avro â”‚ â”œâ”€â”€ v38.metadata.json â”‚ â”œâ”€â”€ v39.metadata.json â”‚ â”œâ”€â”€ v40.metadata.json â”‚ â”œâ”€â”€ v41.metadata.json â”‚ â”œâ”€â”€ v42.metadata.json â”‚ â”œâ”€â”€ v43.metadata.json â”‚ â””â”€â”€ version-hint.text â””â”€â”€ temp metadataæ•°æ§åˆ¶åœ¨Icebergä¸­,æ¯æ¬¡è§¦å‘äº‹åŠ¡æäº¤éƒ½ä¼šç”Ÿæˆä¸€ä¸ªmetadata.json,åº”å½“é¿å…metadataæ–‡ä»¶æ— é™å¢é•¿,å¯ä»¥åœ¨å»ºè¡¨æ—¶æŒ‡å®šå¦‚ä¸‹å‚æ•°: &#39;write.metadata.delete-after-commit.enabled&#39;=&#39;true&#39; # å‘ç”Ÿcommitå,æ˜¯å¦åˆ é™¤æ¯”è¾ƒæ—§çš„metadataæ–‡ä»¶ &#39;write.metadata.previous-versions-max&#39;=&#39;9&#39; # ä¿ç•™çš„æœ€å¤§å†å²metadataæ–‡ä»¶æ•°é‡,è¶…è¿‡è¯¥å†å²ç‰ˆæœ¬æ•°é‡çš„è€çš„metadataæ–‡ä»¶ä¼šè¢«åˆ é™¤ è¿™æ ·å¯ä»¥è‡ªåŠ¨æ§åˆ¶MetadataFileæ–‡ä»¶æ•°ä¸º9ä¸ª. æ¸…ç†è¿‡æœŸsnapshotæ¸…ç†Icebergè¡¨è¿‡æœŸå¿«ç…§çš„DemoFlinkå®ç°:ClearExpiredSnapshotsSparkå®ç°:SparkIcebergTableMaintenance$expireSnapshots æ•°æ®æ–‡ä»¶é‡å†™æµå¼æ•°æ®å†™å…¥å¯èƒ½ä¼šäº§ç”Ÿå¤§é‡å°çš„æ•°æ®æ–‡ä»¶,Icebergæä¾›äº†rewriteDataFiles(Compaction)æ“ä½œ,å¯ä»¥å®šæœŸåˆå¹¶å°æ–‡ä»¶,æé«˜æŸ¥è¯¢æ€§èƒ½.SparkIcebergTableMaintenance$compactDataFiles å…ƒæ•°æ®æ–‡ä»¶é‡å†™æ¯æ¬¡Commitéƒ½ä¼šäº§ç”Ÿä¸€ä¸ªmetadataæ–‡ä»¶,éšç€æ—¶é—´çš„æ¨ç§»,å®æ—¶ä»»åŠ¡å†™å…¥çš„MetadataFileæ•°è¶Šæ¥è¶Šå¤š,åšåˆå¹¶å¯ä»¥é™ä½æ–‡ä»¶æ•°,æå‡æŸ¥è¯¢æ•ˆç‡.SparkIcebergTableMaintenance$rewriteManifests æ¸…ç†å­¤ç«‹æ–‡ä»¶SparkIcebergTableMaintenance$removeOrphanFiles å¼‚å¸¸å¤„ç†ManifestFileæ–‡ä»¶ä¸¢å¤±2023-01-30 09:36:57,558 WARN org.apache.flink.runtime.taskmanager.Task [] - IcebergFilesCommitter -&gt; Sink: IcebergSink (1 /1)#0 (2125b52f518a53194e79e9f5d86dbb78) switched from RUNNING to FAILED with failure cause: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: oss://xxxxx/user/hive/warehouse/iceberg_db/xxxxx/metadata/f86794c3-750d-4def-ad2d-b726c4c210ad-m0.avro at org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183) at org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:100) at org.apache.iceberg.avro.AvroIterable.getMetadata(AvroIterable.java:65) at org.apache.iceberg.ManifestReader.&lt;init&gt;(ManifestReader.java:115) at org.apache.iceberg.ManifestFiles.read(ManifestFiles.java:91) at org.apache.iceberg.SnapshotProducer.newManifestReader(SnapshotProducer.java:448) at org.apache.iceberg.MergingSnapshotProducer$DataFileMergeManager.newManifestReader(MergingSnapshotProducer.java:1005) at org.apache.iceberg.ManifestMergeManager.createManifest(ManifestMergeManager.java:175) at org.apache.iceberg.ManifestMergeManager.lambda$mergeGroup$1(ManifestMergeManager.java:156) at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:402) at org.apache.iceberg.util.Tasks$Builder.access$300(Tasks.java:68) at org.apache.iceberg.util.Tasks$Builder$1.run(Tasks.java:308) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.FileNotFoundException: ErrorCode : 25002 , ErrorMsg: File not found. [RequestId]: ... åŸå› : å¯èƒ½åˆå¹¶ä»»åŠ¡å¼‚å¸¸å¯¼è‡´è§£å†³: // å°†ä¸¢å¤±ManifestFileæ–‡ä»¶ä»å…ƒæ•°æ®ä¸­ç§»é™¤,ä»¥è§£å†³è¡¨ä¸å¯ç”¨çš„é—®é¢˜ String lostManifestFilePath = &quot;xxxxx&quot; Table table = getTable(dbName, tabName); Snapshot snapshot = table.currentSnapshot(); List&lt;ManifestFile&gt; manifestFiles = snapshot.allManifests(table.io()); List&lt;ManifestFile&gt; manifestFileDeletes = new ArrayList&lt;&gt;(); for (ManifestFile manifestFile : manifestFiles) &#123; String path = manifestFile.path(); if (path.equals(lostManifestFilePath)) &#123; manifestFileDeletes.add(manifestFile); break; &#125; &#125; if (manifestFileDeletes.isEmpty()) &#123; throw new Exception(StringUtils.format(&quot;Manifest File:%s not in metadata&quot;,lostManifestFilePath)); &#125; RewriteManifests rewriteManifests = table.rewriteManifests(); for (ManifestFile manifestFile : manifestFileDeletes) &#123; rewriteManifests.deleteManifest(manifestFile); &#125; rewriteManifests.commit(); // ä»£ç æ‰§è¡Œè¿‡ç¨‹ä¸­å¯èƒ½æŠ›å‡ºorg.apache.iceberg.exceptions.ValidationException:Replaced and created manifests must have the same number of active files: 0 (new), 5567 (old) // ä¿®æ”¹iceberg-coreä½äºcore/src/main/java/org/apache/iceberg/BaseRewriteManifests.java activeFilesCountæ–¹æ³•æ³¨é‡Šæ‰å¦‚ä¸‹ä¸¤è¡Œ // activeFilesCount += manifest.addedFilesCount(); // activeFilesCount += manifest.existingFilesCount(); Flinkå†™å…¥Icebergæ— æ³•æ‰¾åˆ°avroæ–‡ä»¶,å¯¼è‡´ä»»åŠ¡æŠ¥é”™æ— æ³•å†™å…¥org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: oss://bucket_name/user/hive/warehouse/iceberg_db/user_experience_report/metadata/32759abff25a1366837ed3d146e27d51-55f7b63bf1c8c02b88d8659b98477e64-00000-2-71-00037.avro at org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183) at org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:100) at org.apache.iceberg.avro.AvroIterable.getMetadata(AvroIterable.java:65) at org.apache.iceberg.ManifestReader.&lt;init&gt;(ManifestReader.java:115) at org.apache.iceberg.ManifestFiles.read(ManifestFiles.java:91) at org.apache.iceberg.ManifestFiles.read(ManifestFiles.java:72) at org.apache.iceberg.flink.sink.FlinkManifestUtil.readDataFiles(FlinkManifestUtil.java:58) at org.apache.iceberg.flink.sink.FlinkManifestUtil.readCompletedFiles(FlinkManifestUtil.java:113) at org.apache.iceberg.flink.sink.IcebergFilesCommitter.commitUpToCheckpoint(IcebergFilesCommitter.java:244) at org.apache.iceberg.flink.sink.IcebergFilesCommitter.initializeState(IcebergFilesCommitter.java:184) at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:119) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286) at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) at java.lang.Thread.run(Thread.java:748) ...... å¯èƒ½åŸå› : æ­¤æ–‡ä»¶ä¸æ˜¯MainfestListæ–‡ä»¶ä¹Ÿä¸æ˜¯ManifestFileæ–‡ä»¶,è€Œæ˜¯Flinkå†™å…¥Icebergæ—¶ä¸€ç§ä¸­é—´çŠ¶æ€çš„æ–‡ä»¶,å¯èƒ½åŸå› æ˜¯checkpointè¶…æ—¶æˆ–æ—¶é—´è¿‡é•¿,ä½†è¯¥å¼‚å¸¸ä¸åˆå¹¶å’Œæ¸…ç†ä»»åŠ¡æ— å…³è§£å†³: hdfs dfs -ls -r -t oss://bucket_name/user/hive/warehouse/iceberg_db/user_experience_report/metadata/ | grep avro | grep -v snap | grep -v m0 | grep -v m1 | grep -v m2 | grep -v m3 æ‰¾åˆ°æœ€æ–°avro æ‹·è´å¹¶é‡å‘½åä¸ºç¼ºå¤±çš„avro åŒæ—¶ä¼˜åŒ–checkpointç¨³å®šæ€§ HiveCatalogä¸‹é”è¡¨é€ æˆæäº¤commitå¤±è´¥23/05/14 03:32:34 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.iceberg.exceptions.CommitFailedException: Timed out after 183898 ms waiting for lock on iceberg_db.user_experience_report_user_details è§£å†³ï¼šåˆ°Hiveå…ƒæ•°æ®åº“select * from metastore.hive_locks; DELETE FROM metastore.hive_locks WHERE HL_DB=â€™iceberg_dbâ€™ AND HL_TABLE=â€™user_experience_report_user_detailsâ€™;å†é‡è·‘icebergä»»åŠ¡å³å¯ å¯¹æ¯”Hudiå’ŒDeltaLake å¯¹æ¯”ç»´åº¦\\æŠ€æœ¯ Iceberg Hudi DeltaLake æ•°æ®ç®¡ç† é€šè¿‡metadataæ–‡ä»¶ç®¡ç† é€šè¿‡metadataæ–‡ä»¶ç®¡ç† é€šè¿‡metadataæ–‡ä»¶ç®¡ç† ä½¿ç”¨åœºæ™¯ æµæ‰¹ä¸€ä½“,é«˜æ€§èƒ½åˆ†æä¸å¯é æ•°æ®ç®¡ç† æµæ‰¹ä¸€ä½“,Upsertåœºæ™¯ æµæ‰¹ä¸€ä½“,èåˆSparkç”Ÿæ€ ACID æ”¯æŒ æ”¯æŒ æ”¯æŒ ACIDéš”ç¦»çº§åˆ« Write Serialization(å†™ä¸²è¡Œæ‰§è¡Œ) Snapshot Isolation(å†™æ•°æ®è‹¥æ— äº¤é›†åˆ™å¹¶å‘å†™,å¦åˆ™ä¸²è¡Œ) Serialization(è¯»å†™éƒ½å¿…é¡»ä¸²è¡Œ)/Write Serialization/Snapshot Isolation Schemaæ¼”åŒ– æ”¯æŒ æ”¯æŒ æ”¯æŒ æ•°æ®æ“ä½œ æ”¯æŒUpdate/Delete æ”¯æŒUpsert/Delete æ”¯æŒUpdate/Delete/Merge æµå¼è¯» æ”¯æŒ æ”¯æŒ æ”¯æŒ æµå¼å†™ æ”¯æŒ æ”¯æŒ æ”¯æŒ å¹¶å‘æ§åˆ¶ ä¹è§‚ ä¹è§‚ ä¹è§‚ æ–‡ä»¶æ¸…ç† æ‰‹åŠ¨ è‡ªåŠ¨ æ‰‹åŠ¨ Compaction æ‰‹åŠ¨ è‡ªåŠ¨ æ‰‹åŠ¨ å¤–éƒ¨ä¾èµ– å®Œå…¨è§£è€¦ ä¾èµ–Spark ä¾èµ–Spark CopyOnWrite æ”¯æŒ æ”¯æŒ æ”¯æŒ MergeOnRead v2è¡¨æ”¯æŒ,v1è¡¨ä¸æ”¯æŒ æ”¯æŒ ä¸æ”¯æŒ å­—æ®µåŠ å¯† v3è¡¨è®¡åˆ’æ”¯æŒ ä¸æ”¯æŒ ä¸æ”¯æŒ å‚è€ƒApache IcebergIcebergæ¦‚è¿°æ·±åº¦å¯¹æ¯” Deltaã€Iceberg å’Œ Hudi ä¸‰å¤§å¼€æºæ•°æ®æ¹–æ–¹æ¡ˆ","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Iceberg","slug":"Iceberg","permalink":"https://shmily-qjj.top/tags/Iceberg/"},{"name":"æ•°æ®æ¹–","slug":"æ•°æ®æ¹–","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E6%B9%96/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Linux BindæœåŠ¡é…ç½®DNSè§£æ","slug":"Linux BindæœåŠ¡é…ç½®DNSè§£æ","date":"2022-06-01T12:12:10.000Z","updated":"2022-12-11T05:35:07.907Z","comments":true,"path":"39a9ed67/","link":"","permalink":"https://shmily-qjj.top/39a9ed67/","excerpt":"","text":"LinuxæœåŠ¡å™¨é…ç½®DNSè§£ææœåŠ¡BindBINDç®€ä»‹BINDæ˜¯ç°åœ¨ä½¿ç”¨æœ€ä¸ºå¹¿æ³›çš„DNSæœåŠ¡å™¨è½¯ä»¶ï¼ˆBerkeley Internet Name Domainï¼‰,æ”¯æŒå…ˆä»Šç»å¤§å¤šæ•°çš„æ“ä½œç³»ç»Ÿï¼ˆLinuxï¼ŒUNIXï¼ŒMacï¼ŒWindowsï¼‰ã€‚BINDæœåŠ¡çš„åç§°ç§°ä¹‹ä¸ºnamedã€‚DNSé»˜è®¤ä½¿ç”¨UDPã€TCPåè®®ï¼Œä½¿ç”¨ç«¯å£ä¸º53ï¼ˆdomainï¼‰ï¼Œ953ï¼ˆmdcï¼Œè¿œç¨‹æ§åˆ¶ä½¿ç”¨ï¼‰ã€‚ DNSæœåŠ¡ç«¯é…ç½® å®‰è£…bindå‡†å¤‡ä¸¤å°DNSæœåŠ¡å™¨ kdc1(10.2.5.3) kdc2(10.2.5.4) å®‰è£… yum -y install bind bind-utils rpm -qa | grep bind é…ç½®bindé…ç½®æ–‡ä»¶åˆ†åˆ«ä½äºä¸¤ä¸ªä½ç½®/etc/named.conf BINDæœåŠ¡ä¸»é…ç½®æ–‡ä»¶/var/named/ zoneæ–‡ä»¶ï¼ˆåŸŸçš„dnsä¿¡æ¯ï¼‰å…ˆä¿®æ”¹ä¸»DNS Serverä¸Šçš„é…ç½®æ–‡ä»¶/etc/named.confcp /etc/named.conf /etc/named.conf.templatevim /etc/named.conf options &#123; listen-on port 53 &#123; 127.0.0.1;10.2.5.3; &#125;; listen-on-v6 &#123;none;&#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query-cache &#123;any;&#125;; allow-query &#123; 10.2.5.0/8; &#125;; forward only; forwarders &#123;10.2.5.2;8.8.8.8;&#125;; recursion yes; allow-recursion &#123; any;&#125;; dnssec-enable no; dnssec-validation no; recursive-clients 1000000; tcp-clients 10000; send-cookie no; require-server-cookie no; /* Path to ISC DLV key */ bindkeys-file &quot;/etc/named.root.key&quot;; managed-keys-directory &quot;/var/named/dynamic&quot;; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;; &#125;; logging &#123; channel default_debug &#123; file &quot;data/named.run&quot;; severity dynamic; &#125;; &#125;; zone &quot;.&quot; IN &#123; type hint; file &quot;named.ca&quot;; &#125;; zone &quot;shmily-qjj.top&quot; IN &#123; type master; file &quot;shmily-qjj.top.zone&quot;; notify yes; also-notify &#123; 10.2.5.4; &#125;; allow-update &#123; none; &#125;; &#125;; zone &quot;10.in-addr.arpa&quot; IN &#123; type master; file &quot;ptr.shmily-qjj.top.zone&quot;; notify yes; also-notify &#123; 10.2.5.4; &#125;; allow-update &#123; none; &#125;; &#125;; #include &quot;/etc/named.rfc1912.zones&quot;; #include &quot;/etc/named.root.key&quot;; ä¸»DNS Serverä¸Šæ·»åŠ è§£æZONEæ–‡ä»¶æ­£å‘è§£ææ–‡ä»¶ vim /var/named/shmily-qjj.top.zone $TTL 1D @ IN SOA dns1.shmily-qjj.top. admin.shmily-qjj.top. ( 2022060101 3H 15M 1W 1D ) @ IN NS dns1.shmily-qjj.top. @ IN NS dns2.shmily-qjj.top. dns1 IN A 10.2.5.3 dns2 IN A 10.2.5.4 ;kdc ldap kdc1.shmily-qjj.top IN A 10.2.5.3 kdc2.shmily-qjj.top IN A 10.2.5.4 ;cdh cdh101 IN A 10.2.5.101 cdh102 IN A 10.2.5.102 cdh103 IN A 10.2.5.103 cdh104 IN A 10.2.5.104 ;other nodes node1.shmily-qjj.top IN A 10.2.5.100 åå‘è§£ææ–‡ä»¶ vim /var/named/ptr.shmily-qjj.top.zone $TTL 1D @ IN SOA dns1.shmily-qjj.top. admin.shmily-qjj.top. ( 2022060101 3H 15M 1W 1D ) @ IN NS dns1.shmily-qjj.top. @ IN NS dns2.shmily-qjj.top. ;kdc ldap 3.5.2 IN PTR kdc1.shmily-qjj.top 4.5.2 IN PTR kdc2.shmily-qjj.top ;cdh 101.5.2 IN PTR cdh101 102.5.2 IN PTR cdh102 103.5.2 IN PTR cdh103 104.5.2 IN PTR cdh104 ;;other nodes 100.5.2 IN PTR node1.shmily-qjj.top ä¿®æ”¹å¤‡DNS Serverä¸Šçš„é…ç½®æ–‡ä»¶/etc/named.conf options &#123; listen-on port 53 &#123; 127.0.0.1;10.2.5.4; &#125;; listen-on-v6 &#123;none;&#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query-cache &#123;any;&#125;; allow-query &#123; 10.2.5.0/8; &#125;; forward only; forwarders &#123;10.2.5.2;8.8.8.8;&#125;; recursion yes; allow-recursion &#123; any;&#125;; dnssec-enable no; dnssec-validation no; recursive-clients 1000000; tcp-clients 10000; send-cookie no; require-server-cookie no; /* Path to ISC DLV key */ bindkeys-file &quot;/etc/named.root.key&quot;; managed-keys-directory &quot;/var/named/dynamic&quot;; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;; &#125;; logging &#123; channel default_debug &#123; file &quot;data/named.run&quot;; severity dynamic; &#125;; &#125;; zone &quot;.&quot; IN &#123; type hint; file &quot;named.ca&quot;; &#125;; zone &quot;shmily-qjj.top&quot; IN &#123; type slave; file &quot;slaves/shmily-qjj.top.zone&quot;; notify no; masters &#123; 10.2.5.3; &#125;; &#125;; zone &quot;10.in-addr.arpa&quot; IN &#123; type slave; file &quot;slaves/ptr.shmily-qjj.top.zone&quot;; notify no; masters &#123; 10.2.5.3; &#125;; &#125;; #include &quot;/etc/named.rfc1912.zones&quot;; #include &quot;/etc/named.root.key&quot;; ä¸¤å°èŠ‚ç‚¹åˆ†åˆ«å¯åŠ¨çš„DNSæœåŠ¡ systemctl enable named systemctl start named systemctl status named rndcåŒæ­¥rndcï¼ˆRemote Name Domain Controllerrï¼‰æ˜¯ä¸€ä¸ªè¿œç¨‹ç®¡ç†bindçš„å·¥å…·ï¼Œé€šè¿‡è¿™ä¸ªå·¥å…·å¯ä»¥åœ¨æœ¬åœ°æˆ–è€…è¿œç¨‹äº†è§£å½“å‰æœåŠ¡å™¨çš„è¿è¡ŒçŠ¶å†µï¼Œä¹Ÿå¯ä»¥å¯¹æœåŠ¡å™¨è¿›è¡Œå…³é—­ã€é‡è½½ã€åˆ·æ–°ç¼“å­˜ã€å¢åŠ åˆ é™¤zoneç­‰æ“ä½œã€‚ ä½¿ç”¨rndcå¯ä»¥åœ¨ä¸åœæ­¢DNSæœåŠ¡å™¨å·¥ä½œçš„æƒ…å†µè¿›è¡Œæ•°æ®çš„æ›´æ–°ï¼Œä½¿ä¿®æ”¹åçš„é…ç½®æ–‡ä»¶ç”Ÿæ•ˆã€‚åœ¨å®é™…æƒ…å†µä¸‹ï¼ŒDNSæœåŠ¡å™¨æ˜¯éå¸¸ç¹å¿™çš„ï¼Œä»»ä½•çŸ­æ—¶é—´çš„åœé¡¿éƒ½ä¼šç»™ç”¨æˆ·çš„ä½¿ç”¨å¸¦æ¥å½±å“ã€‚å› æ­¤ï¼Œä½¿ç”¨rndcå·¥å…·å¯ä»¥ä½¿DNSæœåŠ¡å™¨æ›´å¥½åœ°ä¸ºç”¨æˆ·æä¾›æœåŠ¡ã€‚åœ¨ä½¿ç”¨rndcç®¡ç†bindå‰éœ€è¦ä½¿ç”¨rndcç”Ÿæˆä¸€å¯¹å¯†é’¥æ–‡ä»¶ï¼Œä¸€åŠä¿å­˜äºrndcçš„é…ç½®æ–‡ä»¶ä¸­ï¼Œå¦ä¸€åŠä¿å­˜äºbindä¸»é…ç½®æ–‡ä»¶ä¸­ã€‚rndcçš„é…ç½®æ–‡ä»¶ä¸º/etc/rndc.confï¼Œåœ¨CentOSæˆ–è€…RHELä¸­ï¼Œrndcçš„å¯†é’¥ä¿å­˜åœ¨/etc/rndc.keyæ–‡ä»¶ä¸­ã€‚rndcé»˜è®¤ç›‘å¬åœ¨953å·ç«¯å£ï¼ˆTCPï¼‰ï¼Œå…¶å®åœ¨bind9ä¸­rndcé»˜è®¤å°±æ˜¯å¯ä»¥ä½¿ç”¨ï¼Œä¸éœ€è¦é…ç½®å¯†é’¥æ–‡ä»¶ã€‚rndcä¸DNSæœåŠ¡å™¨å®è¡Œè¿æ¥æ—¶ï¼Œéœ€è¦é€šè¿‡æ•°å­—è¯ä¹¦è¿›è¡Œè®¤è¯ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç”¨æˆ·å/å¯†ç æ–¹å¼ã€‚åœ¨å½“å‰ç‰ˆæœ¬ä¸‹ï¼Œrndcå’Œnamedéƒ½åªæ”¯æŒHMAC-MD5è®¤è¯ç®—æ³•ï¼Œåœ¨é€šä¿¡ä¸¤ç«¯ä½¿ç”¨é¢„å…±äº«å¯†é’¥ã€‚åœ¨å½“å‰ç‰ˆæœ¬çš„rndc å’Œ namedä¸­ï¼Œå”¯ä¸€æ”¯æŒçš„è®¤è¯ç®—æ³•æ˜¯HMAC-MD5ï¼Œåœ¨è¿æ¥çš„ä¸¤ç«¯ä½¿ç”¨å…±äº«å¯†é’¥ã€‚å®ƒä¸ºå‘½ä»¤è¯·æ±‚å’Œåå­—æœåŠ¡å™¨çš„å“åº”æä¾› TSIGç±»å‹çš„è®¤è¯ã€‚æ‰€æœ‰ç»ç”±é€šé“å‘é€çš„å‘½ä»¤éƒ½å¿…é¡»è¢«ä¸€ä¸ªæœåŠ¡å™¨æ‰€çŸ¥é“çš„ key_id ç­¾åã€‚ä¸ºäº†ç”ŸæˆåŒæ–¹éƒ½è®¤å¯çš„å¯†é’¥ï¼Œå¯ä»¥ä½¿ç”¨rndc-confgenå‘½ä»¤äº§ç”Ÿå¯†é’¥å’Œç›¸åº”çš„é…ç½®ï¼Œå†æŠŠè¿™äº›é…ç½®åˆ†åˆ«æ”¾å…¥named.confå’Œrndcçš„é…ç½®æ–‡ä»¶rndc.confä¸­ã€‚ä¿®æ”¹ä¸»èŠ‚ç‚¹ZONEé…ç½®æ–‡ä»¶ï¼ˆéœ€è¦ä¿®æ”¹ZONEæ–‡ä»¶çš„ç¼–å·ï¼‰æ³¨æ„ï¼šä¿®æ”¹äº†ZONEç¼–å·ï¼Œå³ä½¿é…ç½®æ²¡å‘ç”Ÿå˜åŒ–ï¼Œé…ç½®ä»ç„¶ä¼šåŒæ­¥åˆ°å¤‡ç”¨DNSï¼›æœªä¿®æ”¹ZONEç¼–å·ï¼Œå³ä½¿é…ç½®å‘ç”Ÿå˜åŒ–ï¼Œä¹Ÿä¸ä¼šåŒæ­¥åˆ°å¤‡ç”¨DNSvim /var/named/shmily-qjj.top.zoneæ‰§è¡Œrndc reload æç¤ºserver reload successfulè¯æ˜æˆåŠŸ éªŒè¯å¤‡èŠ‚ç‚¹è§£æå°†/etc/resolv.confä¸­nameserveræŒ‡å‘å¤‡èŠ‚ç‚¹10.2.5.4æŸ¥çœ‹å¤‡èŠ‚ç‚¹ç›®å½•ä¿®æ”¹æ—¶é—´ DNSå®¢æˆ·ç«¯èŠ‚ç‚¹é…ç½®å®¢æˆ·ç«¯é…ç½®vim /etc/resolv.conf search shmily-qjj.top nameserver 10.2.5.3 nameserver 10.2.5.4 éªŒè¯ï¼šæ³¨é‡Šæ‰/etc/hostsä¸‹çš„ipåœ°å€æ˜ å°„å®‰è£…bind-utilsyum install bind-utilsæ­£å‘è§£æéªŒè¯nslookup kdc1.shmily-qjj.topnslookup kdc2.shmily-qjj.topnslookup node1.shmily-qjj.topping kdc1.shmily-qjj.top -c 3ping kdc2.shmily-qjj.top -c 1ping node1.shmily-qjj.top -c 1 åå‘è§£æéªŒè¯nslookup 10.2.5.3nslookup 10.2.5.100 å‚è€ƒbindé…ç½®å·¥å…·rndcä½¿ç”¨","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"DNS","slug":"DNS","permalink":"https://shmily-qjj.top/tags/DNS/"},{"name":"Bind","slug":"Bind","permalink":"https://shmily-qjj.top/tags/Bind/"},{"name":"Linux","slug":"Linux","permalink":"https://shmily-qjj.top/tags/Linux/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"KyuubiåŸç†ä¸æ›¿ä»£SparkThriftServerå®è·µ-åŸºäºCDH6","slug":"KyuubiåŸç†ä¸æ›¿ä»£SparkThriftServerå®è·µ","date":"2022-04-29T08:39:12.000Z","updated":"2023-03-19T15:36:21.800Z","comments":true,"path":"ee1c2df4/","link":"","permalink":"https://shmily-qjj.top/ee1c2df4/","excerpt":"","text":"KyuubiåŸç†ä¸æ›¿ä»£SparkThriftServerå®è·µ-åŸºäºCDH6 å‰è¨€Spark ThriftServeråŸç”Ÿä¸æ”¯æŒå¤šç§Ÿæˆ·ã€æƒé™ç®¡ç†ã€ä¸”ç¨³å®šæ€§ä¸€èˆ¬ï¼Œå³ä½¿æˆ‘ä»¬åœ¨æºç åŸºç¡€ä¸Šåšäº†å¾ˆå¤šæƒé™ç®¡æ§ã€SQLæ—¥å¿—å®¡è®¡ã€æ•°æ®è„±æ•ä»¥åŠæ€§èƒ½ä¼˜åŒ–ï¼Œä½†ç”±äºå®ƒè‡ªèº«çš„ç¨³å®šæ€§å’Œå•ç‚¹é—®é¢˜ï¼Œä»ç„¶ä¼šç»å¸¸é€ æˆè°ƒåº¦ã€åˆ†æä»»åŠ¡çš„å¤±è´¥ã€‚å¸¸è§çš„ä¸€äº›é—®é¢˜æœ‰ï¼š Driverç«¯å•ç‚¹æ•…éšœå¯¼è‡´æ•´ä¸ªè°ƒåº¦å¤±è´¥ å•ä¸ªå¤§Queryå ç”¨å¤§é‡å¹¶è¡Œåº¦ï¼Œå¯¼è‡´åç»­ä»»åŠ¡ç¼“æ…¢æˆ–æŒç»­ç­‰å¾… å•ä¸ªJobå‘ç”Ÿæ•°æ®å€¾æ–œæ—¶ï¼Œä¼šæ‹–æ…¢è¯¥Jobçš„Taskæ‰€åœ¨çš„Executorï¼Œå½±å“å…¶ä»–Job å®šæœŸé‡å¯ä»¥é¿å…Kerberoså‡­æ®è¿‡æœŸ é€šè¿‡æºç äºŒæ¬¡å¼€å‘æ‰èƒ½å®ç°çš„ç”¨æˆ·æƒé™ç®¡ç†ã€å¤šç§Ÿæˆ·ç®¡ç† èµ„æºç”³è¯·å’Œé‡Šæ”¾ç²’åº¦è¾ƒç²—ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨æµªè´¹æˆ–ä¸å……åˆ† å¯¹ä¸åŒè´Ÿè½½ä»»åŠ¡å¯åŠ¨å¤šä¸ªThriftå®ä¾‹ï¼Œæ‰èƒ½å®ç°ç²—ç²’åº¦çš„èµ„æºéš”ç¦»ï¼Œå®ä¾‹è¶Šå¤šï¼Œç»´æŠ¤èµ·æ¥è¶Šç¹çé’ˆå¯¹ä»¥ä¸Šç—›ç‚¹ï¼Œç½‘æ˜“è´¡çŒ®äº†Kyuubiè¿™ä¸ªé¡¹ç›®ï¼Œéå¸¸é€‚åˆæ›¿æ¢æ‰åŸæœ‰çš„SparkThriftServeræœåŠ¡ï¼Œè§£å†³ä»¥ä¸Šç—›ç‚¹çš„åŒæ—¶ï¼Œè¿˜æ”¯æŒäº†å¼‚æ„è®¡ç®—å¼•æ“(Flinkã€Trinoç­‰)ã€‚å½“å‰(2022.5)è¯¥é¡¹ç›®è¿˜åœ¨Apacheå­µåŒ–å™¨è¿›è¡Œå­µåŒ–ï¼Œåœ¨æˆ‘çœ‹æ¥æ˜¯ä¸ªæ¯”è¾ƒæœ‰å‰æ™¯çš„é¡¹ç›®ã€‚ Kyuubiæ˜¯ä»€ä¹ˆç½‘æ˜“æ•°å¸†å¼€æºçš„ä¸€æ¬¾æ”¯æŒå¤šç§Ÿæˆ·èµ„æºéš”ç¦»ã€ç»†ç²’åº¦çš„è¡Œçº§ã€åˆ—çº§æƒé™ç®¡ç†ã€æ”¯æŒé«˜å¯ç”¨å’Œè´Ÿè½½å‡è¡¡çš„ç»Ÿä¸€åˆ†æå¼•æ“ï¼Œå¯ä»¥é€šè¿‡SQLã€Scalaå®ŒæˆETLã€æ•°æ®å¤„ç†è·‘æ‰¹ã€åˆ†æç­‰å¤šç§ä»»åŠ¡è´Ÿè½½ã€‚Kyuubiçš„æ„¿æ™¯æ˜¯å»ºç«‹åœ¨Apache Sparkå’ŒData LakeæŠ€æœ¯ä¹‹ä¸Šï¼Œç†æƒ³çš„ç»Ÿä¸€æ•°æ®æ¹–ç®¡ç†å¹³å°ã€‚æ”¯æŒçº¯SQLæ–¹å¼å¤„ç†æ•°æ®ï¼Œå®ç°åœ¨åŒç»Ÿä¸€å¹³å°ä¸Šä½¿ç”¨ä¸€ä»½æ•°æ®å‰¯æœ¬å’Œä¸€ä¸ªSQLæ¥å£ï¼Œå®ŒæˆETLã€åˆ†æã€BIâ€¦â€¦ç­‰å·¥ä½œã€‚ Kyuubiå¯¹æ¯”SparkThriftServerçš„ä¼˜åŠ¿ Kyuubi SparkThriftServer èµ„æºéš”ç¦» æ”¯æŒèµ„æºéš”ç¦» STSæ˜¯å•ä¸ªApplicationï¼Œåªèƒ½æäº¤åˆ°ä¸€ä¸ªYarn Queueï¼›è™½ç„¶Sparkæœ¬èº«ä¹Ÿå…·æœ‰ä¸€å®šèµ„æºå…±äº«èƒ½åŠ›â€”â€”FairScheduleré€šè¿‡è®¾ç½®spark.scheduler.poolèµ„æºæ± ä¼˜å…ˆçº§æ¥ä¸ºä¸åŒç”¨æˆ·åˆ†é…ä¸åŒèµ„æºï¼Œä½†å†…å­˜IOå’ŒCPUç­‰èµ„æºçš„éš”ç¦»æœ¬èº«åº”æ˜¯èµ„æºè°ƒåº¦ç³»ç»ŸYarnæˆ–K8Sè¯¥åšçš„äº‹å„¿ å¹¶å‘å’Œæ‰©å±•èƒ½åŠ› æ”¯æŒæ— é™æ°´å¹³æ‰©å±•çš„å¤šå®¢æˆ·ç«¯å¹¶å‘èƒ½åŠ›ï¼Œå¯è‡ªåŠ¨æ‰©å±•çš„æŸ¥è¯¢å¹¶å‘èƒ½åŠ›ï¼Œæ…¢SQLå½±å“å° å•ä¸ªSTSå¹¶å‘æŸ¥è¯¢èƒ½åŠ›æœ‰é™ã€å¹¶å‘é«˜æ—¶å°±ä¼šå‡ºç°èµ„æºç´§å¼ ï¼Œèµ„æºæŠ¢å ï¼Œä»»åŠ¡ç­‰å¾…ã€å¡æ­»ï¼Œä¸”Driverå•ç‚¹ç“¶é¢ˆæ˜æ˜¾ï¼Œæ…¢SQLå½±å“å¤§ èµ„æºä¼¸ç¼©æ€§ ä¸¤çº§å¼¹æ€§èµ„æºç®¡ç†ï¼ˆKyuubiçš„èµ„æºå¼¹æ€§ç®¡ç†æ”¯æŒè‡ªåŠ¨ç”³è¯·å’Œé‡Šæ”¾Sparkå®ä¾‹+Sparkåº”ç”¨è‡ªèº«åŠ¨æ€èµ„æºç®¡ç†ï¼‰ Sparkè‡ªèº«åŠ¨æ€èµ„æºç®¡ç† æˆæƒæ§åˆ¶ æ”¯æŒæ•°æ®å’Œå…ƒæ•°æ®çš„è®¿é—®æƒé™æ§åˆ¶ï¼Œæ”¯æŒåŸºäºRangerç»†ç²’åº¦æˆæƒï¼Œä¿è¯æ•°æ®å®‰å…¨ STSæ˜¯å•ç”¨æˆ·å¯åŠ¨çš„ï¼Œåªæœ‰ç²—ç²’åº¦æˆæƒï¼Œæ— æ³•ä¿è¯æ•°æ®å®‰å…¨ å®ä¾‹ç®¡ç† æ”¯æŒè¿æ¥çº§åˆ«ã€ç”¨æˆ·çº§åˆ«ã€æœåŠ¡çº§åˆ«å’Œç»„çº§åˆ«çš„SparkApplicationå®ä¾‹ç”³è¯· å•ä¸ªSparkApplicationå®ä¾‹ æ‰§è¡Œå¼•æ“ Sparkã€Flinkã€Trino(Presto) Spark ç”¨æˆ·è¯­è¨€ Scala+SQLçµæ´»æ··åˆä½¿ç”¨ SQL å­˜å‚¨å¼•æ“ Hive+Kudu+DeltaLake+Azure+Presto Hive+DeltaLake é«˜å¯ç”¨æ€§ åŸç”ŸåŸºäºZKå’ŒYarnçš„é«˜å¯ç”¨ï¼ŒKyuubiServeræœ¬èº«æ”¯æŒæ°´å¹³æ‰©å±•é«˜å¯ç”¨ åŸç”Ÿä¸æ”¯æŒï¼Œéœ€è¦æ‰‹åŠ¨é…ç½®LoadBalancerï¼Œä½†å‘ç”Ÿåˆ‡æ¢æ—¶è§†å›¾ã€hivevarå˜é‡ã€ç¼“å­˜ç­‰çŠ¶æ€ä¼šä¸¢å¤± ç³»ç»Ÿæ¶æ„ KyuubiåŸç†Kyuubiæ¶æ„å›¾åœ¨Kyuubiä¸­ï¼Œå®¢æˆ·ç«¯çš„è¿æ¥æ˜¯ä½œä¸ºKyuubiSessionæ¥ç»´æŠ¤çš„ã€‚Kyuubi Sessionçš„åˆ›å»ºå¯ä»¥åˆ†ä¸ºè½»é‡çº§å’Œé‡é‡çº§ä¸¤ç§æƒ…å†µã€‚å¤§å¤šæ•°ä¼šè¯åˆ›å»ºéƒ½æ˜¯è½»é‡çº§ã€ç”¨æˆ·æ— æ„ŸçŸ¥çš„ã€‚å”¯ä¸€çš„é‡é‡çº§æƒ…å†µæ˜¯ç”¨æˆ·çš„å…±äº«åŸŸä¸­æ²¡æœ‰å®ä¾‹åŒ–æˆ–ç¼“å­˜çš„SparkContextï¼Œè¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ç”¨æˆ·ç¬¬ä¸€æ¬¡è¿æ¥æˆ–é•¿æ—¶é—´æœªè¿æ¥æ—¶ã€‚è¿™ç§ä¸€æ¬¡æ€§åˆ›å»ºä¼šè¯çš„æˆæœ¬ï¼Œåœ¨å¤šæ•°AdHocåœºæ™¯ä¸‹ä¹Ÿèƒ½æ¥å—ã€‚ Kyuubiç»´æŠ¤SparkContextçš„æ–¹å¼æ˜¯æ¾æ•£è€¦åˆçš„ï¼Œè¿™äº›SparkContextæ—¢å¯ä»¥æ˜¯æœ¬åœ°Clientæ¨¡å¼åˆ›å»ºçš„ï¼Œä¹Ÿå¯ä»¥æ˜¯Yarnã€K8Sé›†ç¾¤ä¸Šçš„Clusteræ¨¡å¼åˆ›å»ºçš„ã€‚é«˜å¯ç”¨æ¨¡å¼ä¸‹ï¼ŒSparkContextä¹Ÿå¯ä»¥ç”±å…¶ä»–æœºå™¨ä¸Šçš„Kyuubiå®ä¾‹åˆ›å»ºå¹¶å…±äº«å‡ºæ¥ã€‚ Kyuubiå¯ä»¥åˆ›å»ºå’Œæ‰˜ç®¡å¤šä¸ªSparkContextså®ä¾‹ï¼Œå®ƒä»¬æœ‰è‡ªå·±çš„ç”Ÿå‘½å‘¨æœŸï¼Œä¸€å®šæ¡ä»¶ä¸‹ä¼šè¢«è‡ªåŠ¨åˆ›å»ºå’Œå›æ”¶ï¼Œå¦‚æœä¸€æ®µæ—¶é—´æ²¡æœ‰ä»»åŠ¡è´Ÿè½½ï¼Œèµ„æºä¼šå…¨éƒ¨é‡Šæ”¾ã€‚SparkContextçš„çŠ¶æ€ä¸å—Kyuubiè¿›ç¨‹æ•…éšœè½¬ç§»çš„å½±å“ã€‚ Kyuubiæ”¯æŒä¸åŒå…±äº«çº§åˆ«çš„å¼•æ“å…±äº«ã€‚å¦‚æœè®¾ç½®äº†USERçº§åˆ«çš„share.levelï¼ŒåŒä¸€ç”¨æˆ·ä¸Kyuubiå»ºç«‹çš„å¤šä¸ªè¿æ¥ä¼šå¤ç”¨åŒä¸€ä¸ªEngineï¼Œå®ç°ç”¨æˆ·çº§åˆ«çš„èµ„æºéš”ç¦»ã€‚ Kyuubièµ„æºéš”ç¦»å…±äº«çº§åˆ« å…±äº«çº§åˆ« å‚æ•° å›¾è§£ è¯´æ˜ CONNECTION kyuubi.engine.share.level=CONNECTION æ¯ä¸ªè¿æ¥éƒ½åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„Engineï¼Œè¿æ¥åˆ›å»ºå³ç”³è¯·Engineï¼Œè¿æ¥å…³é—­å³é‡Šæ”¾Engine USER kyuubi.engine.share.level=USER åŒä¸€ç”¨æˆ·çš„å¤šä¸ªè¿æ¥å…±äº«ä¸€ä¸ªEngineï¼Œä¸€ä¸ªç”¨æˆ·å¯¹åº”ä¸€ä¸ªEngineï¼Œç”¨æˆ·è¿æ¥å…³é—­åä¸ä¼šç«‹åˆ»é‡Šæ”¾Engineï¼Œåœ¨æ— æ“ä½œè¾¾åˆ°TTLåé‡Šæ”¾Engine GROUP kyuubi.engine.share.level=GROUP å±äºç›¸åŒç»„çš„æ‰€æœ‰ç”¨æˆ·åˆ›å»ºçš„æ‰€æœ‰è¿æ¥å…±äº«åŒä¸€ä¸ªEngineï¼Œä»¥ç»„åä½œä¸ºå¯åŠ¨Engineçš„ç”¨æˆ·åï¼Œæ•°æ®æƒé™æŒ‰ç»„è¿›è¡Œç®¡ç†ï¼Œå¦‚æœç»„åä¸å­˜åœ¨ï¼Œå…±äº«çº§åˆ«é™çº§ä¸ºUSERï¼Œç”¨æˆ·ç»„éµå¾ªHadoop Groups Mappingï¼Œå¯ä»¥é€šè¿‡é…ç½®æŠŠä¸åŒç”¨æˆ·æ˜ å°„åˆ°ä¸€ä¸ªç»„ã€‚ç›¸æ¯”USERçº§åˆ«ç»™æ¯ä¸ªç”¨æˆ·éƒ½åˆ›å»ºå¼•æ“ï¼ŒGROUPçº§åˆ«å¯ä»¥å‡å°‘å¼•æ“å®ä¾‹æ•°ï¼ŒèŠ‚çº¦èµ„æºï¼Œä½†å¼•æ“æ˜¯å…±äº«çš„ï¼ŒåŒç»„æ‰€æœ‰ç”¨æˆ·éƒ½å¤ç”¨è¿™ä¸ªå¼•æ“ï¼Œè®¿é—®æƒé™æ§åˆ¶è‹¥è¦åšåˆ°ç»†ç²’åº¦ï¼Œåˆ™éœ€è¦ç»“åˆApache Rangerï¼Œèµ„æºæ§åˆ¶çš„ç»†ç²’åº¦éœ€è¦ç»“åˆSparkFairScheduler SERVER kyuubi.engine.share.level=SERVER æ¯ä¸ªKyuubiServerä¸­çš„è¿æ¥å…±ç”¨ä¸€ä¸ªEngineï¼Œç±»ä¼¼åŸç”ŸThriftServerçš„é«˜å¯ç”¨ç‰ˆæœ¬ ä¸€ä¸ªKyuubiServerä¸­å¯ä»¥æ··ç”¨å¤šç§éš”ç¦»çº§åˆ«ã€‚ æ¯”å¦‚æ­£å¸¸æƒ…å†µä¸‹å¼•æ“å…±äº«çº§åˆ«è®¾ç½®ä¸ºGROUPï¼ŒåŒä¸€ä¸ªç»„ä¸‹çš„ç”¨æˆ·åªèƒ½ç”³è¯·ä¸€ä¸ªå¼•æ“ï¼›å½“ç»„é‡Œç”¨æˆ·å¤ªå¤šæ—¶ï¼Œå•ä¸ªå¼•æ“ä¹Ÿä¼šå‡ºç°å¹¶å‘ç“¶é¢ˆå’Œèµ„æºæŠ¢å ï¼Œé’ˆå¯¹è¿™ç§é—®é¢˜ï¼ŒKyuubiä¸­å¼•å…¥äº†Subdomainçš„æ¦‚å¿µï¼Œå¼•æ“å…±äº«å­åŸŸï¼ˆkyuubi.engine.share.level.subdomainï¼‰æ˜¯å¯¹å¼•æ“èµ„æºéš”ç¦»å…±äº«çº§åˆ«çš„è¡¥å……ï¼Œèƒ½å®ç°åŒä¸€ä¸ªç”¨æˆ·ã€ç»„åˆ›å»ºå¤šä¸ªå¼•æ“ã€‚Kyuubiçš„JDBCè¿æ¥ä¸²æ¨¡æ¿ï¼šjdbc:hive2://kyuubi-server-ip:10009/default;?conf1=val1;conf2=var2;â€¦;confN=varNKyuubiçš„JDBCè¿æ¥ä¸²ç¤ºä¾‹ï¼šjdbc:hive2://kyuubi-server-ip:10009/default;?spark.driver.memory=5G;spark.app.name=qjj_kyuubi_applicationSubdomainçš„ä½¿ç”¨ï¼š beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?spark.app.name=qjj_kyuubi_sd1;spark.driver.memory=4G;kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd1&quot; -nq00885 -p****** beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?spark.app.name=qjj_kyuubi_sd2;spark.driver.memory=2G;kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd2&quot; -nq00885 -p****** å¯ä»¥çœ‹åˆ°å•ä¸ªç”¨æˆ·å¯åŠ¨äº†ä¸¤ä¸ªEngineå¦‚æœæˆ‘æƒ³åˆ›å»ºä¸€ä¸ªè¿æ¥å¤ç”¨ä¹‹å‰çš„sd2è¿™ä¸ªSubdomainï¼Œå°±å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡å®šSubdomainçš„æ–¹å¼è¿›è¡ŒæŒ‡å®šã€‚ beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd2&quot; -nq00885 -p****** å‚è€ƒï¼šKyuubi Engine Share Level Kyuubi HAKyuubiåŸºäºZKå®ç°é«˜å¯ç”¨å’Œè´Ÿè½½å‡è¡¡ï¼šKyuubiServerå¯åŠ¨ä¼šåˆ°ZKæ³¨å†ŒèŠ‚ç‚¹ï¼Œå®ç°KyuubiServerä¹‹é—´è´Ÿè½½å‡è¡¡å’Œé«˜å¯ç”¨æ¯ä¸ªç”¨æˆ·ç™»å½•é»˜è®¤æ˜¯defaultå­åŸŸï¼Œæ¯ä¸ªå­åŸŸæ³¨å†Œä¸€ä¸ªæ°¸ä¹…èŠ‚ç‚¹ï¼Œå­åŸŸä¸‹é¢ç”³è¯·çš„Engineä¼šæ³¨å†Œä¸´æ—¶èŠ‚ç‚¹ï¼Œå°†Engineä¿¡æ¯å†™å…¥ZKã€‚æ­¤å¤–è¿˜é€šè¿‡ZKå­˜æ”¾ä¸€äº›ç”¨æˆ·çš„é”å’Œç§Ÿçº¦ä¿¡æ¯ã€‚ Kyuubiç›‘æ§Kyuubiæœ¬èº«æ”¯æŒç›‘æ§ï¼Œé…ç½®æ–¹æ³•å‚è€ƒï¼šMonitoring Kyuubi - Server Metrics éƒ¨ç½²Kyuubi On CDH6.3.2Spark 3.2.2 On CDH6.3.2ç¼–è¯‘ä¸éƒ¨ç½²æºç å‡†å¤‡ # Windowsä¸‹è¿›å…¥wslç¯å¢ƒï¼ˆWindowsçš„Linuxå­ç³»ç»Ÿï¼‰ wsl # ä¸‹è½½Sparkæºç  git clone https://github.com/apache/spark.git # åˆ‡æ¢åˆ°spark3.2åˆ†æ”¯å¹¶åˆ›å»ºæ–°åˆ†æ”¯branch-3.2-cdh6.3.2 cd spark git checkout branch-3.2 git checkout -b branch-3.2-cdh6.3.2 pomä¿®æ”¹ &lt;!-- å¢åŠ Clouderaæº --&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;name&gt;Cloudera Repositories&lt;/name&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;pluginRepository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;Cloudera Repositories&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;!-- å¢åŠ hadoop3 profile --&gt; &lt;profile&gt; &lt;id&gt;hadoop-3.0&lt;/id&gt; &lt;properties&gt; &lt;hadoop.version&gt;3.0.0-cdh6.3.2&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;/profile&gt; ç¼–è¯‘ # è½¬æ¢/r/nä¸ºunixç³»ç»Ÿå¯æ­£å¸¸è¿è¡Œçš„/n (CRLFè½¬LF) sudo apt install dos2unix dos2unix ./dev/*.sh dos2unix ./build/* # ç¼–è¯‘ çœ‹èƒ½å¦ç¼–è¯‘é€šè¿‡ï¼š mvn -Pyarn -Dhadoop.version=3.0.0-cdh6.3.2 -Phadoop-3.0 -Phive-thriftserver -DskipTests clean package --settings &quot;/mnt/d/Applications/apache-maven-3.6.3/conf/settings.xml&quot; -Dmaven.repo.local=&quot;/mnt/e/Maven/Repository&quot; # ç¼–è¯‘&amp;æ‰“åŒ… å°†æºç ç¼–è¯‘ä¸ºbinaryåŒ… ç”Ÿæˆspark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2.tgzå®‰è£…åŒ…ï¼š ./dev/make-distribution.sh --name hadoop-3.0.0-cdh6.3.2 --tgz -Phadoop-3.0 -Pyarn -Phive-thriftserver -DskipTests --settings &quot;/mnt/d/Applications/apache-maven-3.6.3/conf/settings.xml&quot; -Dmaven.repo.local=&quot;/mnt/e/Maven/Repository&quot; éƒ¨ç½² tar -zxvf spark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2.tgz -C ../../ mv spark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2 spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2 cd spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2 cd conf cp spark-defaults.conf.template spark-defaults.conf cp spark-env.sh.template spark-env.sh # ä¿®æ”¹spark-defaults.conf (å‚æ•°ç”Ÿæ•ˆä¼˜å…ˆçº§: SparkConf &gt; spark-submit Flags &gt; spark-defaults.conf) vim spark-defaults.conf ## Javaè®¾ç½® spark.executorEnv.JAVA_HOME /usr/java/jdk1.8.0_181 spark.yarn.appMasterEnv.JAVA_HOME /usr/java/jdk1.8.0_181 ## å¼€å¯eventLogç”¨äºé‡æ„å†å²å·²å®Œæˆä»»åŠ¡çš„WebUI spark.eventLog.enabled true spark.eventLog.dir hdfs:///user/spark/applicationHistory spark.eventLog.compress true spark.driver.log.dfsDir /user/spark/driverLogs ï¼ˆæŒä¹…åŒ–driveræ—¥å¿—çš„è·¯å¾„ï¼‰ spark.driver.log.persistToDfs.enabled true ï¼ˆæŒä¹…åŒ–driveræ—¥å¿—ï¼‰ spark.history.fs.cleaner.enabled true (å®šæœŸè‡ªåŠ¨æ¸…ç†æ—¥å¿—ç›®å½•ï¼Œé»˜è®¤ä¸€å¤©æ¸…ç†ä¸€æ¬¡ï¼Œæ¸…ç†7å¤©å‰çš„æ—¥å¿—æ–‡ä»¶) spark.history.fs.logDirectory hdfs:///user/spark/applicationHistory spark.history.ui.port 18080 ï¼ˆè®¿é—®Sparkåº”ç”¨å†å²è®°å½•http://historyServerHost:18080/ï¼‰ spark.history.retainedApplications 30 ï¼ˆç¼“å­˜ä¸­ä¿å­˜çš„åº”ç”¨å†å²è®°å½•ä¸ªæ•°ï¼Œè¶…è¿‡ä¼šå°†æ—§çš„åˆ é™¤ï¼Œè¯»æ›´æ—©çš„æ—¥å¿—å»ç£ç›˜è¯»ä¼šæ…¢äº›ï¼‰ spark.yarn.historyServer.address http://cdh101:18080 ï¼ˆYarn Applicationé¡µé¢Tracking URLé“¾æ¥å¯ä»¥ç›´æ¥è¿›å…¥HistoryServeræŸ¥çœ‹æ—¥å¿—ï¼‰ spark.yarn.historyServer.allowTracking true ï¼ˆYarn Applicationé¡µé¢Tracking URLé“¾æ¥å¯ä»¥ç›´æ¥è¿›å…¥HistoryServeræŸ¥çœ‹æ—¥å¿—ï¼‰ ## local.dirè®¾ç½®ä¸ºæ•°æ®ç›˜ï¼Œé¿å…ä½¿ç”¨ç³»ç»Ÿåˆ†åŒº spark.local.dir /tmp/spark_temp_data ## ä¼˜åŒ–è®¾ç½® spark.kryoserializer.buffer.max 512m spark.serializer org.apache.spark.serializer.KryoSerializer spark.authenticate false ï¼ˆå…³é—­æ•°æ®å—ä¼ è¾“æœåŠ¡SASLåŠ å¯†è®¤è¯ï¼‰ spark.io.encryption.enabled false ï¼ˆå…³é—­I/OåŠ å¯†ï¼‰ spark.network.crypto.enabled false ï¼ˆå…³é—­åŸºäºAESç®—æ³•çš„RPCåŠ å¯†ï¼‰ spark.shuffle.service.enabled true ï¼ˆå¯ç”¨å¤–éƒ¨ShuffleServiceæé«˜Shuffleç¨³å®šæ€§ï¼‰ spark.shuffle.service.port 7337 ï¼ˆè¿™ä¸ªå¤–éƒ¨ShuffleServiceç”±YarnNodeManageræä¾›ï¼Œé»˜è®¤ç«¯å£7337ï¼‰ spark.shuffle.useOldFetchProtocol true ï¼ˆå…¼å®¹æ—§çš„Shuffleåè®®é¿å…æŠ¥é”™ï¼‰ spark.sql.cbo.enabled true (å¯ç”¨CBOåŸºäºä»£ä»·çš„ä¼˜åŒ–-ä»£æ›¿RBOåŸºäºè§„åˆ™çš„ä¼˜åŒ–-Optimizer) spark.sql.cbo.starSchemaDetection true ï¼ˆæ˜Ÿå‹æ¨¡å‹æ¢æµ‹ï¼Œåˆ¤æ–­åˆ—æ˜¯å¦æ˜¯è¡¨çš„ä¸»é”®ï¼‰ spark.sql.datetime.java8API.enabled false spark.sql.sources.partitionOverwriteMode dynamic spark.sql.orc.mergeSchema true ï¼ˆORCæ ¼å¼SchemaåŠ è½½æ—¶ä»æ‰€æœ‰æ•°æ®æ–‡ä»¶æ”¶é›†ï¼‰ spark.sql.parquet.mergeSchema false (æ ¹æ®æƒ…å†µè®¾ç½®ï¼Œæˆ‘ä»¬é›†ç¾¤å¤§å¤šæ•°éƒ½æ˜¯parquetï¼Œä»æ‰€æœ‰æ–‡ä»¶æ”¶é›†Schemaä¼šå½±å“æ€§èƒ½ï¼Œæ‰€ä»¥ä»éšæœºä¸€ä¸ªParquetæ–‡ä»¶æ”¶é›†Schema) spark.sql.parquet.writeLegacyFormat true ï¼ˆå…¼å®¹æ—§é›†ç¾¤ï¼‰ spark.sql.autoBroadcastJoinThreshold 1048576 ï¼ˆå½“å‰ä»…æ”¯æŒè¿è¡Œäº†ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscançš„Hive Metastoreè¡¨ï¼Œä»¥åŠç›´æ¥åœ¨æ•°æ®æ–‡ä»¶ä¸Šè®¡ç®—ç»Ÿè®¡ä¿¡æ¯çš„åŸºäºæ–‡ä»¶çš„æ•°æ®æºè¡¨ï¼‰ spark.sql.adaptive.enabled true ï¼ˆSpark AQE[adaptive query execution]å¯ç”¨ï¼ŒAQEçš„ä¼˜åŠ¿ï¼šæ‰§è¡Œè®¡åˆ’å¯åŠ¨æ€è°ƒæ•´ã€è°ƒæ•´çš„ä¾æ®æ˜¯ä¸­é—´ç»“æœçš„ç²¾ç¡®ç»Ÿè®¡ä¿¡æ¯ï¼‰ spark.sql.adaptive.forceApply false spark.sql.adaptive.logLevel info spark.sql.adaptive.advisoryPartitionSizeInBytes 256m ï¼ˆå€¾æ–œæ•°æ®åˆ†åŒºæ‹†åˆ†ï¼Œå°æ•°æ®åˆ†åŒºåˆå¹¶ä¼˜åŒ–æ—¶ï¼Œå»ºè®®çš„åˆ†åŒºå¤§å°ï¼Œä¸spark.sql.adaptive.shuffle.targetPostShuffleInputSizeå«ä¹‰ç›¸åŒï¼‰ spark.sql.adaptive.coalescePartitions.enabled true ï¼ˆæ˜¯å¦å¼€å¯åˆå¹¶å°æ•°æ®åˆ†åŒºé»˜è®¤å¼€å¯ï¼Œè°ƒä¼˜ç­–ç•¥ä¹‹ä¸€ï¼‰ spark.sql.adaptive.coalescePartitions.minPartitionSize 1m ï¼ˆåˆå¹¶åæœ€å°çš„åˆ†åŒºå¤§å°ï¼‰ spark.sql.adaptive.coalescePartitions.initialPartitionNum 1024 ï¼ˆåˆå¹¶å‰çš„åˆå§‹åˆ†åŒºæ•°ï¼‰ spark.sql.adaptive.fetchShuffleBlocksInBatch true ï¼ˆæ˜¯å¦æ‰¹é‡æ‹‰å–blocks,è€Œä¸æ˜¯ä¸€ä¸ªä¸ªçš„å»å–ï¼Œç»™åŒä¸€ä¸ªmapä»»åŠ¡ä¸€æ¬¡æ€§æ‰¹é‡æ‹‰å–blockså¯ä»¥å‡å°‘io æé«˜æ€§èƒ½ï¼‰ spark.sql.adaptive.localShuffleReader.enabled true ï¼ˆä¸éœ€è¦Shuffleæ“ä½œæ—¶ï¼Œä½¿ç”¨LocalShuffleReaderï¼Œä¾‹å¦‚å°†SortMergeJoinè½¬ä¸ºBrocastJoinï¼‰ spark.sql.adaptive.skewJoin.enabled true ï¼ˆSparkä¼šé€šè¿‡æ‹†åˆ†çš„æ–¹å¼è‡ªåŠ¨å¤„ç†Joinè¿‡ç¨‹ä¸­æœ‰æ•°æ®å€¾æ–œçš„åˆ†åŒºï¼‰ spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes 128m spark.sql.adaptive.skewJoin.skewedPartitionFactor 5 ï¼ˆåˆ¤æ–­å€¾æ–œçš„æ¡ä»¶ï¼šåˆ†åŒºå¤§å°å¤§äºæ‰€æœ‰åˆ†åŒºå¤§å°ä¸­ä½æ•°çš„5å€ï¼Œä¸”å¤§äºspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytesçš„å€¼ï¼‰ ## é»˜è®¤åº”ç”¨èµ„æºè®¾ç½® spark.driver.memory 2G spark.executor.cores 4 spark.executor.memory 4G spark.executor.memoryOverhead 2G spark.memory.offHeap.enabled true spark.memory.offHeap.size 2G ## åŠ¨æ€èµ„æºè®¾ç½® å…·ä½“é€»è¾‘è§ExecutorAllocationManagerè¿™ä¸ªç±» spark.dynamicAllocation.enabled true spark.dynamicAllocation.executorIdleTimeout 60 ï¼ˆexecutoré—²ç½®æ—¶é—´ï¼Œå¦‚æœæŸexecutorç©ºé—²è¶…è¿‡60sï¼Œåˆ™removeæ­¤executorï¼‰ spark.dynamicAllocation.minExecutors 0 spark.dynamicAllocation.schedulerBacklogTimeout 5s ï¼ˆå¦‚æœæœ‰pending taskå¹¶ä¸”ç­‰å¾…äº†5sï¼Œåˆ™ç”³è¯·å¢åŠ executorï¼‰ spark.dynamicAllocation.cachedExecutorIdleTimeout 600 ï¼ˆcacheé—²ç½®æ—¶é—´ï¼Œè¶…è¿‡æ­¤æ—¶é—´ï¼Œå¯é‡Šæ”¾cacheæ‰€åœ¨çš„executorï¼‰ ## å…¶ä»–è®¾ç½® spark.driver.extraLibraryPath /opt/cloudera/parcels/CDH/lib/hadoop/lib/native spark.executor.extraLibraryPath /opt/cloudera/parcels/CDH/lib/hadoop/lib/native spark.yarn.am.extraLibraryPath /opt/cloudera/parcels/CDH/lib/hadoop/lib/native spark.ui.enabled true spark.ui.killEnabled true spark.master yarn spark.sql.hive.metastore.version 2.1.1 spark.sql.hive.metastore.jars /opt/cloudera/parcels/CDH/lib/hive/lib/* # ä¿®æ”¹spark-env.sh vim spark-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_181 HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_DIST_CLASSPATH=$(/opt/cloudera/parcels/CDH/bin/hadoop classpath) export SPARK_LOCAL_DIRS=/tmp/spark_temp_data # è½¯è¿æ¥hiveå’Œhdfsã€yarné…ç½®ï¼š ln -s /etc/hadoop/conf/core-site.xml core-site.xml ln -s /etc/hbase/conf/hbase-site.xml hbase-site.xml ln -s /etc/hadoop/conf/hdfs-site.xml hdfs-site.xml ln -s /etc/hive/conf/hive-site.xml hive-site.xml ln -s /etc/hadoop/conf/mapred-site.xml mapred-site.xml ln -s /etc/hadoop/conf/yarn-site.xml yarn-site.xml # å°†CRLFè½¬LFä»¥ä¿è¯è¿è¡Œæ­£å¸¸ cd spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2 sudo yum -y install dos2unix dos2unix bin/* dos2unix sbin/* dos2unix conf/* dos2unix python/* # è¿è¡ŒSparkHistoryServer (æ³¨æ„è¯¥è¿›ç¨‹ä¼šäº§ç”Ÿæ—¥å¿—ï¼Œä¸ºé¿å…å ç”¨ç³»ç»Ÿåˆ†åŒºç©ºé—´ï¼Œå°½é‡å°†$SPARK_HOME/logsè½¯è¿æ¥åˆ°æ•°æ®ç›˜) sbin/start-history-server.sh # è¿è¡ŒSparkSQL on Yarn bin/spark-sql --master yarn è‡³æ­¤Spark3.2.2 On CDH6.3.2ç¼–è¯‘éƒ¨ç½²å®Œæ¯•æ³¨æ„Yarnå¤–éƒ¨ShuffleServiceä¸€å®šç¡®ä¿å¼€å¯ Kyuubi On Spark3åŸºç¡€éƒ¨ç½²æ›´å¤šé…ç½®å‚è€ƒï¼šKyuubi-Deployment-Settingså®‰è£…ä¸é…ç½®Kyuubi wget https://www.apache.org/dyn/closer.lua/incubator/kyuubi/kyuubi-1.5.1-incubating/apache-kyuubi-1.5.1-incubating-bin.tgz tar -zxvf apache-kyuubi-1.5.1-incubating-bin.tgz -C /opt/modules/ # è®¾ç½®ç¯å¢ƒå˜é‡ vim /etc/profile # KYUUBI_HOME export KYUUBI_HOME=/opt/modules/apache-kyuubi-1.5.1-incubating-bin # é…ç½®æ–‡ä»¶ä¿®æ”¹ cd apache-kyuubi-1.5.1-incubating-bin cd conf cp kyuubi-env.sh.template kyuubi-env.sh;cp kyuubi-defaults.conf.template kyuubi-defaults.conf;cp log4j2.properties.template log4j2.properties # ä¿®æ”¹kyuubi-env.sh vim kyuubi-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_181 export SPARK_HOME=/opt/modules/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2 export HADOOP_CONF_DIR=/etc/hive/conf export KYUUBI_JAVA_OPTS=&quot;-Xmx4g -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4096 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:+UseCondCardMark -XX:MaxDirectMemorySize=1024m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./logs -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:./logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=5M -XX:NewRatio=3 -XX:MetaspaceSize=512m&quot; # ä¿®æ”¹kyuubi-defaults.conf ï¼ˆç”±äºæˆ‘ä¹‹å‰çš„Sparkå®‰è£…ä¸­å·²ç»é…ç½®äº†hive-siteç­‰é…ç½®ï¼Œè¿™é‡Œä¸éœ€è¦æŒ‡å®šhiveç›¸å…³é…ç½®äº†ï¼Œæ­£å¸¸è¿™é‡Œæ˜¯å¯ä»¥æŒ‡å®šhiveé…ç½®çš„ï¼Œå‚è€ƒhttps://kyuubi.apache.org/docs/latest/deployment/hive_metastore.htmlï¼‰ vim kyuubi-defaults.conf spark.master=yarn kyuubi.ha.zookeeper.acl.enabled=true kyuubi.ha.zookeeper.quorum=cdh101:2181,cdh102:2181,cdh103:2181 kyuubi.engine.share.level=USER kyuubi.session.engine.idle.timeout=PT1H spark.dynamicAllocation.enabled=true spark.dynamicAllocation.minExecutors=1 spark.dynamicAllocation.maxExecutors=10 spark.dynamicAllocation.executorIdleTimeout=120 å¯åŠ¨ä¸è¿æ¥Kyuubi # å¯åŠ¨Kyuubi Server bin/kyuubi start # ä½¿ç”¨hiveç”¨æˆ· è¿æ¥Kyuubi beeline -u jdbc:hive2://10.2.5.101:10009 -n hive show databases # è¯¥å‘½ä»¤ç›´æ¥è§¦å‘Sparkå¼•æ“åˆå§‹åŒ– è‡³æ­¤KyuubiåŸºç¡€é…ç½®å®ŒæˆKyuubiç”³è¯·åˆ°Sparkå¼•æ“åï¼Œé»˜è®¤ç©ºé—²30minåè‡ªåŠ¨å›æ”¶ã€‚ Kyuubiç”Ÿäº§ç¯å¢ƒçš„é«˜çº§é…ç½®åœ¨ä¸Šé¢åŸºç¡€é…ç½®çš„åŸºç¡€ä¸Šå¢åŠ ç”Ÿäº§ç¯å¢ƒæ‰€éœ€çš„é«˜çº§é…ç½®ï¼ŒåŒ…æ‹¬å®‰å…¨æ€§é…ç½®ï¼Œç”¨æˆ·çš„é…ç½®ï¼Œæˆæƒé…ç½®ç­‰ã€‚Kerberosè®¤è¯ kyuubi.kinit.keytab=/hadoop/bigdata/kerberos/keytab/hive.keytab kyuubi.kinit.principal=hive/xxx@XXX.COM é‡‡ç”¨LDAPè®¤è¯ ä½¿ç”¨LDAPè®¤è¯ç™»é™†Kyuubi kyuubi.authentication=LDAP kyuubi.authentication.ldap.base.dn=ou=People,dc=xxx,dc=xxx,dc=xxx kyuubi.authentication.ldap.guidKey=uid #kyuubi.authentication.ldap.domain=xxx.xxx.com kyuubi.authentication.ldap.url=ldap://ldap_addr:389 ä½¿ç”¨q00885ç”¨æˆ·ç™»é™†ï¼Œæ‰§è¡ŒsqlæŸ¥è¯¢ï¼Œåå°ä¼šä»¥q00885ç”³è¯·ä¸€ä¸ªSparkApplicationã€‚æŸ¥è¯¢æ—¶ï¼Œæ•°æ®è®¿é—®ã€å…ƒæ•°æ®è®¿é—®éƒ½ä½¿ç”¨è¿™ä¸ªç”¨æˆ·ï¼Œè¦ç¡®ä¿è¿™ä¸ªç”¨æˆ·æœ‰HDFSä¸ŠACLæƒé™(hdfs dfs -getfaclæŸ¥çœ‹)ã€‚è¿˜è¦ç¡®ä¿Linuxä¸Šæœ‰è¯¥ç”¨æˆ·ï¼Œå¦åˆ™å¼•æ“æ— æ³•ç”³è¯·æˆåŠŸã€‚å¦‚æœæ²¡æœ‰HDFSä¸Šçš„ACLæƒé™ï¼Œå¯ä»¥é€šè¿‡setfaclè®¾ç½®ACL,æˆ–è€…é€šè¿‡hiveçš„grantå‘½ä»¤é’ˆå¯¹ç»„æ‰¹é‡æˆæƒã€‚ Scala+SQLæ··åˆä½¿ç”¨beeline -u jdbc:hive2://kyuubi-server-ip:10009/default -n q00885 -p xxx ç™»é™†åé»˜è®¤æ˜¯SQLæ¨¡å¼ CREATE TEMPORARY VIEW qjj_view as select * from qjj_test; set kyuubi.operation.language=scala; val df = spark.table(&quot;qjj_view&quot;).where(&quot;id &gt; 2 and id &lt; 5&quot;); df.registerTempTable(&quot;qjj_view1&quot;) spark.sql(&quot;set kyuubi.operation.language=sql&quot;); select count(1),min(id),max(id) from qjj_view1; é›†æˆKuduKyuubi On Kudu Kyuubiçš„æˆæƒï¼šKyuuibå½“å‰æ”¯æŒä¸‰ç§æˆæƒï¼š1.åŸºäºå­˜å‚¨å±‚é¢çš„æˆæƒ(ä»¥ä¸Šæˆ‘ä»¬ä½¿ç”¨åˆ°çš„æˆæƒæ–¹å¼)2.åŸºäºSQLæ ‡å‡†çš„æˆæƒç±»ä¼¼HiveServer2(åŸºäºSubmarine:Spark Securityå¤–éƒ¨æ’ä»¶)3.åŸºäºRanger(å®˜ç½‘æ¨èï¼Œä¹Ÿæ˜¯åŸºäºSubmarine Sparkï¼Œåªæ˜¯é€šè¿‡Spark-Rangeræ¥å®ç°æ›´ç»†ç²’åº¦çš„è®¿é—®æˆæƒ) é—®é¢˜ä¸å¼‚å¸¸å¤„ç†Kyuubi Trouble Shooting æ‰§è¡Œspark sqlåä¸€ç›´å¡ä½ï¼Œåå°æŠ¥é”™User: root is not allowed to impersonate anonymous Error: org.apache.kyuubi.KyuubiSQLException: Timeout(180000 ms) to launched SPARK_SQL engine with /opt/modules/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2/bin/spark-submit \\ --class org.apache.kyuubi.engine.spark.SparkSQLEngine \\ --conf spark.kyuubi.ha.zookeeper.quorum=cdh101:2181,cdh102:2181,cdh103:2181 \\ --conf spark.kyuubi.client.ip=10.2.5.101 \\ --conf spark.hive.query.redaction.rules=/etc/alternatives/hive-conf/redaction-rules.json \\ --conf spark.kyuubi.engine.submit.time=1651991699800 \\ --conf spark.app.name=kyuubi_USER_SPARK_SQL_anonymous_default_a0c93d16-2718-4791-8205-97fbc35e652a \\ --conf spark.kyuubi.ha.zookeeper.acl.enabled=true \\ --conf spark.kyuubi.ha.engine.ref.id=a0c93d16-2718-4791-8205-97fbc35e652a \\ --conf spark.kyuubi.ha.zookeeper.auth.type=NONE \\ --conf spark.master=yarn \\ --conf spark.yarn.tags=KYUUBI \\ --conf spark.kyuubi.ha.zookeeper.namespace=/kyuubi_1.5.1-incubating_USER_SPARK_SQL/anonymous/default \\ --conf spark.hive.exec.query.redactor.hooks=org.cloudera.hadoop.hive.ql.hooks.QueryRedactor \\ --proxy-user anonymous /opt/modules/apache-kyuubi-1.5.1-incubating-bin/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.5.1-incubating.jar. (state=,code=0) ...... 22/05/08 14:37:17 INFO retry.RetryInvocationHandler: org.apache.hadoop.security.authorize.AuthorizationException: User: root is not allowed to impersonate anonymous, while invoking ApplicationClientProtocolPBClientImpl.getClusterMetrics over null after 5 failover attempts. Trying to failover after sleeping for 37639ms. è§£å†³ï¼šé¿å…ä½¿ç”¨rootç”¨æˆ·å¯åŠ¨Kyuubi Serverã€‚å¯ä½¿ç”¨hiveã€hdfsç”¨æˆ·å¯åŠ¨ï¼Œæˆ–å•ç‹¬å»ºç«‹ä¸€ä¸ªkyuubiç”¨æˆ·å¯åŠ¨KyuubiServerã€‚ ç”¨æˆ·æ— ç›®å½•ä»¥åŠNMèŠ‚ç‚¹æ²¡ç”¨æˆ·å¯¼è‡´å¼•æ“æ— æ³•è¿è¡Œ Caused by: org.apache.kyuubi.KyuubiSQLException: Timeout(180000 ms) to launched SPARK_SQL engine with /data3/bigdata/spark/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2/bin/spark-submit \\ --class org.apache.kyuubi.engine.spark.SparkSQLEngine \\ --conf spark.kyuubi.authentication.ldap.url=ldap://xxx.xx.xxx.xx \\ --conf spark.kyuubi.ha.zookeeper.quorum=zk1:2181,zk2:2181,zk3:2181 \\ --conf spark.kyuubi.client.ip=xxx.xx.xxx.xx \\ --conf spark.kyuubi.kinit.principal=hive/hive02.c6.com@XXX.COM \\ --conf spark.kyuubi.engine.submit.time=1652671391562 \\ --conf spark.app.name=kyuubi_USER_SPARK_SQL_k00877_default_ff13fda9-1a01-4322-8d40-d3bc098d78e4 \\ --conf spark.kyuubi.ha.zookeeper.acl.enabled=true \\ --conf spark.kyuubi.ha.engine.ref.id=ff13fda9-1a01-4322-8d40-d3bc098d78e4 \\ --conf spark.master=yarn \\ --conf spark.yarn.tags=KYUUBI \\ --conf spark.kyuubi.ha.zookeeper.namespace=/kyuubi_1.5.1-incubating_USER_SPARK_SQL/k00877/default \\ --conf spark.kyuubi.kinit.keytab=/hadoop/bigdata/kerberos/keytab/hiveserver2_hive02_c6.keytab \\ --conf spark.kyuubi.authentication.ldap.domain=smyoa.com \\ --proxy-user k00877 /data3/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.5.1-incubating.jar. at org.apache.kyuubi.KyuubiSQLException$.apply(KyuubiSQLException.scala:69) ~[kyuubi-common_2.12-1.5.1-incubating.jar:1.5.1-incubating] ...... Caused by: org.apache.kyuubi.KyuubiSQLException: org.apache.hadoop.security.AccessControlException: Permission denied: user=k00877, access=WRITE, inode=&quot;/user&quot;:hdfs:supergroup:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:400) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:256) at org.apache.sentry.hdfs.SentryINodeAttributesProvider$SentryPermissionEnforcer.checkPermission(SentryINodeAttributesProvider.java:86) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:194) å°è¯•åœ¨HDFSä¸Šåˆ›å»ºå¯¹åº”ç”¨æˆ·ç›®å½• (è¿™é‡Œä¹Ÿå¯ä»¥ä¿®æ”¹Sparkåœ¨æ–‡ä»¶ç³»ç»Ÿä¸­å½“å‰ç”¨æˆ·çš„ä¸»ç›®å½•-æäº¤åº”ç”¨çš„ç¼“å­˜ç›®å½•ï¼šspark.yarn.stagingDir) hdfs dfs -mkdir /user/k00877/ hdfs dfs -chown k00877:k00877 /user/k00877/ å†æ¬¡å°è¯•åˆ›å»ºå¼•æ“ï¼ŒæŠ¥é”™å¦‚ä¸‹ # Kyuubi Server errorï¼š Caused by: org.apache.kyuubi.KyuubiSQLException: org.apache.spark.SparkException: Application application_1637826239096_34377 failed 2 times due to AM Container for appattempt_1637826239096_34377_000002 exited with exitCode: -1000 See more: /hadoop/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/work/k00877/kyuubi-spark-sql-engine.log.4 at org.apache.kyuubi.KyuubiSQLException$.apply(KyuubiSQLException.scala:69) ~[kyuubi-common_2.12-1.5.1-incubating.jar:1.5.1-incubating] at org.apache.kyuubi.engine.ProcBuilder.$anonfun$start$1(ProcBuilder.scala:165) ~[kyuubi-server_2.12-1.5.1-incubating.jar:1.5.1-incubating] Engine log: /hadoop/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/work/k00877/kyuubi-spark-sql-engine.log.4 For more detailed output, check the application tracking page: http://xxxxx:8088/cluster/app/application_1637826239096_34377 Then click on links to logs of each attempt. . Failing the application. org.apache.spark.SparkException: Application application_1637826239096_34377 failed 2 times due to AM Container for appattempt_1637826239096_34377_000002 exited with exitCode: -1000 Failing this attempt.Diagnostics: [2022-05-16 13:00:12.276]Application application_1637826239096_34377 initialization failed (exitCode=255) with output: main : command provided 0 main : run as user is k00877 main : requested yarn user is k00877 User k00877 not found ...... åœ¨ä¸€ä¸ªæ²¡æœ‰å¼€å¯Kerberoså®‰å…¨çš„é›†ç¾¤é‡Œï¼Œå¯åŠ¨containerè¿›ç¨‹å¯ä»¥ä½¿ç”¨DefaultContainerExecutoræˆ–LinuxContainerExecutorï¼›ä½†æ˜¯å¯ç”¨äº†Kerberoså®‰å…¨çš„é›†ç¾¤é‡Œï¼Œå¯åŠ¨containerè¿›ç¨‹åªèƒ½ä½¿ç”¨LinuxContainerExecutorï¼Œåœ¨åº•å±‚ä¼šä½¿ç”¨setuidåˆ‡æ¢åˆ°ä¸šåŠ¡ç”¨æˆ·ä»¥å¯åŠ¨containerè¿›ç¨‹ï¼Œæ‰€ä»¥è¦æ±‚æ‰€æœ‰nodemanagerèŠ‚ç‚¹å¿…é¡»æœ‰ä¸šåŠ¡ç”¨æˆ·ã€‚å¯é€‰æ–¹æ¡ˆ: Ldapæ˜¯æ”¯æŒç®¡ç†Linuxç”¨æˆ·çš„,å¯ä»¥ä½œä¸ºLinuxè‡ªå¸¦ç”¨æˆ·çš„æ‰©å±•,å®ç°ä¸ç”¨æ‰‹åŠ¨useraddå°±èƒ½åœ¨å„èŠ‚ç‚¹ä»¥ldapä¸­çš„ç”¨æˆ·æ¨¡æ‹ŸLinuxç”¨æˆ·å¯åŠ¨Container.ä¸´æ—¶è§£å†³ï¼šé¦–å…ˆä¿è¯ç”¨æˆ·ä¸»ç›®å½•æœ‰æƒé™çš„å‰æä¸‹ï¼Œåœ¨å„ä¸ªNodeManagerèŠ‚ç‚¹åˆ›å»ºk00877ç”¨æˆ·ï¼Œåˆ›å»ºåå¯ä»¥çœ‹åˆ°å¼•æ“æ­£å¸¸å¯åŠ¨ ä½¿ç”¨LDAPç™»å½•çš„ç”¨æˆ·æ— HDFSä¸Šè¡¨æ•°æ®çš„è®¿é—®æƒé™åˆ†æï¼šéœ€è¦ç¡®ä¿å½“å‰ç”¨æˆ·çš„æƒé™æˆ–è€…ACLæƒé™æ˜¯READ_EXECUTEå½“å‰ç”¨æˆ·q00885æ²¡æœ‰è¯¥ç›®å½•çš„ä»»ä½•è¯»æƒé™ã€‚è§£å†³æ–¹å¼ï¼š ä½¿ç”¨hiveç”¨æˆ·ç™»å½•HiveServer2ï¼šbeeline -u &quot;jdbc:hive2://kyuubi-server-ip:10000/default&quot; -nhive -pxxxxx æŸ¥çœ‹q00885æ‰€å±è§’è‰² SHOW ROLE GRANT GROUP group q00885; +--------+---------------+-------------+----------+--+ | role | grant_option | grant_time | grantor | +--------+---------------+-------------+----------+--+ | admin | false | 0 | -- | | d_bd | false | 0 | -- | +--------+---------------+-------------+----------+--+ æˆæƒæƒé™ç»™d_bdè§’è‰² grant select on table t_sai_t_model_log to role d_bd; æŸ¥çœ‹d_bdè§’è‰²æœ‰å“ªäº›æƒé™ SHOW GRANT ROLE d_bd; +-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+ | database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor | +-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+ | default | xxxxxxxxxx | | | d_bd | ROLE | SELECT | false | 1629844007000 | -- | | default | t_sai_t_model_log | | | d_bd | ROLE | SELECT | false | 1652777345000 | -- | | default | xxxxxxxxxx | | | d_bd | ROLE | SELECT | false | 1634268085000 | -- | +-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+ å…ˆå›æ”¶æƒé™ï¼Œæµ‹è¯•å¦ä¸€ç§æ–¹æ³•ï¼šè®¾ç½®acl revoke select on table t_sai_t_model_log from role d_bd; ç»™è¡¨æ•°æ®è·¯å¾„å¢åŠ ACLæƒé™ hdfs dfs -setfacl -R -m group:q00885:r-x /user/hive/warehouse/t_sai_t_model_log è®¾ç½®ACLåå†ç”¨getfaclæŸ¥çœ‹ACLåˆ—è¡¨ï¼Œè®¾ç½®æ²¡ç”Ÿæ•ˆï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬é›†ç¾¤ç”¨äº†Sentryç®¡ç†ACLï¼Œç›´æ¥å¯¹ç›®å½•è®¾ç½®ACLä¸ä¼šç”Ÿæ•ˆï¼Œæ‰€ä»¥è¿˜éœ€ä½¿ç”¨hiveçš„grant+revokeæ–¹å¼æˆæƒã€‚ å†æ¬¡ä½¿ç”¨q00885å³å¯æŸ¥è¯¢ã€‚ æƒé™åˆ—è¡¨: ALL SERVER, TABLE, DB, URI, COLLECTION, CONFIG INSERT DB, TABLE SELECT DB, TABLE, COLUMN æˆæƒä¸å›æ”¶ï¼š GRANT ROLE &lt;role name&gt; [, &lt;role name&gt;] TO GROUP &lt;group name&gt; [,GROUP &lt;group name&gt;] GRANT &lt;privilege&gt; [, &lt;privilege&gt; ] ON &lt;object type&gt; &lt;object name&gt; TO ROLE &lt;role name&gt; [,ROLE &lt;role name&gt;] GRANT SELECT &lt;column name&gt; ON TABLE &lt;table name&gt; TO ROLE &lt;role name&gt;; REVOKE ROLE &lt;role name&gt; [, &lt;role name&gt;] FROM GROUP &lt;group name&gt; [,GROUP &lt;group name&gt;] REVOKE &lt;privilege&gt; [, &lt;privilege&gt; ] ON &lt;object type&gt; &lt;object name&gt; FROM ROLE &lt;role name&gt; [,ROLE &lt;role name&gt;] REVOKE SELECT &lt;column name&gt; ON TABLE &lt;table name&gt; FROM ROLE &lt;role name&gt;; å‚è€ƒApache Kyuubi DocumentsApache Kyuubi Deployment SettingsApache Spark Configurationspark-history-server-configuration-optionsSparkSQLçš„è‡ªé€‚åº”æ‰§è¡ŒMigration Guide: SQL, Datasets and DataFrame","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Kyuubi","slug":"Kyuubi","permalink":"https://shmily-qjj.top/tags/Kyuubi/"},{"name":"Spark","slug":"Spark","permalink":"https://shmily-qjj.top/tags/Spark/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"SeaTunnelå¼€æºæ•°æ®åŒæ­¥å¹³å°","slug":"SeaTunnelå¼€æºæ•°æ®åŒæ­¥å¹³å°","date":"2021-12-15T08:30:00.000Z","updated":"2022-12-11T05:35:07.911Z","comments":true,"path":"84534d72/","link":"","permalink":"https://shmily-qjj.top/84534d72/","excerpt":"","text":"SeaTunnelå¼€æºæ•°æ®åŒæ­¥å¹³å°SeaTunnelç®€ä»‹SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.SeaTunnelæ˜¯ä¸€ä¸ªç®€å•æ˜“ç”¨ä¸”é«˜æ•ˆçš„å¼€æºæ•°æ®é›†æˆå¹³å°ï¼ˆå‰èº«æ˜¯WaterDropï¼‰ï¼Œæ”¯æŒç¦»çº¿å’Œå®æ—¶æ•°æ®åŒæ­¥ã€‚æ”¯æŒå¤šç§Sourceã€Outputã€Filterç»„ä»¶ä»¥åŠè‡ªè¡Œå¼€å‘è¾“å…¥è¾“å‡ºæ’ä»¶å’Œè¿‡æ»¤å™¨æ’ä»¶ã€‚SeaTunnelé…ç½®ç®€å•ï¼ŒåŸºäºå·²æœ‰çš„Sparkã€Flinkç¯å¢ƒå‡ åˆ†é’Ÿå°±å¯ä»¥éƒ¨ç½²å®Œæˆã€‚å› å…¶æœ‰å„ç§çµæ´»çš„æ’ä»¶æ”¯æŒï¼Œåªéœ€è¦èŠ±å‡ åˆ†é’Ÿç¼–å†™ä¸€ä¸ªé…ç½®æ–‡ä»¶å³å¯å®Œæˆä¸€ä¸ªæ•°æ®åŒæ­¥ä»»åŠ¡çš„å¼€å‘ã€‚ SeaTunnelæ¶æ„ SeaTunnelç‰¹æ€§ï¼š ç®€å•æ˜“ç”¨ï¼Œé…ç½®çµæ´»ï¼Œä½ä»£ç  æ”¯æŒå®æ—¶æ•°æ®æµå’Œç¦»çº¿æ•°æ®åŒæ­¥ é«˜æ€§èƒ½åˆ†å¸ƒå¼ã€æµ·é‡æ•°æ®å¤„ç†èƒ½åŠ› æ¨¡å—åŒ–ã€æ’ä»¶åŒ–ï¼Œæ˜“äºæ‰©å±• æ”¯æŒé€šè¿‡SQLåšETLæ“ä½œ SeaTunnelæ”¯æŒçš„ç»„ä»¶ï¼šInput pluginï¼š Fake, File, HDFS, Kafka, S3, Hive, Kudu, MongoDB, JDBC, Alluxio, Socket, self-developed Input pluginFilter pluginï¼š Add, Checksum, Convert, Date, Drop, Grok, Json, Kv, Lowercase, Remove, Rename, Repartition, Replace, Sample, Split, Sql, Table, Truncate, Uppercase, Uuid, Self-developed Filter pluginOutput plugin: Elasticsearch, File, Hdfs, Jdbc, Kafka, Mysql, S3, Stdout, self-developed Output pluginæ”¯æŒçš„æ‰€æœ‰ç»„ä»¶å¯ä»¥å‚è€ƒSeaTunnelé€šç”¨é…ç½® ä½¿ç”¨SeaTunnelå®‰è£…éƒ¨ç½²SeaTunnelä½¿ç”¨SeaTunnelå°†Kuduæ•°æ®å¯¼å…¥ClickHouseä¸‹è½½SeaTunnel:SeaTunneläºŒè¿›åˆ¶åŒ… unzip seatunnel-1.5.5.zip cd seatunnel-1.5.5 # ä¿®æ”¹seatunnelç¯å¢ƒé…ç½® vim config/seatunnel-env.sh SPARK_HOME=/hadoop/bigdata/spark/spark-2.3.2-bin-hadoop2.6 SeaTunnelå°†Kuduè¡¨å¯¼å…¥ClickHouseå‡†å¤‡kuduè¡¨Kuduè¡¨kudu_db.kudu_tableï¼ˆåœ¨KuduWebUIä¸­è¡¨åä¸ºimpala::kudu_db.kudu_tableï¼‰ é¢„å…ˆåˆ›å»ºç›®æ ‡ClickHouseè¡¨ CREATE TABLE test.ch_table ( `cust_no` String, `tag_code` String, `update_datetime` DateTime ) ENGINE = MergeTree ORDER BY cust_no; å‚è€ƒseatunnel-docs-configuration é…ç½®æ•°æ®æŠ½å–ä»»åŠ¡vim config/kudu2ch.batch.confå†…å®¹å¦‚ä¸‹ spark &#123; spark.app.name = &quot;kudu2ch&quot; # executorçš„æ•°é‡ spark.executor.instances = 2 # æ¯ä¸ªexcutoræ ¸æ•° (å¹¶è¡Œåº¦,æ•°æ®é‡å¤§å¯ä»¥é€‚å½“å¢å¤§åˆ°ClickHouseæœåŠ¡å™¨æ ¸æ•°ä¸€åŠä»¥ä¸‹,å°½é‡ä¸è¦å½±å“ClickHouse) spark.executor.cores = 1 # æ¯ä¸ªexcutorå†…å­˜ spark.executor.memory = &quot;1g&quot; &#125; input &#123; kudu&#123; kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot; kudu_table=&quot;impala::kudu_db.kudu_table&quot; # å¯¹åº”è¾“å‡ºä¸­éœ€è¦æŒ‡å®šsource_table_name=&quot;kudu_table_source&quot; result_table_name=&quot;kudu_table_source&quot; &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; # æŒ‡å®šä»å“ªä¸ªæºæŠ½å–æ•°æ® source_table_name=&quot;kudu_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;test&quot; table = &quot;ch_table&quot; fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;] username = &quot;default&quot; password = &quot;123456&quot; # æ¯æ‰¹æ¬¡å†™å…¥ClickHouseæ•°æ®æ¡æ•° bulk_size = 20000 &#125; &#125; æ‰§è¡ŒæŠ½å–ä»»åŠ¡ï¼š /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf æ’é”™1ï¼š Caused by: ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 210, host: ch_jdbc_ip, port: 8123; Connect to ch_jdbc_ip:8123 [/ch_jdbc_ip] failed: Connection refused (Connection refused) åŸå› ï¼šCH Serverç«¯æœªå¼€å¯è¿œç¨‹è®¿é—®æƒé™è§£å†³ï¼šå¼€å¯CH Serveræ”¯æŒè¿œç¨‹è®¿é—®çš„æƒé™ æ’é”™2ï¼š 2021-12-22 15:23:47 ERROR TaskSetManager:70 - Task 2 in stage 0.0 failed 1 times; aborting job Exception in thread &quot;main&quot; java.lang.Exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.ClassCastException: java.sql.Timestamp cannot be cast to java.lang.String at io.github.interestinglab.waterdrop.output.batch.Clickhouse.renderBaseTypeStatement(Clickhouse.scala:351) at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatementEntry(Clickhouse.scala:373) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement$1.apply$mcVI$sp(Clickhouse.scala:403) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160) at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement(Clickhouse.scala:391) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:187) at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:162) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) åŸå› ï¼šå¦‚æœKuduä¸­è¡¨å­—æ®µæ ¼å¼ä¸ºTimestampï¼Œéœ€è¦åœ¨å†™å…¥ClickHouseå‰å…ˆå°†Timestampç±»å‹æ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼å¦åˆ™ä¼šå†™å…¥é”™è¯¯ã€‚ç›¸å…³Git Issue: SeaTunnel-848ç›¸å…³æ–‡æ¡£ï¼šClickHouseç±»å‹å¯¹ç…§è¡¨è§£å†³ï¼šå†™å…¥ClickHouseä¹‹å‰éœ€è¦é€šè¿‡SeaTunnelä¸­çš„ Filteræ’ä»¶ ä¸­çš„ SQL æˆ–è€… Convert æ’ä»¶å°†å„å­—æ®µè½¬æ¢ä¸ºå¯¹åº”æ ¼å¼ï¼Œå¦åˆ™ä¼šäº§ç”ŸæŠ¥é”™æ³¨æ„ï¼šè‹¥é…ç½®ä¸­æœ‰filteræ’ä»¶ä¸”éœ€è¦filterç”Ÿæ•ˆï¼Œåˆ™ä¸è¦åœ¨outputæŒ‡å®šsource_table_nameè¿™ä¸ªé€‰é¡¹ï¼Œè‹¥æŒ‡å®šäº†source_table_nameçš„å€¼ç­‰äºinputä¸­result_table_nameçš„å€¼ï¼Œåˆ™ä¼šç»•è¿‡filter(filterä¸ç”Ÿæ•ˆ)ä¿®æ”¹é…ç½®vim config/kudu2ch.batch.confå†…å®¹å¦‚ä¸‹ spark &#123; spark.app.name = &quot;kudu2ch&quot; # executorçš„æ•°é‡ spark.executor.instances = 2 # æ¯ä¸ªexcutoræ ¸æ•° (å¹¶è¡Œåº¦,æ•°æ®é‡å¤§å¯ä»¥é€‚å½“å¢å¤§åˆ°ClickHouseæœåŠ¡å™¨æ ¸æ•°ä¸€åŠä»¥ä¸‹,å°½é‡ä¸è¦å½±å“ClickHouse) spark.executor.cores = 1 # æ¯ä¸ªexcutorå†…å­˜ spark.executor.memory = &quot;1g&quot; &#125; input &#123; kudu&#123; kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot; kudu_table=&quot;impala::kudu_db.kudu_table&quot; # å¯¹åº”è¾“å‡ºä¸­éœ€è¦æŒ‡å®šsource_table_name=&quot;kudu_table_source&quot; result_table_name=&quot;kudu_table_source&quot; &#125; &#125; filter &#123; sql &#123; sql = &quot;select cust_no,tag_code,date_format(update_datetime, &#39;yyyy-MM-dd&#39;) as update_datetime from kudu_table_source&quot; &#125; &#125; output &#123; clickhouse &#123; # æŒ‡å®šä»å“ªä¸ªæºæŠ½å–æ•°æ® # source_table_name=&quot;kudu_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;test&quot; table = &quot;ch_table&quot; fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;] username = &quot;default&quot; password = &quot;123456&quot; # æ¯æ‰¹æ¬¡å†™å…¥ClickHouseæ•°æ®æ¡æ•° bulk_size = 20000 &#125; &#125; è‹¥ä½¿ç”¨Convertæ¨¡å—ï¼ŒFilterä¸­å†…å®¹ filter &#123; date&#123; source_field = &quot;update_datetime&quot; target_field = &quot;update_datetime&quot; source_time_format = &quot;UNIX&quot; target_time_format = &quot;yyyy-MM-dd HH:mm:ss&quot; &#125; &#125; æ‰§è¡ŒæŠ½å–ä»»åŠ¡ï¼š /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf æ•°æ®éªŒè¯:Kudu:+â€”â€”â€”-+| count(1) |+â€”â€”â€”-+| 714218 |+â€”â€”â€”-+Fetched 1 row(s) in 2.39s ClickHouse:Query id: 8d6bc13d-c49d-408a-8e07-3d2691e3ebbbâ”Œâ”€count()â”€â”â”‚ 714218 â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜1 rows in set. Elapsed: 0.003 sec. ä½†DateTimeç±»å‹ç›¸å·®8å°æ—¶ï¼Œå› ä¸ºClickHouseçš„DateTimeæ—¶åŒºé—®é¢˜ï¼Œæ•…å¯ä»¥åœ¨sqlä¸­å¯¹update_datetimeå­—æ®µå€¼å‡å»8*3600ç§’ filter &#123; sql &#123; sql = &quot;select cust_no, tag_code, date_format(cast(cast(update_datetime as int) - 8*3600 as timestamp), &#39;yyyy-MM-dd HH:mm:ss&#39;) as update_datetime from kudu_table_source&quot; &#125; &#125; ä¸€å¼€å§‹æƒ³è®¾ç½®ClickHouseä¸­DateTimeæ—¶åŒºä¸ºDateTime(â€˜Asia/Hong_Kongâ€™)ï¼Œä½†SeaTunnelä¸æ”¯æŒè¿™æ ¼å¼ï¼Œåªèƒ½ç”¨é»˜è®¤çš„DateTimeæ ¼å¼æ³¨æ„ï¼šSeaTunnelæŠ½å–Kuduçš„SparkTaskæ•°ç­‰äºKuduè¡¨çš„Tabletæ•°ï¼Œå»ºè®®ç»™å®šSparkç¨‹åºå¹¶è¡Œåº¦ä¸ºTabletæ•°çš„ä¸‰åˆ†ä¹‹ä¸€æˆ–äºŒåˆ†ä¹‹ä¸€ã€‚ SeaTunnelå°†Impalaè¡¨å¯¼å…¥ClickHouseSeaTunnelæ”¯æŒInputç±»å‹æ²¡æœ‰Impalaä½†æœ‰JDBCï¼Œæ”¯æŒä»»ä½•JDBCæ•°æ®æºï¼ŒImpalaä¹Ÿå±äºJDBCæ•°æ®æºã€‚é€šè¿‡SeaTunnelå¯ä»¥å°†Impalaç®¡ç†çš„Kuduè¡¨ã€Hiveè¡¨æ•°æ®å¯¼å‡ºåˆ°å…¶ä»–å­˜å‚¨å¼•æ“ã€‚ å‡†å¤‡Impala Hiveè¡¨Impalaè¡¨ default.qjj_test+â€”â€”+â€”â€”â€“+â€”â€”â€”+| name | type | comment |+â€”â€”+â€”â€”â€“+â€”â€”â€”+| id | int | || name | string | |+â€”â€”+â€”â€”â€“+â€”â€”â€”+ åˆ›å»ºå¯¹åº”ç›®æ ‡ClickHouseè¡¨ CREATE TABLE default.qjj_test ( `id` int, `name` String ) ENGINE = MergeTree ORDER BY id; å‚è€ƒSeaTunnel-docs-JDBCç¼–å†™ä»»åŠ¡é…ç½®æ–‡ä»¶é…ç½®æ–‡ä»¶/opt/seatunnel-1.5.5/config/impala2ch.batch.confå¦‚ä¸‹: spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; spark.executor.instances = 2 spark.executor.cores = 1 # æ¯ä¸ªexcutorå†…å­˜ spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; table = &quot;(select * from qjj_test) as source_table&quot; # æˆ–è€…ç›´æ¥å†™è¡¨åä¹Ÿå¯ä»¥table = &quot;qjj_test&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # æ¯æ‰¹æ¬¡å†™å…¥ClickHouseæ•°æ®æ¡æ•° bulk_size = 20000 &#125; &#125; å°†jdbc-jaræ”¾å…¥seatunnelç›®å½•çš„plugins/my_plugins/libç›®å½•Impala-jdbcä¸‹è½½åœ°å€ï¼šDonwload ImpalaJDBC41.jar cd seatunnel-1.5.6/ mkdir -p plugins/my_plugins/lib cd plugins/my_plugins/lib cp /hadoop/bigdata/common/lib/ImpalaJDBC41.jar . æ‰§è¡ŒæŠ½å–ä»»åŠ¡ï¼š /opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master yarn --deploy-mode cluster --config /opt/seatunnel-1.5.5/config/impala2ch.batch.conf æ­¤æ—¶å¯ä»¥æ­£å¸¸æŠ½å–æ•°æ®äº†ï¼Œä½†é€šè¿‡è§‚å¯Ÿç¨‹åºWebUIå‘ç°æ— è®ºç»™äº†å¤šå°‘ExecutorCoreï¼Œåªæœ‰ä¸€ä¸ªTaskï¼Œè¿™æ ·ä½çš„å¹¶è¡Œåº¦ä¼šæå¤§å½±å“æ•°æ®æŠ½å–æ•ˆç‡ï¼Œæ‰€ä»¥éœ€è¦åœ¨é…ç½®ä¸Šåšæ”¹è¿›ï¼šå‚è€ƒSeaTunnel-Spark-jdbc-string å¾—çŸ¥SeaTunnelæ”¯æŒSparkJDBCçš„æ‰€æœ‰å‚æ•°:spark-sql-data-sources-jdbc é…ç½®ä¿®æ”¹æ€è·¯æ˜¯å°†åŸæ¥çš„åªæœ‰ä¸€ä¸ªå¹¶è¡Œåº¦å¢åŠ åˆ°å¤šä¸ªå¹¶è¡Œåº¦æ‰€ä»¥ä½¿ç”¨partitionColumn, lowerBound, upperBoundå’ŒnumPartitionsè¿™å››ä¸ªå‚æ•°è¿›è¡Œè°ƒä¼˜ï¼Œæ³¨æ„è¦å¯¹åˆ†åŒºå­—æ®µå€¼æ•°æ®æœ‰ä¸€å®šäº†è§£ï¼Œé€‰æ‹©åˆé€‚çš„åˆ†åŒºå­—æ®µå’ŒlowerBound, upperBoundå¾ˆå…³é”®ã€‚å½“ç„¶è¿™æ ·å¹¶è¡ŒåŠ è½½æ•°æ®æºä¹Ÿå°†å¹¶è¡Œåˆå§‹åŒ–å¤šä¸ªè¿æ¥ï¼ŒSparkæºç ä¸­æé†’åˆ°ä¸è¦å¹¶è¡Œåº¦è¿‡å¤§ï¼Œå¦åˆ™å®¹æ˜“æŠŠå¤–éƒ¨å­˜å‚¨æå®ã€‚ partitionColumn, lowerBound, upperBoundå’ŒnumPartitionsè¿™å››ä¸ªå‚æ•°èƒ½å†³å®šSparkè¯»å–JDBCæ•°æ®æºçš„å¹¶è¡Œåº¦åŠç­–ç•¥ï¼ŒlowerBoundæ˜¯åˆ†åŒºå­—æ®µå–å€¼çš„ä¸‹é™(åŒ…å«)ï¼ŒupperBoundæ˜¯ä¸Šé™(ä¸åŒ…å«)ï¼ŒnumPatitionsæ˜¯æˆ‘ä»¬å¸Œæœ›æŒ‰ç…§å¤šå°‘åˆ†åŒºæ¥åŠ è½½JDBCã€‚æ³¨æ„ç¬¬0ä¸ªåˆ†åŒºå’Œæœ€åä¸€ä¸ªåˆ†åŒºåŠ è½½çš„æ•°æ®ä¸è¢«lowerBound, upperBoundæ‰€é™åˆ¶ï¼Œä»ç„¶ä¼šæŠŠæ‰€æœ‰æ•°æ®åŠ è½½å‡ºæ¥ã€‚å…·ä½“å®ç°é€»è¾‘å¯ä»¥çœ‹Sparkä¸­JdbcRelationProviderå’ŒJDBCRelationä¸¤ä¸ªæ ¸å¿ƒç±»ã€‚ æ ¹æ®é…ç½®æ ·ä¾‹SeaTunnel-JDBC-Example ä¿®æ”¹é…ç½®å¦‚ä¸‹ï¼š spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; spark.executor.instances = 5 spark.executor.cores = 2 # æ¯ä¸ªexcutorå†…å­˜ spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; table = &quot;(select * from qjj_test) as source_table&quot; # æˆ–è€…ç›´æ¥å†™è¡¨åä¹Ÿå¯ä»¥table = &quot;qjj_test&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; jdbc.partitionColumn = &quot;id&quot; jdbc.numPartitions = &quot;20&quot; jdbc.lowerBound = 0 jdbc.upperBound = 2000000 &#125; &#125; filter &#123; &#125; output &#123; clickhouse &#123; source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # æ¯æ‰¹æ¬¡å†™å…¥ClickHouseæ•°æ®æ¡æ•° bulk_size = 20000 &#125; &#125; å†æ¬¡æ‰§è¡Œï¼Œè§‚å¯ŸWebUIå‘ç°å¹¶è¡Œåº¦å·²ç»æé«˜äº†ï¼Œå†™å…¥é€Ÿåº¦ä¹Ÿå˜å¿«äº†ã€‚ è·‘åˆ°åé¢å‘ç°æœ‰å‘ç”Ÿæ•°æ®å€¾æ–œï¼Œå¯èƒ½æ˜¯å› partitionColumnå‚æ•°è®¾ç½®ä¸åˆç†å¯¼è‡´æ•°æ®å€¾æ–œï¼Œè¦æ³¨æ„å°½é‡é€‰æ‹©ä¸åŒèŒƒå›´æ•°æ®åˆ†å¸ƒå‡åŒ€çš„å­—æ®µä½œä¸ºåˆ†åŒºå­—æ®µï¼Œå¦åˆ™ææ˜“å‘ç”Ÿæ•°æ®å€¾æ–œã€‚ä½†é€šè¿‡è§‚å¯ŸåŸè¡¨æ•°æ®ï¼Œå‘ç°æ²¡æœ‰æ•°æ®åœ¨ä¸åŒèŒƒå›´å†…åˆ†å¸ƒå‡åŒ€çš„å­—æ®µï¼Œæ‰€ä»¥éœ€è¦è‡ªå·±é€ ä¸€ä¸ªåˆ†å¸ƒå‡åŒ€çš„å­—æ®µã€‚å¯ä»¥å¯¹å­—æ®µåšMOD(ASCII(SUBSTR(å­—æ®µå,-1)), åˆ†åŒºæ•°)æ“ä½œã€‚ä¿®æ”¹é…ç½®å¦‚ä¸‹ï¼š spark &#123; spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot; # æé«˜äº†åˆ†åŒºæ•° ç›¸åº”çš„åœ¨jdbcå…è®¸çš„jdbcè¿æ¥æ•°èŒƒå›´å†…è°ƒå¤§executoræ ¸æ•° ä»¥æ›´é«˜çš„å¹¶è¡Œåº¦è·‘æ•°æ® spark.executor.instances = 30 spark.executor.cores = 2 # æ¯ä¸ªexcutorå†…å­˜ spark.executor.memory = &quot;2g&quot; &#125; input &#123; jdbc &#123; driver = &quot;com.cloudera.impala.jdbc41.Driver&quot; url = &quot;jdbc:impala://impalad_ip:21050/default&quot; # æ³¨æ„tableçš„å€¼æ˜¯äº¤ç»™æ•°æ®æºjdbcå»è¿è¡Œçš„è€ŒéSparkï¼Œä¸èƒ½ä½¿ç”¨SparkSQLå‡½æ•°ï¼Œåªèƒ½ä½¿ç”¨æ•°æ®æºæ”¯æŒçš„å‡½æ•° æ¬¡æ•°å°†æ•°æ®æ‰“æ•£æˆ300ä¸ªåŒº å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ•°æ®æ‰“æ•£æ–¹å¼ æœ€å¥½å…ˆgroupbyæµ‹ä¸€ä¸‹æ˜¯å¦å°†æ•°æ®å‡åŒ€æ‰“æ•£ table = &quot;(select id,name,(cast(rand() * 300 as int)) as spark_partition_column from qjj_test) as source_table&quot; result_table_name = &quot;impala_table_source&quot; user = &quot;&quot; password = &quot;&quot; jdbc.partitionColumn = &quot;spark_partition_column&quot; jdbc.numPartitions = &quot;300&quot; jdbc.lowerBound = 0 jdbc.upperBound = 300 &#125; &#125; filter &#123; sql &#123; # ä¸Šé¢å¤„ç†åå¤šå‡ºæ¥ä¸ªå­—æ®µï¼Œå¿½ç•¥æ‰è¯¥å­—æ®µ sql = &quot;select id,name from impala_table_source&quot; &#125; &#125; output &#123; clickhouse &#123; # source_table_name=&quot;impala_table_source&quot; host = &quot;ch_jdbc_ip:8123&quot; clickhouse.socket_timeout = 50000 database = &quot;default&quot; table = &quot;qjj_test&quot; username = &quot;default&quot; password = &quot;123456&quot; # æ¯æ‰¹æ¬¡å†™å…¥ClickHouseæ•°æ®æ¡æ•° bulk_size = 20000 &#125; &#125; å¯¹äºä½¿ç”¨Impala JDBCè¿›è¡Œæ•°æ®æŠ½å–çš„æƒ…å†µï¼ŒæŸ¥è¯¢çš„å¹¶è¡Œåº¦éœ€è¦æ ¹æ®æœåŠ¡å™¨æ•°é‡å’Œèµ„æºæƒ…å†µè®¾ç½®ï¼Œè¿æ¥å¹¶è¡Œåº¦ä¸åº”è¿‡å¤§ï¼ŒImpaladå¯¹å•æ± å†…å­˜å¤§å°æœ‰é™åˆ¶ã€‚å¹¶è¡Œåº¦å¤ªé«˜ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯ï¼š Caused by: java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: ExecQueryFInstances rpc query_id=42464c52f2e2c5dc:fe9ecfe800000000 failed: Failed to get minimum memory reservation of 272.00 MB on daemon data02.smycluster.sa:22000 for query 42464c52f2e2c5dc:fe9ecfe800000000 due to following error: Failed to increase reservation by 272.00 MB because it would exceed the applicable reservation limit for the &quot;Process&quot; ReservationTracker: reservation_limit=39.10 GB reservation=38.91 GB used_reservation=0 child_reservations=38.91 GB The top 5 queries that allocated memory under this tracker are: Query(8a4d40e3a6968443:7ae87ca100000000): Reservation=28.67 GB ReservationLimit=36.80 GB OtherMemory=21.24 MB Total=28.69 GB Peak=28.79 GB Query(bb4dc7b08c698bc3:f4036eb000000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=93.62 MB Total=1.15 GB Peak=2.39 GB Query(8a41df2c931faaec:ae30808c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=68.75 MB Total=1.13 GB Peak=1.37 GB Query(604eddfbd1fd2de5:b7493a7400000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=66.37 MB Total=1.13 GB Peak=1.38 GB Query(4c4ff283b5e12385:903c399c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=47.71 MB Total=1.11 GB Peak=1.39 GB Memory is likely oversubscribed. Reducing query concurrency or configuring admission control may help avoid this error. åœ¨æµ·é‡æ•°æ®ä¸”èµ„æºé…ç½®ä¸ä½³çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨Impala JDBCå¯¼å‡ºæ•°æ®å¹¶ä¸æ˜¯å¾ˆå¥½çš„é€‰æ‹©ï¼ŒImpalaæœ¬èº«ä¸é€‚åˆè·‘æ‰¹ï¼Œè·‘æ‰¹ç¨³å®šæ€§å·®ï¼Œæ— å®¹é”™æœºåˆ¶ã€‚å¯¹äºè¿™æ ·çš„åœºæ™¯å¯ä»¥å°†Impalaè¡¨æ•°æ®å¯¼å‡ºæˆParquetæ–‡ä»¶ï¼Œå†Loadåˆ°ClickHouseã€‚ä¹Ÿå¯ä»¥å¯¼å‡ºParquetè¡¨åˆ°HDFSï¼Œå†ä½¿ç”¨ClickHouseæ˜ å°„HDFSå¼•æ“è¡¨ä»è€Œè·å–æ•°æ®ã€‚ org.apache.kudu.client.NonRecoverableException: Scanner 10150be3c0b944829d4eea1bc2251e24 not found (it may have expired) åŸå› åŠè§£å†³ï¼šé€šå¸¸æˆ‘ä»¬éœ€è¦çŸ¥é“ï¼Œå½“å¸¦å®½å ç”¨æ¥è¿‘æ€»å¸¦å®½çš„90%æ—¶ï¼Œä¸¢åŒ…æƒ…å½¢å°±ä¼šå‘ç”Ÿã€‚ç½‘ç»œç­–ç•¥æœ‰é—®é¢˜æˆ–è€…å¸¦å®½è¿‡ä½ï¼Œå¯¹å¸¦å®½åšäº†é™åˆ¶ï¼Œéƒ½ä¼šå¯¼è‡´è¿™æ ·çš„é—®é¢˜ï¼Œå–æ¶ˆé™åˆ¶å³å¯ã€‚è‹¥æ‹…å¿ƒå¸¦å®½é—®é¢˜ï¼Œå¯ä»¥é€‚å½“é™ä½å¹¶è¡Œåº¦æŠ½å–ã€‚ 2022.1æœˆ-SeaTunnelæ­£å¼è¿›å…¥Apacheå­µåŒ–å™¨ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸ªæ¯”è¾ƒä¼˜ç§€çš„é¡¹ç›®ï¼Œæ˜¯ä¸ªä½ä»£ç å®ç°æ•°æ®æŠ½å–çš„é«˜æ•ˆå¹³å°ï¼Œæœ‰å…´è¶£å¯ä»¥å¤šå…³æ³¨è¿™ä¸ªé¡¹ç›®ã€‚ å‚è€ƒ:SeaTunnel-githubSeaTunnel-docs-configurationä½¿ç”¨WaterDropå°†Kuduæ•°æ®æŠ½å–åˆ°Clickhouse","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"WaterDrop","slug":"WaterDrop","permalink":"https://shmily-qjj.top/tags/WaterDrop/"},{"name":"SeaTunnel","slug":"SeaTunnel","permalink":"https://shmily-qjj.top/tags/SeaTunnel/"},{"name":"æ•°æ®åŒæ­¥","slug":"æ•°æ®åŒæ­¥","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"åŸºäºManjaro KDEç‰ˆæ‰“é€ ç¾è§‚èˆ’é€‚å¼€å‘ç¯å¢ƒ","slug":"åŸºäºManjaro KDEç‰ˆæ‰“é€ ç¾è§‚èˆ’é€‚å¼€å‘ç¯å¢ƒ","date":"2021-07-07T03:22:00.000Z","updated":"2023-06-09T17:26:21.944Z","comments":true,"path":"3f34ebe3/","link":"","permalink":"https://shmily-qjj.top/3f34ebe3/","excerpt":"","text":"åŸºäºManjaro KDEç‰ˆæ‰“é€ ç¾è§‚èˆ’é€‚å¼€å‘ç¯å¢ƒç³»ç»Ÿå®‰è£…ä¸åˆå§‹åŒ–é…ç½®å®‰è£…ç³»ç»Ÿåˆ°Manjaroå®˜ç½‘ä¸‹è½½æœ€æ–°ManjaroLinuxå‘è¡Œç‰ˆï¼ˆæœ¬æ–‡åŸºäºManjaro KDE Plasma 5.21.5ç‰ˆæœ¬ï¼‰åˆ°Rufuså®˜ç½‘ä¸‹è½½é•œåƒå…‹éš†å·¥å…·ï¼Œä½¿ç”¨Rufuså…‹éš†Manjaroé•œåƒåˆ°Uç›˜ï¼Œæ¨¡å¼é€‰æ‹©UEFI ç³»ç»ŸBIOSè®¾ç½®é¡¹ï¼šBooté¡ºåºå°†ç³»ç»Ÿå®‰è£…ç›˜æ”¹ä¸ºç¬¬ä¸€é¡¹å…³é—­å®‰å…¨å¯åŠ¨Security Boot =&gt; å¦åˆ™æ— æ³•å¼•å¯¼è¿›å…¥LinuxSATAæ¨¡å¼ç”±Raid Onåˆ‡æ¢ä¸ºAHCI =&gt; è‹¥ç³»ç»Ÿæœ‰NVMEç¡¬ç›˜åˆ™éœ€è¦æ­¤æ“ä½œï¼Œé¿å…Linuxæ— æ³•è¯†åˆ«åˆ°NVMEç¡¬ç›˜ï¼ˆåŒç³»ç»Ÿç”¨æˆ·å…ˆè¿›å…¥Windows-&gt;cmdè¿è¡Œmsconfig-&gt;å¼•å¯¼-&gt;å‹¾é€‰å®‰å…¨å¼•å¯¼-&gt;é‡å¯çš„è¿‡ç¨‹ä¸­ä¼šä¿®å¤ç¡¬ç›˜çš„AHCIé©±åŠ¨é¿å…å› åˆ‡æ¢AHCIå¯¼è‡´æ— æ³•å¯åŠ¨Windowsç³»ç»Ÿ-&gt;é‡å¯åå†å–æ¶ˆå‹¾é€‰å®‰å…¨å¼•å¯¼ï¼‰ åŒæ˜¾å¡ç”¨æˆ·æ³¨æ„äº‹é¡¹(å•æ˜¾å¡å¿½ç•¥æ­¤æ­¥éª¤)ï¼šNvidia+IntelåŒæ˜¾å¡ç¬”è®°æœ¬å®‰è£…éœ€è¦è¿™æ­¥ï¼šå®‰è£…å‰ç»™å†…æ ¸ä¼ å‚=&gt;æŒ‰eåœ¨quietååŠ ï¼šacpi_osi=! acpi_osi=â€Windows 2009â€ æŒ‰F10å¯åŠ¨ï¼Œå¦åˆ™ä¼šå¡æ­»æ— æ³•è¿›å…¥æ¡Œé¢ åŒå‡» Install Manjaro Linuxæ‰“å¼€å®‰è£…å‘å¯¼æ—¶åŒºé€‰æ‹©Asia/ShangHaié”®ç›˜é»˜è®¤å³å¯æ¥ä¸‹æ¥æ˜¯å…³é”®æ­¥éª¤ ç£ç›˜è¦é€‰æ‰‹åŠ¨åˆ†åŒºç£ç›˜ç©ºé—´è§„åˆ’ï¼š/boot/efiåˆ†åŒºæŒ‚è½½åˆ°åŸEFIåˆ†åŒºï¼Œå…±384Gç©ºé—²ç©ºé—´ï¼Œæ ¹åˆ†åŒºxfsæ ¼å¼192Gï¼Œhomeåˆ†åŒºxfsæ ¼å¼160Gï¼Œvaråˆ†åŒºext4æ ¼å¼24Gï¼Œswapç»™8Gï¼ˆxfsè¯»å–æ•ˆç‡å’Œæ–­ç”µå®¹é”™è¾ƒå¥½ä½†å†™æ•ˆç‡ç•¥å¾®ä½äºext4ï¼Œext4å†™æ•ˆç‡é«˜äº›è¯»æ•ˆç‡ä½äºxfsï¼‰åœ¨ç©ºé—²åŒºåŸŸåˆ›å»ºåˆ†åŒº æ­¥éª¤å¦‚ä¸‹ï¼šæ•°æ®åˆ†åŒºæœ€ç»ˆç»“æœå¦‚ä¸‹ï¼šå¯åŠ¨åˆ†åŒº(EFIåˆ†åŒº) å¦‚ä¸‹è®¾ç½®ï¼šæœ€åè®¾ç½®ç³»ç»Ÿç®¡ç†å‘˜ç”¨æˆ·å®‰è£…å®Œæˆå å¯ä»¥é‡å¯ åŒæ˜¾å¡ç”¨æˆ·æ³¨æ„äº‹é¡¹(å•æ˜¾å¡å¿½ç•¥æ­¤æ­¥éª¤)ï¼šé‡å¯ç¬¬ä¸€æ¬¡è¿›å…¥ç³»ç»Ÿä¹Ÿéœ€è¦æŒ‰eåœ¨quietååŠ ï¼šacpi_osi=! acpi_osi=â€Windows 2009â€ æŒ‰F10å¯åŠ¨ï¼Œå¦åˆ™ä¼šå¡æ­»æ— æ³•è¿›å…¥æ¡Œé¢è¿›å…¥ç³»ç»Ÿåï¼šsudo vim /boot/grub/grub.cfg åœ¨æ‰€æœ‰quietååŠ acpi_osi=! acpi_osi=â€Windows 2009â€å‚æ•°ï¼Œä¸‹æ¬¡å¼€æœºåˆ™ä¸éœ€è¦å†åŠ å†…æ ¸å‚æ•°ï¼ˆè‹¥ç³»ç»Ÿæ›´æ–°äº†å†…æ ¸ï¼Œgrub.cfgä¹Ÿä¼šè¢«æ›´æ–°ï¼Œéœ€è¦é‡æ–°åŠ å†…æ ¸å‚æ•°è¿›å…¥ç³»ç»Ÿï¼Œé‡æ–°ä¿®æ”¹grub.cfgæ–‡ä»¶ï¼‰å»ºè®®æ¯æ¬¡æ›´æ–°ç³»ç»Ÿæ‰§è¡Œå¦‚ä¸‹è„šæœ¬(update_grub.sh)è‡ªåŠ¨å¢åŠ å†…æ ¸å‚æ•°ï¼š #!/bin/bash echo &quot;åŒæ˜¾å¡ç¬”è®°æœ¬æ›´æ–°Manjaroç³»ç»Ÿåéœ€è¦æ·»åŠ grubå‚æ•°é¿å…æ— æ³•å¼€æœº&quot; if [[ $(whoami) != root ]]; then echo -e &quot;\\033[41;37m[ERROR] Need sudo or root privilege.\\033[0m&quot; exit 1 fi GRUB_CFG=&quot;/boot/grub/grub.cfg&quot; GRUB_CFG_BACKUP=&quot;$GRUB_CFG&quot;_bak echo &quot;Backup path: $GRUB_CFG_BACKUP&quot; cp $GRUB_CFG $GRUB_CFG_BACKUP if [ -n &quot;$(grep &quot;Windows 2009&quot; $GRUB_CFG)&quot; ]; then echo &quot;Grub config is ok,no update.&quot; else echo &quot;Grub config need to be updated.Go to update it.&quot; # é˜²æ­¢ä¸èƒ½åŠ è½½æ˜¾å¡ä¸èƒ½è¿›æ¡Œé¢ sed -i &#39;s/quiet/quiet acpi_osi=! acpi_osi=&quot;Windows 2009&quot;/g&#39; $GRUB_CFG # å¼€æœºç­‰å¾…ç•Œé¢è¶…æ—¶æ—¶é—´è®¾ä¸º3s sed -i &#39;s/timeout=10/timeout=3/g&#39; $GRUB_CFG fi echo -e &quot;\\033[42;3mAll Done. Result:\\033[0m&quot; cat $GRUB_CFG | grep &quot;quiet&quot; cat $GRUB_CFG | grep &quot;timeout=&quot; åŒæ˜¾å¡ç”¨æˆ·ç›®å‰æ— è®ºå®‰è£…ä»»ä½•Linuxå‘è¡Œç‰ˆéƒ½å¾ˆå‘ï¼Œå¯¹äºManjaro,å¥‰ä¸Šè®¾ç½®æ˜¾å¡åˆ‡æ¢çš„æ•™ç¨‹ï¼šManjaro ç¬”è®°æœ¬é…ç½®Intelä¸NvidiaåŒæ˜¾å¡åˆ‡æ¢ï¼Œé˜²è¸©å‘æ•™ç¨‹ è‡³æ­¤ Manjaro Linuxç³»ç»Ÿå®‰è£…å®Œæˆ åˆå§‹åŒ–ç³»ç»Ÿå®‰è£…å¿…å¤‡ç³»ç»Ÿé•œåƒæº cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup # æ›´æ–°é•œåƒæ’å sudo pacman-mirrors -i -c China -m rank cp /etc/pacman.conf /etc/pacman.conf.backup # æ·»åŠ ArchLinuxä¸­æ–‡ç¤¾åŒºæº sudo vi /etc/pacman.conf [archlinuxcn] SigLevel = Optional TrustedOnly #æ¸…åæº Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch #ä¸­ç§‘å¤§æº #Server = https://mirrors.ustc.edu.cn/archlinuxcn/$arch # é…ç½®ç”Ÿæ•ˆ sudo pacman-mirrors -g # æ›´æ–°pacmanæ•°æ®åº“å…¨é¢æ›´æ–°ç³»ç»Ÿå¹¶ç­¾å sudo pacman -Syyu &amp;&amp; sudo pacman -S archlinuxcn-keyring # å®‰è£…ä¸€äº›å¸¸ç”¨çš„åŒ… sudo pacman -S acpi vim æ³¨æ„ï¼Œå¦‚æœé‡åˆ°â€œæ— æ•ˆæˆ–å·²æŸåçš„è½¯ä»¶åŒ… (PGP ç­¾å)â€è¿™æ ·çš„æŠ¥é”™ï¼Œå¯ä»¥å°è¯•ä¿®æ”¹â€œOptional TrustedOnlyâ€ä¸ºâ€œOptional TrustAllâ€ æ ¹æ®ä¸ªäººä¹ æƒ¯åˆ›å»ºä¸€äº›ç›®å½•ï¼ˆæˆ‘çš„ä¹ æƒ¯å¹¶ä¸å¥½ï¼ŒæŠŠä¸€éƒ¨åˆ†é¡¹ç›®æ–‡ä»¶æ”¾åœ¨/optä¸‹ï¼Œ/optæœ¬èº«æ˜¯ç”¨äºå®‰è£…ä¸€äº›å¤§å‹è½¯ä»¶çš„ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç”¨æˆ·çš„ä¸ªäººæ–‡ä»¶æ”¾åœ¨å®¶ç›®å½•ä¸‹æ¯”è¾ƒè§„èŒƒï¼‰ su root chmod 1777 /opt # å­˜æ”¾æˆ‘çš„åº”ç”¨ mkdir /opt/apps chmod 1777 -R /opt/apps/ usermod -a -G root shmily # ç”¨æˆ·ä»£ç é¡¹ç›®å­˜æ”¾ç›®å½• su shmily sudo mkdir -p /opt/Projects/ sudo mkdir -p /opt/Projects/OpenSourceProjects sudo mkdir -p /opt/Projects/MyProjects sudo mkdir -p /opt/Projects/EnterpriseProjects sudo chmod 1777 -R /opt/Projects/ # ç³»ç»Ÿç¯å¢ƒæ‰€éœ€ç›®å½• sudo mkdir -p /opt/Env/ # å·¥å…·ç›®å½• sudo mkdir -p /opt/Tools/ é…ç½®å…å¯† # sudoå…å¯† é¿å…é¢‘ç¹è¾“å…¥å¯†ç  ä»¥æˆ‘ç”¨æˆ·åshmilyä¸ºä¾‹(sudo suåªåˆ‡ç”¨æˆ·ä¸å¸¦rootçš„ç¯å¢ƒå˜é‡ sudo su -å¸¦rootç¯å¢ƒå˜é‡è·Ÿrootç”¨æˆ·ä¸€æ ·) sudo vim /etc/sudoers shmily ALL=(ALL) NOPASSWD: ALL æ¸…å¾%sudo ALL=(ALL) ALLå‰çš„æ³¨é‡Š# sudo vim /etc/sudoers.d/10-installer åœ¨%wheel ALL=(ALL) ALLè¡Œåæ·»åŠ å¦‚ä¸‹é…ç½® shmily ALL=(ALL) NOPASSWD: ALL %shmily ALL=(ALL) NOPASSWD: ALL è½¯ä»¶å•†åº—å‹¾é€‰å¯ç”¨AURå’ŒSnapæº ä¸­æ–‡è¾“å…¥æ³•å®‰è£…è¾“å…¥æ³•é¦–é€‰å®‰è£…:sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-qt fcitx5-gtk kcm-fcitx5 fcitx5-material-coloryay -S noto-color-emoji-fontconfig æ‰“å­—æ”¯æŒemojiè¡¨æƒ…ğŸ˜˜ sudo vim ~/.pam_environment ï¼ˆåœ¨å½“å‰æ¡Œé¢ç™»é™†çš„ç”¨æˆ·ä¸‹æ‰§è¡Œï¼‰ INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=\\@im=fcitx5 åç»­å¦‚æœå…¶ä»–ç”¨æˆ·éœ€è¦ä¸­æ–‡è¾“å…¥æ³• ä¹Ÿéœ€è¦åœ¨æ¯ä¸ªç”¨æˆ·çš„å®¶ç›®å½•ä¸‹åŠ ä»¥ä¸Šç¯å¢ƒå˜é‡æ³¨é”€é‡æ–°ç™»é™†åç”Ÿæ•ˆé…ç½®è¾“å…¥æ³•ï¼šå°†æ‹¼éŸ³ä¸Šç§»ï¼Œä½œä¸ºé»˜è®¤è¾“å…¥æ³•è®¾ç½®shiftæŒ‰é”®ä¸ºåˆ‡æ¢ä¸­è‹±æ–‡è¾“å…¥çš„æŒ‰é”®æ ¹æ®ä¸ªäººä¹ æƒ¯ è®¾ç½®å…±äº«è¾“å…¥çŠ¶æ€ (æˆ‘ä¸€èˆ¬ä¸è®¾ç½®)æ›´æ¢è¾“å…¥æ³•ä¸»é¢˜: vim ~/.config/fcitx5/conf/classicui.conf # å‚ç›´å€™é€‰åˆ—è¡¨ Vertical Candidate List=False # æŒ‰å±å¹• DPI ä½¿ç”¨ PerScreenDPI=True # Font (è®¾ç½®æˆä½ å–œæ¬¢çš„å­—ä½“) Font=&quot;æ€æºé»‘ä½“ CN Medium 13&quot; # ä¸»é¢˜åç§°(å¯¹åº”~/.local/share/fcitx5/themesä¸‹çš„ä¸»é¢˜ç›®å½•åç§°) Theme=Material-Color-DeepPurple ç„¶åé‡å¯è¾“å…¥æ³•å³å¯ç”Ÿæ•ˆ ä¹Ÿå¯ä»¥ä¸‹è½½ä¸»é¢˜åŒ…æ–‡ä»¶è§£å‹åˆ°~/.local/share/fcitx5/themesç›®å½•ä¸‹å¹¶ä¿®æ”¹~/.config/fcitx5/conf/classicui.confæ¥è®¾ç½®ä¸åŒä¸»é¢˜ ç®€çº¦é»‘ç™½ä¸»é¢˜ä¸‹è½½åœ°å€:fcitx5-simple-themes.zip å¯ç”¨äº‘æ‹¼éŸ³è¯´æ˜ï¼šfcitx5ä¸ºä¸»ä½“ï¼Œfcitx5-chinese-addonsä¸­æ–‡è¾“å…¥æ–¹å¼æ”¯æŒfcitx5-qtï¼Œå¯¹Qt5ç¨‹åºçš„æ”¯æŒfcitx5-gtkï¼Œå¯¹GTKç¨‹åºçš„æ”¯æŒfcitx5-qt4-gitAURï¼Œå¯¹Qt4ç¨‹åºçš„æ”¯æŒkcm-fcitx5æ˜¯KDEä¸‹çš„é…ç½®å·¥å…·ï¼Œä¸è¿‡åœ¨gnomeä¸‹ä¹Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æç¤ºï¼šä¸€èˆ¬æƒ…å†µä¸‹ï¼Œåªå®‰è£…fcitx5-qtå’Œfcitx5-gtkå°±å¯ä»¥äº†ï¼Œé…ç½®å·¥å…·fcitx5çš„é…ç½®æ–‡ä»¶ä½äº~/.local/share/fcitx5ï¼Œå°½ç®¡æ‚¨å¯ä»¥ä½¿ç”¨æ–‡æœ¬ç¼–è¾‘å™¨ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯ä½¿ç”¨ GUI é…ç½®æ˜¾ç„¶æ›´æ–¹ä¾¿ï¼Œkcm-fcitx5é›†æˆåˆ° KCM ä¸­çš„é…ç½®å·¥å…·ï¼Œä¸“ä¸ºKDEè€Œç”Ÿfcitx5-config-qt-git AURï¼šQtå‰ç«¯çš„fcitx5é…ç½®å·¥å…·ï¼Œä¸kcm-fcitx5ç›¸å†²çªã€‚æ³¨æ„ï¼šå¯¹äºéKDEç•Œé¢ï¼Œå¯ä»¥ä½¿ç”¨fcitx5-config-qt-gitAUR,è¯¥è½¯ä»¶åŒ…ä¸kcm-fcitx5ç›¸å†²çªï¼Œä½ éœ€è¦æ‰‹åŠ¨å¸è½½å®ƒã€‚å…¶ä»–å¯é€‰è¾“å…¥æ³•ç»„ä»¶ï¼šsunpinyin+sunpinyin-datafcitx-sunpinyinibus-sunpinyinkcm-fcitx Gité…ç½®git config --global user.name &quot;shmily&quot; git config --global user.email 710552907@qq.com git config --global http.version HTTP/1.1 git config --global core.autocrlf false git config --global core.safecrlf false git config --global core.autocrlf input #æäº¤æ—¶è½¬æ¢ä¸ºLFï¼Œæ£€å‡ºæ—¶ä¸è½¬æ¢ git config http.proxy socks5://127.0.0.1:7891 # å› ä¸ºæˆ‘çš„Clashä»£ç†sockç«¯å£æ˜¯7891 git config --global --add remote.origin.proxy &quot;&quot; git config --global core.editor &quot;vim&quot; å¼€æœºè‡ªå¯è„šæœ¬éƒ¨ç½²su rootvim /etc/systemd/system/rc-local.service åˆ›å»ºè¯¥æ–‡ä»¶ [Unit] Description=&quot;/etc/rc.local Compatibility&quot; [Service] Type=oneshot ExecStart=/etc/rc.local start TimeoutSec=0 StandardInput=tty RemainAfterExit=yes SysVStartPriority=99 [Install] WantedBy=multi-user.target vim /etc/rc.local åˆ›å»ºè¯¥æ–‡ä»¶ #!/bin/sh # /etc/rc.local if test -d /etc/rc.local.d; then for rcscript in /etc/rc.local.d/*.sh; do test -r &quot;$&#123;rcscript&#125;&quot; &amp;&amp; sh $&#123;rcscript&#125; done unset rcscript fi chmod a+x /etc/rc.localmkdir /etc/rc.local.dsystemctl enable rc-local.serviceè‡ªå®šä¹‰è„šæœ¬æ”¾åœ¨/etc/rc.local.d/é‡Œå°±å¯ä»¥äº† ç³»ç»Ÿå¸¸è§„ä¼˜åŒ–# 1.å¯ç”¨TRIMä¼šå¸®åŠ©æ¸…ç†SSDä¸­çš„å—ï¼Œä»è€Œå»¶é•¿SSDçš„ä½¿ç”¨å¯¿å‘½ sudo systemctl enable fstrim.timer # 2.å®‰è£…ä¸­æ–‡å­—ä½“ sudo pacman -S wqy-zenhei sudo pacman -S wqy-bitmapfont sudo pacman -S wqy-microhei sudo pacman -S ttf-wps-fonts sudo pacman -S adobe-source-han-sans-cn-fonts sudo pacman -S adobe-source-han-serif-cn-fonts # ls -lå‘½ä»¤ç®€å†™ll sudo vim /etc/profileå’Œ~/.bashrc alias ls=&#39;ls --color&#39; alias ll=&#39;ls -l --color&#39; åŒç³»ç»Ÿä¼˜åŒ–Windows+LinuxåŒç³»ç»Ÿå¯ä»¥åŠ å¦‚ä¸‹å‚æ•°ä½¿WindowsæŠŠç¡¬ä»¶æ—¶é—´å½“ä½œUTCï¼ˆé¿å…åŒç³»ç»Ÿåˆ‡æ¢å¯¼è‡´çš„æ—¶é—´é”™ä¹±ï¼‰Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1 LinuxéŸ³é¢‘éŸ³è´¨ä¼˜åŒ–åŠè“ç‰™è¿æ¥é—®é¢˜è§£å†³1.è“ç‰™è¿æ¥æ€»æ˜¯å¤±è´¥ï¼Œæå°æ¦‚ç‡æˆåŠŸï¼švim /etc/bluetooth/main.conf ä¿®æ”¹ControllerModeä¸ºbredr # Restricts all controllers to the specified transport. Default value # is &quot;dual&quot;, i.e. both BR/EDR and LE enabled (when supported by the HW). # Possible values: &quot;dual&quot;, &quot;bredr&quot;, &quot;le&quot; # ControllerMode = dual ControllerMode = bredr 2.éŸ³è´¨æå‡ç›®å‰Linuxç³»ç»Ÿæ’­æ”¾éŸ³è´¨å·®æ˜¯é€šç—…ï¼Œä½¿ç”¨pulseaudioå¯ä»¥è®©LinuxéŸ³è´¨æ¢å¤åˆ°ä¸Macã€Windowsç›¸è¿‘çš„æ°´å¹³ï¼Œæ›´å¤špulseaudioçš„é«˜é˜¶ä½¿ç”¨å¯å‚è€ƒï¼šPulseAudio - ArchWikiè½¯ä»¶åŒ…ç®¡ç†å™¨ä¸­å®‰è£…pulseaudioã€pulseaudio-bluetoothï¼Œä»¥åŠå…¶ä»–pulseaudioç›¸å…³åŒ…(å¯è‡ªè¡Œé€‰æ‹©)å®‰è£…åé‡å¯è“ç‰™æœåŠ¡å¹¶å¯åŠ¨pulseaudioï¼š systemctl restart bluetooth systemctl status bluetooth pulseaudio --start # å°†æ­¤å‘½ä»¤æ”¾åˆ°å¼€æœºè‡ªå¯è„šæœ¬ 3.è“ç‰™è®¾å¤‡ç»å¸¸éœ€è¦é‡æ–°é…å¯¹è“ç‰™é¼ æ ‡ã€è“ç‰™è€³æœºç»å¸¸éœ€è¦é‡æ–°é…å¯¹,æ¯æ¬¡é‡å¯éƒ½éœ€è¦é‡æ–°é…å¯¹,è§£å†³åŠæ³•:sudo vim /etc/bluetooth/main.confä¿®æ”¹FastConnectable = falseï¼Œå–æ¶ˆ#æ³¨é‡Šï¼Œæ”¹ä¸ºFastConnectable = trueä¿®æ”¹AutoEnable=falseï¼Œå–æ¶ˆ#æ³¨é‡Šï¼Œæ”¹ä¸ºAutoEnable=truesystemctl restart bluetooth å®‰è£…åº”ç”¨å’Œå·¥å…·å¸¸ç”¨è½¯ä»¶å®‰è£…# å¾®ä¿¡ã€TIM sudo pacman -S yay yay --aururl https://aur.tuna.tsinghua.edu.cn --save sudo pacman -Sy base-devel yay -S com.qq.weixin.spark yay -S linuxqq yay -S ocs-url å¢å¤§dpié¿å…çª—å£å’Œå­—ä½“è¿‡å°ï¼ˆåœ¨æ‰“å¼€çš„çª—å£ä¸­è®¾ç½® 2kå±å¹•å»ºè®®å€¼168-192ï¼‰ï¼š env WINEPREFIX=/home/shmily/.deepinwine/Spark-WeChat/ deepin-wine5 winecfg env WINEPREFIX=/home/shmily/.deepinwine/Spark-TIM/ deepin-wine5 winecfg ------------------------------------------------------------------------------------------------------- sudo pacman -S google-chrome # Chrome yay -S yesplaymusic # (æ¨èä»£æ›¿sudo pacman -S netease-cloud-musicç½‘æ˜“äº‘éŸ³ä¹)ä»£æ›¿ç½‘æ˜“äº‘éŸ³ä¹Linuxå®¢æˆ·ç«¯çš„éŸ³ä¹è½¯ä»¶ yay -S tenvideo # è…¾è®¯è§†é¢‘ sudo pacman -S unrar unzip p7zip # è§£å‹ ### å®‰è£…WPSï¼šè½¯ä»¶å•†åº—å®‰è£…å¦‚ä¸‹åŒ…ï¼šwps-office-cn wps-office-mui-zh-cn wps-office-mime-cn ttf-wps-fonts sudo pacman -S gimp # ä¿®å›¾ sudo pacman -S kdenlive # è§†é¢‘ç¼–è¾‘ (å¯åœ¨åº”ç”¨å•†åº—å®‰è£…é€‰æ›´å¤šåŠŸèƒ½æ’ä»¶) sudo pacman -S neofetch screenfetch # è¾“å‡ºç³»ç»Ÿä¿¡æ¯ ------------------------------------------------------------------------------------------------------- #è¿œç¨‹æ¡Œé¢å·¥å…· 1. x11vnc linux x11vnc è¿œç¨‹æ¡Œé¢ï¼ˆX11æ¡Œé¢ç¯å¢ƒé€‚ç”¨ï¼Œä¸é€‚ç”¨äºwaylandï¼‰ æœåŠ¡ç«¯é…ç½®ï¼š sudo pacman -S x11vnc # å®‰è£…x11vnc x11vnc -storepasswd # è®¾ç½®VNCçš„è¿æ¥å¯†ç  vim x11vnc-start.sh # x11vncæœåŠ¡å¯åŠ¨è„šæœ¬ #!/bin/bash x11vnc -display :0 -forever -shared -rfbauth ~/.vnc/passwd -bg -o /tmp/x11vnc.log -rfbport 5900 -ncache 10 -ncache_cr å°†VNC Serverè®¾ç½®æˆéšç³»ç»Ÿå¯åŠ¨åè‡ªåŠ¨åœ¨åå°å¯åŠ¨ å°†x11vnc-start.shæ·»åŠ åˆ° è®¾ç½®-&gt;å¼€æœºä¸å…³æœº-&gt;è‡ªåŠ¨å¯åŠ¨ ä¸­ æ‰‹åŠ¨æ‰§è¡Œx11vnc-start.shæˆ–é‡å¯Linuxç³»ç»Ÿ è¿œç¨‹è¿æ¥å·¥å…· Windowsï¼šRealVNC Viewer Linuxï¼š Remmina 2.anydesk yay -S anydesk 3.todesk yay -S todesk;sudo systemctl enable todeskd.service;sudo systemctl start todeskd.service;sudo systemctl status todeskd.service ### è¿œç¨‹æ¡Œé¢è¿æ¥å®¢æˆ·ç«¯remmina sudo pacman -S remmina å®‰è£…å·¥ç¨‹ä¼šæç¤º remmina çš„å¯é€‰ä¾èµ– freerdp: RDP plugin libsecret: Secret plugin [å·²å®‰è£…] libvncserver: VNC plugin libxkbfile: NX plugin [å·²å®‰è£…] nxproxy: NX plugin spice-gtk: Spice plugin telepathy-glib: Telepathy plugin xorg-server-xephyr: XDMCP plugin gnome-terminal: external tools é€‰æ‹©è‡ªå·±æƒ³è¦çš„ä¾èµ–ï¼Œå¦‚RDPè¿œç¨‹æ¡Œé¢è¿æ¥: sudo pacman -S freerdp ------------------------------------------------------------------------------------------------------- ### ä¼ä¸šå¾®ä¿¡å®‰è£… https://aur.archlinux.org/packages/com.qq.weixin.work.deepin/ ä¸‹è½½debåŒ… ç”¨Arkæ‰“å¼€debåŒ… è§£å‹å‡ºdata.tar.xz å†è§£å‹data.tar.xzä¸­çš„opt/apps/com.qq.weixin.work.deepinè§£å‹åˆ°/opt/apps/ cd /opt/apps/com.qq.weixin.work.deepin ä¿®æ”¹/opt/apps/com.qq.weixin.work.deepin/entries/applications/com.qq.weixin.work.deepin.desktopä¸­Iconçš„å€¼ï¼š/opt/apps/com.qq.weixin.work.deepin/entries/icons/hicolor/48x48/apps/com.qq.weixin.work.deepin.svg sudo cp /opt/apps/com.qq.weixin.work.deepin/entries/applications/com.qq.weixin.work.deepin.desktop /usr/share/applications å¢å¤§dpié¿å…çª—å£å’Œå­—ä½“è¿‡å°ï¼ˆåœ¨æ‰“å¼€çš„çª—å£ä¸­è®¾ç½® 2kå±å¹•å»ºè®®å€¼168-192ï¼‰ï¼š env WINEPREFIX=/home/shmily/.deepinwine/Deepin-WXWork/ deepin-wine5 winecfg ------------------------------------------------------------------------------------------------------- è®¾ç½®shellæ¬¢è¿è¯­ ç™»é™†Shellç¯å¢ƒ ä¼šè‡ªåŠ¨è¾“å‡º cat &lt;&lt;EOT &gt;/etc/motd QJJåŠ æ²¹ï¼Œä½ æœ€æœˆåŠï¼ EOT ------------------------------------------------------------------------------------------------------- è½¯ä»¶ä»“åº“å®‰è£…ï¼šTyporaï¼ŒShotcutï¼Œlaptop-mode-tools(å¯é€‰ æœ‰tlpå¯ä»¥ä¸ç”¨) è½¯ä»¶ä»“åº“å®‰è£…:timeshift (ç³»ç»Ÿå¯èƒ½å·²ç»è‡ªå¸¦äº†) è½¯ä»¶ä»“åº“å®‰è£…:æ·±åº¦å½±é™¢ æ·±åº¦ç›¸æœº BaiduNetDiskç™¾åº¦ç½‘ç›˜ ------------------------------------------------------------------------------------------------------- # å®‰è£…æ–‡ä»¶åŒæ­¥å·¥å…· å¤šç«¯åŒæ­¥ sudo pacman -S syncthing å‚è€ƒhttps://github.com/syncthing/syncthing/tree/main/etc/linux-desktopåˆ›å»ºå¿«æ·æ–¹å¼ å¯åŠ¨Syncthingçš„å¿«æ·æ–¹å¼syncthing-start.desktop [Desktop Entry] Name=Start Syncthing GenericName=File synchronization Comment=Starts the main syncthing process in the background. Exec=/usr/bin/syncthing serve --no-browser --logfile=default Icon=syncthing Terminal=false Type=Application Keywords=synchronization;daemon; Categories=Network;FileTransfer;P2P æŸ¥çœ‹Syncthing UIçš„å¿«æ·æ–¹å¼syncthing-ui.desktop [Desktop Entry] Name=Syncthing Web UI GenericName=File synchronization UI Comment=Opens Syncthing&#39;s Web UI in the default browser (Syncthing must already be started). Exec=/usr/bin/syncthing -browser-only Icon=syncthing Terminal=false Type=Application Keywords=synchronization;interface; Categories=Network;FileTransfer;P2P ç”Ÿæ•ˆå¿«æ·æ–¹å¼ sudo desktop-file-install syncthing-start.desktop sudo desktop-file-install syncthing-ui.desktop Clashç§‘å­¦ä¸Šç½‘æ–¹æ¡ˆä¸€.Clashå®¢æˆ·ç«¯yay -S clash-for-windows-chinese æ–¹æ¡ˆäºŒ.åŸç‰ˆClashå®¢æˆ·ç«¯:ä¸‹è½½Clash cd ~/ä¸‹è½½ gunzip clash-linux-amd64-v1.6.5.gz mkdir /opt/apps/Clash mv clash-linux-amd64-v1.6.5 /opt/apps/Clash/ cd /opt/apps/Clash chmod +x clash-linux-amd64-v1.6.5 ./clash-linux-amd64-v1.6.5 ç›´åˆ°å‡ºç°INFO[0003] Mixed(http+socks5) proxy listening at: 127.0.0.1:7890å³å¯å…³é—­ ls ~/.config/clash ä¼šæœ‰config.yaml Country.mmdb å¦‚æœæ²¡å‡ºç°ä¸Šè¿°INFOæ—¥å¿—åˆ™å¯èƒ½æ˜¯Country.mmdbä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½ sudo touch /usr/share/applications/Clash.desktop chmod a+x /usr/share/applications/Clash.desktop cat&gt;/usr/share/applications/Clash.desktop&lt;&lt;EOF [Desktop Entry] Name=Clash For Linux Comment=clash-for-linux Encoding=UTF-8 Exec=/opt/apps/Clash/clash-linux-amd64-v1.6.5 Icon=/opt/apps/Clash/logo_64.png Categories=System;Application;Network; StartupNotify=true Terminal=false Type=Application EOF # ç”Ÿæ•ˆæˆ‘ä»¬çš„ä»£ç†é…ç½®æ–‡ä»¶ cp ~/ä¸‹è½½/Clash_1625991739.yaml ~/.config/clash/config.yaml ä½¿ç”¨WebUIç®¡ç†è¿æ¥ï¼šæ ¹æ®cat ~/.config/clash/config.yaml | grep external-controllerçš„ç»“æœï¼Œé€šè¿‡http://clash.razord.topè¿›è¡Œç­–ç•¥ç»„èŠ‚ç‚¹çš„åˆ‡æ¢ åªæµè§ˆç½‘é¡µæ¨èä½¿ç”¨Chromeæµè§ˆå™¨æ’ä»¶Proxy SwitchyOmega:å¿…è¦æ—¶å¯ä»¥ä½¿ç”¨ç³»ç»Ÿå…¨å±€ä»£ç†ï¼šè¿›å…¥ç³»ç»Ÿè®¾ç½®-&gt;ç½‘ç»œè®¾ç½®-&gt;ä½¿ç”¨ç³»ç»Ÿä»£ç†æœåŠ¡å™¨é…ç½®(æˆ–ä½¿ç”¨æ‰‹åŠ¨è®¾ç½®çš„ä»£ç†æœåŠ¡å™¨)-&gt;httpä»£ç†è®¾ä¸º127.0.0.1:7890 Socksä»£ç†è®¾ç½®ä¸º127.0.0.1:7891é…ç½®Clashå¼€æœºè‡ªå¯ï¼š cp /usr/share/applications/Clash.desktop ~/.config/autostart/ æˆªå±å½•å± åŠŸèƒ½å…¨é¢çš„å½•å±å’Œä¸²æµç›´æ’­è½¯ä»¶sudo pacman -S obs-studio kazamsudo pacman -S kazam å¯ä»¥æˆªå›¾å’Œå½•å±çš„å·¥å…· æ·±åº¦æˆªå›¾sudo pacman -S deepin-screenshot è‡ªå¸¦æˆªå›¾å·¥å…·Spectacleæ—¥å¸¸æˆªå›¾è‡ªå¸¦æˆªå›¾å·¥å…·å°±è¶³å¤Ÿäº†ï¼Œåªæ˜¯ä¸Windowsç«¯æˆ‘ä»¬å¸¸ç”¨çš„Ctrl+Alt+Aä¸å¤ªä¸€æ ·ï¼Œå¯ä»¥è®°ä½å®ƒçš„å¿«æ·é”®ï¼Œç”¨èµ·æ¥ä¹Ÿå¾ˆæ–¹ä¾¿ä¸»è¦å¸¸ç”¨çš„å°±æ˜¯Meta+Print ï¼ˆå³Win+PrtScnï¼‰ æˆªå–å½“å‰æ´»åŠ¨çš„çª—å£ æ·±åº¦å½•å±sudo pacman -S deepin-screen-recorderln -s /usr/bin/deepin-screen-recorder /usr/bin/sr è¿è¡Œsrå‘½ä»¤ä½¿ç”¨æ·»åŠ æˆªå›¾åŠŸèƒ½åˆ°ç³»ç»Ÿå…¨å±€å¿«æ·æ–¹å¼:è®¾ç½®-&gt;å¿«æ·é”®-&gt;è‡ªå®šä¹‰å¿«æ·é”®-&gt;ç¼–è¾‘-&gt;æ–°å»º-&gt;å…¨å±€å¿«æ·é”®-&gt;å‘½ä»¤/URL-&gt;å‘½ä»¤/usr/bin/deepin-screen-recorder è§¦å‘å™¨Ctrl+Alt+A è¯å…¸ç¿»è¯‘è¯å…¸ä¸å±å¹•å–è¯ç¿»è¯‘å·¥å…·sudo pacman -S goldendictä¸‹è½½æœ—æ–‡è¯å…¸æ–‡ä»¶:æå–ç 9m43 (å¦‚æœé“¾æ¥å¤±æ•ˆä¹Ÿå¯ä»å…¶ä»–é€”å¾„ä¸‹è½½)mkdir /home/shmily/tools/.LDOCE_Dict å°†è¯å…¸æ–‡ä»¶æ”¾è¿›å»åœ¨GoldenDictä¸Šé…ç½®è¯å…¸å³å¯.!alt ä¹…åæé†’å¯ä»¥è®¾ç½®ä¹…åæé†’,å°æ†©æ—¶é—´çš„å·¥å…·,å¯å¼ºåˆ¶ä¼‘æ¯ yay -S stretchly-bin !alt å¼€å‘ç¯å¢ƒå®‰è£…sudo pacman -S net-tools dnsutils inetutils iproute2 stress python-pip screen htop bat tree ncdu tig tldr sudo pacman -S nodejs sudo pacman -S npm sudo pacman -S make sudo pacman -S cmake sudo pacman -S clang sudo pacman -S maven ---------------------------------------------------------------------------------- # Javaã€Scalaç¯å¢ƒå®‰è£… # ä¸‹è½½jdk-8u181-linux-x64.tar.gz # å¸è½½ç³»ç»Ÿé»˜è®¤jdkï¼ˆç³»ç»Ÿé»˜è®¤ä½¿ç”¨Java8ä¼šå¯¼è‡´éƒ¨åˆ†ä¾èµ–javaè¿è¡Œçš„è½¯ä»¶å‡ºç°ä¸å…¼å®¹ç°è±¡ï¼Œå»ºè®®å°†ç³»ç»ŸJAVA_HOMEæ”¹ä¸ºjdk11+ç‰ˆæœ¬ï¼‰ sudo archlinux-java unset # å¦åˆ™ä¸ä¼šä»ç¯å¢ƒå˜é‡è¯»javaåœ°å€ sudo tar -zxvf jdk-8u181-linux-x64.tar.gz -C /opt/Env/ # ä¸‹è½½scala-2.12.12.tgz sudo tar -zxvf scala-2.12.12.tgz -C /opt/Env/ # ä¸‹è½½Golang https://golang.google.cn/dl/go1.18.3.linux-amd64.tar.gz sudo tar -zxvf go1.18.3.linux-amd64.tar.gz -C /opt/Env/ mkdir -p /home/shmily/.gopath sudo vim /etc/profile export JAVA8_HOME=/opt/Env/jdk1.8.0_181 export JAVA_HOME=/opt/Env/jdk-11.0.11 export PATH=$PATH:$JAVA_HOME/bin export SCALA_HOME=/opt/Env/scala-2.12.12 export PATH=$PATH:$SCALA_HOME/bin source /etc/profile export GO_HOME=/opt/Env/go1.18.3 export PATH=$PATH:$GO_HOME/bin sudo pacman -S pkg-config # Javaåç¼–è¯‘å·¥å…·å®‰è£… yay -S jd-gui ------------------------------------------------------------------- Pythonæºåˆ‡æ¢ sudo pip config --global set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple sudo pip config --global set install.trusted-host pypi.tuna.tsinghua.edu.cn Anacondaå®‰è£… sudo pacman -S anaconda sudo vim /etc/profileå¢åŠ  export PATH=/opt/anaconda/bin:$PATH source /etc/profile conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ Pythonç¯å¢ƒé…ç½® conda create -n py310_auto_gpt python=3.10 æ¿€æ´»ç¯å¢ƒconda activate py310_auto_gpt å–æ¶ˆæ¿€æ´»conda deactivate æŸ¥çœ‹å·²æœ‰ç¯å¢ƒconda info -e åˆ é™¤æŸä¸ªç¯å¢ƒconda remove -n py310_auto_gpt --all ---------------------------------------------------------------------------------------- Mysqlå®‰è£…ï¼š pacman -Si mysql # æŸ¥çœ‹ä»“åº“ä¸­çš„MySQLç‰ˆæœ¬å· sudo pacman -S mysql # å®‰è£…mysql sudo mkdir /opt/mysql-data sudo chmod 1777 /opt/mysql-data sudo mysqld --initialize --user=shmily --basedir=/usr --datadir=/opt/mysql-data --character-set-server=UTF8MB4 vim /etc/mysql/my.cnf ä¿®æ”¹datadir=/opt/mysql-data chmod -R 777 /opt/mysql-data sudo systemctl start mysqld.service systemctl status mysqld.service åˆå§‹å¯†ç ç™»é™† alter user &#39;root&#39;@&#39;localhost&#39; identified with mysql_native_password by &#39;123456&#39; mysql -uroot -p123456 sudo systemctl enable mysqld.service # è®¾ç½®å¼€æœºå¯åŠ¨mysql server (å¯é€‰) æ”¯æŒUbuntuç³»å®‰è£…åŒ…(.debåŒ…)git clone https://github.com/helixarch/debtapcd debtapsudo cp debtap /usr/local/binï¼ˆå‰ä¸‰æ­¥å¯ä»¥ç”¨yay -S debtapæˆ–yaourt -S debtapä»£æ›¿ï¼‰sudo debtap -u æ›´æ–°è½¯ä»¶åŒ…å¦‚æœsudo debtap -uè¿‡ç¨‹ä¸‹è½½å¾ˆæ…¢ï¼Œéœ€è¦æ¢æº vim /usr/bin/debtap æ›¿æ¢ï¼šhttp://ftp.debian.org/debian/dists https://mirrors.ustc.edu.cn/debian/dists æ›¿æ¢ï¼šhttp://archive.ubuntu.com/ubuntu/dists https://mirrors.ustc.edu.cn/ubuntu/dists/ ç„¶åå°±å¯ä»¥æ“ä½œå®‰è£….debåŒ…äº†debtap xxx.deb ï¼ˆä¸€è·¯ä¸‹ä¸€æ­¥ï¼Œè¯ä¹¦é€‰GPLï¼‰å¾—åˆ°è§£æåçš„å®‰è£…åŒ…(Final Package)sudo pacman -U è§£æåçš„å®‰è£…åŒ… å¼€å‘å·¥å…·å®‰è£…# JetBrainså…¨å®¶æ¡¶ å‘½ä»¤è¡Œæ–¹å¼å®‰è£… sudo pacman -S intellij-idea-ultimate-edition # å®‰è£…IDEAæœ€æ–°æ——èˆ°ç‰ˆ sudo pacman -S pycharm-community-edition # å®‰è£…PyCharm sudo pacman -S goland # å®‰è£…Goland # JetBrainså…¨å®¶æ¡¶ æ‰‹åŠ¨ä¸‹è½½æ–¹å¼å®‰è£… [æ¨è] # ä»¥GoLandä¸ºä¾‹ tar -zxvf goland-xxx.tar.gz -C /opt/apps/ mv /opt/apps/goland-2021.3.4 /opt/apps/GoLand åˆ›å»ºå¿«æ·æ–¹å¼--è‡ªåŠ¨(æ¨è):æ‰“å¼€IDE,Tools -&gt; Create Desktop Entry. åˆ›å»ºå¿«æ·æ–¹å¼--æ‰‹åŠ¨: cd /opt/apps/GoLand touch GoLand.desktop å†…å®¹å¦‚ä¸‹ [Desktop Entry] Name=GoLand Comment=GoLand Exec=/opt/apps/GoLand/bin/goland.sh Icon=/opt/apps/GoLand/bin/goland.png Terminal=false Type=Application Categories=Development ç„¶åæ‰§è¡Œsudo desktop-file-install GoLand.desktop å®‰è£…å¿«æ·æ–¹å¼ ----------------------------------------------------------------------- # å®‰è£…VSCode(yay -S visual-studio-code-bin)ï¼š # é¦–å…ˆå®˜ç½‘å»ä¸‹è½½å®‰è£…åŒ…vscodeå®˜ç½‘https://code.visualstudio.com å¾—åˆ°code-stable-xxxxxxx.tar.gz tar -zxvf code-stable-x64-1623937300.tar.gz -C /opt/apps/ sudo chmod +x /opt/apps/VSCode-linux-x64/code ln -s /opt/apps/VSCode-linux-x64/code /usr/local/bin/code touch /usr/share/applications/VSCode.desktop chmod +x /usr/share/applications/VSCode.desktop cp /opt/apps/VSCode-linux-x64/resources/app/resources/linux/code.png /usr/share/icons/ cat&gt;/usr/share/applications/VSCode.desktop&lt;&lt;EOF [Desktop Entry] Name=Visual Studio Code Comment=Multi-platform code editor for Linux Exec=/opt/apps/VSCode-linux-x64/code Icon=/usr/share/icons/code.png Type=Application StartupNotify=true Categories=TextEditor;Development;Utility; MimeType=text/plain; EOF ----------------------------------------------------------------------- # Typora markdownå·¥å…· yay -S typora # Sublimeå®‰è£… å‚è€ƒhttps://www.sublimetext.com/docs/3/linux_repositories.html#pacman æ¿€æ´»ç ï¼š ----- BEGIN LICENSE ----- Member J2TeaM Single User License EA7E-1011316 D7DA350E 1B8B0760 972F8B60 F3E64036 B9B4E234 F356F38F 0AD1E3B7 0E9C5FAD FA0A2ABE 25F65BD8 D51458E5 3923CE80 87428428 79079A01 AA69F319 A1AF29A4 A684C2DC 0B1583D4 19CBD290 217618CD 5653E0A0 BACE3948 BB2EE45E 422D2C87 DD9AF44B 99C49590 D2DBDEE1 75860FD2 8C8BB2AD B2ECE5A4 EFC08AF2 25A9B864 ------ END LICENSE ------â€‹ # DBeaver æ•°æ®åº“ç®¡ç†å·¥å…·å®‰è£… sudo pacman -S dbeaver ä¿®æ”¹DBeaveré»˜è®¤JDKç‰ˆæœ¬ä¸ºJDK11ä»¥ä¸Š: sudo vim /usr/lib/dbeaver/dbeaver.ini æ·»åŠ  -vm /usr/java/jdk-11.0.16/bin è™šæ‹Ÿæœºè½¯ä»¶å®‰è£…å®‰è£…VirtualBox:mhwd-kernel -li (å½“å‰ç³»ç»Ÿæ˜¯linux510ï¼Œåˆ™å®‰è£…linux510-virtualbox-host-modules)sudo pacman -Syu virtualbox linux510-virtualbox-host-modulesé‡å¯æˆ–æ‰§è¡Œsudo vboxreload å®‰è£…KVMï¼ˆå¤‡é€‰ï¼‰ï¼špacman -S qemu libvirt ovmf virt-managerï¼ˆkvmè´Ÿè´£CPUå’Œå†…å­˜çš„è™šæ‹ŸåŒ–ï¼Œqemuå‘Guest OSæ¨¡æ‹Ÿç¡¬ä»¶ï¼Œovmfä¸ºè™šæ‹Ÿæœºå¯ç”¨UEFIæ”¯æŒï¼Œlibvirtæä¾›ç®¡ç†è™šæ‹Ÿæœºå’Œå…¶å®ƒè™šæ‹ŸåŒ–åŠŸèƒ½çš„å·¥å…·å’ŒAPIï¼Œvirt-manageræ˜¯ç®¡ç†è™šæ‹Ÿæœºçš„GUIï¼‰systemctl enable libvirtdsystemctl start libvirtdusermod -a -G kvm shmilyå¯åŠ¨qem/virt-manager å®‰å“åº”ç”¨æ”¯æŒå‚è€ƒUOS(Deepin)å¯¹äºå®‰å“åº”ç”¨æ”¯æŒçš„è§£å†³æ–¹æ¡ˆï¼Œé‡‡ç”¨XDroidä½œä¸ºå®‰å“åº”ç”¨æ”¯æŒè½¯ä»¶ã€‚å…ˆä¸‹è½½XDroidè½¯ä»¶ï¼šå®˜ç½‘ä¸‹è½½XDroidæ‰§è¡Œtar -zxvf xDroidInstall-x86_64-vxxxx.tar.gz è§£å‹æ‰§è¡Œ ./xDroidInstall-x86_64-vxxxx.run å®‰è£…XDroidå®‰è£…åé‡å¯ä¸€åˆ°ä¸¤æ¬¡å³å¯å®Œæˆå®‰è£…åº”ç”¨å•†åŸå·²å®‰è£…åº”ç”¨ä½¿ç”¨Android APP ç³»ç»Ÿç•Œé¢ç¾åŒ–Manjaro Linuxæ˜¯å¯ä»¥éšç”¨æˆ·å¿ƒæƒ…éšæ„å®šåˆ¶çš„ï¼Œå¯å®šåˆ¶åŒ–ç¨‹åº¦æé«˜ï¼Œæ˜¯æ¡Œé¢æ§çš„ç¦éŸ³ã€‚ä¸‹é¢åšä¸€äº›ç®€å•çš„ç•Œé¢è®¾ç½®ã€‚ Dockæ sudo pacman -S latte-dockæ ¹æ®åå¥½è®¾ç½®latte dockæ•ˆæœlatte-dockå…¼å®¹IDEA,PyCharm,Golandå¯åŠ¨å›¾æ ‡æ‰“å¼€IDE,Tools -&gt; Create Desktop Entry.ç„¶åæ‰“å¼€ä¸Šä¸€æ­¥åˆ›å»ºçš„å¿«æ·æ–¹å¼,å†å›ºå®šåˆ°latte-dockå³å¯. æ›¿ä»£å“: plankä¼˜ç‚¹: èµ„æºå ç”¨è¾ƒlatte-dockå°å¾ˆå¤š,æ”¯æŒå¤šç§çš®è‚¤ä¸‹è½½ç¼ºç‚¹: ç‚¹å›¾æ ‡åæ‰€æœ‰è¯¥ç¨‹åºçš„çª—å£éƒ½è¢«æ‰“å¼€sudo pacman -S plankè®¾ç½®plank:plank â€“preferences oh-my-zsh sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; #å®‰è£…powerlevel10kä¸»é¢˜ sudo pacman -Sy --noconfirm zsh-theme-powerlevel10k #é…ç½®powerlevel10k echo &#39;source /usr/share/zsh-theme-powerlevel10k/powerlevel10k.zsh-theme&#39; &gt;&gt;! ~/.zshrc #ä½¿é…ç½®ç«‹å³ç”Ÿæ•ˆ source ~/.zshrc # æŒ‰æç¤ºè®¾ç½®å³å¯ è®¾ç½®æ—¶å»ºè®®ä¸è¦å¸¦å›¾æ ‡ï¼Œå› ä¸ºåœ¨å…¶ä»–ç”¨åˆ°zshçš„ç»ˆç«¯ä¸Šå¯èƒ½ä¼šå‡ºç°ä¹±ç æˆ–æ–¹æ¡†ï¼Œå¾ˆä¸ç¾è§‚ï¼Œå¯ä»¥åœ¨å…¶ä»–ç”¨åˆ°zshçš„ç»ˆç«¯ä¸Šé‡æ–°æ‰§è¡Œp10k configureå‘½ä»¤æ¥è®¾ç½®æ›´åˆé€‚çš„æ ·å¼ã€‚ # å®‰è£…è¯­æ³•é«˜äº®æ’ä»¶ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting echo &quot;source $ZSH_CUSTOM/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot; &gt;&gt; $&#123;ZDOTDIR:-$HOME&#125;/.zshrc source ~/.zshrc å‘½ä»¤è¡Œç»ˆç«¯å¯é€‰ä»£æ›¿Konsoleçš„æ›´å¥½çœ‹çš„å‘½ä»¤è¡Œç»ˆç«¯Tabby(åŸTerminus)ï¼šhttps://github.com/Eugeny/tabbyç»§ç»­é…ç½®Konsole: å…¨å±€ä¸»é¢˜ æ‰“å¼€è®¾ç½®-&gt;å¤–è§‚-&gt;å…¨å±€ä¸»é¢˜ä¹‹æ‰€ä»¥å–œæ¬¢ç”¨Macçš„å…¨å±€ä¸»é¢˜ï¼ˆMcSur-darkï¼‰æ˜¯å› ä¸ºå®ƒçš„ä»»åŠ¡æ æ¯”è¾ƒå¥½çœ‹æ¯”è¾ƒç²¾è‡´ çª—å£çª—å£é€‰æ‹©äº†è¿™æ¬¾ï¼ŒæŒ‰é’®æ¯”è¾ƒç®€æ´ï¼Œæ˜¯å’ŒMacç›¸è¿‘çš„ï¼Œä½†æŒ‰é’®ä½ç½®åœ¨å³è¾¹ï¼Œæˆ‘è¿˜æ˜¯æ¯”è¾ƒä¹ æƒ¯è¿™ç§æŒ‰é”®ä½ç½®ã€‚ç‚¹å‡»ä¸»é¢˜ä¸Šçš„æŒ‰é’®å¯ä»¥è°ƒèŠ‚ä¸»é¢˜æŒ‰é”®å¤§å°ï¼Œä¸‹æ–¹å¯ä»¥è°ƒèŠ‚çª—å£è¾¹æ¡†å¤§å°çª—å£é…è‰²æ–¹æ¡ˆæˆ‘é€‰çš„Ambiance-ISHï¼Œäº®è‰²çœ‹èµ·æ¥æ›´æ•äº®ï¼Œå¿ƒæƒ…æ›´å¥½ã€‚ ç™»é™†é¡µç™»é™†é¡µé¢åªæœ‰æ¯æ¬¡å¼€æœºæ—¶æ‰ä¼šå‡ºç°ï¼Œé”å±æ˜¯å•ç‹¬çš„é”å±é¡µé¢ã€‚æ‰“å¼€è®¾ç½®-&gt;å¼€æœºä¸å…³æœº-&gt;ç™»å½•å±å¹•(SDDM) åœ¨è¿™é‡Œè®¾ç½®æ•ˆæœï¼š æ¬¢è¿å±å¹•å¼€æœºåœ¨ç™»é™†é¡µé¢è¾“å…¥å¯†ç åä¼šè¿›å…¥æ¬¢è¿å±å¹•ï¼Œå¤§æ¦‚æœ‰2ç§’å·¦å³åœç•™åœ¨æ¬¢è¿å±å¹•ï¼Œéšä¾¿é€‰ä¸ªå¥½çœ‹çš„å°±å¯ä»¥äº†ã€‚ é”å±ç•Œé¢æ¯æ¬¡é”å±(Meta+L)åéƒ½ä¼šæ˜¾ç¤ºè¿™ä¸ªé¡µé¢ã€‚æ‰“å¼€è®¾ç½®-&gt;å·¥ä½œåŒºè¡Œä¸º-&gt;é”å±-&gt;å¤–è§‚ï¼šé…ç½® æ¡Œé¢ç‰¹æ•ˆè¿™é‡Œä¼šæœ‰ä¸€äº›ç¥å¥‡çš„ç•Œé¢æ•ˆæœï¼Œå¦‚çª—å£æƒ¯æ€§æ‹–åŠ¨ï¼Œæœ€å°åŒ–ç¥ç¯æ•ˆæœï¼Œçª—å£åˆ‡æ¢æ•ˆæœç­‰ã€‚æ‰“å¼€è®¾ç½®-&gt;å·¥ä½œåŒºè¡Œä¸º-&gt;æ¡Œé¢ç‰¹æ•ˆä¸»è¦æ”¹åŠ¨çš„åœ°æ–¹ï¼šæ°”æ³¡ç›¸å…³ã€çª—å£èƒŒæ™¯è™šåŒ–ã€çª—å£é€æ˜åº¦ã€çª—å£æƒ¯æ€§æ™ƒåŠ¨ã€æœ€å°åŒ–è¿‡æ¸¡åŠ¨ç”»(ç¥ç¯)ã€çª—å£åæ»‘ç‰¹æ•ˆã€çª—å£æ‰“å¼€\\å…³é—­åŠ¨æ•ˆã€è™šæ‹Ÿæ¡Œé¢åˆ‡æ¢åŠ¨æ•ˆ æ‰‹åŠ¨å®‰è£…ä¸»é¢˜ã€å£çº¸ã€æ’ä»¶æœ¬èŠ‚ä¸»è¦ä½¿ç”¨kpackagetool5å‘½ä»¤æ‰“å¼€https://store.kde.org/ å¯ä»¥å®‰è£…ä¸€äº›ä¸»é¢˜æ¯”å¦‚å®‰è£…åŠ¨æ€å£çº¸æ’ä»¶ï¼šä¸‹è½½Smart Video Wallpaperæ’ä»¶çš„taråŒ…ï¼šhttps://store.kde.org/p/1316299/kpackagetool5 -t Plasma/Wallpaper -i smartvideowallpaper.tar.gzç„¶åè¿›å…¥å£çº¸è®¾ç½®é€‰æ‹©å£çº¸ç±»å‹è¿˜æœ‰è§†é¢‘è·¯å¾„å³å¯æ¯”å¦‚å®‰è£…ä¸»é¢˜ï¼škpackagetool5 -t Plasma/Theme -i Gently.tar.gz å¨±ä¹å‘½ä»¤yay -S oneko # ä¸€åªè·Ÿç€é¼ æ ‡èµ°çš„å°çŒ« sudo pacman -S nyancat # shellçª—å£ä¸­çš„å½©è™¹çŒ« yay -S hollywood # å¥½è±åæ•ˆæœshell sudo pacman -S cmatrix # é»‘å®¢å¸å›½æ•ˆæœshell sudo pacman -S sl # lsé”™å†™æˆslåä¼šæ˜¾ç¤ºå°ç«è½¦ sudo pacman -S fortune-mod # éšæœºä¸€å¥ç¬‘è¯\\åè¨€ (ä¸­æ–‡ç‰ˆæœ¬å¯ä»¥åœ¨åº”ç”¨å•†åº—å®‰è£…fortune-mod-zh) sudo pacman -S cowsay # cowsay -f tux haha ; cowsay haha è®©cowsayè¯´å‡ºfortuneçš„å†…å®¹: fortune | cowsay sudo pacman -S figlet # è‰ºæœ¯å­—ç”Ÿæˆå™¨ï¼Œç”±ASCIIå­—ç¬¦ç»„æˆï¼ŒæŠŠæ–‡æœ¬æ˜¾ç¤ºæˆæ ‡é¢˜æ ,å¯åŠ é¢œè‰² figlet haha sudo pacman -S toilet # è‰ºæœ¯å­—ç”Ÿæˆå™¨ï¼Œç”±ASCIIå­—ç¬¦ç»„æˆï¼ŒæŠŠæ–‡æœ¬æ˜¾ç¤ºæˆæ ‡é¢˜æ ,å¯åŠ é¢œè‰² toilet haha sudo pacman -S xorg-xeyes # æ‰§è¡Œxeyes ä¸€åŒå¤§çœ¼ç›ç›¯ç€ä½ çš„é¼ æ ‡ cal 1 1998 æ˜¾ç¤ºæœˆæ—¥å† shred /tmp/aaa æŸåæ–‡ä»¶,ç ´åæ–‡ä»¶å†…å®¹è®©äººæ— æ³•è¯†åˆ«å’ŒæŸ¥çœ‹ yay -S boxes # ASCIIè‰ºæœ¯æ¡† echo &quot;Haha&quot; | boxes ; echo &quot;Haha&quot; | boxes -d dog yay -S aview # å›¾ç‰‡è½¬ASCIIå›¾åƒ åŠ¨æ€å£çº¸å·¥å…·KDEæœ¬èº«æœ‰åŠ¨æ€å£çº¸æ’ä»¶,å¯ä»¥åœ¨å£çº¸è®¾ç½®ä¸­ä¸‹è½½SmartER video Wallpaperæ’ä»¶:è¿˜æœ‰ä¸€æ¬¾èµ„æºå¼€é”€è¾ƒä½çš„åŠ¨æ€å£çº¸è½¯ä»¶:fantascene-dynamic-wallpaper å¯ä»¥åœ¨åº”ç”¨å•†åº—æ‰¾åˆ°åŠ¨æ€å£çº¸å°†åŠ¨æ€è§†é¢‘æ–‡ä»¶ååºåˆ—åŒ–åˆ°å†…å­˜ä¸­,ä¼šä¸€å®šç¨‹åº¦å ç”¨æ˜¾å¡\\CPUå’Œå†…å­˜èµ„æº.ä½é…ç”µè„‘ä¸å»ºè®®ä½¿ç”¨. ç³»ç»Ÿä½¿ç”¨å°æŠ€å·§ä¸é—®é¢˜å¤„ç†å¿…é¡»æŒæ¡çš„ç³»ç»Ÿå¤‡ä»½å’Œæ¢å¤æŠ€å·§Linuxå„ä¸ªä¾èµ–åŒ…ä¹‹é—´å­˜åœ¨å¤æ‚çš„ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶æˆ‘ä»¬ç»å¸¸ä½¿ç”¨è¾ƒé«˜çš„æƒé™æ“ä½œï¼Œå¯èƒ½ä¼šå› ä¸ºç§ç§åŸå› å¯¼è‡´ç³»ç»Ÿå‡ºç°å„ç§é—®é¢˜ï¼Œæ‰€ä»¥å¤‡ä»½è¿˜åŸæ˜¯å¿…å¤‡çš„æŠ€å·§ï¼Œèƒ½åœ¨ç³»ç»Ÿå®•æœºæˆ–æ»šæŒ‚åå¯ä»¥è¿˜åŸåˆ°æŸä¸ªå…ˆå‰çš„æ—¶é—´èŠ‚ç‚¹ï¼Œæ¥ä¿æŠ¤æˆ‘ä»¬è¾›è¾›è‹¦è‹¦è°ƒæ•™äº†å¾ˆä¹…çš„ç³»ç»Ÿä¸ä¼šå‡ºæ„å¤–ã€‚æ³¨æ„ï¼šç³»ç»Ÿæ£€æµ‹åˆ°æœ‰å¤§æ›´æ–°æ—¶ï¼Œä¸è¦æ€¥äºæ›´æ–°ï¼Œè¦é¦–å…ˆä½¿ç”¨timeshiftåšä¸€ä¸ªå¿«ç…§ï¼Œå†æ›´æ–°ã€‚åŸå› æ˜¯éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç³»ç»Ÿæ›´æ–°åçœ‹ä¼¼æ²¡é—®é¢˜ï¼Œä½†å®é™…ä¸Šè½¯ä»¶çš„ä¾èµ–åº“ç‰ˆæœ¬å‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¼è‡´æœ‰éƒ¨åˆ†è½¯ä»¶æ— æ³•æ­£å¸¸è¿è¡Œäº†ï¼Œè¿™ç§æƒ…å†µä¸æ˜“å‘ç°ã€‚è¦å…»æˆå…ˆå¿«ç…§å†å‡çº§çš„ä¹ æƒ¯ã€‚ç³»ç»Ÿå¤‡ä»½å’Œè¿˜åŸä¸¤ç§æ–¹å¼ï¼šä½¿ç”¨tarå‹ç¼©åŒ…æ‰“åŒ…å¤‡ä»½ç³»ç»Ÿ https://www.cnblogs.com/smlile-you-me/p/13601039.htmlä½¿ç”¨timeshiftçš„å¿«ç…§å¤‡ä»½å’Œè¿˜åŸç³»ç»Ÿ æŒ‰ç…§å‘å¯¼è®¾ç½®ï¼šé€‰æ‹©å¿«ç…§ç±»å‹:RSYNC-&gt;é€‰æ‹©å¿«ç…§ä½ç½®(é€‰ä¸€ä¸ªåˆ†åŒºï¼Œæ³¨æ„åªèƒ½é€‰Linuxæ–‡ä»¶ç³»ç»Ÿçš„åˆ†åŒºï¼Œä¸æ”¯æŒè¿œç¨‹ã€NTFSç­‰)-&gt;é€‰æ‹©å¿«ç…§ç­‰çº§(æ ¹æ®é‡è¦æ€§å’Œç£ç›˜ç©ºé—´é€‰æ‹©å¤‡ä»½å‘¨æœŸå’Œä¿ç•™å¿«ç…§æ•°)-&gt;ç”¨æˆ·ä¸»ç›®å½•(é»˜è®¤å…¨éƒ¨) ç‚¹å‡»åˆ›å»º ä¼šç«‹åˆ»è¿è¡Œå¿«ç…§åˆ›å»ºç¨‹åºï¼Œåˆ›å»ºå®Œå¦‚å›¾ å®¶ç›®å½•æœ‰äº›æ–‡ä»¶å¯èƒ½ä¸éœ€è¦å¤‡ä»½ï¼Œéœ€è¦æ’é™¤ä¸€éƒ¨åˆ†æ–‡ä»¶ï¼šè®¾å®š-&gt;ç­›é€‰ å¯ä»¥è‡ªå®šä¹‰ä¸å¯¹ç‰¹å®šæ¨¡å¼çš„æ–‡ä»¶åˆ›å»ºå¿«ç…§ æ¢å¤å¿«ç…§: é€‰ä¸­è¦æ¢å¤çš„å¿«ç…§ ç‚¹å‡»æ¢å¤å³å¯ å½“é”™è¯¯æ“ä½œå¯¼è‡´ç³»ç»Ÿå´©æºƒæ— æ³•è¿›å…¥ç•Œé¢æ—¶ï¼Œéœ€è¦è¿›å…¥å‘½ä»¤è¡Œä½¿ç”¨timeshiftç›¸å…³å‘½ä»¤æ¢å¤:é€šè¿‡Ctrl+Alt+F1ï¼ˆä¸€èˆ¬æ˜¯F1-F6éƒ½å¯ï¼‰è¿›å…¥ttyç»ˆç«¯ è¾“å…¥ç”¨æˆ·å’Œå¯†ç ç™»å½• *** æŸ¥çœ‹å¯è¿˜åŸçš„è¿˜åŸç‚¹ sudo timeshift --list /dev/nvme0n1p9 is mounted at: /run/timeshift/backup, options: rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota Device : /dev/nvme0n1p9 UUID : d4fa3365-62fe-4488-ba18-b36ddac64c4d Path : /run/timeshift/backup Mode : RSYNC Status : OK 2 snapshots, 75.5 GB free Num Name Tags Description ------------------------------------------------------------------------------ 0 &gt; 2021-08-12_12-24-49 O 1 &gt; 2021-08-12_14-00-01 M *** è¿˜åŸå¿«ç…§ --skip-grubé€‰é¡¹ä¸ºè·³è¿‡grubå®‰è£…ï¼Œä¸€èˆ¬æ¥è¯´grubä¸éœ€è¦é‡æ–°å®‰è£…ï¼Œé™¤ébioså¯åŠ¨æ— æ³•æ‰¾åˆ°æ­£ç¡®çš„grubå¯åŠ¨é¡¹ï¼Œæ‰éœ€è¦å®‰è£… sudo timeshift --restore --snapshot &#39;2021-08-12_14-00-01&#39; --skip-grub æ— æ³•è¿›å…¥ç³»ç»Ÿä¹Ÿæ— æ³•è¿›å…¥ttyå‘½ä»¤è¡Œå‚ç…§æ–‡ç« å¼€å§‹çš„éƒ¨åˆ†åˆ›å»ºManjaroå®‰è£…ç›˜ï¼Œè¿›å…¥LiveCDæ¡Œé¢ï¼Œå®‰è£…timeshift æŒ‰ä¸Šä¸€æ­¥çš„æ­¥éª¤è¿›è¡Œæ¢å¤æ¢å¤å®Œæˆåæ¡Œé¢æ— æ³•åŠ è½½ç¨‹åºå¿«æ·æ–¹å¼-&gt;è§£å†³ï¼šyay -Syuuæ‰§è¡Œç³»ç»Ÿæ›´æ–°å³å¯ plasmashellä»¥åŠkwin_x11çš„é‡å¯# å³é”®æ—¶ä¸å‡ºç°èœå•, æ–°å»ºæ–‡ä»¶å¤¹ä¹Ÿä¸æ˜¾ç¤º, éœ€è¦é‡å¯ plasma, åœ¨ç»ˆç«¯ä¸­è¿è¡Œ plasmashell --replace 2&gt;&amp;1 &amp; # æ¡Œé¢åŠ¨ç”»æ•ˆæœæ¶ˆå¤±é¦–å…ˆå°è¯•Alt+Shift+F12æ¥æ‰“å¼€&quot;æ˜¾ç¤ºç‰¹æ•ˆæ··åˆå™¨&quot;,å¦‚æœæ— æ•ˆæˆ–kwinçª—å£æ‰å¸§å¡é¡¿ # éœ€è¦é‡å¯ kwin, åœ¨ç»ˆç«¯ä¸­è¿è¡Œ kwin_x11 --replace 2&gt;&amp;1 &amp; æˆ–è€…ç®€å•å‘½ä»¤ systemctl --user restart plasma-kwin_x11 è§£å†³æ— æ³•å†™å’Œæ›´æ–°NTFSç›˜æ•°æ®çš„é—®é¢˜ï¼šåˆ›å»º /usr/bin/fix_ntfs_disk_rw.sh å†…å®¹ï¼š #!/bin/bash # Fix NTFS Disk which can not be writen on linux system. # Usage: sh fix_ntfs_disk_rw.sh /run/media/shmily/Entertainment /Entertainment DEFAULT_MOUNT_POINT=$1 TARGET_MOUNT_POINT=$2 if [ &quot;$(whoami)&quot; != &quot;root&quot; ];then echo User root is necessary. exit 1 fi current_point=$(df -h | grep $DEFAULT_MOUNT_POINT | awk &#39;&#123;print $1&#125;&#39;) echo &quot;Remounting point $current_point from $DEFAULT_MOUNT_POINT to $TARGET_MOUNT_POINT&quot; sudo ntfsfix $current_point sudo umount $DEFAULT_MOUNT_POINT sudo mkdir -p $TARGET_MOUNT_POINT sudo chmod 1777 $TARGET_MOUNT_POINT sudo mount -t ntfs -o rw $current_point $TARGET_MOUNT_POINT echo &quot;All Done&quot; å°†ç³»ç»Ÿé»˜è®¤æŒ‚è½½ç‚¹é‡æ–°æŒ‚è½½ä¸ºè‡ªå®šä¹‰çš„æŒ‚è½½ç‚¹ ç”¨æ³•sh fix_ntfs_disk_rw.sh /run/media/shmily/Entertainment /Entertainment ç³»ç»Ÿæ¸…ç†å†…å­˜æ¸…ç†sudo susyncecho 1 &gt; /proc/sys/vm/drop_cachesecho 2 &gt; /proc/sys/vm/drop_cachesecho 3 &gt; /proc/sys/vm/drop_cachesæ—¥å¿—æ¸…ç†journalctl â€“disk-usage æŸ¥çœ‹æ—¥å¿—å ç”¨sudo journalctl â€“vacuum-size=500M é™åˆ¶å½’æ¡£æ—¥å¿—å¤§å°ï¼Œå¯¹æ—¥å¿—åšæ¸…é™¤æ“ä½œï¼Œé€‚ç”¨äº/varå ç”¨è¾ƒå¤§çš„åœºæ™¯pacmanå®‰è£…åŒ…æ¸…ç†sudo pacman -Sccå…¶ä»–æ•°æ®æ¸…ç†ä¸ç©ºé—´é‡Šæ”¾-&gt;ç»“åˆFilelightå·¥å…·æŸ¥çœ‹æ–‡ä»¶å¤§å°åˆ†å¸ƒ,è¿›è€Œæ‰‹åŠ¨æ¸…ç† å†…å­˜ç®¡ç†ä¼˜åŒ–ç³»ç»ŸåŸç”Ÿå†…å­˜ç®¡ç†é—®é¢˜: å†…å­˜ç”¨å°½æ—¶ä¼šå¯¼è‡´ç³»ç»Ÿå¡æ­»æ— æ³•æ“ä½œ,ç­‰å¾…OOM-Killeræœºåˆ¶é‡Šæ”¾è¦å¾ˆä¹…,å½±å“ç³»ç»Ÿä½¿ç”¨ä½“éªŒç”±äºLinuxå†…æ ¸çš„oom-killeræœºåˆ¶ä¸å¤Ÿå®Œå–„,ä¸€æ—¦ç³»ç»Ÿå†…å­˜çˆ†æ»¡,ä¼šå‡ºç°ç³»ç»Ÿå¡æ­»,ç­‰å¾…å¾ˆé•¿æ—¶é—´æ‰ä¼škillæ‰å ç”¨è¾ƒå¤§çš„è¿›ç¨‹.æ‰€ä»¥ä½¿ç”¨earlyoomè¿›è¡Œæ—©æœŸçš„å†…å­˜ç®¡ç†,åŠæ—¶æ€è¿›ç¨‹ä»¥é¿å…ç³»ç»Ÿå¡æ­» sudo pacman -S earlyoom systemctl enable earlyoom;systemctl start earlyoom;systemctl status earlyoom é…ç½®earlyoom:é»˜è®¤é…ç½®:å‰©ä½™å¯ç”¨ç‰©ç†å†…å­˜å°äº 10% å’Œ å‰©ä½™äº¤æ¢å†…å­˜å°äº10%æ—¶ï¼Œå°±ä¼šè§¦å‘æ—©æœŸOOMSIGTERM when mem &lt;= 10 % and swap &lt;= 10 %,SIGKILL when mem &lt;= 5 % and swap &lt;= 5 %å¦‚æœè§‰å¾—å‰©ä½™å†…å­˜ä¿ç•™å¤ªå¤šï¼Œå¯ä»¥ç¼–è¾‘ earlyoom çš„é…ç½®æ–‡ä»¶ vim /etc/default/earlyoomç¼–è¾‘å¥½é…ç½®æ–‡ä»¶å é‡å¯æœåŠ¡ç”Ÿæ•ˆé…ç½®systemctl restart earlyoomå»ºè®®é…ç½®:EARLYOOM_ARGS=â€-r 60 -m 1 -s 5 â€“avoid â€˜(^|/)(init|Xorg|sshd)$â€™â€å‚æ•°è§£é‡Š:-r 60 ä»£è¡¨æ¯60ç§’æ‰“å°ä¸€æ¬¡å†…å­˜ç»Ÿè®¡ä¿¡æ¯-m 1 ä»£è¡¨ä¿ç•™ç‰©ç†å†…å­˜ä¸º1%-s 5 ä»£è¡¨ä¿ç•™äº¤æ¢å†…å­˜ä¸º5%â€“avoid â€˜(^|/)(init|Xorg|sshd)$â€™ ä»£è¡¨ä»»ä½•æ—¶å€™ä¸è¦æ€æ­»åå­—å¸¦æœ‰ initï¼ŒXorgï¼Œsshd çš„è¿›ç¨‹ SWAPä¼˜åŒ–# ä¸´æ—¶é™ä½ä½¿ç”¨swapçš„æƒé‡(ä½¿ç”¨swapçš„ç§¯æç¨‹åº¦) é…ç½® sudo sysctl vm.swappiness=20 (0-100 æ•°å€¼è¶Šå¤§,è¶Šç§¯æä½¿ç”¨Swap) æŸ¥çœ‹ cat /proc/sys/vm/swappiness # æ°¸ä¹…ç”Ÿæ•ˆé™ä½ä½¿ç”¨swapæƒé‡ vim /etc/sysctl.conf vm.swappiness=20 # åˆ·æ–°ç”Ÿæ•ˆ sysctl -p æœç´¢å·¥å…·Alt+Space å…¨å±€æœç´¢å·¥å…· ä¼šåœ¨æ¡Œé¢ä¸Šæ–¹å¼¹å‡ºæœç´¢æ¡† å¯ä»¥æœç´¢åº”ç”¨ã€æ–‡ä»¶ã€ç›®å½•ã€æœåŠ¡ã€è®¾ç½®ç­‰ è§£å†³thermalè¯¯æŠ¥å¯¼è‡´è‡ªåŠ¨å…³æœºæŠ¥é”™kernel: thermal thermal_zone3: critical temperature reached (125 C), shutting down ç›´æ¥è¢«å…³æœºsudo chmod 665 /sys/class/thermal/thermal_zone3/modesudo echo â€œdisabledâ€ &gt; /sys/class/thermal/thermal_zone3/modeè¿™ä¸ªå‚æ•°éœ€è¦æ¯æ¬¡ç³»ç»Ÿå¯åŠ¨æ—¶é‡æ–°å†™å…¥,æ”¾å…¥å¼€æœºå¯åŠ¨è„šæœ¬è·¯å¾„/etc/rc.local.d/ bashä¸‹æ‰€æœ‰å‘½ä»¤å‡ä¸å¯ç”¨åŸå› :å¯èƒ½å­˜åœ¨é”™è¯¯çš„ç¯å¢ƒå˜é‡é…ç½®,ä¸è¿‡ä¸å½±å“zshå› ä¸ºzshæ˜¯å•ç‹¬çš„.zshrcé…ç½®æ–‡ä»¶ä¿®å¤: åœ¨~/.bashrcæœ«å°¾åŠ ä¸Šå¦‚ä¸‹å†…å®¹ ä¿®å¤PATHexport PATH=â€$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/gamesâ€ åŒ…ç®¡ç†å™¨ä¸è½¯ä»¶ä¾èµ–ç®¡ç†# å¸è½½å­¤åŒ…:ï¼ˆå­¤åŒ…:å­¤ç«‹åŒ…,ä¸è¢«å¼•ç”¨çš„åŒ…,æ— ç”¨çš„åŒ…ï¼‰ pacman -R $(pacman -Qtdq) # æ¸…é™¤å·²ä¸‹è½½çš„å®‰è£…åŒ… sudo pacman -Scc # å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œæ›´æ–° sudo pacman -Syu # å‡çº§è½¯ä»¶åŒ… sudo pacman -Syu # æ¸…ç†è½¯ä»¶åŒ…ç¼“å­˜ sudo pacman -Sc # æ¸…ç†æ‰€æœ‰çš„ç¼“å­˜æ–‡ä»¶ sudo pacman -Scc # å®‰è£…æˆ–è€…å‡çº§å•ä¸ªè½¯ä»¶åŒ…ï¼Œæˆ–è€…ä¸€åˆ—è½¯ä»¶åŒ…ï¼ˆåŒ…å«ä¾èµ–åŒ…ï¼‰ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š sudo pacman -S package_name1 package_name2 ... # å®‰è£…æˆ–è€…å‡çº§å•ä¸ªè½¯ä»¶åŒ…ï¼Œæˆ–è€…ä¸€åˆ—è½¯ä»¶åŒ…,åŒæ­¥åŒ…æ•°æ®åº“åå†æ‰§è¡Œå®‰è£… sudo pacman -Sy package_name package_name2 ... # å®‰è£…æœ¬åœ°åŒ… å…¶æ‰©å±•åä¸ºpkg.tar.gzæˆ–pkg.tar.xz sudo pacman -U local_package_name # å®‰è£…ä¸€ä¸ªè¿œç¨‹åŒ… sudo pacman -U url http://www.example.com/repo/example.pkg.tar.xz # åœ¨ä»“åº“ä¸­æœç´¢å«å…³é”®å­—çš„åŒ… sudo pacman -Ss keyword # æŸ¥çœ‹å·²å®‰è£…è½¯ä»¶ sudo pacman -Qs keyword # åˆ é™¤å•ä¸ªè½¯ä»¶åŒ…ï¼Œä¿ç•™å…¶å…¨éƒ¨å·²ç»å®‰è£…çš„ä¾èµ–å…³ç³» sudo pacman -R package_name # åˆ é™¤æŒ‡å®šè½¯ä»¶åŒ…ï¼ŒåŠå…¶æ‰€æœ‰æ²¡æœ‰è¢«å…¶ä»–å·²å®‰è£…è½¯ä»¶åŒ…ä½¿ç”¨çš„ä¾èµ–å…³ç³»(è¦åˆ é™¤è½¯ä»¶åŒ…å’Œæ‰€æœ‰ä¾èµ–è¿™ä¸ªè½¯ä»¶åŒ…çš„ç¨‹åºï¼Œè­¦å‘Š: æ­¤æ“ä½œæ˜¯é€’å½’çš„ï¼Œè¯·å°å¿ƒæ£€æŸ¥ï¼Œå¯èƒ½ä¼šä¸€æ¬¡åˆ é™¤å¤§é‡çš„è½¯ä»¶åŒ…) sudo pacman -Rs package_name # ä» AUR å®‰è£…è½¯ä»¶åŒ… yay -S package # yayå¸è½½åŒ… yay -Rns package # å‡çº§æ‰€æœ‰å·²å®‰è£…çš„åŒ…(åŒ…æ‹¬AURæº) yay -Syu # æ›´æ–°AURä»“åº“çš„è½¯ä»¶ yay -Syyu # æ‰“å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ yay -Ps # æ£€æŸ¥å®‰è£…åŒ…çš„ç‰ˆæœ¬ yay -Qi package æ‰‹æœºå¤šå±ååŒå·¥å…·-åŒå±å·¥å…·1.å®‰è£…scrcpy sudo pacman -S scrcpy scrcpy --help scrcpy --max-size 1024 ä¿®æ”¹åˆ†è¾¨ç‡(è¿™é‡ŒæŒ‡é•¿æˆ–å®½æœ€å¤§å€¼1024) 1920*1080ä¼šè¢«ç¼©æ”¾æˆ1024x576 scrcpy --bit-rate 6M ä¿®æ”¹æ¯”ç‰¹ç‡ scrcpy --max-fps 15 é™åˆ¶å¸§ç‡ scrcpy --record file.mp4 å¤šå±ååŒçš„åŒæ—¶å½•å±(æŒ‰Ctrl+Cä»¥åœæ­¢å½•åˆ¶) scrcpy -r file.mkv å¤šå±ååŒçš„åŒæ—¶å½•å±(æŒ‰Ctrl+Cä»¥åœæ­¢å½•åˆ¶) 2.æ‰‹æœºç«¯è¿æ¥åˆ°scrcpy æ‰‹æœºæ‰“å¼€USBè°ƒè¯• æ•°æ®çº¿è¿æ¥æ‰‹æœºusbåˆ°ç”µè„‘ adb devices æŸ¥çœ‹adb æ‰“å¼€å ç”µè„‘ç«¯æ‰§è¡Œadb usb æŠ¥é”™ä¿¡æ¯:error: device unauthorized. æ‰‹æœºä¸Šç¡®è®¤æˆæƒ å†æ¬¡adb usb æ˜¾ç¤ºrestarting in USB mode è¡¨ç¤ºæˆåŠŸ 3.å¼€å¯å¤šå±ååŒ scrcpy scrcpy -m 1920 scrcpy -m 1024 scrcpy --turn-screen-off ç†„å±é•œåƒ scrcpy --turn-screen-off -m 1024 scrcpy -S ç†„å±é•œåƒ æ›´å¤šå‚æ•°å¯æŸ¥çœ‹å¸®åŠ©æ–‡æ¡£:man scrcpy æ¯æ¬¡é€šè¿‡å‘½ä»¤è¿æ¥æ‰‹æœºä¼šæ¯”è¾ƒéº»çƒ¦,å¯ä»¥ä½¿ç”¨å¸¦GUIçš„Scrcpy: yay -S qtscrcpy åŠŸèƒ½ç›¸åŒåªæ˜¯å¸¦GUIæ›´æ–¹ä¾¿ ä½¿ç”¨Wineè¿è¡ŒWindowsç¨‹åºWine ï¼ˆâ€œWine Is Not an Emulatorâ€ çš„é¦–å­—æ¯ç¼©å†™ï¼‰æ˜¯ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤šç§POSIX-compliantæ“ä½œç³»ç»Ÿï¼ˆè¯¸å¦‚Linuxï¼ŒmacOSåŠBSDç­‰ï¼‰ä¸Šè¿è¡ŒWindowsåº”ç”¨çš„å…¼å®¹å±‚ã€‚Wineä¸æ˜¯åƒè™šæ‹Ÿæœºæˆ–è€…æ¨¡æ‹Ÿå™¨ä¸€æ ·æ¨¡ä»¿å†…éƒ¨çš„Windowsé€»è¾‘ï¼Œè€Œæ˜¯å°‡Windows APIè°ƒç”¨ç¿»è¯‘æˆä¸ºåŠ¨æ€çš„POSIXè°ƒç”¨ï¼Œå…é™¤äº†æ€§èƒ½å’Œå…¶ä»–ä¸€äº›è¡Œä¸ºçš„å†…å­˜å ç”¨ï¼Œè®©ä½ èƒ½å¤Ÿå¹²å‡€åœ°é›†åˆWindowsåº”ç”¨åˆ°ä½ çš„æ¡Œé¢ã€‚å®‰è£…wine å¿½ç•¥ ä¹‹å‰çš„æ­¥éª¤å·²ç»æœ‰wineäº†sudo pacman -S wine wine_gecko wine-monosudo pacman -S lib32-mesa lib32-nvidia-utilsä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ä¿®å¤ï¼š åœ¨windowsä¸‹æ‹·è´å­—ä½“æ–‡ä»¶â€”â€”simsun.ttcï¼ˆc:\\windows\\fonts\\simsun.ttcï¼‰ï¼Œå¤åˆ¶åˆ°~/.wine/drive_c/windows/Fontsä¸‹ï¼›ç„¶åï¼Œç¼–è¾‘regæ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š REGEDIT4 [HKEY_LOCAL_MACHINE\\Software\\Microsoft\\NT\\CurrentVersion\\FontSubstitutes] &quot;Arial&quot;=&quot;simsun&quot; &quot;Arial CE,238&quot;=&quot;simsun&quot; &quot;Arial CYR,204&quot;=&quot;simsun&quot; &quot;Arial Greek,161&quot;=&quot;simsun&quot; &quot;Arial TUR,162&quot;=&quot;simsun&quot; &quot;Courier New&quot;=&quot;simsun&quot; &quot;Courier New CE,238&quot;=&quot;simsun&quot; &quot;Courier New CYR,204&quot;=&quot;simsun&quot; &quot;Courier New Greek,161&quot;=&quot;simsun&quot; &quot;Courier New TUR,162&quot;=&quot;simsun&quot; &quot;FixedSys&quot;=&quot;simsun&quot; &quot;Helv&quot;=&quot;simsun&quot; &quot;Helvetica&quot;=&quot;simsun&quot; &quot;MS Sans Serif&quot;=&quot;simsun&quot; &quot;MS Shell Dlg&quot;=&quot;simsun&quot; &quot;MS Shell Dlg 2&quot;=&quot;simsun&quot; &quot;System&quot;=&quot;simsun&quot; &quot;Tahoma&quot;=&quot;simsun&quot; &quot;Times&quot;=&quot;simsun&quot; &quot;Times New Roman CE,238&quot;=&quot;simsun&quot; &quot;Times New Roman CYR,204&quot;=&quot;simsun&quot; &quot;Times New Roman Greek,161&quot;=&quot;simsun&quot; &quot;Times New Roman TUR,162&quot;=&quot;simsun&quot; &quot;Tms Rmn&quot;=&quot;simsun&quot; ï¼ˆæ³¨ï¼šæŒ‰ç…§windowsçš„æ ¼å¼ï¼Œæœ€åä¸€è¡Œä¹‹åè¦æ•²å›è½¦ç¬¦ï¼‰ä¿å­˜æ–‡ä»¶åä¸ºfonts.regï¼Œä¿å­˜åœ¨/.wineä¸‹ï¼›ç„¶åå¯¼å…¥regeditï¼šæ‰“å¼€gnome-terminalï¼Œè¾“å…¥æŒ‡ä»¤ cd ~/.wine ; regedit fonts.reg æœ€åï¼Œæ‰“å¼€regeditï¼Œ/.wine/drive_c/windows/regedit.exe,ä¾æ¬¡æ‰¾åˆ° HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows NT\\CurrentVersion\\FontSubstitutesï¼Œå°†è¯¥é”®ä¸‹çš„MS Shell Dlgå’ŒMS Shell Dlg2é”®å€¼åˆ é™¤ã€‚ Wineä½¿ç”¨Wineè¿è¡Œç¨‹åº: wine xxx.exemsiå®‰è£…åŒ…è¿è¡Œ: msiexec -i &lt;msiå®‰è£…åŒ…&gt; Deepin-wineDeepin-Wineæ˜¯Deepinå›¢é˜Ÿç§»æ¤çš„Wineï¼Œåœ¨å…¶åŸºç¡€ä¸Šç§»æ¤çš„å¾ˆå¤šè½¯ä»¶å¦‚å¾®ä¿¡ã€TIM/QQã€ç½‘æ˜“äº‘éŸ³ä¹ç­‰æœ‰ç€æ›´å¥½çš„å…¼å®¹æ€§å’Œä½¿ç”¨ä½“éªŒã€‚æ³¨æ„ï¼ŒDeepin-Wineæ˜¯32ä½çš„ï¼Œå¹¶ä¸”å…¶ä¾èµ–äºWineï¼Œå› æ­¤æœ¬æœºä¸Šå®‰è£…çš„Wineæœ€å¥½æ˜¯32ä½çš„ï¼Œå¦åˆ™Deepin-Wineä½¿ç”¨å‘½ä»¤æ—¶ä¼šæœ‰ä¸ä¾¿ã€‚å®‰è£…(å¿½ç•¥ ä¹‹å‰çš„æ­¥éª¤å·²æœ‰æ­¤ä¾èµ–) yaourt deepin-wineä½¿ç”¨æ–¹æ³•ä¸wineåŸºæœ¬ä¸€è‡´ æ›´å¤šWineè¿›é˜¶ä½¿ç”¨å¯ä»¥äº†è§£Wineå®˜æ–¹ç½‘ç«™ Linuxä½¿ç”¨Wine AURä»“åº“ä¸è½¯ä»¶åŒ…æŸ¥è¯¢https://aur.archlinux.org/packages ä¿®å¤Manjaroç³»ç»Ÿæ›´æ–°åæ²¡å£°éŸ³äº†-ä»»åŠ¡æ å–‡å­æ˜¯ç°è‰²çš„systemctl --user status pulseaudio systemctl --user status pipewire # å‘ç°pulseaudioæœåŠ¡å¯åŠ¨å¤±è´¥ # å»é™¤æ‰‹åŠ¨é…ç½®å¼€æœºå¯åŠ¨pulseaudioçš„è„šæœ¬ # å¦‚æœéŸ³é¢‘æŒ‚äº†,å¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤å°è¯•ä¿®å¤: sudo killall pulseaudio systemctl --user restart pulseaudio.service systemctl --user restart pulseaudio.socket ç³»ç»Ÿä¿¡æ¯æŸ¥è¯¢ å…¨éƒ¨ç¡¬ä»¶ä¿¡æ¯è¾“å‡ºï¼šsudo dmidecode &gt;&gt; hardware.info hwinfo æŸ¥çœ‹å¼€æœºè®°å½•last reboot é’‰é’‰Linuxç‰ˆæ— æ³•è¾“å…¥ä¸­æ–‡sudo vim /usr/bin/dingtalk åœ¨ export PATH åé¢åŠ å¦‚ä¸‹ä¸¤è¡Œ export XMODIFIERS=&quot;@im=ibus&quot; export QT_IM_MODULE=&quot;ibus&quot; å‚è€ƒé“¾æ¥Manjaro WikiManjaro Gnome ä¸‹fcitx5çš„å®‰è£…Fcitx5-Material-ColorArchLinux WikiSyncthingManjaroå®‰è£…Mysql8.0ï¼ˆè¡€æ³ªç¯‡ï¼‰archlinux Timeshiftç³»ç»Ÿå¤‡ä»½ä¸è¿˜åŸè½»æ¾ä¸Šæ‰‹Manjaroä¹‹Manjaroä¸‹ä½¿ç”¨WineManjaro-KDEå®‰è£…åŠ¨æ€æ¡Œé¢æ’ä»¶å¦‚ä½•è®©Ubuntuç³»ç»Ÿæ”¯æŒLDACï¼ŒAPTXï¼ŒAACç¼–ç ï¼ˆæå‡è“ç‰™éŸ³è´¨ï¼‰PulseAudio - ArchLinux Wiki","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://shmily-qjj.top/tags/Linux/"},{"name":"Manjaro","slug":"Manjaro","permalink":"https://shmily-qjj.top/tags/Manjaro/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Presto-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“","slug":"Presto-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“","date":"2021-03-12T06:46:00.000Z","updated":"2022-12-11T05:35:07.910Z","comments":true,"path":"4c197c46/","link":"","permalink":"https://shmily-qjj.top/4c197c46/","excerpt":"","text":"Presto-é«˜æ•ˆSQLäº¤äº’å¼æŸ¥è¯¢å¼•æ“Prestoç®€ä»‹&emsp;&emsp;Prestoæ˜¯Facebookå¼€æºçš„åˆ†å¸ƒå¼SQLæŸ¥è¯¢å¼•æ“ï¼Œé€‚ç”¨äºäº¤äº’å¼åˆ†ææŸ¥è¯¢çš„åœºæ™¯ï¼ˆOLAPï¼‰ï¼Œå“åº”æ—¶é—´åœ¨å°äº 1 ç§’åˆ°å‡ åˆ†é’Ÿçš„åœºæ™¯ï¼Œæ•°æ®é‡æ”¯æŒGBåˆ°PBå­—èŠ‚ã€‚ç±»ä¼¼çš„å·¥å…·æœ‰Impalaã€ClickHouseç­‰â€¦ Prestoä¼˜ç¼ºç‚¹ä¼˜ç‚¹ï¼š æ¶æ„æ¸…æ™°ï¼Œå¯ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨ç³»ç»Ÿç‹¬ç«‹è¿è¡Œã€‚ Prestoè‡ªèº«æä¾›äº†å¯¹é›†ç¾¤çš„ç›‘æ§ã€‚ åŸºäºçº¯å†…å­˜è®¡ç®—ï¼Œä¸éœ€è¦å†™ç£ç›˜ï¼Œæ•ˆç‡é«˜ è‡ªèº«æ›´åŠ è½»é‡çº§èµ„æºè°ƒåº¦ï¼Œçº¿ç¨‹çº§åˆ«çš„Taskï¼Œæ•ˆç‡é«˜ è½®è¯¢æŸ¥è¯¢ç»“æœå¹¶ç«‹åˆ»è¿”å›ç»“æœï¼Œæ•ˆç‡é«˜ è§£è€¦æ•°æ®æºï¼Œç»Ÿä¸€æŸ¥è¯¢å…¥å£ï¼Œæ”¯æŒå¤šä¸ªæ•°æ®æºä¸åŒè¡¨çš„è”é‚¦æŸ¥è¯¢åˆ†æ MPPæ¶æ„çš„ä¼˜åŠ¿-æ‰©å±•æ€§ï¼ŒèŠ‚ç‚¹ç‹¬ç«‹ï¼Œæ— é”èµ„æºç«äº‰ï¼Œæ— IOå†²çªï¼Œæ— å…±äº«æ•°æ® ç®€å•çš„æ•°æ®ç»“æ„ï¼Œåˆ—å¼å­˜å‚¨ï¼Œé€»è¾‘è¡Œï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½å¯ä»¥è½»æ˜“çš„è½¬åŒ–æˆPrestoæ‰€éœ€è¦çš„è¿™ç§æ•°æ®ç»“æ„ã€‚ ä¸°å¯Œçš„æ¥å£ï¼Œå¯å®Œç¾å¯¹æ¥å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿï¼Œä»¥åŠæ·»åŠ è‡ªå®šä¹‰çš„å‡½æ•°ã€‚ ç¼ºç‚¹ï¼š æ— å®¹é”™èƒ½åŠ›ï¼Œæ— é‡è¯•æœºåˆ¶ ä¸æ”¯æŒæ•°æ®ç±»å‹éšå¼è½¬æ¢ ä¸Hiveç›¸æ¯”å­˜åœ¨ä¸å°çš„è¯­æ³•å·®å¼‚ã€å‡½æ•°å’ŒUDFå·®å¼‚ä»¥åŠè¿ç®—ç»“æœå·®å¼‚(å¦‚1/2åœ¨Hiveç»“æœä¸º0.5åœ¨Prestoç»“æœä¸º0) Hive views are not supported.éœ€è¦åˆ›å»ºPrestoè§†å›¾ å› ä¸ºçº¯å†…å­˜è®¡ç®—ï¼Œä¸é€‚åˆå¤šä¸ªå¤§è¡¨Join(èšåˆæ“ä½œè¾¹è¯»æ•°æ®è¾¹è®¡ç®—ï¼Œå†æ¸…å†…å­˜ï¼Œå†è¯»æ•°æ®å†è®¡ç®—ï¼Œè¿™ç§è€—çš„å†…å­˜å¹¶ä¸é«˜ï¼›ä½†å…³è”æ“ä½œå¯èƒ½äº§ç”Ÿå¤§é‡ä¸´æ—¶æ•°æ®ï¼Œå¯èƒ½æ¯”Hiveæ…¢) Coordinatorå•ç‚¹é—®é¢˜ï¼ˆå¸¸è§æ–¹æ¡ˆï¼šipæ¼‚ç§»ã€Nginxä»£ç†åŠ¨æ€è·å–ç­‰ï¼‰ PrestoåŸç†åœ¨å­¦ä¹ PrestoåŸç†å‰æ¨èå…ˆçœ‹çœ‹æˆ‘ä¹‹å‰å…³äºImpalaçš„æ–‡ç« ï¼šã€ŠImpala-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“ã€‹ Prestoæ¶æ„å’Œè¿›ç¨‹&emsp;&emsp;Prestoä¸Impalaçš„æ¶æ„æå…¶ç›¸ä¼¼ï¼Œéƒ½æ˜¯é‡‡ç”¨Master-Slaveæ¨¡å‹ä»¥åŠMPPæ¶æ„,è€Œä¸”Prestoçš„å·¥ä½œè§’è‰²ä¹Ÿä¸ImpalaDaemonçš„è§’è‰²åŸºæœ¬ç›¸åŒï¼ŒPrestoæœ‰ä¸‰ç§å·¥ä½œè§’è‰²ï¼šCoordinator,Workerå’ŒDiscoveryServerï¼š Coordinatorï¼šå³Masterï¼Œè´Ÿè´£ç®¡ç†Metaå…ƒæ•°æ®ï¼ŒWorkerèŠ‚ç‚¹ï¼ŒSQLçš„è§£æå’Œè°ƒåº¦ï¼Œç”ŸæˆStageå’ŒTaskåˆ†å‘ç»™Workersï¼Œè´Ÿè´£åˆå¹¶ç»“æœé›†å¹¶è¿”å›ç»™å®¢æˆ·ç«¯ã€‚ç›¸å½“äºç»“åˆäº†Impaladçš„Coordinatorè§’è‰²å’ŒPlannerè§’è‰²çš„åŠŸèƒ½ï¼ŒåŒºåˆ«æ˜¯æ¯ä¸ªImpaladèŠ‚ç‚¹éƒ½å¯ä»¥æ˜¯Coordinatorï¼Œè€ŒPrestoåªèƒ½æœ‰ä¸€ä¸ªCoordinatorï¼Œå¤šä¸ªåè°ƒè€…è¿›ç¨‹ä¼šå¯¼è‡´è„‘è£‚ï¼ŒæŸ¥è¯¢ä»»åŠ¡ä¼šæ­»é”ã€‚ Workerï¼šè´Ÿè´£è®¡ç®—å’Œè¯»å†™æ•°æ®ã€‚ç›¸å½“äºImpaladçš„Executorè§’è‰²çš„åŠŸèƒ½ã€‚ DiscoveryServerï¼šé€šå¸¸å†…åµŒäºCoordinatorèŠ‚ç‚¹ï¼Œä¹Ÿå¯ä»¥ç‹¬ç«‹å‡ºæ¥éƒ¨ç½²ï¼ŒåŠŸèƒ½ç±»ä¼¼ZKï¼Œç±»ä¼¼Impalaä¸­çš„ImpalaStateStoreï¼Œç”¨äºç›‘æ§èŠ‚ç‚¹å¿ƒè·³ï¼Œä¸€èˆ¬DSå’ŒCoordinatoråœ¨åŒä¸€èŠ‚ç‚¹ã€‚Workerå¯åŠ¨ä¼šå‘DSè¿›ç¨‹æ³¨å†Œï¼ŒCoordinatorå¯ä»¥ä»DSè·å–åˆ°æ‰€æœ‰æ­£å¸¸æä¾›æœåŠ¡çš„Workerã€‚Coordinator ä¸ Workerã€Client é€šä¿¡æ˜¯é€šè¿‡ REST APIã€‚ Prestoæ•°æ®æ¨¡å‹Prestoä½¿ç”¨Catalogã€Schemaå’ŒTableè¿™3å±‚ç»“æ„æ¥ç®¡ç†æ•°æ®ï¼š Catalogï¼šæ¯ä¸ªæ•°æ®æºéƒ½æœ‰ä¸€ä¸ªåå­—ï¼Œä¸€ä¸ªCatalogå¯åŒ…å«å¤šä¸ªSchemaã€‚é€šè¿‡show catalogså‘½ä»¤æŸ¥çœ‹Prestoå·²è¿æ¥çš„æ‰€æœ‰æ•°æ®æº Schemaï¼šç›¸å½“äºä¸€ä¸ªæ•°æ®åº“å®ä¾‹ï¼Œä¸€ä¸ªSchema(æ•°æ®åº“)ä¸­æœ‰å¤šä¸ªTableè¡¨ï¼Œé€šè¿‡show schemas from hiveå‘½ä»¤æŸ¥çœ‹hiveæ•°æ®æºæ‰€æœ‰åº“ Tableï¼šç›¸å½“äºä¸€å¼ è¡¨ï¼Œé€šè¿‡show tables from catalog_name.schema_nameæ¥æŸ¥çœ‹åº“ä¸‹æœ‰å“ªäº›è¡¨ã€‚å®šä½ä¸€å¼ è¡¨ï¼šæ•°æ®æºçš„ç±»åˆ«.æ•°æ®åº“.æ•°æ®è¡¨ Prestoæœ‰ä¸¤ç§å­˜å‚¨å•å…ƒï¼šPageå’ŒBlock Pageï¼šå¤šè¡Œæ•°æ®çš„é›†åˆï¼ŒåŒ…å«å¤šä¸ªåˆ—çš„æ•°æ®ï¼Œè¿™é‡Œçš„å¤šè¡Œæ•°æ®æ˜¯é€»è¾‘è¡Œï¼Œå®é™…æ˜¯ä»¥åˆ—å¼å­˜å‚¨ã€‚ Blockï¼šä¸€åˆ—æ•°æ®ï¼Œæ ¹æ®ä¸åŒç±»å‹çš„æ•°æ®ï¼Œé€šå¸¸é‡‡å–ä¸åŒçš„ç¼–ç æ–¹å¼ã€‚ï¼ˆKuduä¹Ÿæœ‰ç±»ä¼¼çš„æ€æƒ³ï¼‰ arrayç±»å‹çš„Blockï¼šåº”ç”¨äºå›ºå®šé•¿åº¦çš„ç±»å‹å¦‚intã€longã€doubleï¼Œç”±ä¸¤éƒ¨åˆ†ç»„æˆ boolean valueIsNull[]è¡¨ç¤ºæ¯ä¸€è¡Œæ˜¯å¦æœ‰å€¼ã€‚ T values[] æ¯ä¸€è¡Œçš„å…·ä½“å€¼ã€‚ å¯å˜é•¿åº¦çš„Blockï¼šStringç±»å‹ï¼Œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š Sliceï¼šæ‰€æœ‰è¡Œæ•°æ®æ‹¼æ¥èµ·æ¥çš„å­—ç¬¦ä¸² int offsets[]ï¼šæ¯è¡Œæ•°æ®çš„èµ·å§‹åç§»ä½ç½®(æ¯ä¸€è¡Œçš„é•¿åº¦ç­‰äºä¸‹ä¸€è¡Œçš„èµ·å§‹åç§»é‡å‡å»å½“å‰è¡Œçš„èµ·å§‹åç§»é‡) boolean valueIsNull[]ï¼šæ˜¯å¦ç©ºå€¼ï¼Œå¦‚æœæ— å€¼ï¼Œåç§»é‡ä¸ä¸Šä¸€è¡Œç›¸ç­‰ å›ºå®šé•¿åº¦çš„stringç±»å‹çš„blockï¼šæ‰€æœ‰è¡Œçš„æ•°æ®æ‹¼æ¥æˆä¸€é•¿ä¸²Sliceï¼Œæ¯ä¸€è¡Œçš„é•¿åº¦å›ºå®š å­—å…¸ç±»å‹çš„Blockï¼šå¯ä»¥åµŒå¥—ä»»æ„ç±»å‹çš„Blockï¼Œç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š å­—å…¸ int ids[] æ•°æ®çš„ç¼–å·ï¼ŒæŸ¥æ‰¾æ—¶å…ˆæ‰¾åˆ°idï¼Œå†ä»å­—å…¸ä¸­æ‹¿åˆ°çœŸå®å€¼ Prestoæ’ä»¶äº†è§£äº†Prestoçš„æ•°æ®æ¨¡å‹åï¼Œå°±å¯ä»¥åˆ©ç”¨æ’ä»¶æ¥å¯¹æ¥è‡ªå·±çš„ç³»ç»Ÿã€‚Prestoæä¾›äº†ä¸€å¥—Connectoræ¥å£ï¼Œæ”¯æŒä»è‡ªå®šä¹‰å­˜å‚¨ä¸­è¯»å–å…ƒæ•°æ®ï¼Œä»¥åŠåˆ—å¼å­˜å‚¨æ•°æ®ã€‚ ConnectorMetadata:ç®¡ç†è¡¨çš„å…ƒæ•°æ®ï¼Œè¡¨çš„å…ƒæ•°æ®ï¼Œpartitionç­‰ä¿¡æ¯ã€‚åœ¨å¤„ç†è¯·æ±‚æ—¶ï¼Œéœ€è¦è·å–å…ƒä¿¡æ¯ï¼Œä»¥ä¾¿ç¡®è®¤è¯»å–çš„æ•°æ®çš„ä½ç½®ã€‚Prestoä¼šä¼ å…¥filteræ¡ä»¶ï¼Œä»¥ä¾¿å‡å°‘è¯»å–çš„æ•°æ®çš„èŒƒå›´ã€‚å…ƒä¿¡æ¯å¯ä»¥ä»ç£ç›˜ä¸Šè¯»å–ï¼Œä¹Ÿå¯ä»¥ç¼“å­˜åœ¨å†…å­˜ä¸­ã€‚ ConnectorSplit:ä¸€ä¸ªIO Taskå¤„ç†çš„æ•°æ®çš„é›†åˆï¼Œæ˜¯è°ƒåº¦çš„å•å…ƒã€‚ä¸€ä¸ªsplitå¯ä»¥å¯¹åº”ä¸€ä¸ªpartitionï¼Œæˆ–å¤šä¸ªpartitionã€‚ SplitManager:æ ¹æ®è¡¨çš„metaï¼Œæ„é€ splitã€‚ SlsPageSource:æ ¹æ®splitçš„ä¿¡æ¯ä»¥åŠè¦è¯»å–çš„åˆ—ä¿¡æ¯ï¼Œä»ç£ç›˜ä¸Šè¯»å–0ä¸ªæˆ–å¤šä¸ªpageï¼Œä¾›è®¡ç®—å¼•æ“è®¡ç®—ã€‚ åŸºäºPrestoçš„æ’ä»¶æˆ‘ä»¬å¯ä»¥å¼€å‘è¿™äº›åŠŸèƒ½ï¼š å¯¹æ¥è‡ªå·±çš„å­˜å‚¨ç³»ç»Ÿã€‚ æ·»åŠ è‡ªå®šä¹‰æ•°æ®ç±»å‹ã€‚ æ·»åŠ è‡ªå®šä¹‰å¤„ç†å‡½æ•°ã€‚ è‡ªå®šä¹‰æƒé™æ§åˆ¶ã€‚ è‡ªå®šä¹‰èµ„æºæ§åˆ¶ã€‚ æ·»åŠ queryäº‹ä»¶å¤„ç†é€»è¾‘ã€‚ ç›®å‰Prestoå·²ç»æ”¯æŒå¾ˆå¤šç±»å‹çš„Connectorï¼Œå…·ä½“å¯è§å®˜æ–¹æ–‡æ¡£ï¼šPresto-Connectors Prestoå†…å­˜ç®¡ç†æœºåˆ¶&emsp;&emsp;Prestoä½œä¸ºåŸºäºå†…å­˜çš„è®¡ç®—å¼•æ“ï¼Œå¯¹å†…å­˜çš„åˆ†é…å¾ˆç²¾ç»†ã€‚Prestoé‡‡ç”¨é€»è¾‘ä¸Šçš„å†…å­˜æ± ï¼Œæ¥ç®¡ç†ä¸åŒç±»å‹çš„å†…å­˜éœ€æ±‚ã€‚PrestoæŠŠæœºå™¨çš„å†…å­˜åˆ’åˆ†æˆä¸‰ä¸ªå†…å­˜æ± ï¼Œåˆ†åˆ«æ˜¯System Pool,Reserved Pool,General Poolã€‚ System Poolï¼šä¿ç•™ç»™ç³»ç»Ÿä½¿ç”¨çš„å†…å­˜ï¼Œé»˜è®¤æ˜¯Xmxçš„40% General Poolï¼šå¤§éƒ¨åˆ†Queryä½¿ç”¨è¿™ä¸ªå†…å­˜æ± ä¸­çš„å†…å­˜ï¼Œå› ä¸ºå¤§éƒ¨åˆ†Queryæ¶ˆè€—å†…å­˜å¹¶ä¸é«˜ Reserved Poolï¼šç”¨äºç»™æ¶ˆè€—å†…å­˜æœ€å¤§çš„ä¸€ä¸ªQueryä½¿ç”¨ï¼Œè¿™ä¸ªå†…å­˜æ± é»˜è®¤å 10%çš„æ€»å†…å­˜ï¼Œä¹Ÿè¡¨ç¤ºä¸€ä¸ªQueryåœ¨ä¸€å°æœºå™¨ä¸Šæœ€å¤§çš„å†…å­˜ä½¿ç”¨é‡ &emsp;&emsp;ä¸ºä»€ä¹ˆPrestoä¼šä½¿ç”¨å†…å­˜æ± æœºåˆ¶ï¼Ÿé¦–å…ˆï¼ŒSystem Poolä¸ºäº†ç³»ç»Ÿæ­£å¸¸è¿è¡Œä»¥åŠæ•°æ®ä¼ è¾“æ—¶ç³»ç»Ÿç¼“å­˜æ¶ˆè€—ï¼›åœ¨èµ„æºä¸å……è¶³æ—¶ï¼Œä¸€ä¸ªæ¶ˆè€—å†…å­˜è¾ƒå¤§çš„Queryå¼€å§‹è¿è¡Œï¼Œå› ä¸ºæ²¡è¶³å¤Ÿç©ºé—´æ‰€ä»¥ä¼šæŒ‚èµ·ç­‰å¾…æ‰§è¡Œï¼Œç­‰ä¸€äº›æ¶ˆè€—å†…å­˜å°çš„Queryæ‰§è¡Œå®Œï¼Œåˆæœ‰æ–°çš„Queryè¯·æ±‚ï¼Œå†…å­˜ä¸€ç›´ä¸å……è¶³ï¼Œå¦‚æœæ²¡æœ‰Reserved Poolï¼Œè¿™ä¸ªæ¶ˆè€—å†…å­˜å¤§çš„Queryå°±ä¼šä¸€ç›´è¢«æŒ‚èµ·ç›´åˆ°å¤±è´¥ã€‚ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œé¢„ç•™å‡ºReserved Poolå†…å­˜æ± ä¾›å¤§Queryæ‰§è¡Œã€‚Prestoæ¯ç§’é’ŸæŒ‘å‡ºæ¥ä¸€ä¸ªå†…å­˜å ç”¨æœ€å¤§çš„queryï¼Œå…è®¸å®ƒåœ¨æ‰€æœ‰æœºå™¨ä¸Šéƒ½èƒ½ä½¿ç”¨Reserved poolï¼Œé¿å…ä¸€ç›´æ²¡æœ‰å¯ç”¨å†…å­˜ä¾›å¤§Queryä½¿ç”¨ã€‚ &emsp;&emsp;å¦‚æœå¤§Queryä¸åœ¨æŸäº›èŠ‚ç‚¹ä½¿ç”¨Reserved Poolå°±ä¼šæµªè´¹é‚£å°èŠ‚ç‚¹çš„é¢„ç•™å†…å­˜ï¼Œæ‰€ä»¥ä¸ºä»€ä¹ˆä¸æ˜¯å•å°æœºå™¨ä¸­æŒ‘å‡ºå ç”¨å†…å­˜æœ€å¤§çš„Taskæ¥ä½¿ç”¨Reserved Poolï¼Ÿè¿™æ ·è®¾è®¡ä¼šæ­»é”ï¼Œå‡è®¾ä¸€ä¸ªå¤§Queryçš„ä¸€ä¸ªTaskåœ¨æŸå°æœºå™¨å¯ç”¨Reserved Poolå¾ˆå¿«æ‰§è¡Œå®Œï¼Œè€Œå¦å¤–ä¸€å°æœºå™¨çš„Taskè¿˜æ˜¯æŒ‚èµ·çŠ¶æ€ï¼Œè¿™ä¸ªQueryä¹Ÿä¼šä¸€ç›´å¤„äºæŒ‚èµ·çŠ¶æ€ï¼Œæ•ˆç‡é™ä½ã€‚ &emsp;&emsp;Prestoå†…å­˜ç®¡ç†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šQueryå†…å­˜ç®¡ç†å’Œæœºå™¨å†…å­˜ç®¡ç†ï¼Œæ˜¯ç”±Coordinatorè´Ÿè´£çš„Queryå†…å­˜ç®¡ç†ï¼šQueryä¼šåˆ’åˆ†ä¸ºå¤šä¸ªTaskï¼Œæ¯ä¸ªTaskä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹å¾ªç¯è·å–TaskçŠ¶æ€åŒ…æ‹¬å†…å­˜ä½¿ç”¨æƒ…å†µï¼ŒQueryå†…å­˜ç®¡ç†å°±æ˜¯æ±‡æ€»è¿™äº›Taskçš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚æœºå™¨å†…å­˜ç®¡ç†ï¼šCoordinatoræœ‰ä¸€ä¸ªçº¿ç¨‹å®šæ—¶è½®è¯¢æ¯å°æœºå™¨çš„å†…å­˜çŠ¶æ€ Prestoæ‰§è¡Œè®¡åˆ’Prestoä¸Sparkã€Hiveä¸€æ ·ï¼Œéƒ½æ˜¯ä½¿ç”¨Antlrè¿›è¡Œè¯­æ³•è§£æï¼Œä¸€æ¡SQLç»è¿‡å¦‚ä¸‹æ­¥éª¤æœ€ç»ˆç”Ÿæˆåœ¨æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œçš„LocalExecutionPlané€»è¾‘è®¡åˆ’ã€‚ æ ·ä¾‹ï¼š select c1.rank, count(*) from dim.city c1 join dim.city c2 on c1.id = c2.id where c1.id &gt; 10 group by c1.rank limit 10; ç”Ÿæˆçš„é€»è¾‘è®¡åˆ’ï¼š ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼šé€»è¾‘è®¡åˆ’çš„æ¯ä¸€ä¸ªSubPlanéƒ½ä¼šæäº¤åˆ°ä¸€ä¸ªæˆ–è€…å¤šä¸ªWorkerèŠ‚ç‚¹ä¸Šæ‰§è¡Œï¼Œä¸€ä¸ªSubPlanä¹Ÿå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªStageï¼ŒSubPlanæœ‰å‡ ä¸ªé‡è¦çš„å±æ€§planDistributionã€outputPartitioningã€partitionByå±æ€§ã€‚ planDistributionæœ‰ä¸‰ç§ç±»å‹ Sourceï¼šæ•°æ®æºï¼Œä¼šæ ¹æ®æ•°æ®æºå¤§å°ç¡®å®šåˆ†é…å¤šå°‘ä¸ªèŠ‚ç‚¹ Fixedï¼šåˆ†é…åˆ°å›ºå®šä¸ªæ•°çš„èŠ‚ç‚¹æ‰§è¡Œï¼ˆConfigé…ç½®ä¸­çš„query.initial-hash-partitionså‚æ•°é…ç½®ï¼Œé»˜è®¤æ˜¯8ï¼‰ Noneï¼šè¿™ä¸ªSubPlanåªåˆ†é…åˆ°ä¸€ä¸ªèŠ‚ç‚¹æ‰§è¡Œ outputPartitioningæœ‰ä¸¤ç§ç±»å‹ï¼Œè¡¨ç¤ºè¿™ä¸ªSubPlançš„è¾“å‡ºæ˜¯å¦æŒ‰ç…§partitionByå±æ€§çš„keyå€¼å¯¹æ•°æ®è¿›è¡ŒShuffleã€‚ Hashï¼šå‘ç”ŸShuffle Noneï¼šä¸è¿›è¡ŒShuffle åœ¨ä¸‹é¢çš„æ‰§è¡Œè®¡åˆ’ä¸­ï¼ŒSubPlan1å’ŒSubPlan0 PlanDistribution=Sourceï¼Œè¿™ä¸¤ä¸ªSubPlanéƒ½æ˜¯æä¾›æ•°æ®æºçš„èŠ‚ç‚¹ï¼ŒSubPlan1æ‰€æœ‰èŠ‚ç‚¹çš„è¯»å–æ•°æ®éƒ½ä¼šå‘å‘SubPlan0çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼›SubPlan2åˆ†é…8ä¸ªèŠ‚ç‚¹æ‰§è¡Œæœ€ç»ˆçš„èšåˆæ“ä½œï¼›SubPlan3åªè´Ÿè´£è¾“å‡ºæœ€åè®¡ç®—å®Œæˆçš„æ•°æ®ã€‚åªæœ‰SubPlan0çš„OutputPartitioning=HASHï¼ˆå­˜åœ¨AggregateNodeè®¡åˆ’ï¼‰ï¼Œæ‰€ä»¥SubPlan2æ¥æ”¶åˆ°çš„æ•°æ®æ˜¯æŒ‰ç…§rankå­—æ®µPartitionåçš„æ•°æ® SQLæäº¤å¹¶è§£æä¸ºSubPlanåçš„æ‰§è¡Œæµç¨‹ï¼šæ¯”å¦‚ä¸€æ¡SQLæœ€ç»ˆç”Ÿæˆ4ä¸ªSubPlanï¼ˆ0-3ï¼‰ï¼Œå…¶ä¸­0ï¼Œ1å¹¶è¡Œæ‰§è¡ŒJoinæˆ–èšåˆæ“ä½œï¼Œå…¶ä½™ä¸²è¡Œæ‰§è¡Œï¼Œæ¯ä¸ªSubPlanéƒ½ä¼šåˆ†å‘åˆ°å¤šä¸ªå·¥ä½œèŠ‚ç‚¹æ‰§è¡Œã€‚ Coordinatoré€šè¿‡HTTPåè®®è°ƒç”¨WorkerèŠ‚ç‚¹çš„/v1/taskæ¥å£å°†æ‰§è¡Œè®¡åˆ’åˆ†é…ç»™æ‰€æœ‰WorkerèŠ‚ç‚¹ï¼ˆå›¾ä¸­è“è‰²ç®­å¤´ï¼‰ SubPlan1çš„æ¯ä¸ªèŠ‚ç‚¹è¯»å–ä¸€ä¸ªSplitçš„æ•°æ®å¹¶è¿‡æ»¤åå°†æ•°æ®åˆ†å‘ç»™æ¯ä¸ªSubPlan0èŠ‚ç‚¹è¿›è¡ŒJoinæˆ–èšåˆæ“ä½œ SubPlan1çš„æ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å®ŒæˆåæŒ‰GroupBy Keyçš„Hashå€¼å°†æ•°æ®åˆ†å‘åˆ°ä¸åŒçš„SubPlan2èŠ‚ç‚¹ æ‰€æœ‰SubPlan2èŠ‚ç‚¹è®¡ç®—å®Œæˆåå°†æ•°æ®åˆ†å‘åˆ°SubPlan3èŠ‚ç‚¹ SubPlan3èŠ‚ç‚¹è®¡ç®—å®Œæˆåé€šçŸ¥Coordinatorç»“æŸæŸ¥è¯¢ï¼Œå¹¶å°†æ•°æ®å‘é€ç»™Coordinator æ€»ç»“ä¸€æ¡SQLåœ¨Prestoæ‰§è¡Œçš„å®Œæ•´æµç¨‹ï¼š å®¢æˆ·ç«¯é€šè¿‡HTTPåè®®å‘é€ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ç»™Prestoé›†ç¾¤çš„Coordinator Coordinatoræ¥æ”¶åˆ°å®¢æˆ·ç«¯ä¼ æ¥çš„æŸ¥è¯¢è¯­å¥ï¼Œå¯¹è¯¥è¯­å¥è¿›è¡Œè§£æã€ç”ŸæˆæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ä¾æ¬¡ç”ŸæˆSqlQueryExecution -&gt; SqlStageExecution -&gt; HttpRemoteTask Coordinatorå°†æ¯ä¸ªTaskåˆ†å‘åˆ°æ‰€éœ€è¦å¤„ç†çš„æ•°æ®æ‰€åœ¨çš„Workerä¸Šè¿›è¡Œåˆ†æ æ‰§è¡ŒSource Stageçš„Taskï¼Œè¿™äº›Taské€šè¿‡Connectorä»æ•°æ®æºä¸­è¯»å–æ‰€éœ€è¦çš„æ•°æ® å¤„äºä¸‹æ¸¸Stageä¸­ç”¨çš„Taskä¼šè¯»å–ä¸Šæ¸¸Stageäº§ç”Ÿçš„è¾“å‡ºç»“æœï¼Œå¹¶åœ¨è¯¥Stageçš„æ¯ä¸ªTaskæ‰€åœ¨çš„Workerå†…å­˜ä¸­è¿›è¡Œåç»­çš„è®¡ç®—å’Œå¤„ç† Coordinatorä»åˆ†å‘çš„Taskä¹‹åï¼Œä¸€ç›´æŒç»­ä¸æ–­çš„ä»æœ€åçš„Stageä¸­çš„Taskè·å¾—è®¡ç®—ç»“æœï¼Œå¹¶å°†ç»“æœå†™å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç›´åˆ°æ‰€æœ‰çš„è®¡ç®—ç»“æŸ Clientä»æäº¤æŸ¥è¯¢åï¼Œå°±ä¸€ç›´ç›‘å¬Coordinatorä¸­çš„æœ¬æ¬¡æŸ¥è¯¢ç»“æœé›†ï¼Œç«‹å³è¾“å‡ºã€‚ç›´åˆ°æ‰€æœ‰çš„ç»“æœéƒ½è¿”å›ï¼Œæœ¬æ¬¡æŸ¥è¯¢ç»“æŸ éƒ¨ç½²ä¸ä½¿ç”¨Prestoé›†ç¾¤éƒ¨ç½²ä¸‹è½½Prestoä½¿ç”¨GUIDç”Ÿæˆå·¥å…·ç”Ÿæˆæ‰€éœ€æœºå™¨æ•°é‡ç›¸ç­‰çš„GUIDï¼Œç”¨äºé…ç½®node.properties cd $PRESTO_HOME mkdir etc # node.propertiesé…ç½®èŠ‚ç‚¹ä¿¡æ¯(æ¯ä¸ªèŠ‚ç‚¹ä¸åŒ) vim etc/node.properties node.environment=cdh ä¸€ä¸ªPrestoé›†ç¾¤æœ‰ç›¸åŒçš„envåç§°ï¼Œæˆ‘è¿™é‡Œèµ·åå«cdh node.id=AB1C6EAC-8CF6-B397-EFD3-77C8AB041CD5 åˆšåˆšç”Ÿæˆçš„GUIDï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½è¦ä¸åŒçš„GUID node.data-dir=/var/presto/data å­˜æ”¾æ•°æ®çš„ç›®å½• # JVMé…ç½®(æ¯ä¸ªèŠ‚ç‚¹ç›¸åŒ) vim etc/jvm.config -server -Xmx4G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError # Prestoé…ç½® CoordinatorèŠ‚ç‚¹ éç”Ÿäº§é›†ç¾¤å•ä¸ªèŠ‚ç‚¹å¯ä»¥æ—¢ä¸ºCoordinatoråˆä¸ºWorkerå¯è®¾node-scheduler.include-coordinator=true vim etc/config.properties(æ¯ä¸ªèŠ‚ç‚¹ä¸åŒ) coordinator=true æ˜¯å¦ä¸ºCoordinator node-scheduler.include-coordinator=false æ˜¯å¦åœ¨CoordinatorèŠ‚ç‚¹æ‰§è¡Œè®¡ç®—ï¼ˆä¼šå½±å“æ€§èƒ½ï¼Œä¸å»ºè®®trueï¼‰ http-server.http.port=8080 è¯·æ±‚å‘é€çš„HTTPç«¯å£ query.max-memory=4GB å•æ¡Queryå ç”¨é›†ç¾¤å†…å­˜çš„æœ€å¤§å€¼ query.max-memory-per-node=1GB å•æ¡Queryå•ä¸ªèŠ‚ç‚¹å ç”¨å†…å­˜çš„æœ€å¤§å€¼ query.max-total-memory-per-node=2GB å•æ¡Queyå•ä¸ªèŠ‚ç‚¹å ç”¨çš„æ‰§è¡Œå†…å­˜å’Œç³»ç»Ÿå†…å­˜(readers, writers, and network buffers, etc.)æ€»å’Œçš„æœ€å¤§å€¼ discovery-server.enabled=true æ•´åˆCoordinatorå’ŒDiscoveryServerä¸ºä¸€ä¸ªè¿›ç¨‹ï¼Œä½¿ç”¨åŒä¸€ä¸ªç«¯å£ discovery.uri=http://cdh101:8080 # Prestoé…ç½® WorkerèŠ‚ç‚¹ vim etc/config.properties coordinator=false http-server.http.port=8080 query.max-memory=4GB query.max-memory-per-node=1GB query.max-total-memory-per-node=2GB discovery.uri=http://cdh101:8080 æŒ‡å®šé›†ç¾¤ä¸­å·²æœ‰çš„DSæœåŠ¡HTTPåœ°å€ # æ—¥å¿—ç­‰çº§è®¾ç½®(æ¯ä¸ªèŠ‚ç‚¹ç›¸åŒ) vim etc/log.properties com.facebook.presto=INFO # é…ç½®æ”¯æŒHiveæ•°æ®æº (æ¯ä¸ªèŠ‚ç‚¹ç›¸åŒ)(å…¶ä»–æ•°æ®æºå¯å‚è€ƒhttps://prestodb.io/docs/current/connector) mkdir etc/catalog vim etc/catalog/hive.properties connector.name=hive-hadoop2 hive.metastore.uri=thrift://cdh101:9083 hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml # ############################################### ln -s /var/presto/data/var/log log å°†æ—¥å¿—é“¾æ¥åˆ°å®‰è£…ç›®å½• åœ¨å„ä¸ªèŠ‚ç‚¹åå°å¯åŠ¨Prestoï¼šbin/launcher start ä¹Ÿå¯ä»¥åœ¨å‰å°è¿è¡Œ,æŸ¥çœ‹å…·ä½“çš„æ—¥å¿—ï¼šbin/launcher run åœæ­¢æœåŠ¡è¿›ç¨‹å‘½ä»¤ï¼šbin/laucher stop å¯åŠ¨è„šæœ¬ç¼–å†™ #!/bin/bash # éœ€è¦rootç”¨æˆ·å…å¯† # ä½¿ç”¨ sh presto-server.sh start PRESTO_HOME=/opt/modules/presto-server-0.248 OP=$1 if [ &quot;$OP&quot; == &quot;start&quot; ] || [ &quot;$OP&quot; == &quot;stop&quot; ] || [ &quot;$OP&quot; == &quot;status&quot; ] || [ &quot;$OP&quot; == &quot;restart&quot; ]; then echo &quot;Begin to $OP Presto Coordinator and Workers.&quot; for((host=101; host&lt;=104; host++)); do echo --- &quot;$OP&quot; presto server on cdh$host --- ssh -l root cdh$host $PRESTO_HOME/bin/launcher &quot;$OP&quot; done else echo &quot;Usage: ./presto-server.sh [start|stop|restart|status]&quot; exit 1 fi # ############################################### æ ¹æ®è‡ªå·±çš„ç‰ˆæœ¬ä¸‹è½½prestoå®¢æˆ·ç«¯ï¼šhttps://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.248/presto-cli-0.248-executable.jar chmod a+x presto-cli-0.248-executable.jar mv presto-cli-0.248-executable.jar presto ln -s /opt/modules/presto-server-0.248/presto /usr/bin/presto è¿›å…¥Prestoå®¢æˆ·ç«¯(æŒ‡å®šæ•°æ®æºhiveï¼ŒæŒ‡å®šåº“ådefault) presto --server cdh101:8080 --catalog hive --schema default # ############################################### # é…ç½®MySQLæ•°æ®æº vim etc/catalog/mysql.properties connector.name=mysql connection-url=jdbc:mysql://cdh102:3306 connection-user=root connection-password=123456 Presto On Yarnéƒ¨ç½²å¾…è¡¥å…… Presto On kerberized HiveHiveé›†ç¾¤æ˜¯Kerberosè®¤è¯çš„å®‰å…¨é›†ç¾¤,Presto Hiveæ’ä»¶éœ€è¦è®¾ç½®kerberosä»¥åŠsaslç›¸å…³å±æ€§,å¦åˆ™ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯ -æ‰§è¡Œ ./presto --debug --execute &#39;show schemas&#39; --server presto-server:8080 --catalog hive Caused by: org.apache.thrift.transport.TTransportException: hms:9083: null at com.facebook.presto.hive.metastore.thrift.Transport.rewriteException(Transport.java:92) at com.facebook.presto.hive.metastore.thrift.Transport.access$000(Transport.java:32) at com.facebook.presto.hive.metastore.thrift.Transport$TTransportWrapper.readAll(Transport.java:169) at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:380) at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:230) at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_databases(ThriftHiveMetastore.java:1187) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_databases(ThriftHiveMetastore.java:1175) at com.facebook.presto.hive.metastore.thrift.ThriftHiveMetastoreClient.getAllDatabases(ThriftHiveMetastoreClient.java:89) at com.facebook.presto.hive.metastore.thrift.ThriftHiveMetastore.getMetastoreClientThenCall(ThriftHiveMetastore.java:1068) at com.facebook.presto.hive.metastore.thrift.ThriftHiveMetastore.lambda$getAllDatabases$0(ThriftHiveMetastore.java:219) at com.facebook.presto.hive.metastore.thrift.HiveMetastoreApiStats.lambda$wrap$0(HiveMetastoreApiStats.java:48) at com.facebook.presto.hive.RetryDriver.run(RetryDriver.java:139) at com.facebook.presto.hive.metastore.thrift.ThriftHiveMetastore.getAllDatabases(ThriftHiveMetastore.java:218) ... 57 more Suppressed: org.apache.thrift.transport.TTransportException: hms:9083: null ... 71 more Caused by: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) at com.facebook.presto.hive.metastore.thrift.Transport$TTransportWrapper.readAll(Transport.java:166) ... 68 more å‚è€ƒ:Presto hive-securityPrestoé»˜è®¤æ²¡æœ‰ä½¿ç”¨æƒé™è®¤è¯,æ‰€æœ‰Queryéƒ½æ˜¯ä»¥è¿è¡ŒPrestoServerè¿›ç¨‹çš„ç”¨æˆ·èº«ä»½æäº¤çš„.Presto Hive Pluginæ”¯æŒè¿æ¥Kerberosé›†ç¾¤ä»¥æ‰©å±•æƒé™ç®¡ç†,å¯¹æ•°æ®åšè®¿é—®æƒé™æ§åˆ¶.é›†æˆKerberoså,Prestoå¯ä»¥æ¨¡æ‹Ÿæ‰§è¡ŒQueryçš„ç”¨æˆ·æ¥è®¿é—®æ•°æ®,æ•°æ®æƒé™å¾—åˆ°æ§åˆ¶.vim etc/catalog/hive.properties æ·»åŠ kerberosç›¸å…³å‚æ•°,å¹¶é‡å¯PrestoServers connector.name=hive-hadoop2 hive.metastore.uri=thrift://metastoreIP:9083 hive.config.resources=/etc/ecm/hadoop-conf/core-site.xml,/etc/ecm/hadoop-conf/hdfs-site.xml hive.metastore.authentication.type=KERBEROS hive.metastore.service.principal=hive/metastoreIP@REALM.COM hive.metastore.client.principal=hive/prestoServerIp@REALM.COM hive.metastore.client.keytab=/opt/keytabs/hive.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=true hive.hdfs.presto.principal=hive/prestoServerIp@REALM.COM hive.hdfs.presto.keytab=/opt/keytabs/hive.keytab é”™è¯¯ä¸å¼‚å¸¸æ’æŸ¥è§£å†³:åœ¨ä½¿ç”¨OSSå­˜å‚¨çš„Hiveé›†ç¾¤ä½¿ç”¨PrestoæŠ¥é”™ Query 20220808_061604_00004_dp628 failed: java.lang.ClassNotFoundException: Class com.aliyun.jindodata.oss.JindoOssFileSystem not found Query 20220809_023611_00025_43gsw failed: java.lang.NoClassDefFoundError: com/aliyun/jindodata/api/spec/JdoException è§£å†³:æ‹·è´jindo-sdk-4.4.1.jarå’Œjindo-core-4.4.1.jaråˆ°$PRESTO_HOME/plugin/hive-hadoop2/ presto:db_name&gt; select * from table_name limit 1;Query 20220808_061604_00004_dp628, FAILED, 1 nodeSplits: 17 total, 0 done (0.00%)0:01 [0 rows, 0B] [0 rows/s, 0B/s]æŸ¥çœ‹Presto WebUIå‘ç°å¦‚ä¸‹æŠ¥é”™: com.facebook.presto.spi.PrestoException: For input string: &quot;30s&quot; at com.facebook.presto.hive.BackgroundHiveSplitLoader$HiveSplitLoaderTask.process(BackgroundHiveSplitLoader.java:128) at com.facebook.presto.hive.util.ResumableTasks.safeProcessTask(ResumableTasks.java:47) at com.facebook.presto.hive.util.ResumableTasks.access$000(ResumableTasks.java:20) at com.facebook.presto.hive.util.ResumableTasks$1.run(ResumableTasks.java:35) at com.facebook.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.NumberFormatException: For input string: &quot;30s&quot; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:589) at java.lang.Long.parseLong(Long.java:631) at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1311) at org.apache.hadoop.hdfs.DFSClient$Conf.&lt;init&gt;(DFSClient.java:502) æ£€æŸ¥hive.config.resourcesä¸­çš„hdfs-site.xmlç­‰æ–‡ä»¶,å‘ç°dfs.client.datanode-restart.timeoutç­‰é…ç½®å‚æ•°å€¼ä¸º30s,ä¿®æ”¹ä¸º30å¹¶é‡å¯PrestoServerå³å¯. Presto On IcebergPrestoå…¼å®¹Iceberg,å‚è€ƒIceberg Connectoré…ç½®Icebergè¿æ¥å™¨é…ç½®vim etc/catalog/iceberg.properties æ”¯æŒçš„Iceberg Catalog Typeæœ‰hiveå’Œhadoopä¸¤ç§,é…ç½®æ–¹å¼åˆ†åˆ«ä¸ºå¦‚ä¸‹iceberg.catalog.type=hiveçš„é…ç½®æ–¹å¼: connector.name=iceberg hive.metastore.uri=thrift://metastoreIP:9083 hive.metastore.authentication.type=KERBEROS hive.metastore.service.principal=hive/metastoreIP@REALM.COM hive.metastore.client.principal=hive/prestoServerIp@REALM.COM hive.metastore.client.keytab=/opt/keytabs/hive.keytab iceberg.catalog.type=hive iceberg.file-format=PARQUET iceberg.compression-codec=SNAPPY hive.config.resources=/etc/ecm/hadoop-conf/core-site.xml,/etc/ecm/hadoop-conf/hdfs-site.xml iceberg.catalog.type=hadoopçš„é…ç½®æ–¹å¼: connector.name=iceberg hive.metastore.uri=thrift://metastoreIP:9083 iceberg.catalog.type=hadoop iceberg.file-format=PARQUET iceberg.catalog.cached-catalog-num=10 iceberg.hadoop.config.resources=/etc/ecm/hadoop-conf/core-site.xml,/etc/ecm/hadoop-conf/hdfs-site.xml iceberg.catalog.warehouse=hdfs://nameservice/user/iceberg/warehouse ç„¶åé‡å¯PrestoServer ä¸Hiveæ•´åˆï¼šåˆ°Iceberg-Releasesé¡µé¢ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ï¼ˆå¯åœ¨$PRESTO_HOME/plugins/icebergä¸‹æŸ¥çœ‹Presto Icebergç‰ˆæœ¬ï¼‰çš„Hive runtime Jarã€‚ä¸‹è½½libfb303-0.9.3.jarä¾èµ–åŒ…ã€‚åœ¨hive-site.xmlä¸­æ·»åŠ  &lt;property&gt; &lt;name&gt;iceberg.engine.hive.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; æ‰€æœ‰hiveèŠ‚ç‚¹åˆ›å»º/etc/hive/auxlibç›®å½•ï¼Œå°†ä¸¤ä¸ªjaræ”¾å…¥é…ç½®HiveServer2ç”Ÿæ•ˆjarï¼Œåœ¨hive-site.xmlæ·»åŠ  &lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt;/etc/hive/auxlib&lt;/value&gt; &lt;/property&gt; é…ç½®hiveCliç”Ÿæ•ˆjarhive-env.shå¢åŠ export HIVE_AUX_JARS_PATH=/etc/hive/auxlibé‡å¯HiveServer2åœ¨hiveåˆ›å»ºIcebergè¡¨ CREATE TABLE iceberg_db.hive_iceberg_table ( id BIGINT, name STRING ) STORED BY &#39;org.apache.iceberg.mr.hive.HiveIcebergStorageHandler&#39;; ä½¿ç”¨Hiveå’ŒPrestoè¯»å†™å’Œæ“ä½œIcebergè¡¨ hive&gt; insert into hive_iceberg_table values (1,&#39;qjj&#39;),(2,&#39;abc&#39;); presto&gt; select * from hive_iceberg_table; presto&gt; insert into hive_iceberg_table values (3,&#39;jjq&#39;),(4,&#39;def&#39;); hive&gt; select count(1) from hive_iceberg_table; ç›¸å…³TroubleShooting Vectorization only supported for Hive 3+è§£å†³ï¼šè®¾ç½®set hive.vectorized.execution.enabled=false; Prestoæ’å…¥Icebergè¡¨Stringç±»å‹æ•°æ®åï¼ŒHiveè¯»å–ä¸åˆ°Stringçš„å€¼è€Œæ˜¯æ˜¾ç¤ºjava.nio.HeapByteBufferå¾…æ’æŸ¥â€¦ Prestoè¯­æ³•ï¼šSHOW CATALOGS; æŸ¥çœ‹Prestoé›†ç¾¤å½“å‰å¯ç”¨æ•°æ®æº SHOW SCHEMAS; æŸ¥çœ‹å½“å‰æ•°æ®æºæœ‰å“ªäº›åº“ SHOW SCHEMAS FROM hive; æŸ¥çœ‹hiveæ•°æ®æºçš„æ‰€æœ‰åº“ ï¼ˆFROMå¯ä»¥ç”¨INæ›¿æ¢) SHOW TABLES; æŸ¥çœ‹å½“å‰Schemaåº“ä¸‹æœ‰å“ªäº›è¡¨ SHOW TABLES FROM hive.default; æŸ¥çœ‹hiveæ•°æ®æºä¸‹defaultåº“ä¸‹çš„æ‰€æœ‰è¡¨ CREATE SCHEMA hive.web WITH (location = &#39;hdfs:///user/hive/warehouse/web/&#39;) # å»ºåº“ ALTER SCHEMA old_db_name RENAME TO new_db_name # æ”¹åº“å -- å»ºè¡¨ CREATE TABLE hive.test.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar ) WITH ( format = &#39;Parquet&#39;, partitioned_by = ARRAY[&#39;ds&#39;, &#39;country&#39;], bucketed_by = ARRAY[&#39;user_id&#39;], bucket_count = 50 ); -- æŸ¥çœ‹è¡¨ desc hive.test.page_views; -- æŸ¥çœ‹è¡¨å­—æ®µ SHOW COLUMNS IN hive.test.page_views; -- ç»Ÿè®¡è¡¨ä¿¡æ¯ï¼ˆç±»ä¼¼äºSparkçš„Analyzed tableï¼‰ï¼ˆæ•°æ®å¤§å°ï¼Œè¡Œæ•°ï¼Œæœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œæ— é‡å¤å€¼ä¸ªæ•°ï¼ŒNULLå€¼å æ¯”ï¼‰ SHOW STATS FOR table SHOW STATS FOR ( SELECT * FROM table [ WHERE condition ] ) -- æŸ¥çœ‹é€»è¾‘è®¡åˆ’ï¼ˆç›¸æ¯”Sparkçš„é€»è¾‘è®¡åˆ’ï¼Œå¤šäº†æ¯ä¸ªèŠ‚ç‚¹çš„è¡Œæ•°åŠæ•°æ®å¤§å°,CPUæ“ä½œæ•°,å†…å­˜æ¶ˆè€—,ç½‘ç»œä¼ è¾“å¤§å°ï¼‰ EXPLAIN sql EXPLAIN ANALYZE sql -- æ›´å…¨é¢ ä½†ä¼šè§¦å‘è®¡ç®— -- æšä¸¾è¡¨ SELECT * FROM ( VALUES (1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;) ) t (id, name); SELECT * FROM ( VALUES (1, &#39;a&#39;, ARRAY[1, 2, 3]), (2, &#39;b&#39;, ARRAY[4, 5, 6]), (3, &#39;c&#39;, ARRAY[7, 8, 9]) ) AS t (id, name, arr); -- å‡†å¤‡å¥½æ‰§è¡Œè¯­å¥,å»¶è¿Ÿæ‰§è¡Œ PREPARE statement_1 FROM show tables; PREPARE statement_2 FROM select * from test; EXECUTE statement_2; EXECUTE statement_1; DEALLOCATE PREPARE statement_1; DEALLOCATE PREPARE statement_2; -- timestampå­—æ®µè¿‡æ»¤æ¡ä»¶ kafka_timestampå­—æ®µç±»å‹ä¸ºtimestamp(6) select appid from iceberg.iceberg_db.iceberg_table where ds=&#39;2022120910&#39; and kafka_timestamp = from_unixtime(1670625207.945); -- æ•ˆç‡æ›´é«˜ select appid from iceberg.iceberg_db.iceberg_table where ds=&#39;2022120910&#39; and to_unixtime(kafka_timestamp) = 1670525207.945; Prestoæ”¯æŒäº‹åŠ¡ï¼Œç›¸å…³å‘½ä»¤æœ‰COMMIT,START TRANSACTION,ROLLBACKæ›´å¤šè¯­æ³•ï¼šSQL Statement Syntax Presto WEBUIè®¿é—®WEBUIåœ°å€å³ä¸ºDiscoveryServeråœ°å€ï¼šhttp://cdh101:8080/ui/é€è¿‡WEB UIå¯ä»¥æŸ¥çœ‹åˆ°æ¯ä¸ªSQL Queryçš„æ‰§è¡Œç›¸å…³çŠ¶æ€ä¿¡æ¯ä»¥åŠPrestoé›†ç¾¤çš„è¿è¡ŒçŠ¶æ€ä¿¡æ¯ã€‚ ä»»åŠ¡çŠ¶æ€ åŸå›  QUEUED ç­‰å¾…æ‰§è¡Œ PLANNING æ­£åœ¨è½¬æˆæ‰§è¡Œè®¡åˆ’ RUNNING æ­£åœ¨è¿è¡ŒQuery BLOCKED é˜»å¡ä¸­ï¼Œç­‰å¾…å†…å­˜ã€Bufferç­‰èµ„æº FINISHING å³å°†å®Œæˆæ‰§è¡Œï¼Œæ­£åœ¨è¿”å›æ•°æ® FINISHED æ‰§è¡Œå®Œæˆ FAILED æ‰§è¡Œå¤±è´¥ BLOCKEDçŠ¶æ€æ˜¯æ­£å¸¸çš„ï¼Œä½†æŒç»­å¾ˆé•¿æ—¶é—´éƒ½æ˜¯è¿™ä¸ªçŠ¶æ€å°±éœ€è¦æ’æŸ¥ä¸‹åŸå› ï¼Œæœ‰å¾ˆå¤šå¯èƒ½çš„åŸå› ï¼š1.å†…å­˜ä¸è¶³2.ç£ç›˜æˆ–ç½‘ç»œI/Oç“¶é¢ˆ3.æ•°æ®å€¾æ–œ(æ‰€æœ‰æ•°æ®éƒ½è½¬ç§»åˆ°å‡ ä¸ªworkerä¸Š)4.å¹¶è¡Œåº¦ä½(åªæœ‰å‡ ä¸ªworkerå¯ç”¨)5.æŸä¸ªStageæŸ¥è¯¢å¼€é”€è¾ƒé«˜ï¼ˆå¦‚select *æ“ä½œæ•°æ®è¿‡å¤šï¼‰å¯¹äºæŸä¸ªQueryçš„æ‰§è¡Œè¿‡ç¨‹ç›¸å…³ç›‘æ§ä¿¡æ¯ï¼Œå¯ä»¥åœ¨WebUIä¸Šç‚¹é‚£ä¸ªQuery IDå³å¯æŸ¥çœ‹ è¿æ¥Prestoç”¨æˆ·è¿æ¥Prestoçš„ä¸»è¦æ–¹å¼ï¼šPresto-Cli,JDBC,PyHive,PrestoOnSparkç­‰ã€‚Presto-Cli: wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.248/presto-cli-0.248-executable.jar mv presto-cli-0.248-executable.jar presto chmod a+x presto presto --server cdh101:8080 --catalog hive --schema default --user admin JDBC: &lt;dependency&gt; &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt; &lt;artifactId&gt;presto-jdbc&lt;/artifactId&gt; &lt;version&gt;0.248&lt;/version&gt; &lt;/dependency&gt; public class PrestoConnetToJDBC &#123; public static void main(String[] args) throws SQLException &#123; // 1.ç®€å•åˆ›å»ºè¿æ¥ // String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users&quot;; // Connection connection = DriverManager.getConnection(url, &quot;root&quot;, null); // connection.prepareStatement(&quot;show tables&quot;); // 2.å¸¦å‚æ•°åˆ›å»ºè¿æ¥ String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users&quot;; Properties properties = new Properties(); properties.setProperty(&quot;user&quot;, &quot;root&quot;); properties.setProperty(&quot;password&quot;, &quot;&quot;); properties.setProperty(&quot;SSL&quot;, &quot;false&quot;); Connection connection = DriverManager.getConnection(url, properties); // 3.å¸¦å‚æ•°åˆ›å»ºè¿æ¥ // String url = &quot;jdbc:presto://cdh101:8080/hive/staging_db_users?user=root&amp;password=secret&amp;SSL=true&quot;; // Connection connection = DriverManager.getConnection(url); // è¯»æ•°æ®æˆ–åšå…¶ä»–æ“ä½œ Statement stmt = connection.createStatement(); ResultSet rs = stmt.executeQuery(&quot;select * from tb_user_info limit 10&quot;); while (rs.next())&#123; System.out.println(rs.getString(1) + &quot;--&quot; + rs.getString(2)); &#125; &#125; &#125; python:pip3 install saslpip3 install thriftpip3 install thrift-saslpip3 install PyHivepip3 install sqlalchemypip3 install requests from sqlalchemy import * from sqlalchemy.engine import create_engine from sqlalchemy.schema import * import pandas as pd # Presto engine = create_engine(&#39;presto://admin:123456@cdh101:8080/mysql/db_users&#39;) # å¯†ç è¿æ¥ engine = create_engine(&#39;presto://cdh101:8080/mysql/db_users&#39;) df = pd.read_sql(&quot;select * from tb_user_records limit 10&quot;,engine) print(df) PrestoOnSpark:Presto on Sparkå³åˆ©ç”¨Sparkä½œä¸ºPrestoæŸ¥è¯¢çš„æ‰§è¡Œæ¡†æ¶æ“ä½œï¼šExecuting Presto on Spark æœ€ä½³å®è·µPrestoå‚æ•°è°ƒä¼˜ï¼šProperties Referenceï¼Œå®˜æ–¹è¯¦ç»†ä»‹ç»äº†Prestoçš„config.propertiesä¸­çš„å¸¸è§„å‚æ•°å¦‚joinå‚æ•°ï¼Œå†…å­˜ç®¡ç†å‚æ•°ï¼ŒSpillingæº¢å‡ºç£ç›˜ç›¸å…³å‚æ•°ï¼Œæ•°æ®ç½‘ç»œäº¤æ¢å‚æ•°ï¼ˆä¸€ä¸ªæŸ¥è¯¢ä»»åŠ¡ä¸åŒStageä¼šæœ‰ä¸åŒèŠ‚ç‚¹äº¤æ¢æ•°æ®ï¼Œè¿™äº›å‚æ•°æé«˜ç½‘ç»œåˆ©ç”¨ç‡ï¼‰ï¼Œä»»åŠ¡å‚æ•°ï¼ŒèŠ‚ç‚¹è°ƒåº¦å‚æ•°ï¼Œä¼˜åŒ–å™¨å‚æ•°ä»¥åŠæ­£åˆ™ç›¸å…³å‚æ•° Prestoä¸æ˜¯çº¯å†…å­˜è®¡ç®—å—ï¼Ÿä¸ºä»€ä¹ˆè¦æº¢å†™åˆ°ç£ç›˜ï¼Ÿæ­£å¸¸æƒ…å†µPrestoæ‰§è¡ŒQueryè¯·æ±‚çš„å†…å­˜èµ„æºè¶…è¿‡query_max_memoryæˆ–query_max_memory_per_nodeè¿™ä¸ªQueryå°±ä¼šè¢«ç»ˆæ­¢ã€‚æº¢å†™ç£ç›˜æœºåˆ¶ï¼šPrestoèŠ‚ç‚¹ç©ºé—²æ—¶Queryä¼šåˆ©ç”¨å…¨éƒ¨å†…å­˜èµ„æºï¼Œå¦‚æœæ²¡è¶³å¤Ÿå†…å­˜ï¼ŒQueryè¢«è¿«ä½¿ç”¨ç£ç›˜æ¥å­˜å‚¨ä¸­é—´æ•°æ®ï¼Œå†™å…¥ç£ç›˜å†ä»ç£ç›˜è¯»å–å›æ¥ï¼Œæœ‰è¾ƒé«˜çš„IOå¼€é”€ã€‚è§£å†³IOå¼€é”€é«˜çš„æ–¹æ³•ï¼šspiller-spill-pathå¯è®¾ç½®å¤šä¸ªç£ç›˜å¤šä¸ªè·¯å¾„ï¼Œå¹¶è¡Œè¯»å†™æé«˜IOæ•ˆç‡ï¼›spill-encryption-enabledå¯ç”¨å‹ç¼©ç”¨CPUå¼€é”€æ¢IOå¼€é”€å±€é™ï¼šç³»ç»Ÿæ— æ³•å°†ä¸­é—´æ•°æ®åˆ’åˆ†æˆè¶³å¤Ÿå°çš„å—ï¼Œå¯¼è‡´ä»ç£ç›˜åŠ è½½å—æ•°æ®æ—¶å‘ç”ŸOOMï¼›åªæœ‰Joinå’Œèšåˆæ“ä½œå¯ä»¥è½ç›˜ èµ„æºéš”ç¦»æœºåˆ¶ï¼ŸPrestoå¯ä»¥åƒYarnä¸€æ ·å°†å…¨éƒ¨èµ„æºåˆ†ä¸ºå¤šä¸ªèµ„æºç»„ï¼ˆé€šè¿‡é…ç½®æ–‡ä»¶etc/resource-groups.propertiesï¼‰ï¼Œèµ„æºç»„ä¹Ÿå¯ä»¥æœ‰å­ç»„ã€‚é…ç½®å¯å‚è€ƒï¼šResource Groups Sessioné…ç½®ç®¡ç†é€šè¿‡é…ç½®etc/session-property-config.propertieså¯ä»¥å°†ä»»åŠ¡åˆ†ä¸ºä¸åŒç±»å‹ï¼ˆå¦‚å³æ—¶æŸ¥è¯¢ï¼Œetlï¼Œé«˜æ¶ˆè€—etlç­‰â€¦ï¼‰ï¼Œç„¶åå¯¹ä¸åŒç±»å‹çš„ä»»åŠ¡é…ç½®ä¸åŒçš„èµ„æºï¼Œå‚æ•°ï¼ˆconfiguration propertyï¼‰ã€‚å…·ä½“è§ï¼šSession Property Managersï¼ˆPrestoå‚æ•°åˆ†ä¸ºconfiguration propertyå’Œsession propertyï¼‰ åˆ†å¸ƒå¼æ’åºéœ€è¦æ’åºæ•°æ®è¶…è¿‡å•èŠ‚ç‚¹query.max-memory-per-nodeå¤§å°é™åˆ¶ï¼Œé»˜è®¤ä¼šå¯ç”¨åˆ†å¸ƒå¼æ’åºï¼ˆå‚æ•°distributed-sortï¼‰ã€‚æ’åºé€Ÿåº¦ä¸ä¼šéšèŠ‚ç‚¹æ•°é‡å¢åŠ è€Œçº¿æ€§åŠ å¿«ï¼Œå› ä¸ºæ’åºåæ•°æ®åœ¨å•ä¸ªèŠ‚ç‚¹åˆå¹¶ ä½¿ç”¨AlluxioåŸºäºå†…å­˜ç¼“å­˜çƒ­ç‚¹æ•°æ®å’Œé™ä½è¿œç¨‹æœºæˆ¿ç½‘ç»œIOå½±å“æ³¨æ„:è®¡ç®—ä¸å­˜å‚¨èŠ‚ç‚¹å…±ç½®çš„åœºæ™¯ä¸‹ï¼ŒAlluxioå¯¹Prestoçš„åŠ é€Ÿæ•ˆæœå¹¶ä¸æ˜æ˜¾Alluxioåˆ†å¸ƒå¼ç¼“å­˜æ•°æ®æ¹–ç›¸å…³çŸ¥è¯†å¯ä»¥å‚è€ƒæˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« ï¼šAlluxio-åŸºäºå†…å­˜çš„è™šæ‹Ÿåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»ŸPrestoç»“åˆAlluxioé…ç½®å’Œä½¿ç”¨å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šAlluxio Cache Service åŸºäºæˆæœ¬çš„ä¼˜åŒ–Joinæ“ä½œå¯¹æŸ¥è¯¢æ€§èƒ½å½±å“å¤§ï¼ŒPrestoä¹Ÿä¼šåƒSparkä¸€æ ·ï¼Œè¯„ä¼°Joinçš„è¡¨çš„é¡ºåºï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½æˆæœ¬çš„Joinè¡¨é¡ºåºã€‚å¯¹åº”çš„ConfigurationProperty(optimizer.join-reordering-strategy)å¯¹åº”çš„SessionProperty(join_reordering_strategy)å‚æ•°å€¼ï¼šAUTOMATICå…¨è‡ªåŠ¨çš„Joinä¼˜åŒ–ï¼ŒELIMINATE_CROSS_JOINSé»˜è®¤å‚æ•°æ¶ˆé™¤ä¸å¿…è¦çš„ç¬›å¡å°”ç§¯ï¼ŒNONEæŒ‰SQLè¯­æ³•çš„é¡ºåºJoin åˆ†å¸ƒå¼Joinç®—æ³•é€‰æ‹©Prestoçš„Joinæ˜¯åŸºäºHashçš„ï¼Œåˆ†ä¸ºä¸¤ç§æ–¹å¼ï¼šPartitionedå’ŒBroadcastPartitionedï¼šæ¯ä¸ªèŠ‚ç‚¹éƒ½æŒæœ‰ä¸€éƒ¨åˆ†Hashåçš„æ•°æ®ï¼Œç„¶åJoinBroadcastï¼šä¸€ä¸ªè¡¨è¢«å¹¿æ’­åˆ°æ‰€æœ‰å‚ä¸Joinè®¡ç®—èŠ‚ç‚¹å¯¹åº”çš„ConfigurationProperty(join-distribution-type)å¯¹åº”çš„SessionProperty(join_distribution_type)å‚æ•°å€¼ï¼šAUTOMATICå…¨è‡ªåŠ¨çš„Joinç®—æ³•é€‰æ‹©ï¼ŒBROADCASTï¼ŒPARTITIONED(é»˜è®¤) Prestoä¼šæ ¹æ®å…ƒæ•°æ®ä¿¡æ¯è¯»å–åˆ†åŒºæ•°æ®ï¼Œåˆç†è®¾ç½®åˆ†åŒºèƒ½å‡å°‘Prestoæ•°æ®è¯»å–é‡ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½ Prestoå¯¹ORCæ ¼å¼æ–‡ä»¶çš„è¯»å–è¿›è¡Œäº†ç‰¹å®šä¼˜åŒ–ï¼Œç›¸å¯¹äºParquetï¼ŒPrestoå¯¹ORCæ”¯æŒæ›´å¥½(Impalaå¯¹Parquetæ”¯æŒæ›´å¥½) æ•°æ®å‹ç¼©å¯ä»¥å‡å°‘èŠ‚ç‚¹é—´æ•°æ®ä¼ è¾“å¯¹ç½‘ç»œå¸¦å®½çš„å‹åŠ›ï¼Œå¯¹äºå³å¸­æŸ¥è¯¢éœ€è¦å¿«é€Ÿè§£å‹ï¼Œå»ºè®®é‡‡ç”¨Snappyå‹ç¼©ç®—æ³• é¢„å…ˆæ’åºä»¥æé«˜æ€§èƒ½å¯¹äºå·²ç»æ’åºçš„æ•°æ®ï¼Œåœ¨æŸ¥è¯¢çš„æ•°æ®è¿‡æ»¤é˜¶æ®µï¼ŒORCæ ¼å¼æ”¯æŒè·³è¿‡è¯»å–ä¸å¿…è¦çš„æ•°æ®ã€‚æ¯”å¦‚å¯¹äºç»å¸¸éœ€è¦è¿‡æ»¤çš„å­—æ®µå¯ä»¥é¢„å…ˆæ’åºã€‚INSERT INTO table nation_orc partition(p) SELECT * FROM nation SORT BY n_name;å¦‚æœéœ€è¦è¿‡æ»¤ n_name å­—æ®µï¼Œåˆ™æ€§èƒ½å°†æå‡ï¼šSELECT count(*) FROM nation_orc WHERE n_name=â€™AUSTRALIAâ€™; ä¸€äº›Prestoä¼˜åŒ–å¸¸è¯† å› ä¸ºåˆ—å¼å­˜å‚¨ï¼Œå°½é‡é¿å…select *ï¼Œè€Œæ˜¯åªæŸ¥è¯¢æœ‰ç”¨å­—æ®µ å¯¹äºæœ‰åˆ†åŒºçš„è¡¨ï¼Œwhereè¯­å¥ä¸­ä¼˜å…ˆä½¿ç”¨åˆ†åŒºå­—æ®µè¿›è¡Œè¿‡æ»¤ åˆç†å®‰æ’__group byå­—æ®µçš„é¡ºåº__æœ‰åŠ©äºæé«˜æŸ¥è¯¢æ•ˆç‡ï¼Œè¿™äº›å­—æ®µæŒ‰ç…§æ¯ä¸ªå­—æ®µdistinctæ•°æ®å¤šå°‘è¿›è¡Œé™åºæ’åˆ— å¤šè¡¨Joinæ—¶ï¼Œæ•°æ®è¶Šå¤šçš„è¡¨è¶Šå¾€åæ”¾ï¼ŒLeft joinæ—¶ï¼Œæ¡ä»¶è¿‡æ»¤å°½é‡åœ¨ONé˜¶æ®µå®Œæˆï¼Œè€Œå°‘ç”¨WHEREï¼ŒJoinå·¦è¾¹å°½é‡æ”¾å°æ•°æ®é‡çš„è¡¨ï¼Œè€Œä¸”æœ€å¥½æ˜¯é‡å¤å…³è”é”®å°‘çš„è¡¨ å°†ä½¿ç”¨é¢‘ç¹çš„è¡¨ä½œä¸ºä¸€ä¸ªå­æŸ¥è¯¢æŠ½ç¦»å‡ºæ¥ï¼Œé¿å…å¤šæ¬¡è¯»å–IO Order byæ—¶ä½¿ç”¨limitï¼šOrder byéœ€è¦æ‰«ææ•°æ®åˆ°å•ä¸ªworkerèŠ‚ç‚¹è¿›è¡Œæ’åºï¼Œå¯¼è‡´å•ä¸ªworkeréœ€è¦å¤§é‡å†…å­˜ã€‚å¦‚æœæ˜¯æŸ¥è¯¢Top Næˆ–è€…Bottom Nï¼Œä½¿ç”¨limitå¯å‡å°‘æ’åºè®¡ç®—å’Œå†…å­˜å‹åŠ› ç²¾åº¦è¦æ±‚ä½çš„åœºæ™¯ä½¿ç”¨è¿‘ä¼¼èšåˆå‡½æ•°ï¼šPrestoæœ‰ä¸€äº›è¿‘ä¼¼èšåˆå‡½æ•°ï¼Œå¯¹äºå…è®¸æœ‰å°‘é‡è¯¯å·®çš„æŸ¥è¯¢åœºæ™¯ï¼Œä½¿ç”¨è¿™äº›å‡½æ•°å¯¹æŸ¥è¯¢æ€§èƒ½æœ‰å¤§å¹…æå‡ã€‚æ¯”å¦‚ä½¿ç”¨approx_distinct()å‡½æ•°æ¯”Count(distinct x)æœ‰å¤§æ¦‚2~3%çš„è¯¯å·®ã€‚ ç”¨regexp_likeä»£æ›¿å¤šä¸ªlikeè¯­å¥ï¼šPrestoæŸ¥è¯¢ä¼˜åŒ–å™¨æ²¡æœ‰å¯¹å¤šä¸ªlikeè¯­å¥è¿›è¡Œä¼˜åŒ–ï¼Œä½¿ç”¨regexp_likeå¯¹æ€§èƒ½æœ‰è¾ƒå¤§æå‡ [GOOD] SELECT xxx FROM access WHERE regexp_like(method, &#39;GET|POST|PUT|DELETE&#39;) [BAD] SELECT xxx FROM access WHERE method LIKE &#39;%GET%&#39; OR method LIKE &#39;%POST%&#39; OR method LIKE &#39;%PUT%&#39; OR method LIKE &#39;%DELETE%&#39; ä½¿ç”¨Rankå‡½æ•°ä»£æ›¿row_numberå‡½æ•°æ¥è·å–Top N:åœ¨è¿›è¡Œä¸€äº›åˆ†ç»„æ’åºåœºæ™¯æ—¶ï¼Œä½¿ç”¨rankå‡½æ•°æ€§èƒ½æ›´å¥½ [GOOD] SELECT checksum(rnk) FROM ( SELECT rank() OVER (PARTITION BY l_orderkey, l_partkey ORDER BY l_shipdate DESC) AS rnk FROM lineitem ) t WHERE rnk = 1 [BAD] SELECT checksum(rnk) FROM ( SELECT row_number() OVER (PARTITION BY l_orderkey, l_partkey ORDER BY l_shipdate DESC) AS rnk FROM lineitem ) t WHERE rnk = 1 ä½¿ç”¨Prestoåˆ†æç»Ÿè®¡æ•°æ®æ—¶ï¼Œå¯è€ƒè™‘æŠŠå¤šæ¬¡æŸ¥è¯¢åˆå¹¶ä¸ºä¸€æ¬¡æŸ¥è¯¢ï¼Œç”¨Prestoæä¾›çš„å­æŸ¥è¯¢å®Œæˆã€‚ WITH subquery_1 AS ( SELECT a1, a2, a3 FROM Table_1 WHERE a3 between 20180101 and 20180131 ), /*å­æŸ¥è¯¢subquery_1,æ³¨æ„ï¼šå¤šä¸ªå­æŸ¥è¯¢éœ€è¦ç”¨é€—å·åˆ†éš”*/ subquery_2 AS ( SELECT b1, b2, b3 FROM Table_2 WHERE b3 between 20180101 and 20180131 ) /*æœ€åä¸€ä¸ªå­æŸ¥è¯¢åä¸è¦å¸¦é€—å·ï¼Œä¸ç„¶ä¼šæŠ¥é”™ã€‚*/ SELECT subquery_1.a1, subquery_1.a2, subquery_2.b1, subquery_2.b2 FROM subquery_1 JOIN subquery_2 ON subquery_1.a3 = subquery_2.b3; å­—æ®µåä¸å…³é”®å­—å†²çªï¼šMySQLå¯¹äºå…³é”®å­—å†²çªçš„å­—æ®µååŠ åå¼•å·ï¼ŒPrestoå¯¹ä¸å…³é”®å­—å†²çªçš„å­—æ®µååŠ åŒå¼•å·ã€‚ Hiveåˆ†æä»»åŠ¡å¦‚ä½•è¿ç§»PrestoPrestoä½¿ç”¨ANSIæ ‡å‡†çš„SQLè¯­æ³•ï¼ŒHiveä½¿ç”¨ç±»SQLè¯­æ³•HQLå®˜æ–¹æ¡ˆä¾‹ï¼šMigrating From Hive -- 1.Prestoä½¿ç”¨ä¸‹æ ‡å–æ•°ç»„å…ƒç´  ä¸‹æ ‡ä»1å¼€å§‹ select id, arr[1] as arr2, arr[3] as arr3 from (SELECT * FROM ( VALUES (1, &#39;a&#39;, ARRAY[1, 2, 3]), (2, &#39;b&#39;, ARRAY[4, 5, 6]), (3, &#39;c&#39;, ARRAY[7, 8, 9]) ) AS t (id, name, arr)) a; -- 2.ä¸æ”¯æŒéšå¼æ•°æ®ç±»å‹è½¬æ¢ï¼Œéœ€è¦æ‰‹åŠ¨è½¬æ¢ SELECT CAST(x AS varchar) , CAST(x AS bigint) , CAST(x AS double) , CAST(x AS boolean) FROM ... SELECT CAST(5 AS DOUBLE) / 2;SELECT 5 / 2; -- 3.WITH ASè¯­æ³• WITH a AS (SELECT uploader,videos FROM tb_user_info LIMIT 10) select * from a; -- 4.UNNESTå…³é”®å­—ä»£æ›¿LATERAL VIEW explode()è¿›è¡Œè¡Œè½¬åˆ— Hiveå†™æ³•: SELECT student, score FROM tests LATERAL VIEW explode(scores) t AS score; Prestoå†™æ³•: SELECT student, score FROM tests CROSS JOIN UNNEST(scores) AS t (score); -- 5.Hiveè§†å›¾ä¸æ”¯æŒé€šè¿‡PrestoæŸ¥è¯¢ï¼Œæ‰€ä»¥è¦åœ¨Prestoåˆ›å»ºåŒåè§†å›¾ï¼ˆå³åœ¨prestoè¯»å–è§†å›¾å®šä¹‰(StatementAnalyzer.java)çš„æ—¶å€™ï¼Œè§£æåŸå§‹çš„sqlå®šä¹‰çš„è¯­å¥ï¼Œè½¬æ¢æˆprestoçš„è§†å›¾ç»“æ„ï¼‰ -- 6.cast as stringä¸æ”¯æŒï¼Œå› ä¸ºPrestoçš„æ˜¯Varcharï¼Œéœ€è¦åœ¨ASTBuilder.javaä¸­æŠŠstringæ›¿æ¢ä¸ºäº†varcharç±»å‹ -- 7.select 1 = &#39;1&#39;;åœ¨Hiveå’ŒPrestoè®¡ç®—ç»“æœåˆ†åˆ«ä¸ºtrue,cannot be applied to integer, varchar(1) éœ€è¦é¢å¤–æ“ä½œå®ç°é€æ˜çš„éšå¼è½¬æ¢ -- 8.UDFæ”¯æŒã€nullå€¼å¤„ç† -- 9.å¯¹äºtimestampç±»å‹å­—æ®µåšwhereæ¡ä»¶æ¯”è¾ƒï¼Œhiveå¯ä»¥ç›´æ¥æ¯”è¾ƒï¼Œprestoéœ€è¦åŠ timestampå…³é”®å­— /*Hiveçš„å†™æ³•*/ SELECT t FROM a WHERE t &gt; &#39;2021-01-01 00:00:00&#39;; /*Prestoä¸­çš„å†™æ³•*/ SELECT t FROM a WHERE t &gt; timestamp &#39;2021-01-01 00:00:00&#39;; -- 10.Prestoçš„MD5ä¼ å…¥binaryç±»å‹åˆ™ä¼šè¿”å›binaryç±»å‹ï¼Œæ‰€ä»¥å¯¹å­—ç¬¦ä¸²çš„MD5éœ€è¦è½¬æ¢ï¼š SELECT to_hex(md5(to_utf8(&#39;abcd&#39;))); -- 11.Prestoä¸æ”¯æŒINSERT OVERWRITEï¼Œåªèƒ½å…ˆDELETEå†INSERT Hiveæ•°ä»“çš„æ•°æ®å®‰å…¨æ€§å’Œæƒé™å‚è€ƒBuilt-in System Access Controlåœ¨æˆ‘çœ‹æ¥hive.security=fileå½¢å¼çš„æˆæƒæ¯”è¾ƒçµæ´»å…ˆé…ç½®å…¨å±€çš„Catalogè®¿é—®æƒé™ï¼šuser:å¯é€‰å‚æ•°ï¼Œæ­£åˆ™åŒ¹é…ç”¨æˆ·åï¼Œé»˜è®¤.*catalog:å¯é€‰å‚æ•°ï¼Œæ­£åˆ™åŒ¹é…Catalogåï¼Œé»˜è®¤.*.allow:å¿…é€‰å‚æ•°ï¼Œç”¨æˆ·æ˜¯å¦å¯¹calalogæœ‰è®¿é—®æƒé™true\\false # å¯ç”¨åŸºäºæ–‡ä»¶çš„æƒé™æ§åˆ¶ vim /opt/modules/presto-server-0.248/etc/access-control.properties access-control.name=file security.config-file=/opt/modules/presto-server-0.248/etc/rules.json security.refresh-period=10s # é…ç½®æƒé™è‡ªåŠ¨åˆ·æ–°æ—¶é—´é—´éš” 10s # è®¾ç½®æƒé™æ§åˆ¶è§„åˆ™ï¼šå…è®¸åªadminç”¨æˆ·æœ‰mysql catalogçš„æƒé™ï¼Œæ‰€æœ‰ç”¨æˆ·æœ‰hive catalogæƒé™ï¼Œæ‰€æœ‰ç”¨æˆ·æ— system catalogæƒé™ vim /opt/modules/presto-server-0.248/etc/rules.json &#123; &quot;catalogs&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;catalog&quot;: &quot;(mysql|system)&quot;, &quot;allow&quot;: true &#125;, &#123; &quot;catalog&quot;: &quot;hive&quot;, &quot;allow&quot;: true &#125;, &#123; &quot;catalog&quot;: &quot;system&quot;, &quot;allow&quot;: false &#125; ] &#125; åˆ†å‘access-control.propertieså’Œrules.jsonï¼Œé‡å¯PrestoServerç”Ÿæ•ˆ è¿æ¥PrestoClientå¹¶æŒ‡å®šç”¨æˆ·presto --server cdh101:8080 --catalog hive --schema default --user qjj é…ç½®hiveæ•°æ®æºçš„æƒé™ï¼Œå‚è€ƒHive Security Configurationæœ‰legacy,read-only,file,sql-standardå››ç§å½¢å¼ï¼Œä»ç„¶æ˜¯fileçš„æˆæƒå½¢å¼æ¯”è¾ƒçµæ´» vim etc/catalog/hive.properties hive.security=file security.config-file=/opt/modules/presto-server-0.248/etc/catalog/hive-security.json vim /opt/modules/presto-server-0.248/etc/catalog/hive-security.json &#123; &quot;schemas&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;schema&quot;: &quot;.*&quot;, &quot;owner&quot;: true &#125;, &#123; &quot;user&quot;: &quot;staging&quot;, &quot;owner&quot;: false &#125;, &#123; &quot;user&quot;: &quot;test&quot;, &quot;schema&quot;: &quot;test&quot;, &quot;owner&quot;: false &#125; ], &quot;tables&quot;: [ &#123; &quot;user&quot;: &quot;admin&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;, &quot;OWNERSHIP&quot;] &#125;, &#123; &quot;user&quot;: &quot;staging&quot;, &quot;table&quot;: &quot;(staging_db_users|staging_db_videos).*&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;] &#125;, &#123; &quot;user&quot;: &quot;test&quot;, &quot;table&quot;: &quot;test.*&quot;, &quot;privileges&quot;: [&quot;SELECT&quot;] &#125; ] &#125; åˆ†å‘catalog/hive.propertiesã€catalog/hive-security.json é‡å¯Presto-server æ€»ç»“ Hive\\Sparkçš„SQLä»»åŠ¡è¿ç§»åˆ°Prestoåœ¨è¯­æ³•ã€è®¡ç®—ç»“æœã€è§†å›¾ä½¿ç”¨ã€ç±»å‹è½¬æ¢ã€UDFåŠç©ºå€¼å¤„ç†ä¸Šæœ‰å·®å¼‚ Hive\\Sparkä»»åŠ¡è¿ç§»Prestoï¼Œå¦‚æœè¦åšåˆ°å¯¹ä¸šåŠ¡é€æ˜ï¼Œè¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ° å‚è€ƒèµ„æ–™Presto DocumentationIceberg Hive Docæ·±å…¥ç†è§£PrestoPrestoå®ç°åŸç†å’Œç¾å›¢çš„ä½¿ç”¨å®è·µHiveè¿ç§»Prestoåœ¨OPPOçš„å®è·µé›¶åŸºç¡€ç†Ÿæ‚‰ Prestoçš„æ¦‚å¿µã€å®‰è£…ã€ä½¿ç”¨åŠä¼˜åŒ–","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"äº¤äº’å¼åˆ†æ","slug":"äº¤äº’å¼åˆ†æ","permalink":"https://shmily-qjj.top/tags/%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%88%86%E6%9E%90/"},{"name":"SQLå¼•æ“","slug":"SQLå¼•æ“","permalink":"https://shmily-qjj.top/tags/SQL%E5%BC%95%E6%93%8E/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"å¤§æ•°æ®å¹³å°å¸¸è§å¼‚å¸¸å¤„ç†æ±‡æ€»","slug":"å¤§æ•°æ®å¹³å°å¸¸è§å¼‚å¸¸å¤„ç†æ±‡æ€»","date":"2021-01-30T04:50:00.000Z","updated":"2023-08-02T14:51:42.958Z","comments":true,"path":"BigdataExceptionsSummary/","link":"","permalink":"https://shmily-qjj.top/BigdataExceptionsSummary/","excerpt":"","text":"å¤§æ•°æ®å¹³å°å¸¸è§å¼‚å¸¸å¤„ç†æ±‡æ€»æœ¬åšå®¢è®°å½•å·¥ä½œä¸­é‡åˆ°çš„ï¼Œå¤§æ•°æ®ç›¸å…³å„ä¸ªç»„ä»¶çš„å¼‚å¸¸å¤„ç†è¿‡ç¨‹ï¼Œå…»æˆè‰¯å¥½çš„é—®é¢˜å½’çº³æ€»ç»“ä¹ æƒ¯ï¼Œç´¯ç§¯é—®é¢˜è§£å†³ç»éªŒä¸æ€è·¯ã€‚ Sparkç›¸å…³ Shuffleå¼‚å¸¸å¯¼è‡´ä»»åŠ¡å¤±è´¥æŠ¥é”™ï¼šorg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1åŸå› ï¼šshuffleåˆ†ä¸ºshuffle writeå’Œshuffle readä¸¤éƒ¨åˆ†ã€‚shuffle writeçš„åˆ†åŒºæ•°ç”±ä¸Šä¸€é˜¶æ®µçš„RDDåˆ†åŒºæ•°æ§åˆ¶ï¼Œshuffle readçš„åˆ†åŒºæ•°åˆ™æ˜¯ç”±Sparkæä¾›çš„ä¸€äº›å‚æ•°æ§åˆ¶ã€‚shuffle writeå¯ä»¥ç®€å•ç†è§£ä¸ºç±»ä¼¼äºsaveAsLocalDiskFileçš„æ“ä½œï¼Œå°†è®¡ç®—çš„ä¸­é—´ç»“æœæŒ‰æŸç§è§„åˆ™ä¸´æ—¶æ”¾åˆ°å„ä¸ªexecutoræ‰€åœ¨çš„æœ¬åœ°ç£ç›˜ä¸Šã€‚shuffle readçš„æ—¶å€™æ•°æ®çš„åˆ†åŒºæ•°åˆ™æ˜¯ç”±sparkæä¾›çš„ä¸€äº›å‚æ•°æ§åˆ¶ã€‚å¯ä»¥æƒ³åˆ°çš„æ˜¯ï¼Œå¦‚æœè¿™ä¸ªå‚æ•°å€¼è®¾ç½®çš„å¾ˆå°ï¼ŒåŒæ—¶shuffle readçš„é‡å¾ˆå¤§ï¼Œé‚£ä¹ˆå°†ä¼šå¯¼è‡´ä¸€ä¸ªtaskéœ€è¦å¤„ç†çš„æ•°æ®éå¸¸å¤§ã€‚ç»“æœå¯¼è‡´JVM crashï¼Œä»è€Œå¯¼è‡´å–shuffleæ•°æ®å¤±è´¥ï¼ŒåŒæ—¶executorä¹Ÿä¸¢å¤±äº†ï¼Œçœ‹åˆ°Failed to connect to hostçš„é”™è¯¯ï¼Œä¹Ÿå°±æ˜¯executor lostçš„æ„æ€ã€‚æœ‰æ—¶å€™å³ä½¿ä¸ä¼šå¯¼è‡´JVM crashä¹Ÿä¼šé€ æˆé•¿æ—¶é—´çš„gcã€‚è§£å†³æ€è·¯ï¼šå‡å°‘shuffleçš„æ•°æ®é‡å’Œå¢åŠ å¤„ç†shuffleæ•°æ®çš„åˆ†åŒºæ•°â‘ spark.sql.shuffle.partitionsæ§åˆ¶åˆ†åŒºæ•°ï¼Œé»˜è®¤ä¸º200ï¼Œæ ¹æ®shuffleçš„é‡ä»¥åŠè®¡ç®—çš„å¤æ‚åº¦æé«˜è¿™ä¸ªå€¼ shuffleå¹¶è¡Œåº¦â‘¡æé«˜spark.executor.memoryâ‘¢map side joinæˆ–æ˜¯broadcast joinæ¥è§„é¿shuffleçš„äº§ç”Ÿâ‘£åˆ†ææ•°æ®å€¾æ–œ è§£å†³æ•°æ®å€¾æ–œâ‘¤å¢åŠ å¤±è´¥çš„é‡è¯•æ¬¡æ•°å’Œé‡è¯•çš„æ—¶é—´é—´éš”é€šè¿‡spark.shuffle.io.maxRetriesæ§åˆ¶é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤æ˜¯3ï¼Œå¯é€‚å½“å¢åŠ ï¼Œä¾‹å¦‚10ã€‚é€šè¿‡spark.shuffle.io.retryWaitæ§åˆ¶é‡è¯•çš„æ—¶é—´é—´éš”ï¼Œé»˜è®¤æ˜¯5sï¼Œå¯é€‚å½“å¢åŠ ï¼Œä¾‹å¦‚10sã€‚â‘¥ç±»ä¼¼RemoteShuffleServiceçš„æœåŠ¡ï¼Œè§£å†³Shuffleå•å°æœºå™¨IOç“¶é¢ˆï¼Œè®°å½•ShuffleçŠ¶æ€ï¼Œå¤§æ‰¹é‡æå‡Shuffleæ•ˆç‡å’Œç¨³å®šæ€§ã€‚ SparkSQLæŠ¥awaitResultå¼‚å¸¸æŠ¥é”™ï¼šorg.apache.spark.SparkException: Exception thrown in awaitResultåŸå› ï¼šå¹¿æ’­æ•°æ®è¶…æ—¶è§£å†³ï¼šspark.sql.broadcastTimeout=1200 é»˜è®¤å¤§å°300 HiveOnSparkä¸èƒ½åˆ›å»ºSparkClientåŠReturn Code 1å¼‚å¸¸æŠ¥é”™ï¼šFAILEDï¼šSemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTaskåŸå› ï¼šä»¥ä¸ŠæŠ¥é”™è¯æ˜åˆå§‹åŒ–Sparkå¤±è´¥ï¼Œè€Œä»¥å‰ä¸ä¼šå¤±è´¥ï¼Œæ‰€ä»¥å¤§æ¦‚ç‡æ˜¯èµ„æºé—®é¢˜è€Œä¸æ˜¯ä»£ç é—®é¢˜ï¼ŒæŸ¥çœ‹Yarné˜Ÿåˆ—å‘ç°æ‰€æäº¤çš„é˜Ÿåˆ—å·²æ»¡ä¸”å·²è¶…è¿‡èƒ½ç”³è¯·èµ„æºçš„ä¸Šé™ï¼ˆè™šçº¿éƒ¨åˆ†ï¼‰ï¼Œæ•…ä»»åŠ¡å¯åŠ¨å¤±è´¥è§£å†³ï¼šCMç•Œé¢-&gt;ç¾¤é›†-&gt;åŠ¨æ€èµ„æºæ± é…ç½®-&gt;æé«˜é˜Ÿåˆ—çš„èµ„æºæƒé‡ï¼ˆä¸Šé™ä¹Ÿä¼šå“åº”æé«˜ï¼‰-&gt;åˆ·æ–°åŠ¨æ€èµ„æºæ± é…ç½® ExecutorLostã€Task LostæŠ¥é”™ï¼š1.[executor lost] WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, aa.local): ExecutorLostFailure (executor lost)2.[task lost] WARN TaskSetManager: Lost task 69.2 in stage 7.0 (TID 1145, 192.168.47.217): java.io.IOException: Connection from /xx.xxx.xx.xxx:xxxxx closed3.[timeout] java.util.concurrent.TimeoutException: Futures timed out after [120 secondERROR TransportChannelHandler: Connection to /xxx.xxx.xx.xxx:xxxxx has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrongåŸå› ï¼šèŠ‚ç‚¹èµ„æºä¸è¶³ã€ç½‘ç»œå»¶è¿Ÿæ³¢åŠ¨ã€GCå¯¼è‡´Executorè¿è¡Œæ…¢ç­‰åŸå› è§£å†³ï¼šâ‘ spark.network.timeoutçš„å€¼ï¼ˆé»˜è®¤ä¸º120s,é…ç½®æ‰€æœ‰ç½‘ç»œä¼ è¾“çš„å»¶æ—¶ï¼‰ï¼Œæ ¹æ®æƒ…å†µæ”¹æˆ300(5min)æˆ–æ›´é«˜ â‘¡åˆ†åˆ«å¢åŠ å„ç±»è¶…æ—¶å‚æ•°spark.core.connection.ack.wait.timeoutspark.akka.timeoutspark.storage.blockManagerSlaveTimeoutMsspark.shuffle.io.connectionTimeoutspark.rpc.askTimeout or spark.rpc.lookupTimeout SparkThriftServeræ— æ³•é“¾æ¥jdbc,åå°æŠ¥é”™Task has been rejected by ExecutorServiceæŠ¥é”™ï¼š2021-08-01 02:26:32.028 WARN [Thread-43] [10.139.53.62] org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:185) :- Task has been rejected by ExecutorService 9 times till timedout, reason: java.util.concurrent.RejectedExecutionException: Task org.apache.thrift.server.TThreadPoolServer$WorkerProcess@6b4f8abf rejected from java.util.concurrent.ThreadPoolExecutor@48ca75d3[Running, pool size = 500, active threads = 500, queued tasks = 0, completed tasks = 917]åŸå› ï¼šæ¯æ¬¡è¿æ¥éƒ½æ˜¯ä¸€ä¸ªsocketè¿æ¥ï¼Œéƒ½ä¼šæäº¤ä¸€ä¸ªRunnableå¯¹è±¡åˆ°ExecutorServiceçº¿ç¨‹æ± ï¼Œçº¿ç¨‹æ± é»˜è®¤æœ€å¤§500,è¿æ¥ä¸ä½¿ç”¨ä¸”æœªå…³é—­å°±ä¼šå ç”¨ä¸€ä¸ªçº¿ç¨‹ï¼Œå æ»¡å°±æ— æ³•å†è¿æ¥è§£å†³ï¼šhive-site.xmlè°ƒæ•´ï¼šhive.server2.session.check.interval 6h-&gt;1hhive.server2.idle.session.timeout 7d-&gt;1dhive.server2.thrift.max.worker.threads (500-&gt;800) (æ ¹æ®ç”¨æˆ·é‡å¤§æ¦‚ä¼°ï¼Œä¸€ä¸ªç”¨æˆ·å¯èƒ½å¤šä¸ªSocket/JDBCè¿æ¥)ç›®çš„ï¼šé¿å…å› socketè¿æ¥å¯¹è±¡çº¿ç¨‹æ± è¢«å æ»¡å¯¼è‡´æ— æ³•è¿æ¥jdbc &lt;property&gt; &lt;name&gt;hive.server2.session.check.interval&lt;/name&gt; &lt;value&gt;1h&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.idle.session.timeout&lt;/name&gt; &lt;value&gt;1d&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt; &lt;value&gt;800&lt;/value&gt; &lt;/property&gt; æ³¨æ„ï¼šhive.server2.session.check.interval &lt; hive.server2.idle.operation.timeout &lt; hive.server2.idle.session.timeout Spark SQLè§£æé”™è¯¯unresolved object, tree: ArrayBuffer(a).* Error executing query, currentState RUNNING, org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: ArrayBuffer(a).* at org.apache.spark.sql.catalyst.analysis.Star.toAttribute(unresolved.scala:245) at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicLogicalOperators.scala:52) at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicLogicalOperators.scala:52) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.immutable.List.map(List.scala:296) ...... SQL: select xxx from table_a a LEFT JOIN table_b b ON a.cust_no = b.cust_no LEFT JOIN table_e e ON a.cust_no = e.cust_no LEFT JOIN table_f f ON a.cust_no = f.cust_no LEFT JOIN table_g g ON a.cust_no = g.cust_no ... ... LEFT JOIN table_n n ON a.cust_no = n.cust_no; åŸå› ï¼šSpark2.4ç‰ˆæœ¬Catalyseæ¨¡å—çš„ä¸€ä¸ªBugï¼Œtable_b ~ table_n ä¸­ä½†å‡¡æœ‰ä¸€å¼ è¡¨ä¸å­˜åœ¨éƒ½ä¼šæŠ›è¯¥å¼‚å¸¸ï¼Œè€ŒéNoSuchTableExceptionã€‚åº•å±‚è¡¨ä¸å­˜åœ¨ï¼Œå°±æ— æ³•å°†â€œunresolved objectâ€è½¬æ¢ä¸ºâ€œresolved objectâ€ï¼Œäºæ˜¯æŠ¥äº†è¯¥é”™è¯¯ã€‚è§£å†³ï¼šç¡®ä¿table_b~table_nä¸­æ¯ä¸ªè¡¨éƒ½å­˜åœ¨å³å¯è§£å†³ã€‚ SparkSQLå»ºè¡¨æŠ¥è·¯å¾„å·²å­˜åœ¨é—®é¢˜å‘ç”Ÿåœ¨spark2.3è¿ç§»åˆ°spark2.4åŠä»¥ä¸Šç‰ˆæœ¬org.apache.spark.sql.AnalysisException: Can not create the managed table xxxx. The associated location hdfs://ns1/user/hive/warehouse/xxxx already exists åŸå› :Spark2.3è¿ç§»åˆ°Spark2.4å,Spark2.4é»˜è®¤ä¸æ”¯æŒåœ¨éç©ºæˆ–è€…æ˜¯å·²å­˜åœ¨çš„HDFSè·¯å¾„ä¸Šåˆ›å»ºå†…éƒ¨è¡¨è§£å†³:è®¾ç½®spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation=trueå‚æ•°æˆ–ä»£ç é‡Œè®¾ç½®å‚æ•°spark.conf.set(â€œspark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocationâ€,â€trueâ€)å½±å“ç‰ˆæœ¬: 2.4.0 &lt;= spark.version &lt; 3.0.0å‚è€ƒ:https://spark.apache.org/docs/2.4.0/sql-migration-guide-upgrade.htmlå‚è€ƒ:https://stackoverflow.com/questions/63837289/apache-spark-sql-table-overwrite-issue HDFSç›¸å…³ æ•°æ®å—ä¸¢å¤±ä¸”å‘½ä»¤æ— æ³•ä¿®å¤èµ·å› ï¼šå¤šå¼ è¡¨æŸ¥è¯¢å‘ç°å¦‚ä¸‹æŠ¥é”™ï¼Œæç¤ºå—ä¸¢å¤±åˆ†æï¼šCMç•Œé¢çœ‹HDFSä¸¢å¤±å—ï¼Œå‘ç°æœ‰2500å¤šï¼Œå¤§æ‰¹é‡å—ä¸¢å¤±å¯èƒ½çš„åŸå› ï¼š 1.DataNodeä¸NameNodeæœªé€šä¿¡ï¼ŒDataNodeè¿›ç¨‹æœªå¯åŠ¨ 2.DataNodeæ•°æ®ç£ç›˜æŸåï¼Œæ•°æ®ä¸¢å¤± 3.ä¸€ä¸ªæ–‡ä»¶çš„å…¨éƒ¨å‰¯æœ¬ä¸¢å¤±è§£å†³è¿‡ç¨‹ï¼šå°è¯•ä¿®å¤ä¸¢å¤±å—ï¼šhdfs debug recoverLease -path -retries æ˜¾ç¤ºä¿®å¤æˆåŠŸï¼Œä½†ä½¿ç”¨hadoop fs -text è¿˜æ˜¯æŠ¥MissingBlockæ— æ³•è¯»å–ä½¿ç”¨fsckæ£€æµ‹åå— hdfs fsck /user/hive/warehouseå‘ç°ç»å¤§å¤šæ•°blockåç§°éƒ½å¸¦æœ‰172.xxx.xxx.11 å®šä½åˆ°å¯èƒ½æ˜¯172.xxx.xxx.11èŠ‚ç‚¹çš„DataNodeå¯èƒ½å­˜åœ¨é—®é¢˜é€šè¿‡CMæ—¥å¿—å’Œæœºå™¨ä¸Šè¿›ç¨‹çŠ¶æ€åˆ¤æ–­172.xxx.xxx.11çš„DataNodeå·²ä¸NameNodeä¿æŒå¿ƒè·³ï¼Œè¿è¡Œæ­£å¸¸ï¼Œè¿›è€Œæ€€ç–‘ç£ç›˜åäº†ï¼ˆæ¦‚ç‡å¤ªå°ï¼‰æŸ¥çœ‹CMé…ç½®å’Œæœºå™¨ç£ç›˜ï¼Œå‘ç°å°‘é…ç½®äº†äº›ç¡¬ç›˜è·¯å¾„ï¼ŒåŸå› æ˜¯åœ¨é…ç½®æ–°èŠ‚ç‚¹ç£ç›˜è·¯å¾„æ—¶è¯¯ä¿®æ”¹æ•´ä¸ªé…ç½®ç»„çš„ç£ç›˜è·¯å¾„ï¼Œå¯¼è‡´è¯¥é…ç½®ç»„ä¸­æ‰€æœ‰DataNodeç¼ºå°‘ç£ç›˜ï¼Œè¿›è€Œå‡ºç°å—ä¸¢å¤±ä¸”æ— æ³•ä¿®å¤çš„é—®é¢˜ã€‚è§£å†³ï¼šè¿˜åŸç£ç›˜é…ç½®ï¼Œæ»šåŠ¨é‡å¯è¯¥é…ç½®ç»„ä¸­çš„DataNodeï¼Œå°†ä¸åŒæœºå™¨é…ç½®åˆ†æˆå¤šä¸ªé…ç½®ç»„ï¼Œé‡æ–°ä¿®æ”¹é…ç½®é‡å¯è¿‡ç¨‹ä¸­ä¸¢å¤±å—æ•°ä¸€ç›´åœ¨å‡å°‘ï¼šæœ€ç»ˆæ¢å¤æ­£å¸¸æ€»ç»“ï¼š1.CMä¸Šä¿®æ”¹é…ç½®ä¸€å®šè¦æ…é‡ï¼Œæ³¨æ„ä¿®æ”¹é…ç½®ç»„ä¸­æŸå°èŠ‚ç‚¹çš„é…ç½®ä¼šå½±å“æ•´ä¸ªé…ç½®ç»„ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„é…ç½®2.CMæ˜¾ç¤ºDataNodeé‡å¯æˆåŠŸåªæ˜¯è¿›ç¨‹å¯åŠ¨æˆåŠŸï¼Œä½†æ—¥å¿—å‡ºç°â€œTotal time to add all replicas to mapâ€å­—çœ¼æ‰æ˜¯çœŸæ­£å®Œæˆå¯åŠ¨3.https://hdfs-site/dfshealth.html#tab-overview ä»HDFS WebUIè·å–æ›´å¤šä¿¡æ¯ï¼ˆä¸¢å¤±å—çš„ä¿¡æ¯ä¸€ç›®äº†ç„¶ï¼‰ Winå¼€å‘Hadoopç¯å¢ƒwinutilsé”™è¯¯ï¼šCould not locate executable null\\bin\\winutils.exe in the Hadoop binariesè§£å†³ï¼šå°†winutils.exeæ”¾åœ¨HADOOP_HOME\\binä¸‹ï¼Œç„¶åä»£ç é‡ŒSystem.setProperty(â€œhadoop.home.dirâ€, â€œD:\\Programming\\Env\\Hadoop\\hadoop-2.7.2\\â€œ)æˆ–è®¾ç½®ç¯å¢ƒå˜é‡HADOOP_HOMEå’ŒPATHåé‡å¯ç”µè„‘winutils.exeä¸‹è½½åœ°å€ï¼šwinutils-master Namenode is not formattedé”™è¯¯Namenode is not formattedåˆ†æï¼šä¸€èˆ¬æ–°éƒ¨ç½²çš„HDFSæ‰ä¼šæç¤ºéœ€è¦æ ¼å¼åŒ–(hadoop namenode -format)ï¼Œè€Œç”Ÿäº§ç¯å¢ƒNamenodeä¸€æ—¦æœ‰å¼‚å¸¸ï¼Œä¹Ÿå¯èƒ½å‡ºç°è¿™æ ·çš„é—®é¢˜ï¼Œä½†æˆ‘ä¸èƒ½æ ¼å¼åŒ–ç”Ÿäº§ç¯å¢ƒå•Šï¼NNæ—¥å¿—å¦‚ä¸‹ï¼šå¯çŸ¥è¯†ç”±äºåŠ è½½fsimageå¼‚å¸¸ï¼Œå»NameNodeçš„å­˜å‚¨è·¯å¾„/dfs/nn/å‘ç°æ²¡æœ‰æ–‡ä»¶ï¼Œè€Œå…¶ä»–èŠ‚ç‚¹/dfs/nnç›®å½•ä¸‹æœ‰currentç›®å½•ï¼Œcurrentç›®å½•ä¸‹æœ‰fsimageã€‚æ‰€ä»¥å¯ä»¥åˆ¤å®šè¯¥Namenodeæ— æ³•å¯åŠ¨å°±æ˜¯å› ä¸ºfsimageä¸¢å¤±ã€‚è§£å†³ï¼šscpè¿™ä¸ªcurrentç›®å½•ï¼Œæƒé™ä¸ä¹‹å‰çš„ä¸€è‡´ï¼ˆä¸€èˆ¬ä¸ºhdfs:hdfsæƒé™ï¼‰ ä¿®å¤HDFSåå—æŸ¥çœ‹åå—å¯é€šè¿‡ hdfs fsck / hadoop dfsadmin -report ä¿®å¤åå—é™¤äº†ä¸Šé¢è®²è¿‡çš„hdfs debug recoverLease -path -retries å‘½ä»¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡hadoop fs -setrep -R 3 / å‘½ä»¤ï¼Œè®¾ç½®å‰¯æœ¬ï¼Œå‰¯æœ¬å°äº3çš„æƒ…å†µä¼šè‡ªåŠ¨æ¢å¤ä¸‰å‰¯æœ¬ï¼Œä¿®å¤åè¿˜æœ‰åå—é‚£å°±æ˜¯ä¸€ä¸ªå‰¯æœ¬éƒ½æ²¡æœ‰äº†ï¼Œåªèƒ½ä¸¢äº†ã€‚ ä¸¤ä¸ªNNå‡ä¸ºStandbyï¼Œé‡å¯åè¿‡ä¸€ä¼šåˆéƒ½StandbyNNå‡æŠ¥é”™ï¼šjava.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!Caused by: java.io.IOException: Cannot find any valid remote NN to service request!Exception from remote name node RemoteNameNodeInfo [nnId=namenode37, ipcAddress=xxxx, httpAddress=xxxx], try next.org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-errorè§£å†³ï¼šå¼ºåˆ¶åˆ‡æ¢ä¸»å¤‡sudo -uhdfs hdfs haadmin -transitionToActive â€“forcemanual namenode37ï¼ˆnnIdåœ¨hdfs-siteæœ‰é…ï¼‰ åˆ‡æ¢åé‡å¯Failover Controlleræˆ–ZKFCã€‚ åŸºäºSentryè®¤è¯çš„HDFSé›†ç¾¤NNå¯åŠ¨å¼‚å¸¸NNè¿›ç¨‹å¯åŠ¨æˆåŠŸä½†æ˜¯ä¸€ç›´å¤„äºå¿™ç¢ŒçŠ¶æ€ï¼ŒæŸ¥çœ‹NNçš„æ—¥å¿— æ™šä¸Š9ç‚¹53:24.664åˆ† INFO SentryAuthorizationInfo Refresh interval [500]ms, retry wait [30000] æ™šä¸Š9ç‚¹53:24.664åˆ† INFO SentryAuthorizationInfo stale threshold [60000]ms æ™šä¸Š9ç‚¹53:24.669åˆ† INFO HMSPaths HMSPaths:[/user/hive/warehouse, /user/hive/xxxx] Initialized æ™šä¸Š9ç‚¹53:24.681åˆ† INFO SentryTransportPool Creating pool for xxx:8038,xxx:8038 with default port 8038 æ™šä¸Š9ç‚¹53:24.683åˆ† INFO SentryTransportPool Adding endpoint xxx:8038 æ™šä¸Š9ç‚¹53:24.683åˆ† INFO SentryTransportPool Adding endpoint xxx:8038 æ™šä¸Š9ç‚¹53:24.683åˆ† INFO SentryTransportPool Connection pooling is disabled æ—¥å¿—å¡åœ¨è¿™ä¸ç»§ç»­äº†ï¼Œå¡åœ¨è¿™ ä¸æ­¤åŒæ—¶ä¸¤ä¸ªSentryServeræœ‰ä¸€å°å‘ç”ŸOOMï¼Œä¸ç¨³å®šåŸå› ï¼šç”±äºåŸºäºSentryè®¤è¯ï¼ŒHDFSçš„NNåœ¨å¯åŠ¨æ—¶ï¼ŒSentryä¼šå…¨é‡æ”¶é›†HMSPathsä¸‹çš„ACLä¿¡æ¯ï¼Œä¼šé©»ç•™åœ¨Sentryå†…å­˜ä¸­ï¼Œå¦‚æœSentryå†…å­˜ä¸è¶³å®¹æ˜“OOMï¼Œéœ€è¦é€šè¿‡å¢åŠ å †å†…å­˜çš„æ–¹å¼è§£å†³ã€‚ä¸¤ä¸ªSentryServerè™½ç„¶å¯ä»¥è´Ÿè½½å‡è¡¡é«˜å¯ç”¨ï¼Œå½“ä¸€ä¸ªSentryServerå®•æ‰è¯·æ±‚æ‰ä¼šè½¬ç§»åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè€ŒOOMæ—¶ï¼Œè¯·æ±‚ä¼šä¸€ç›´å¡ä½ï¼Œä¸ä¼šè‡ªåŠ¨è½¬ç§»ï¼Œå¯¼è‡´NNå¯åŠ¨å¼‚å¸¸ã€‚è§£å†³ï¼šå¢åŠ Sentryå †å†…å­˜ï¼Œä¸ç›²ç›®åŠ ï¼Œæ ¹æ®å½“å‰HiveæœåŠ¡å™¨æ•°ã€åº“æ•°ã€è¡¨æ•°ã€åˆ†åŒºæ•°ã€åˆ—æ•°åŠè§†å›¾æ•°è¿›è¡Œè¯„ä¼°ï¼Œå…·ä½“è¯„ä¼°æ ‡å‡†å¦‚ä¸‹ï¼šå¦‚å›¾æ¨ç®—ï¼šæ¯ç™¾ä¸‡ä¸ªHiveå¯¹è±¡ï¼ˆåº“ã€è¡¨ã€åˆ†åŒºæ•°ï¼‰éœ€è¦é…ç½®2.25Gçš„Sentryæœ€å¤§å †å†…å­˜ distcpæ•°æ®æ‹·è´æŠ¥é”™distcpæŠ¥é”™file might have been written to during copy, consider enabling HDFS Snapshots to avoid this error.hdfs-site.xmlé‡Œå¢åŠ dfs.namenode.snapshot.capture.openfiles å€¼ä¸ºtrue å¼€å¯Immutable Snapshotï¼Œä¿è¯å¿«ç…§ç›®å½•é‡Œæ‰€æœ‰æ–‡ä»¶çš„çŠ¶æ€éƒ½æ˜¯å…³é—­çš„ï¼Œæ–‡ä»¶å¤§å°éƒ½æ˜¯åˆ›å»ºå¿«ç…§æ—¶çš„çŠ¶æ€ï¼Œè§£å†³åŒæ­¥hdfsæ•°æ®æŠ¥é”™æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…æŠ¥ä¸èƒ½å¤åˆ¶ä¸€ä¸ªæ‰“å¼€çš„æˆ–æ­£åœ¨å†™å…¥çš„æ–‡ä»¶è¿™äº›é—®é¢˜ã€‚ NameNodeå¯åŠ¨åæ— æ³•RPCå’Œåˆ‡æ¢çŠ¶æ€å¹¶ä¸€ç›´FullGCï¼Œå‡ ä¹80%ä»¥ä¸Šæ—¶é—´éƒ½åœ¨FullGC 2021-07-14 09:38:35,803 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47892ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48186ms 2021-07-14 09:39:25,780 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47976ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48428ms 2021-07-14 09:40:15,526 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47744ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47840ms 2021-07-14 09:41:04,652 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47126ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47437ms 2021-07-14 09:41:55,106 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 48452ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48767ms 2021-07-14 09:42:48,440 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 51332ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=51719ms 2021-07-14 09:43:40,527 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 50086ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=50450ms 2021-07-14 09:44:34,750 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 52222ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=52462ms 2021-07-14 09:45:30,759 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 53999ms GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=54179ms åŸå› ï¼šåŠ è½½FSImageåˆ°å †å†…å­˜çš„è¿‡ç¨‹ä¸­ç”±äºå †å†…å­˜ä¸è¶³å¯¼è‡´æ— æ³•å¯åŠ¨ã€‚è§£å†³ï¼šâ‘ å¢åŠ NameNodeå †å†…å­˜ï¼Œå¯åŠ¨åæŸ¥çœ‹HDFS WebUI-&gt;Overview-&gt;SummaryæŸ¥çœ‹ä¸»NNå †å†…å­˜ä½¿ç”¨æƒ…å†µå’Œç›®å‰Blocksæ•°é‡ï¼Œå¯¹NNå†…å­˜éœ€æ±‚é‡æ–°åšè¯„ä¼°ï¼Œè°ƒæ•´åˆé€‚çš„å †å¤§å°ã€‚æ ¹æ®Blockæ•°é‡å’Œå¢é‡ç²¾ç»†åŒ–è®¡ç®—NNçš„å †å†…å­˜ï¼Œé¿å…å†æ¬¡OOMã€‚â‘¡å¦‚æœå¯èƒ½ï¼Œå¯ä»¥å‡å°HDFSåƒåœ¾å›æ”¶æ—¶é—´ã€‚â‘¢æŸ¥çœ‹HDFS WebUI-&gt;Snaoshotæ ï¼Œåˆ é™¤æ— ç”¨çš„å¿«ç…§ï¼Œé¿å…å¿«ç…§å˜åŒ–æ–‡ä»¶å ç”¨å¤§é‡ç©ºé—´å’ŒBlocksã€‚ HDFS DataNodeç£ç›˜åˆ‡æ¢åœæ­¢DNã€JN -&gt; å°†/dfsç›®å½•ç§»è‡³ç›®æ ‡ç£ç›˜ï¼Œå¹¶ä¿®æ”¹hdfså¯¹åº”çš„ç£ç›˜ä¸ºç›®æ ‡è·¯å¾„ï¼ˆå¯¹åº”èŠ‚ç‚¹çš„dfs.journalnode.edits.dirå’Œdfs.datanode.data.dirï¼‰ -&gt; å¯åŠ¨DNã€JN HDFSè¿œç¨‹Kerberoså®¢æˆ·ç«¯è¿æ¥éKerberosé›†ç¾¤æŠ¥é”™ï¼šhdfs dfs -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/ ls: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.; Host Details : local host is: &quot;client_host/client_ip&quot;; destination host is: &quot;remote_nn_ip&quot;:8020; åŸå› :æœ¬åœ°å®¢æˆ·ç«¯é‡‡ç”¨Kerberosè®¤è¯ï¼Œä½†è¿œç¨‹Serveræœªå¼€å¯Kerberosè®¤è¯ï¼Œè¿œç¨‹ä¸ºSIMPLEè®¤è¯è§£å†³:è®¾ç½®ipc.client.fallback-to-simple-auth-allowed=trueå‚æ•°å³å¯ å‚æ•°åŠ åœ¨dfsåé¢ å¦‚ä¸‹ hdfs dfs -D ipc.client.fallback-to-simple-auth-allowed=true -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/ HDFSæ— æ³•ä¸Šä¼ æ–‡ä»¶ # hdfs dfs -copyFromLocal xxx /tmp/ 22/05/16 18:10:05 WARN hdfs.DataStreamer: DataStreamer Exception org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/qjj._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 4 datanode(s) running and 4 node(s) are excluded in this operation. at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2102) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2673) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:872) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:550) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:815) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2675) at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499) at org.apache.hadoop.ipc.Client.call(Client.java:1445) at org.apache.hadoop.ipc.Client.call(Client.java:1355) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) at com.sun.proxy.$Proxy9.addBlock(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:497) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) at com.sun.proxy.$Proxy10.addBlock(Unknown Source) at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1085) at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1865) at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716) copyFromLocal: File /tmp/qjj._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 4 datanode(s) running and 4 node(s) are excluded in this operation. # æœåŠ¡ç«¯æ—¥å¿— ä¸Šåˆ10ç‚¹15:18.249åˆ† INFO BlockPlacementPolicy Not enough replicas was chosen. Reason:&#123;NOT_ENOUGH_STORAGE_SPACE=3&#125; ä¸Šåˆ10ç‚¹15:18.249åˆ† WARN BlockPlacementPolicy Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy&#123;HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]&#125;, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology ä¸Šåˆ10ç‚¹15:18.250åˆ† WARN BlockPlacementPolicy Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy&#123;HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]&#125;, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology ä¸Šåˆ10ç‚¹15:18.250åˆ† WARN BlockStoragePolicy Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK, DISK], policy=BlockStoragePolicy&#123;HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]&#125;) ä¸Šåˆ10ç‚¹15:18.250åˆ† WARN BlockPlacementPolicy Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy&#123;HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]&#125;, newBlock=false) All required storage types are unavailable: unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy&#123;HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]&#125; æ‰§è¡Œhdfs dfsadmin -reportç»“æœå‘ç°DFS Used%è¶…è¿‡äº†100%åŸå› ï¼šdfs.datanode.du.reservedå‚æ•°è¢«åŒäº‹ä¿®æ”¹æˆäº†3072GBï¼Œè¿™ä¸ªå‚æ•°æ˜¯æŒ‡è¯¥DataNodeä¸Šå•ä¸ªæ•°æ®ç›˜é¢„ç•™çš„ç©ºé—´ï¼Œå•èŠ‚ç‚¹ä¸Šæœ‰8å—ç›˜ï¼Œæ¯å—å°‘3Tï¼Œæ€»å…±å°‘äº†24Tï¼ŒDNèŠ‚ç‚¹å­˜å‚¨å®¹é‡ä¹Ÿéšä¹‹ä¸‹é™ã€‚ä¿®æ”¹åï¼Œç£ç›˜å‰©ä½™ç©ºé—´ä¸è¶³3072Gï¼Œåˆ™è¯¥DataNodeæ— æ³•å†™å…¥ã€‚è§£å†³ï¼šé™ä½dfs.datanode.du.reservedï¼Œé‡å¯DNèŠ‚ç‚¹ã€‚ HDFSæ— æ³•ä¸Šä¼ æ–‡ä»¶,æŠ¥é”™DataNodeè¢«æ’é™¤ 22/08/15 18:05:53 WARN hdfs.DataStreamer: DataStreamer Exception org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/hive-e.log._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 3 datanode(s) running and 3 node(s) are excluded in this operation. at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892) 22/08/15 19:40:38 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted =false, remoteHostTrusted = false 22/08/15 19:40:38 INFO hdfs.DataStreamer: Exception in createBlockOutputStream blk_1074772338_1031527 ...... java.io.IOException: Invalid token in javax.security.sasl.qop: at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessage(DataTransferSaslUtil.java:220) ä¹‹å‰é‡åˆ°è¿‡å› DataNodeæ— å¯ç”¨ç©ºé—´å¯¼è‡´çš„DataNodeè¢«æ’é™¤è¿›è€Œæ— æ³•å†™å…¥æ•°æ®,ä½†è¿™æ¬¡ä¸åŒ,è¿™æ¬¡å¤šäº†â€not support SASL data transferâ€ 2022-08-15 17:59:47,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /client-ip:42960. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protec tion org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50ed instead of deadbeef from client. at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:372) at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getSaslStreams(SaslDataTransferServer.java:306) at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:133) at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:234) at java.lang.Thread.run(Thread.java:748) 2022-08-15 18:00:20,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /client-ip:43638. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protec tion org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50ed instead of deadbeef from client. åŸå› :As of version 2.6.0, SASL can be used to authenticate the data transfer protocol. In this configuration, it is no longer required for secured clusters to start the DataNode as root using jsvc and bind to privileged ports. To enable SASL on data transfer protocol, set dfs.data.transfer.protection in hdfs-site.xml, set a non-privileged port for dfs.datanode.address, set dfs.http.policy to HTTPS_ONLY and make sure the HADOOP_SECURE_DN_USER environment variable is not defined. Note that it is not possible to use SASL on data transfer protocol if dfs.datanode.address is set to a privileged port. This is required for backwards-compatibility reasons.è§£å†³:hdfs-site.xmlå¢åŠ dfs.data.transfer.protection=integrity Hiveç›¸å…³ HiveMetaStoreçŠ¶æ€ä¸è‰¯å¯¼DDLSQLè€—æ—¶200sä»¥ä¸ŠHMSè¿›ç¨‹æŠ¥é”™ï¼šhive metastore server Failed to sync requested HMS notifications up to the event ID xxxåŸå› åˆ†æï¼šæŸ¥çœ‹sentryå¼‚å¸¸CounterWaitæºç å‘ç°ä¼ é€’çš„idæ¯”currentidå¤§å¯¼è‡´ä¸€ç›´ç­‰å¾…è¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´é»˜è®¤ä¸º200sï¼ˆsentry.notification.sync.timeout.msï¼‰ã€‚å¼€å¯äº†hdfs-sentry aclåŒæ­¥åï¼Œhdfsï¼Œsentryï¼ŒHMSä¸‰è€…é—´æƒé™åŒæ­¥çš„æ¶ˆæ¯å¤„ç†ã€‚å½“çªç„¶å¤§æ‰¹é‡çš„ç›®å½•æƒé™æ¶ˆæ¯éœ€è¦å¤„ç†ï¼Œåå°çº¿ç¨‹å¤„ç†ä¸è¿‡æ¥ï¼Œæ¶ˆæ¯ç§¯å‹æ»åå°±ä¼šå‡ºç°è¿™ä¸ªå¼‚å¸¸ã€‚è¿™ä¸ªå¼‚å¸¸ä¸å½±å“é›†ç¾¤ä½¿ç”¨ï¼Œåªæ˜¯ä¼šå¯¼è‡´createï¼Œdrop tableæ…¢éœ€è¦ç­‰200sï¼Œè¿™æ ·ç­‰å¾…ä¹Ÿæ˜¯ä¸ºäº†è¿½ä¸Šæœ€æ–°çš„idã€‚æˆ‘ä»¬è¿™æ¬¡åŒæ—¶å‡ºç°äº†HMSå‚ä¸åŒæ­¥æ¶ˆæ¯å¤„ç†çš„çº¿ç¨‹è¢«å¼‚å¸¸é€€å‡ºï¼Œå¯¼è‡´sentryçš„sentry_hms_notification_idè¡¨æ•°æ®ä¸€ç›´æ²¡æ›´æ–°ï¼Œéœ€è¦é‡å¯HMSã€‚å¦‚æœç§¯å‹äº†å¤ªå¤šæ¶ˆæ¯ï¼Œè®©å®ƒæ…¢æ…¢æ¶ˆè´¹å¤„ç†éœ€è¦çš„æ—¶é—´å¤ªé•¿ï¼Œå¯èƒ½ä¸€ç›´è¿½ä¸ä¸Šï¼Œè¿™æ—¶å¯ä»¥é€‰æ‹©ä¸¢æ‰è¿™äº›æ¶ˆæ¯ã€‚è§£å†³ï¼š â‘ å¯ä»¥é€šè¿‡è®¾ç½®sentry.notification.sync.timeout.mså‚æ•°è°ƒå°è¶…æ—¶æ—¶é—´ï¼Œå‡å°ç­‰å¾…æ—¶é—´ï¼Œç§¯å‹ä¸å¤šçš„è¯å¯ä»¥è®©å®ƒè‡ªè¡Œæ¶ˆè´¹å¤„ç†æ‰ã€‚ â‘¡ä¸¢æ‰æœªå¤„ç†çš„æ¶ˆæ¯ï¼Œåœ¨sentryçš„sentry_hms_notification_idè¡¨ä¸­æ’å…¥ä¸€æ¡æœ€å¤§å€¼(ç­‰äºå½“å‰æ¶ˆæ¯çš„idï¼Œä»notification_sequenceè¡¨ä¸­è·å–) ï¼Œé‡å¯sentryæœåŠ¡ã€‚ï¼ˆnotification_log è¡¨å­˜å‚¨äº†æ¶ˆæ¯æ—¥å¿—ä¿¡æ¯ï¼‰ HBaseå¤–éƒ¨è¡¨æŠ¥Unexpected end-of-inputèµ·å› ï¼šä½¿ç”¨Hiveåˆ›å»ºHBaseå¤–éƒ¨è¡¨æ—¶æ­£å¸¸ï¼Œä½†ä½¿ç”¨HBaseå¤–éƒ¨è¡¨æ—¶æŠ¥Unexpected end-of-input: was expecting closingåˆ†æè¿‡ç¨‹ï¼šç¿»é˜…æºç éƒ¨åˆ†å‘ç°å¼‚å¸¸æ˜¯åœ¨è§£æå¤–éƒ¨è¡¨åˆ›å»ºJSONæ—¶å‘ç”Ÿï¼Œäºæ˜¯å¯¹æ¯”å»ºè¡¨è¯­å¥å’ŒHiveå…ƒæ•°æ®åº“ä¸­çš„TABLE_PARAMSè¡¨ä¿¡æ¯å¾—åˆ°åŸå› åŸå› ï¼šåˆ›å»ºhbaseå¤–éƒ¨è¡¨catalogå¤ªé•¿å¯¼è‡´schemaå¤ªé•¿ï¼Œè€Œhiveå…ƒæ•°æ®è¡¨mysqlé‡Œçš„table_paramså­—æ®µparam_valueå­—æ®µç±»å‹æ˜¯varchar(4000)å»ºè¡¨æ—¶ç”±äºschemaå¤ªé•¿ï¼Œè¶…è¿‡4000å­—ç¬¦çš„éƒ¨åˆ†è¢«æˆªæ–­ã€‚è€Œä½¿ç”¨è¯¥è¡¨çš„æ—¶å€™ä¼šè¯»å…ƒæ•°æ®ï¼Œä½†å› ä¸ºå…ƒæ•°æ®ä¸å®Œæ•´è€ŒæŠ¥é”™ã€‚è§£å†³ï¼šâ‘ åˆ†å¤šæ¬¡å»ºè¡¨â‘¡æ”¹varchar(4000)ä¸ºlongtextç„¶åé‡å»ºè¡¨ï¼ˆå½±å“æ— æ³•è¯„ä¼°ï¼Œæ²¡å°è¯•ï¼‰ Hiveå’ŒSparkæŸ¥è¯¢Hiveè¡¨æŠ¥java.net.UnknownHostException: nameservice1åˆ†æï¼šç”±äºæœºå™¨HDFS HAé…ç½®å‘ç”Ÿå˜åŠ¨ï¼Œå…³æ‰äº†é«˜å¯ç”¨ï¼Œæ‰€ä»¥nameserviceå˜æˆäº†ip:8020ï¼ŒæŠ¥è¿™ä¸ªé”™ï¼Œé¦–å…ˆæ£€æŸ¥äº†/etc/hadoop/conf/hdfs-site.xmlé‡Œé¢æ— dfs.nameservicesé…ç½®ï¼Œè¯æ˜é…ç½®æ–‡ä»¶æ²¡é—®é¢˜ã€‚ä½¿ç”¨hivecliï¼Œdesc formattedæŸå‡ å¼ è¡¨ï¼Œå‘ç°è¡¨çš„LOCATIONå‡ä¸ºhdfs://nameservice1/xx æ‰¾åˆ°äº†é—®é¢˜çš„åŸå› ï¼Œå…ƒæ•°æ®é”™è¯¯ï¼Œæ‰€ä»¥éœ€è¦å»Hiveå…ƒæ•°æ®åº“æ‰¹é‡ä¿®æ”¹å…ƒæ•°æ®ã€‚è§£å†³ï¼šè¿æ¥åˆ°Hive metastoreå…ƒæ•°æ®åº“ï¼Œæ‰¹é‡ä¿®æ”¹LOCATIONï¼šupdate hive.SDS set LOCATION=concat(â€œhdfs://ip:8020â€,substring(LOCATION,20,length(LOCATION)-19)) where LOCATION like â€œhdfs://nameservice1%â€;update hive.DBS set DB_LOCATION_URI=concat(â€œhdfs://ip:8020â€,substring(DB_LOCATION_URI,20,length(DB_LOCATION_URI)-19)) where DB_LOCATION_URI like â€œhdfs://nameservice1%â€;æ— éœ€é‡å¯æœåŠ¡å³å¯ç”Ÿæ•ˆï¼Œé—®é¢˜è§£å†³ã€‚ SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS] Exception in thread &quot;main&quot; java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS] at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:651) at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591) å¢åŠ å¦‚ä¸‹å‡ ä¸ªå‚æ•°: &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;KERBEROS&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt; &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; Kerberosé›†ç¾¤Hiveå®¢æˆ·ç«¯æ— æ³•è¿æ¥é›†ç¾¤org.apache.thrift.transport.TTransportException: null 2022-07-25T19:24:37,992 WARN [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: set_ugi() not successful, Likely cause: new client talking to old server. Continuing without it. org.apache.thrift.transport.TTransportException: null at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:380) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:230) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_set_ugi(ThriftHiveMetastore.java:4787) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.set_ugi(ThriftHiveMetastore.java:4773) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:534) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:224) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:94) ~[hive-exec-3.1.2.jar:3.1.2] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_252] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:95) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:119) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientFactory.createMetaStoreClient(SessionHiveMetaStoreClientFactory.java:49) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4324) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4390) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4370) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] 2022-07-25T19:24:37,993 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Connected to metastore. 2022-07-25T19:24:37,993 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=xxx@REALM_NAME (auth:KERBEROS) retries=1 delay=1 lifetime=0 2022-07-25T19:24:38,217 WARN [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. getAllFunctions org.apache.thrift.transport.TTransportException: null at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:4357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:4345) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllFunctions(HiveMetaStoreClient.java:2861) ~[hive-exec-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] 2022-07-25T19:24:39,217 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient trying reconnect as xxx@REALM_NAME (auth:KERBEROS) 2022-07-25T19:24:39,221 DEBUG [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Unable to shutdown metastore client. Will try closing transport directly. org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed) at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:161) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.sendBaseOneway(TServiceClient.java:66) ~[hive-exec-3.1.2.jar:3.1.2] at com.facebook.fb303.FacebookService$Client.send_shutdown(FacebookService.java:436) ~[libfb303-0.9.3.jar:?] at com.facebook.fb303.FacebookService$Client.shutdown(FacebookService.java:430) ~[libfb303-0.9.3.jar:?] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:591) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:366) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187) ~[hive-exec-3.1.2.jar:3.1.2] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_252] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_252] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] Caused by: java.net.SocketException: Broken pipe (Write failed) at java.net.SocketOutputStream.socketWrite0(Native Method) ~[?:1.8.0_252] at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[?:1.8.0_252] at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[?:1.8.0_252] at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) ~[?:1.8.0_252] at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) ~[?:1.8.0_252] at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:159) ~[hive-exec-3.1.2.jar:3.1.2] ... 34 more 2022-07-25T19:24:39,221 WARN [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] transport.TIOStreamTransport: Error closing output stream. java.net.SocketException: Socket closed at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118) ~[?:1.8.0_252] at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[?:1.8.0_252] at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) ~[?:1.8.0_252] at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) ~[?:1.8.0_252] at java.io.FilterOutputStream.close(FilterOutputStream.java:158) ~[?:1.8.0_252] at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TSocket.close(TSocket.java:235) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:599) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:366) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187) ~[hive-exec-3.1.2.jar:3.1.2] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_252] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_252] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] 2022-07-25T19:24:39,221 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0 2022-07-25T19:24:39,221 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://xxxxxx:9083 2022-07-25T19:24:39,221 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Opened a connection to metastore, current connections: 1 2022-07-25T19:24:39,230 WARN [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: set_ugi() not successful, Likely cause: new client talking to old server. Continuing without it. org.apache.thrift.transport.TTransportException: null at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:380) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:230) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_set_ugi(ThriftHiveMetastore.java:4787) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.set_ugi(ThriftHiveMetastore.java:4773) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:534) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:379) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient$1.run(RetryingMetaStoreClient.java:187) ~[hive-exec-3.1.2.jar:3.1.2] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_252] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_252] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:183) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] 2022-07-25T19:24:39,231 INFO [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metastore.HiveMetaStoreClient: Connected to metastore. 2022-07-25T19:24:39,245 WARN [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metadata.Hive: Failed to register all functions. org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4629) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] Caused by: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:4357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:4345) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllFunctions(HiveMetaStoreClient.java:2861) ~[hive-exec-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] ... 15 more Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. 2022-07-25T19:24:39,245 ERROR [0824f8ba-8eaf-494d-9ca4-d430d4d0376f main] metadata.HiveMaterializedViewsRegistry: Problem connecting to the metastore when initializing the view registry org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:281) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:437) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:377) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:333) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.init(HiveMaterializedViewsRegistry.java:133) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:755) ~[hive-cli-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.util.RunJar.run(RunJar.java:323) ~[hadoop-common-3.2.1.jar:?] at org.apache.hadoop.util.RunJar.main(RunJar.java:236) ~[hadoop-common-3.2.1.jar:?] Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4629) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] ... 13 more Caused by: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:4357) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:4345) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllFunctions(HiveMetaStoreClient.java:2861) ~[hive-exec-3.1.2.jar:3.1.2] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2] at com.sun.proxy.$Proxy34.getAllFunctions(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4626) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:293) ~[hive-exec-3.1.2.jar:3.1.2] at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:276) ~[hive-exec-3.1.2.jar:3.1.2] ... 13 more çœ‹åˆ°å…³é”®æ—¥å¿—â€œProblem connecting to the metastore when initializing the view registryâ€ï¼Œè¯æ˜Metastoreè¿æ¥å‡ºç°é—®é¢˜ã€‚åŸå› :hive.metastore.sasl.enabledå‚æ•°æœªè®¾ç½®è§£å†³æ–¹å¼ï¼šHiveå®¢æˆ·ç«¯å¢åŠ hive.metastore.sasl.enabledå‚æ•°å¹¶è®¾ç½®ä¸ºtrueï¼ŒåŒæ—¶ç”±äºé›†ç¾¤æ˜¯Kerberosè®¤è¯çš„ï¼Œå¹¶ä¸”saslå¼€å¯åè®¤è¯ç±»å‹åªèƒ½æ˜¯Kerberosï¼Œæ‰€ä»¥ä¸ºä¿è¯èƒ½æ­£å¸¸è¿æ¥Metastoreï¼ŒHiveå®¢æˆ·ç«¯é…ç½®è¿˜éœ€è¦è¿™å‡ ä¸ªå‚æ•° &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;KERBEROS&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt; &lt;!-- &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; --&gt; &lt;!-- &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; --&gt; HiveClientæ­£å¸¸é€€å‡ºæ—¶,æ ‡å‡†è¾“å‡ºå¦‚ä¸‹warn(ä¼šå½±å“hive -eå°†ç»“æœå¯¼å‡ºæ–‡ä»¶çš„æƒ…å†µ) WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked. WARN: Please see http://www.slf4j.org/codes.html#release for an explanation. åŸå› : hiveä¸­å¼•ç”¨äº†HBaseçš„jaråŒ…jcl-over-slf4j-1.7.30.jar ä¼šå¯¼è‡´è¾“å‡ºè¿™ä¸ªWARNè§£å†³:æ–¹æ³•1:hive clientå¯åŠ¨å»æ‰hbase classpathå¼•å…¥: ä¿®æ”¹hiveclientå¯åŠ¨è„šæœ¬ vim $(which hive) ä¿®æ”¹å‚æ•°SKIP_HBASECP=trueæ–¹æ³•2:å¦‚æœå¿…é¡»å¼•å…¥HBASEçš„Classpath,åˆ™ä¿®æ”¹jcl-over-slf4j-1.7.30.jaræºç å»æ‰æ‰“å° Mapç«¯èšåˆæˆ–MapJoinå¯¼è‡´MRå†…å­˜æº¢å‡º 2023-03-28 08:49:01,202 Stage-1 map = 100%, reduce = 95%, Cumulative CPU 24417.19 sec 2023-03-28 08:49:02,228 Stage-1 map = 100%, reduce = 99%, Cumulative CPU 24506.89 sec 2023-03-28 08:49:04,275 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 24532.72 sec MapReduce Total cumulative CPU time: 0 days 6 hours 48 minutes 52 seconds 720 msec Ended Job = job_1676281359678_1134544 SLF4J: Found binding in [jar:file:/opt/apps/ecm/service/hive/3.1.2-hadoop3.1-1.3.1/package/apache-hive-3.1.2-hadoop3.1-1.3.1-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 2023-03-28 08:49:18 Processing rows: 300000 Hashtable size: 299999 Memory usage: 242011504 percentage: 0.253 2023-03-28 08:49:19 Processing rows: 500000 Hashtable size: 499999 Memory usage: 307602344 percentage: 0.322 2023-03-28 08:49:20 Processing rows: 700000 Hashtable size: 699999 Memory usage: 504480144 percentage: 0.528 2023-03-28 08:49:22 Processing rows: 900000 Hashtable size: 899999 Memory usage: 691092576 percentage: 0.724 Execution failed with exit status: 3 Obtaining error information Task failed! Task ID: Stage-12 Logs: FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask åŸå› :å¼€å¯äº†set hive.auto.convert.join=true; å¼€å¯è‡ªåŠ¨mapjoin,å³è¡¨å†™å…¥å†…å­˜,å†…å­˜ä¸è¶³å¯¼è‡´ä»»åŠ¡å¤±è´¥è§£å†³:å…³é—­è‡ªåŠ¨mapjoinè®¾ç½®set hive.auto.convert.join=false; Hiveå¼€å¯è‡ªåŠ¨è·å–è¡¨ä¸Šæ¬¡è®¿é—®æ—¶é—´(lastAccessTime) &lt;property&gt; &lt;name&gt;hive.security.authorization.sqlstd.confwhitelist.append&lt;/name&gt; &lt;!-- &lt;value&gt;hive\\.exec\\.pre\\.hooks&lt;/value&gt; --&gt; &lt;value&gt;hive\\.*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.pre.hooks&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec&lt;/value&gt; &lt;/property&gt; åŸç†ï¼šHiveå†…éƒ¨å·²å®ç°ç›¸åº”hookï¼Œé€šè¿‡hookå®ç°ã€‚å¯èƒ½é‡åˆ°çš„å¼‚å¸¸: 2023-08-02T19:29:27,608 INFO [fee06e51-d86f-4686-acf1-d4be127aebab main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false 2023-08-02T19:29:27,609 ERROR [fee06e51-d86f-4686-acf1-d4be127aebab main] ql.Driver: FAILED: Hive Internal Error: org.apache.hadoop.hive.ql.metadata.InvalidTableException(Table not found _dummy_table) org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1135) at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1105) at org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec.run(UpdateInputAccessTimeHook.java:64) at org.apache.hadoop.hive.ql.HookRunner.invokeGeneralHook(HookRunner.java:296) at org.apache.hadoop.hive.ql.HookRunner.runPreHooks(HookRunner.java:273) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2364) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2099) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1797) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1791) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:218) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:787) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:323) at org.apache.hadoop.util.RunJar.main(RunJar.java:236) 2023-08-02T19:29:27,609 INFO [fee06e51-d86f-4686-acf1-d4be127aebab main] ql.Driver: Completed executing command(queryId=user_20230802192923_daf65682-5164-45ce-80f4-51e79c094973); Time taken: 1.045 seconds è§£å†³ï¼šå•ç‹¬æå–æºç ä¸­çš„org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHookç±»ï¼Œå¢åŠ å¼‚å¸¸å¤„ç†é€»è¾‘ï¼Œä¿®æ”¹åä»£ç ï¼ˆä½¿ç”¨æ–¹æ³•åœ¨æ³¨é‡Šï¼‰ï¼šUpdateInputAccessTimeHook.java Hiveå¼€å¯è¡¨ç»Ÿè®¡ä¿¡æ¯hive.stats.autogather=trueåˆ™ä¼šå¼€å¯è¡¨ç»Ÿè®¡ä¿¡æ¯å¦‚ä¸‹last_modified_by xxxxx last_modified_time 1690965029 numFiles 11 numRows 10 rawDataSize 34 totalSize 2433 transient_lastDdlTime 1690965096 ä¸å»ºè®®åœ¨å…¨å±€å‚æ•°è®¾ç½®è¯¥å‚æ•°ï¼Œä¼šå½±å“å†™å…¥æ€§èƒ½ï¼Œä½†å¯ä»¥é’ˆå¯¹éƒ¨åˆ†è¡¨å•ç‹¬è®¾ç½®ALTER TABLE table_name SET TBLPROPERTIES (&#39;hive.stats.autogather&#39;=&#39;true&#39;); Yarnç›¸å…³ åº”ç”¨æäº¤æŠ¥Retrying connect to server 0.0.0.0åŸå› ï¼šåº”ç”¨æ²¡æœ‰è®¤åˆ°yarn-site.xmlæˆ–è€…yarn-site.xmlé…ç½®ä¸æ­£ç¡®è§£å†³ï¼šâ‘ æŒ‡å®šHADOOP_CONF_DIR â‘¡ç¡®è®¤yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; å†…å­˜ä¸è¶³Containeré€€å‡ºæŠ¥é”™ï¼šDiagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.Dump of the process-tree for container_e23_1574819880505_43157_01_000001åˆ†æï¼šâ€physical memory usedâ€ä¸ºç‰©ç†å†…å­˜å ç”¨ï¼ˆåº”ç”¨å·²å æ»¡9Gï¼‰ï¼Œâ€virtual memory usedâ€ä¸ºè™šæ‹Ÿå†…å­˜å ç”¨ï¼Œ18.9GBæ˜¯å–å†³äºyarn.nodemanager.vmem-pmem-ratioï¼ˆyarn-site.xmlä¸­è®¾ç½®çš„è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜æ¯”ä¾‹ï¼Œé»˜è®¤2.1ï¼‰ï¼ŒæŠ¥é”™æ˜¯å› ä¸ºç‰©ç†å†…å­˜ä¸è¶³ï¼Œæ˜¯ä»»åŠ¡è®¾ç½®çš„å†…å­˜å°‘äº†è§£å†³æ€è·¯ï¼šâ‘ å¦‚æœèµ„æºå……è¶³ï¼Œå¢åŠ ä»»åŠ¡å¹¶è¡Œåº¦åˆ†æ‹…ä»»åŠ¡è´Ÿè½½â‘¡å¢å¤§ä»»åŠ¡å¯ç”¨èµ„æºï¼ˆæ³¨æ„ä¸è¦è¶…è¿‡å•å°NMå¯åˆ†é…ä¸Šé™yarn.scheduler.maximum-allocation-mbçš„å€¼ï¼‰â‘¢é€‚å½“å¢å¤§yarn.nodemanager.vmem-pmem-ratioï¼Œé€‚å½“è°ƒé«˜è™šæ‹Ÿå†…å­˜æ¯”ä¾‹â‘£[ä¸å»ºè®®]å–æ¶ˆå†…å­˜çš„æ£€æŸ¥ï¼šåœ¨yarn-site.xmlæˆ–è€…ç¨‹åºä¸­ä¸­è®¾ç½®yarn.nodemanager.vmem-check-enabledä¸ºfalse yarn logsæ— æ³•æ­£å¸¸è¾“å‡ºæ—¥å¿—åŸå› ï¼šyarn.nodemanager.remote-app-log-dirè¦è®¾ç½®ä¸º/tmp/logs ä¸èƒ½ä¿®æ”¹ä¸ºå…¶ä»–ç›˜ä¿®æ”¹åå†å²ApplicationæŸ¥çœ‹æ—¥å¿—æ¢å¤æ­£å¸¸ã€‚ CapacityScheduleræ–°å¢é˜Ÿåˆ—å‚è€ƒYARN-CapacityScheduleræ–°å¢é˜Ÿåˆ—root.analyse &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt; &lt;value&gt;default,analyse&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue).&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt; &lt;value&gt;40&lt;/value&gt; &lt;description&gt;Default queue target capacity.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.capacity&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;analyse queue target capacity.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.user-limit-factor&lt;/name&gt; &lt;value&gt;0.8&lt;/value&gt; &lt;description&gt;Default queue user limit a percentage from 0.0 to 1.0.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.user-limit-factor&lt;/name&gt; &lt;value&gt;1.5&lt;/value&gt; &lt;description&gt;The multiple of the queue capacity which can be configured to allow a single user to acquire more resources. Default is 1,it can be more than 1&lt;/description&gt; &lt;!-- user-limit-factoræ˜¯é™åˆ¶å•ä¸ªç”¨æˆ·å¯ä»¥å ç”¨è¿™ä¸ªé˜Ÿåˆ—èµ„æºçš„å€æ•° å¯ä»¥å¤§äº1--&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt; &lt;value&gt;40&lt;/value&gt; &lt;description&gt;The maximum capacity of the default queue.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.maximum-capacity&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt; &lt;description&gt;The maximum capacity of the analyse queue is 100 percent.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.state&lt;/name&gt; &lt;value&gt;RUNNING&lt;/value&gt; &lt;description&gt;The state of the default queue. State can be one of RUNNING or STOPPED.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.state&lt;/name&gt; &lt;value&gt;RUNNING&lt;/value&gt; &lt;description&gt;The state of the analyse queue. State can be one of RUNNING or STOPPED.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.acl_submit_applications&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The ACL of who can submit jobs to the default queue.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.acl_submit_applications&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The ACL of who can submit jobs to the default queue.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.acl_administer_queue&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The ACL of who can administer jobs on the default queue.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.acl_administer_queue&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The ACL of who can administer jobs on the default queue.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.analyse.ordering-policy&lt;/name&gt; &lt;value&gt;fair&lt;/value&gt; &lt;description&gt;è¦åŸºäºæ¯ä¸ªé˜Ÿåˆ—æŒ‡å®šæ’åºç­–ç•¥ï¼Œè¯·å°†ä»¥ä¸‹å±æ€§è®¾ç½®ä¸º fifoæˆ–fairã€‚é»˜è®¤è®¾ç½®ä¸º fifoã€‚&lt;/description&gt; &lt;/property&gt; è®¾ç½®ä½¿ç”¨çš„èµ„æºåˆ†é…ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¼šåŒæ—¶è€ƒè™‘CPUä»¥åŠå†…å­˜èµ„æºï¼Œè®©æ‰€æœ‰Applicationçš„â€œä¸»è¦èµ„æºå æ¯”â€èµ„æºå°½å¯èƒ½çš„å‡ç­‰ã€‚ &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt; &lt;/property&gt; åˆ·æ–°Queue,hiveå®¢æˆ·ç«¯æŒ‡å®šé˜Ÿåˆ—analyse yarn rmadmin -refreshQueues hive --hiveconf mapreduce.job.queuename=analyse Yarn WebUIæ˜¾ç¤ºæœ‰LostNodesä¸”è¿™äº›LostNodesåŒæ—¶ä¹Ÿåœ¨ActiveNodesä¸­åŸå› : æœªå›ºå®šNodeManagerç«¯å£,è€Œæ˜¯é‡‡ç”¨é»˜è®¤çš„éšå³ç«¯å£,å¯¼è‡´ä¸€ä¸ªèŠ‚ç‚¹çš„NMå¯èƒ½æœ‰å¤šç§çŠ¶æ€(LOST/Active).è§£å†³: yarn-site.xmlä¸­æ·»åŠ é…ç½®yarn.nodemanager.address=${yarn.nodemanager.hostname}:8041,å›ºå®šNMç«¯å£ä¸º8041 HBaseç›¸å…³ Couldnâ€™t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxxå¯¼è‡´Masterä¸å¯ç”¨hbase shellæ‰§è¡Œlistå‘½ä»¤æŠ¥ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializingè·Ÿè¿›Masteræ—¥å¿—çœ‹åˆ°Couldnâ€™t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxxï¼ŒMasterçŠ¶æ€ä¸å¯ç”¨åˆ†æï¼šç”±äºHDFSçš„åŸå› å¯¼è‡´HBaseçš„Snapshotæ–‡ä»¶ä¸¢å¤±ï¼Œè¯¦ç»†çœ‹äº†ä¸€ä¸‹ï¼Œå…¶ä»–snapshotç›®å½•ä¸‹æœ‰.snapshotä»¥åŠdata.manifestï¼Œè€Œè¿™ä¸ªæ²¡æœ‰æ“ä½œï¼šåˆ é™¤è¿™ä¸ªsnapshotç›®å½• é‡å¯HMasterä¿®å¤åscanæ“ä½œåˆæŠ¥Unknown table xxxï¼Œè€Œä¸”å‡ ä¹æ‰€æœ‰è¡¨éƒ½ä¸èƒ½scanæ“ä½œï¼šé¦–å…ˆæƒ³åˆ°çš„å°±æ˜¯ä¿®å¤HBaseå…ƒæ•°æ®hbase hbck -fixMetaä¿®å¤åç»§ç»­scanï¼ŒæŠ¥é”™ERROR: No server address listed in hbase:meta for region t1,,1536659773616.09db0b8b3b7f8cd81dde86c9f1e41306. containing row rowkey001: 1 timeâ€¦åˆ†æï¼šæŸ¥è¯¢hbase:metaè¡¨scan â€˜hbase:metaâ€™,{LIMIT=&gt;10,FILTER=&gt;â€PrefixFilter(â€˜test_rayâ€™)â€}å‘ç°è¡¨å…ƒæ•°æ®ä¸­éƒ½æ²¡æœ‰regionserverçš„ä¿¡æ¯ï¼Œæ­£å¸¸æƒ…å†µæ˜¯è¿™æ ·çš„ï¼šè§£å†³ï¼šé‡æ–°å¯¹regionåˆ†åŒºï¼šhbase hbck -fixAssignments æœ€ç»ˆæœåŠ¡å’Œæ•°æ®å‡æ¢å¤æ­£å¸¸ HBaseRSèŠ‚ç‚¹å®•æœºåæœåŠ¡å—åˆ°å¾ˆå¤§å½±å“ï¼Œæ— æ³•æ¢å¤æœåŠ¡æ­£å¸¸æƒ…å†µä¸‹ï¼ŒZookeeperæ„ŸçŸ¥åˆ°RegionServerå®•æœºä¹‹åï¼Œç¬¬ä¸€æ—¶é—´é€šçŸ¥Masterï¼ŒMasteré¦–å…ˆä¼šå°†è¿™å°RegionServerä¸Šæ‰€æœ‰Regionå…³é—­å¹¶ç§»åˆ°å…¶ä»–RegionServerä¸Šï¼ˆRegionå…ˆé‡æ–°åˆ†é…ï¼‰ï¼Œåˆ†é…å¥½åçš„Regionè¦åœ¨å„ä¸ªèŠ‚ç‚¹é€æ¸åˆå§‹åŒ–ï¼ŒRegionåœ¨å„ä¸ªèŠ‚ç‚¹Openåå¯ä»¥è¯»ä½†æš‚æ—¶ä¸èƒ½å†™å…¥ï¼ŒOpenRegionåå°†HLogåˆ†å‘ç»™å…¶ä»–RegionServerè¿›è¡Œå›æ”¾ï¼Œæ•´ä¸ªè¿‡ç¨‹é€šå¸¸ä¸ä¼šå¾ˆæ…¢ã€‚å®Œæˆä¹‹åå†ä¿®æ”¹è·¯ç”±ï¼Œå®¢æˆ·ç«¯çš„è¯»å†™æ‰ä¼šå®Œå…¨æ¢å¤æ­£å¸¸ã€‚ 13:20 èŠ‚ç‚¹åº•å±‚ç»„ä»¶å‡ºç°å…¼å®¹æ€§é—®é¢˜ï¼Œå‡ºç°å®•æœºç°è±¡ 13:21åˆ† å› ä¸»æœºæ²¡æœ‰è·¯ç”±å¯¼è‡´hbase04èŠ‚ç‚¹ RSå¼€å§‹å¼‚å¸¸ç»ˆæ­¢ã€‚Masteré€šçŸ¥å…¶ä»–RSæ£€æµ‹åˆ°å®•æœºçš„RSèŠ‚ç‚¹ZNodeå¤±æ•ˆï¼Œå…¶ä»–èŠ‚ç‚¹å¼€å§‹è·å–åˆ°SplitWALä»»åŠ¡ï¼Œå¼€å§‹è¿›è¡ŒWALåˆ‡åˆ†ï¼Œä¸ºWALé‡æ’­åšå‡†å¤‡ã€‚ 13:22å¼€å§‹ æœ‰è¡¨æ•°æ®æ–‡ä»¶ç›¸å…³çš„Blockä¸¢å¤±ï¼Œè®¿é—®è¶…æ—¶åä¼šæ”¾å¼ƒè¯»å–å½“å‰Blockå‰¯æœ¬ï¼Œè½¬è€Œè¯»å–è¯¥Blockçš„å…¶ä»–å‰¯æœ¬å…ƒæ•°æ®ï¼Œå†åˆ°å¯¹åº”çš„å…¶ä»–DNä¸Šè¯»å–æ•°æ®ï¼Œè¿™ä¹Ÿç›¸åº”å¢åŠ äº†HBaseè®¿é—®è€—æ—¶ï¼Œä½†å½±å“ä¸å¤§ã€‚2022-03-22 13:21:19,967 WARN org.apache.hadoop.hdfs.DFSClient: Connection failure: Failed to connect to /xxx.xx.xxx.196:1004 for file /hbase/data/default/table_name/45e41a518ec793f5eef56e0e2ff7d37d/cri/4347398d03a84cfda88cd821d2ffe957_SeqId_8465_ for block BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1141233866_67493375:org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/xxx.xx.xxx.196:1004] org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/xxx.xx.xxx.196:1004] at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534) at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2854) ...... 2022-03-22 13:21:28,560 WARN org.apache.hadoop.hdfs.DataStreamer: Abandoning BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1141323985_67583494 2022-03-22 13:21:28,564 WARN org.apache.hadoop.hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[xxx.xx.xxx.196:1004,DS-e9e8ac9e-58a0-497f-9045-7e1a8eaa8fdb,DISK] 13:25 å®•æœºRSå®Œæˆæ‰€æœ‰Regionå…³é—­ä¸‹çº¿ã€‚ä»ç›‘æ§ä¸Šçœ‹ï¼Œå®•æœºåï¼Œå„RegionServerä¸­çš„æ€»åŒºåŸŸæš‚æ—¶ä¸‹é™äº”åˆ†ä¹‹ä¸€ï¼Œ13:40å·¦å³Regionæ€»æ•°æ¢å¤åˆ°åŸå§‹æ•°é‡ï¼Œè€Œè¯»å†™è¯·æ±‚æœ‰å¤§é‡å¤±è´¥ï¼Œä¸”è¯»å†™ååé‡å¾ˆä½ã€‚ç£ç›˜IOæƒ…å†µï¼Œå„ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€æ³¢é«˜å³°ï¼ŒåŸå› æ˜¯WALé‡æ’­ï¼šHBaseå†™MemStoreå‰å…ˆå†™WALï¼ŒWALä¿å­˜åœ¨HDFSï¼ŒRSæŒ‚äº†ï¼ŒWALä¼šä»HDFSè¯»å–ï¼Œåœ¨RSå„ä¸ªèŠ‚ç‚¹è¿›è¡Œé‡æ’­ï¼Œå¹¶å†™å…¥åˆ°HFileï¼Œè¿™ä¹Ÿå°±æ˜¯ä»¥ä¸Šç›‘æ§å›¾è¡¨ä¸­ç£ç›˜IOå‡ºç°å³°å€¼ä»¥åŠIã€Oå³°å€¼å¤§å°ç›¸è¿‘çš„åŸå› ã€‚ç”±äºä½¿ç”¨SSDä½œä¸ºå­˜å‚¨ï¼Œä»¥ä¸ŠèŠ‚ç‚¹IOå³°å€¼ä¸ä¼šå¯¹HBaseæœåŠ¡çš„è¯»å†™é€ æˆå¾ˆå¤§å½±å“(ä»¥ä¸ŠIOè¯»å–é‡æœªè¾¾åˆ°SSDçš„è¯»å†™é€Ÿç‡ä¸Šé™)ï¼Œæ‰€ä»¥IOä¸æ˜¯é€ æˆHBaseé˜»å¡å’Œè¶…æ—¶çš„æ ¹æœ¬åŸå› ã€‚ 13:22å„ä¸ªèŠ‚ç‚¹å¼€å§‹æœ‰å¤§é‡è¶…æ—¶çš„å®¢æˆ·ç«¯è¯·æ±‚ã€‚å®¢æˆ·ç«¯å¼€å§‹é¢‘ç¹æŠ›å‡ºorg.apache.hadoop.hbase.NotServingRegionException 13:33:31 å…¶ä»–èŠ‚ç‚¹å¼€å§‹æ‰“å¼€hbase04ä¸‹çº¿çš„Regionç”±äºHBaseæœåŠ¡ç«¯Regionæ— æ³•æä¾›æœåŠ¡ï¼Œå¯¼è‡´å®¢æˆ·ç«¯å‡ºç°è¯·æ±‚é˜Ÿåˆ—ç§¯å‹ï¼šéé«˜å³°æœŸ(RSå‹åŠ›ä¸æ˜¯ç‰¹åˆ«å¤§çš„æƒ…å†µä¸‹)ä¸éœ€è¦è°ƒæ•´call queueï¼Œå¦‚æœè¿›è¡Œè°ƒä¼˜å¯ä»¥é€‚å½“å¢å¤§hbase.regionserver.handler.countçš„å€¼ä¸è¶…CPU Coreæ•°çš„2å€ã€‚hbase.ipc.server.max.callqueue.lengthé»˜è®¤ä¸º10*hbase.regionserver.handler.countå¤§å°ï¼Œå¯ä»¥é€‚å½“å¢åŠ callqueueå¤§å°ï¼Œä½†å¯¹äºæœ¬æ¬¡äº‹æ•…çš„æƒ…å†µï¼Œå¢å¤§callqueueæ²»æ ‡ä¸æ²»æœ¬ã€‚ 13:40 è§‚å¯Ÿå‘ç°WEBUIä¸Šå‡ºç°äº†é•¿æ—¶é—´çš„RITçŠ¶æ€ *RIT(Region-In-Transition):Regionåœ¨è¿›å…¥Openï¼ŒCloseï¼ŒSplittingï¼Œrebalanceç­‰æ“ä½œå‰çš„ä¸€ç§çŠ¶æ€**åŒæ—¶å‘ç°RS WebUIä¸Šæœ‰å¤§é‡Regionåˆå§‹åŒ–å¤±è´¥ï¼ˆException during region xxx initializationï¼‰ï¼Œä½†ä¹Ÿæœ‰å°‘éƒ¨åˆ†æ­£å¸¸Running2022-03-22 13:47:58,640 ERROR org.apache.hadoop.hbase.regionserver.HRegion: Could not initialize all stores for the region=jxl_orig_calls,77160950131161443142060106150C,1551868600765.249e28a92e9e4bc23b4adc1ce51fea03. 2022-03-22 13:47:58,640 WARN org.apache.hadoop.hbase.regionserver.HRegion: Failed initialize of region= jxl_orig_calls,77160950131161443142060106150C,1551868600765.249e28a92e9e4bc23b4adc1ce51fea03., starting to roll back memstore java.io.IOException: java.io.IOException: org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile Trailer from file hdfs://nameservice1/hbase/archive/data/default/jxl_orig_calls/249e28a92e9e4bc23b4adc1ce51fea03/cri/a49f1498d37042e6b2c458ed8f25ebaa at org.apache.hadoop.hbase.regionserver.HRegion.initializeStores(HRegion.java:1095) at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:943) at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:899) Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074443230_702456 file=/hbase/archive/data/default/jxl_orig_calls/249e28a92e9e4bc23b4adc1ce51fea03/cri/a49f1498d37042e6b2c458ed8f25ebaa at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:880) ...... 2022-03-22 13:52:49,773 INFO org.apache.hadoop.hbase.regionserver.RSRpcServices: Open cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5. 2022-03-22 13:52:49,792 WARN org.apache.hadoop.hdfs.DFSClient: No live nodes contain block BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 after checking nodes = [], ignoredNodes = null 2022-03-22 13:52:49,792 INFO org.apache.hadoop.hdfs.DFSClient: No node available for BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 file=/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 2022-03-22 13:52:49,792 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 from any node: No live nodes contain current block Block locations: Dead nodes: . Will get new block locations from namenode and retry... 2022-03-22 13:52:49,792 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1121.4412474110932 msec. 2022-03-22 13:52:50,914 INFO org.apache.hadoop.hdfs.DFSClient: No node available for BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 file=/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 2022-03-22 13:52:50,914 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 from any node: No live nodes contain current block Block locations: Dead nodes: . Will get new block locations from namenode and retry... org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 file=/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 2022-03-22 13:53:05,772 ERROR org.apache.hadoop.hbase.regionserver.HRegion: Could not initialize all stores for the region=cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5. 2022-03-22 13:53:05,772 WARN org.apache.hadoop.hbase.regionserver.HRegion: Failed initialize of region= cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5., starting to roll back memstore java.io.IOException: java.io.IOException: org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile Trailer from file hdfs://nameservice1/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1701342213-xxx.xx.xxx.193-1622692928774:blk_1074490423_749649 file=/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:880) 2022-03-22 13:53:05,777 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5. java.io.IOException: The new max sequence id 1 is less than the old max sequence id 2380665 at org.apache.hadoop.hbase.wal.WALSplitter.writeRegionSequenceIdFile(WALSplitter.java:697) â€¦â€¦â€¦ 2022-03-22 14:26:30,302 ERROR org.apache.hadoop.hbase.regionserver.HRegion: Could not initialize all stores for the region=cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5. 2022-03-22 14:26:30,302 WARN org.apache.hadoop.hbase.regionserver.HRegion: Failed initialize of region= cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5., starting to roll back memstore java.io.IOException: java.io.IOException: org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile Trailer from file hdfs://nameservice1/hbase/archive/data/default/cst_cust_relation_info/76c8f8961f500e02029cf3d731b8cdb5/cri/b9f57338b8ff42f891727ca1ef9ca378 at org.apache.hadoop.hbase.regionserver.HRegion.initializeStores(HRegion.java:1095) 2022-03-22 14:26:30,308 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=cst_cust_relation_info,30612942006100091,1611896946166.76c8f8961f500e02029cf3d731b8cdb5. java.io.IOException: The new max sequence id 1 is less than the old max sequence id 2380665 2022-03-22 14:20:43,278 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Failed report transition server &#123; host_name: &quot;hbase01.c6.com&quot; port: 16020 start_code: 1647536118456 &#125; transition &#123; transition_code: FAILED_OPEN region_info &#123; region_id: 1611896946166 table_name &#123; namespace: &quot;default&quot; qualifier: &quot;cst_cust_relation_info&quot; &#125; start_key: &quot;30612942006100091&quot; end_key: &quot;309204000051869584160052106171C&quot; offline: false split: false replica_id: 0 &#125; &#125;; retry (#0) after 1006ms delay (Master is coming online...). HLogç»„æˆï¼šsequenceidæ˜¯ä¸€ä¸ªStoreçº§åˆ«çš„è‡ªå¢åºåˆ—å·ï¼Œéå¸¸é‡è¦ï¼ŒRegionçš„æ•°æ®æ¢å¤ã€WALæœ‰åºé‡æ’­å’ŒHLogè¿‡æœŸæ¸…é™¤éƒ½è¦ä¾èµ–sequenceidã€‚è€Œä»¥ä¸ŠæŠ¥é”™â€œThe new max sequence id 1 is less than the old max sequence id 2380665â€ï¼ŒnewMaxSeqId = 1Læ ¹æ®æ—¥å¿— æŠ¥é”™ä¸»è¦å‘ç”Ÿåœ¨OpenRegionçš„è¿‡ç¨‹ï¼Œè¯æ˜initializeRegionInternalsæ–¹æ³•è¿è¡Œå¼‚å¸¸ï¼ŒnextSeqId = -1ç»§ç»­è¿½æº¯ERRORï¼š Could not initialize all stores for the region=xxx,xxx,xxx.76c8f8961f500e02029cf3d731b8cdb5.ç»§ç»­è¿½æº¯getMaxSequenceIdæ–¹æ³•ç»§ç»­è¿½æº¯Collection sfs çœ‹çœ‹åˆ°åº•è·å–äº†å“ªäº›HStoreFileï¼Œä¸ºä½•ä¼šå»/hbase/archiveè¯»å–æ–‡ä»¶(æ­£å¸¸æƒ…å†µä¸‹/hbase/archiveåªå­˜æ”¾è¿‡æœŸHfileå’ŒSnapshotæ•°æ®)ï¼Œè¿½æº¯åˆ°HStore.getStorefiles()æ–¹æ³•é€šè¿‡æºç ç¡®è®¤äº†å®é™…ä¸Šè¯»å–çš„TableDirå¹¶éarchiveè€Œæ˜¯dataç›®å½•ã€‚åˆ†æï¼šæ²¡æœ‰è¯»/hbase/archiveç›®å½•ï¼ŒOpen Regionä»ç„¶å¤±è´¥æ˜¯å› ä¸ºHBaseçš„Snapshotæœºåˆ¶ã€‚Snapshotæœºåˆ¶å¹¶ä¸ä¼šç‰©ç†æ‹·è´æ•°æ®ï¼Œè€Œæ˜¯åŸå§‹æ•°æ®çš„ä¸€ä»½æŒ‡é’ˆ(é“¾æ¥)ï¼Œæˆ‘ä»¬çš„æ–°é›†ç¾¤æ˜¯é€šè¿‡åˆ›å»ºå¿«ç…§+ExportSnapshot+restore_snapshotä¸‰ä¸ªæ­¥éª¤å°†å…¨é‡å†å²æ•°æ®è¿ç§»è‡³äº‘å±•é›†ç¾¤çš„ï¼Œè€Œrestore_snapshotå‰ï¼Œè¡¨çš„çœŸå®æ•°æ®å°±å­˜åœ¨/hbase/archiveä¸­ï¼Œrestore_snapshotæ“ä½œå‡ ä¹ç¬é—´å®Œæˆï¼Œä¸ä¼šçœŸå®ç§»åŠ¨æ•°æ®ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä»½å…ƒæ•°æ®é“¾æ¥(link)ï¼ŒHBaseåœ¨Compcatæ—¶ä¹Ÿä¼šå°†æ•°æ®å¤åˆ¶åˆ°archiveç›®å½•ä¸‹å†è¿›è¡ŒCompactï¼ŒåŒ…æ‹¬Open Regionæ—¶è¯»å–/hbase/dataå®é™…è¯»å–çš„æ˜¯LinkFileï¼ˆè¿™ä¹Ÿèƒ½è§£é‡Šä¸ºä½•æ–°é›†ç¾¤/hbase/dataæ™®éæ¯”æ—§é›†ç¾¤å°ï¼Œä½†è¡¨æ•°æ®é‡ä¸€è‡´ï¼‰ï¼Œæ‰€ä»¥å¦‚æœarchiveä¸­åŸå§‹æ–‡ä»¶çš„Blockå› æœºå™¨å®•æœºè€Œä¸¢å¤±ï¼Œåˆ™è¯»å–/hbase/dataä¹Ÿä¼šå¤±è´¥ï¼ŒOpen Regionå°±ä¼šå¤±è´¥ã€‚HBaseçš„HDFSé»˜è®¤ä¸‰å‰¯æœ¬ï¼Œä½†archiveç›®å½•åªæœ‰å•ä¸ªå‰¯æœ¬ï¼ŒåŸå› æ˜¯åœ¨é›†ç¾¤è¿ç§»æ—¶ä¸ºäº†é™ä½é›†ç¾¤IOï¼Œè®¾ç½®äº†å•å‰¯æœ¬è¿ç§»hbase org.hadoop.hbase.dataExport.ExportSnapshot -D dfs.replication=1 -snapshot snap_name -copy-from hdfs://nameservice1/hbase -copy-to hdfs://xxx.xx.xxx.194/hbase -mappers 10 -bandwidth 100æ‰€ä»¥archiveç›®å½•åªæœ‰å•å‰¯æœ¬ã€‚æ€»ç»“ï¼šRegion Stateæ— æ³•ç”±ClOSEDåŠæ—¶æˆåŠŸè½¬æ¢ä¸ºOPENæ˜¯å¯¼è‡´HBaseæ— æ³•åŠæ—¶æ¢å¤è¯»å†™çš„ä¸»è¦åŸå› ã€‚é€ æˆRegion Openå¤±è´¥çš„ä¸»è¦åŸå› æ˜¯archiveç›®å½•ä¸­æ–‡ä»¶Blockä¸¢å¤±ã€å•å‰¯æœ¬ã€‚HBaseæ˜¯é€šè¿‡åˆ†æ•£Regionçš„æ–¹å¼åˆ†æ•£è¯·æ±‚ï¼Œè¾¾åˆ°åœ¨å»‰ä»·PCä¸Šè¾¾åˆ°å‡ åè‡³ç™¾æ¯«ç§’å†…è¿”å›æ•°æ®çš„æ•ˆç‡ï¼Œæ•…Regionæœ‰é—®é¢˜ä¼šç›´æ¥å½±å“æ•´ä½“çš„è¯·æ±‚æ•ˆç‡ï¼Œæœ‰é—®é¢˜çš„Regionè¶Šå°‘ï¼Œå¯¹è¯·æ±‚çš„å½±å“è¶Šå°ã€‚åº”å½“å°½é‡æ§åˆ¶æ¯å°RSä¸ŠRegionçš„ä¸ªæ•°ï¼Œè‹¥æ¯å°RSä¸Šçš„Regionæ•°é‡è¾ƒå¤šï¼Œå®•æœºå¯¼è‡´Regionè¿ç§»ï¼ˆRegionè¿ç§»è¿‡ç¨‹ä¸­ä¹Ÿä¼šå¯¼è‡´Regionä¸‹çº¿ï¼ŒRegionä¸‹çº¿ä¸å¯æœåŠ¡ï¼‰éœ€è¦èŠ±è´¹çš„æ—¶é—´è¶Šé•¿ï¼Œå½±å“ä¹Ÿè¶Šå¤§ã€‚è¡¥æ•‘æªæ–½ï¼šåœ¨é›†ç¾¤IOè¾ƒä¸ºç©ºé—²æ—¶ä¿®æ”¹/hbase/archiveä¸ºä¸‰å‰¯æœ¬ï¼ˆä»¥ä¸‹æ“ä½œä¸ºå¼‚æ­¥æ“ä½œï¼Œæ“ä½œåç›‘æ§æ˜¾ç¤ºé›†ç¾¤æœ‰ä¸¢å¤±çš„Blockï¼Œä¸¢å¤±Blockæ•°é€æ¸å‡å°‘å½’é›¶ï¼‰hadoop fs -setrep -R 3 /hbase/archive è®¾ç½®archiveä¸‰å‰¯æœ¬åï¼Œè¿›è¡Œå®•æœºæ¼”ç»ƒå®•æœºåï¼ŒRegionè¿…é€Ÿè¿ç§»ï¼Œä¸Šçº¿æ­£å¸¸ã€‚Open Regionæ—¶å¯ä»¥æ‹¿åˆ°æ­£ç¡®çš„SequenceIdï¼Œæœªå‡ºç°æ‰“å¼€Regionå¤±è´¥çš„é—®é¢˜ï¼Œè¯»å†™è¯·æ±‚ä¹Ÿéšç€Regionæ¢å¤è€Œæ¢å¤æ­£å¸¸æœºå™¨å¯åŠ¨ å…¶ä»–èŠ‚ç‚¹å…³é—­RegionRSå¹³å‡Regionæ•°ä¸‹é™å®•æœºæ¢å¤çš„èŠ‚ç‚¹ï¼ŒRegionåˆå§‹åŒ–æ­£å¸¸è¿‡ä¸€ä¼šå„¿åœ¨å®•æœºæ¢å¤çš„èŠ‚ç‚¹Region OpenæˆåŠŸï¼ŒRegionå‡åŒ€åˆ†é…è¿‡æ¥ é›†ç¾¤æ–°å¢RSè‡´å¦ä¸€RSå¼‚å¸¸é€€å‡ºä¸Šçº¿çš„RegionServerä¼šè§¦å‘Regionç§»åŠ¨ï¼ŒæŠ¥é”™å‰æœ‰Flushæ“ä½œï¼Œå› ä¸ºRegionç§»åŠ¨å‰ä¼šå…ˆFlush Regionå¼‚å¸¸ç›¸å…³æºç ï¼šFlush Memstoreåˆ°HFileè¿™ä¸ªè¿‡ç¨‹æœªå‘ç”Ÿå¼‚å¸¸ï¼Œä½†Flushä¸€ä¸ªMemstoreåè·Ÿè¸ªMemstoreæ€»å¤§å°æœªå‘ç”Ÿå˜åŒ–ï¼Œå³å†…å­˜æ¸…ç†å¤±è´¥ è¶…è¿‡5æ¬¡å°±ä¸­æ­¢è¿™ä¸ªRSåˆ†æï¼šç‰ˆæœ¬BUG Kafkaç›¸å…³ ConsumerRebalanceFailedExceptionå¼‚å¸¸è§£å†³æŠ¥é”™ï¼škafka.common.ConsumerRebalanceFailedException: group_xxx-1446432618163-2746a209 canâ€™t rebalance after 5 retriesåˆ†æï¼šå®˜ç½‘ç»™å‡ºäº†å¼‚å¸¸çš„ç›¸å…³è¯´æ˜å’Œè§£å†³æ–¹æ¡ˆconsumer rebalancing fails (you will see ConsumerRebalanceFailedException): This is due to conflicts when two consumers are trying to own the same topic partition. The log will show you what caused the conflict (search for â€œconflict in â€œ).If your consumer subscribes to many topics and your ZK server is busy, this could be caused by consumers not having enough time to see a consistent view of all consumers in the same group. If this is the case, try Increasing rebalance.max.retries and rebalance.backoff.ms.Another reason could be that one of the consumers is hard killed. Other consumers during rebalancing wonâ€™t realize that consumer is gone after zookeeper.session.timeout.ms time. In the case, make sure that rebalance.max.retries * rebalance.backoff.ms &gt; zookeeper.session.timeout.ms.ç®€å•æ¥è¯´å°±æ˜¯åŒä¸€ä¸ªtopicçš„åŒä¸€ä¸ªåˆ†åŒºè¢«å¤šä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹ï¼Œå‘ç”Ÿå†²çªè§£å†³ï¼šâ‘ å¢åŠ ç›¸å…³topicçš„partitionæ•°â‘¡æé«˜kafkaçš„consumerå¦‚ä¸‹ä¸¤é¡¹é…ç½®rebalance.backoff.ms=2000rebalance.max.retries=10 MirrorMakeræ— æ•…æŒ‚æ‰ # MirrorMakeræŒ‚æ‰ä¸”æ— æ³•é‡å¯ï¼Œé€šè¿‡æ—¥å¿—æ— æ³•æ˜ç¡®åŸå›  ä¸Šåˆ10ç‚¹27:04.393åˆ† ERROR MirrorMaker$MirrorMakerThread [mirrormaker-thread-0] Mirror maker thread failure due to java.lang.IllegalStateException: Unexpected error code 2 while fetching data at org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(Fetcher.java:998) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:491) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1238) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1200) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1176) at kafka.tools.MirrorMaker$ConsumerWrapper.receive(MirrorMaker.scala:332) at kafka.tools.MirrorMaker$MirrorMakerThread.run(MirrorMaker.scala:221) ... ä¸Šåˆ10ç‚¹27:04.697åˆ† ERROR MirrorMaker$MirrorMakerThread [mirrormaker-thread-1] Mirror maker thread exited abnormally, stopping the whole mirror maker. ä¸Šåˆ10ç‚¹27:04.697åˆ† ERROR MirrorMaker$MirrorMakerThread [mirrormaker-thread-0] Mirror maker thread exited abnormally, stopping the whole mirror maker. # å‘ç°sourceç«¯Kafka Brokeré”™è¯¯æ—¥å¿— [ReplicaManager broker=30] Error processing fetch with max size 10485760 from replica [31] on partition event-topic-2: (fetchOffset=27000304654, logStartOffset=26879257739, maxBytes=11534336, CurrentLeaderEpoch=Optional.empty) org.apache.kafka.common.errors.CorruptRecordException: Found record size 0 smaller than minimum record overhead (14) in file /xxx/xxx/0000000000xxxxx.log æ€€ç–‘å‰¯æœ¬æˆ–ISRæŸåï¼ŒæŸ¥çœ‹topicçŠ¶æ€kafka-topics â€“topic event_topic â€“describe â€“bootstrap-server s1:9092,s2:9092,s3:9092åŸå› ï¼šISRç¼ºå¤±ï¼ŒISRæ— æ³•è‡ªåŠ¨æ¢å¤å’Œä¿®å¤ï¼Œç³»ç»Ÿç£ç›˜å­˜åœ¨åé“ï¼Œéœ€è¦æ›´æ¢ç£ç›˜è§£å†³ï¼›è™½ç„¶Leaderå‰¯æœ¬æ˜¯å¯ç”¨çš„ï¼Œä½†MirrorMakerå¯èƒ½å¡åœ¨åé“å¤„çš„Offsetï¼Œå°è¯•è·³è¿‡Offsetæ¶ˆè´¹ã€‚å¦‚æœå…è®¸æ•°æ®ä¸¢å¤±ä¸€æ®µæ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨æ–°çš„æ¶ˆè´¹ç»„ï¼Œä»æœ€æ–°çš„Offsetå¼€å§‹æ¶ˆè´¹ã€‚è§£å†³ï¼šæ¢æ–°çš„MirrorMakeræ¶ˆè´¹ç»„ï¼Œæ¶ˆè´¹æ­£å¸¸ï¼Œå¯åŠ¨æ­£å¸¸ã€‚åç»­è¡¥å……ä¸¢å¤±æ•°æ®ã€‚ æ¶ˆè´¹Kafkaæ—¶æŠ¥é”™org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic_name-7=5741542729}åŸå› ï¼šå½“å‰æ¶ˆè´¹è€…æ¶ˆè´¹çš„Offsetå¤§äºæˆ–å°äºå½“å‰Kafkaé›†ç¾¤çš„Offsetæ—¶ä¼šæŠ¥æ­¤é”™è¯¯ã€‚å¯èƒ½æƒ…å†µï¼šâ‘ æ¶ˆæ¯ä¸€ç›´ç§¯å‹ï¼Œå½“å‰æ¶ˆè´¹çš„offsetå·²ç»å°äºKafkaæŸä¸€ä¸ªæˆ–å¤šä¸ªåˆ†åŒºä¸­æœ€è€çš„offsetï¼Œè¿™éƒ¨åˆ†æ•°æ®å·²è¢«Kafkaåˆ é™¤ï¼Œæ•…æ— æ³•è¢«æ¶ˆè´¹ï¼ŒæŠ¥æ­¤é”™è¯¯ â‘¡æ¶ˆè´¹ä¸å‡è¡¡ï¼Œéƒ¨åˆ†åˆ†åŒºç§¯å‹lagè¾ƒå¤šï¼Œè¿™äº›åˆ†åŒºæ¶ˆè´¹çš„offsetå¯¹åº”æ•°æ®å·²è¿‡æœŸï¼Œè¢«kafkaåˆ é™¤ï¼Œæ— æ³•æ¶ˆè´¹ã€‚è§£å†³ï¼šæé«˜æ¶ˆè´¹é€Ÿç‡ï¼Œè§£å†³åˆ†åŒºæ¶ˆè´¹ä¸å‡è¡¡çš„é—®é¢˜ï¼Œå°†offseté‡ç½®åˆ°ä¸€å®šå€¼ä»¥ä¿è¯ç¨‹åºå¯åŠ¨(å·²ç»ä¸¢å¤±çš„offsetæ— æ³•æ¢å¤)ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´Topicçš„æ¶ˆæ¯ä¿ç•™æ—¶é—´(retention.ms=86400000)ã€‚ Flinkç›¸å…³ Checkpoint expired before completing.org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.åŸå› : å¯èƒ½è®¾ç½®Checkpointæ—¶é—´é—´éš”è¿‡çŸ­æˆ–ä»£ç é—®é¢˜.è§£å†³: è®¾ç½®æ›´é•¿çš„checkpoint interval;æ£€æŸ¥ä»£ç ä¸­æ˜¯å¦æœ‰å½±å“checkpointæˆåŠŸæ‰§è¡Œçš„é€»è¾‘. Exceeded checkpoint tolerable failure threshold.åŸå› : æ£€æŸ¥ç‚¹ä»»åŠ¡å¤±è´¥æ¬¡æ•°è¶…é˜ˆå€¼;åœ¨è¾¾åˆ°æ£€æŸ¥ç‚¹å¤±è´¥æ¬¡æ•°é˜ˆå€¼(execution.checkpointing.tolerable-failed-checkpoints)å‰,Flinkä¼šç»Ÿè®¡æ£€æŸ¥ç‚¹å¤±è´¥çš„æ¬¡æ•°,å¦‚æœä¸­é—´æœ‰ä¸€æ¬¡æ£€æŸ¥ç‚¹æˆåŠŸ,åˆ™ä¹‹å‰ç»Ÿè®¡çš„å¤±è´¥æ•°å½’é›¶,è‹¥è¾¾åˆ°æœ€å¤§é˜ˆå€¼åˆ™è§¦å‘Jobé‡å¯.è§£å†³: å¦‚æœå¶å°”å¤±è´¥ä¸€æ¬¡,å¯ä»¥å®¹å¿,åˆ™è°ƒé«˜æ£€æŸ¥ç‚¹å¤±è´¥æ•°é˜ˆå€¼execution.checkpointing.tolerable-failed-checkpoints Flink Checkpointç›®å½•æ— chk-xxxç›®å½•ï¼Œæ— æ³•ä»è¯¥CPæ¢å¤åŸå› ï¼šå¦‚æœè¯¥Flinkä»»åŠ¡ä¸æ¶‰åŠçŠ¶æ€çš„ä¿å­˜ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹ï¼Œexecution.checkpointing.externalized-checkpoint-retention=NO_EXTERNALIZED_CHECKPOINTSï¼Œå‚è€ƒRetained Checkpointsï¼ŒFlinkå°†ä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶æ¸…ç†æ‰€æœ‰CPï¼Œæ•…chk-xxxç›®å½•ä¸å­˜åœ¨ã€‚å¦‚æœè¯¥Flinkä»»åŠ¡çš„ç®—å­æ˜¯æœ‰çŠ¶æ€çš„ï¼Œåˆ™CheckPointç›®å½•chk-xxxä¸ä¼šè¢«åˆ é™¤ã€‚è§£å†³ï¼šè‹¥éœ€è¦å¼ºè¡Œä¿ç•™æ— çŠ¶æ€ä»»åŠ¡çš„CPç›®å½•ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®-Dexecution.checkpointing.externalized-checkpoint-retention=RETAIN_ON_CANCELLATIONæ¥ä½¿å¾—ç¨‹åºä¸æ¸…ç†chk-xxxç›®å½•ï¼Œä»¥ä¾¿èƒ½ä»è¯¥CPæ¢å¤ä»»åŠ¡ã€‚ å…¶ä»– Zookeeperæ—¥å¿—å’Œå¿«ç…§è¿‡å¤§è§£å†³1ï¼šcd $ZOOKEEPER_HOME;bin/zkCleanup.sh {dataDir} 5;bin/zkCleanup.sh {snapshotDir} 5 å¹¶è®¾ç½®æˆå®šæ—¶è°ƒåº¦è§£å†³2ï¼šä»3.4.0å¼€å§‹ï¼Œzookeeperæä¾›äº†è‡ªåŠ¨æ¸…ç†snapshotå’Œäº‹åŠ¡æ—¥å¿—çš„åŠŸèƒ½ï¼Œé€šè¿‡é…ç½®autopurge.snapRetainCountå’Œautopurge.purgeIntervalè¿™ä¸¤ä¸ªå‚æ•°èƒ½å¤Ÿå®ç°å®šæ—¶æ¸…ç†äº†ã€‚è¿™ä¸¤ä¸ªå‚æ•°éƒ½æ˜¯åœ¨zoo.cfgä¸­é…ç½®çš„ï¼š autopurge.purgeInterval è¿™ä¸ªå‚æ•°æŒ‡å®šäº†æ¸…ç†é¢‘ç‡ï¼Œå•ä½æ˜¯å°æ—¶ï¼Œéœ€è¦å¡«å†™ä¸€ä¸ª1æˆ–æ›´å¤§çš„æ•´æ•°ï¼Œé»˜è®¤æ˜¯0ï¼Œè¡¨ç¤ºä¸å¼€å¯è‡ªå·±æ¸…ç†åŠŸèƒ½ã€‚ autopurge.snapRetainCount è¿™ä¸ªå‚æ•°å’Œä¸Šé¢çš„å‚æ•°æ­é…ä½¿ç”¨ï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šäº†éœ€è¦ä¿ç•™çš„æ–‡ä»¶æ•°ç›®ã€‚é»˜è®¤æ˜¯ä¿ç•™3ä¸ªã€‚åŸç†ï¼šZookeeperä¸ä¼šåˆ é™¤æ—§çš„å¿«ç…§å’Œæ—¥å¿—æ–‡ä»¶ï¼ŒzkCleanup.shå¯ä»¥å¸®åŠ©åˆç†æ¸…ç†å½“å‰èŠ‚ç‚¹çš„æ—§çš„æ—¥å¿—å’Œå¿«ç…§æ–‡ä»¶è§£å†³ç£ç›˜ç©ºé—´ã€‚å…¶ä¸­dataDiræ˜¯ç‰¹å®šæœåŠ¡é›†åˆçš„znodeå­˜å‚¨çš„æ°¸ä¹…å‰¯æœ¬ï¼Œå¯¹znodeçš„æ‰€æœ‰æ›´æ”¹ä¼šé™„åŠ åˆ°logæ—¥å¿—ä¸­ï¼ŒsnapshotDiræ˜¯æ°¸ä¹…å¿«ç…§ã€‚zkCleanup.shå°†ä¿ç•™æœ€åä¸€ä¸ªå¿«ç…§å’Œç›¸å…³çš„logæ—¥å¿—ï¼Œæ¸…é™¤å…¶ä»–çš„å¿«ç…§å’Œæ—¥å¿—ã€‚æ³¨æ„æœ€åä¸€ä¸ªå‚æ•°å€¼è¦å¤§äºç­‰äº3ï¼Œæ—¥å¿—æŸåçš„æƒ…å†µä¸‹ä¿è¯æœ‰3ä¸ªä»¥ä¸Šå¤‡ä»½ã€‚å‚è€ƒ:zookeeperAdmin.html#sc_maintenance ZookeeperæœåŠ¡æŠ¥é”™java.io.IOException: Unreasonable length æ— æ³•å¯åŠ¨ 2022-07-25 15:38:56,652 [myid:3] - INFO [main:QuorumPeer@2479] - QuorumPeer communication is not secured! (SASL auth disabled) 2022-07-25 15:38:56,652 [myid:3] - INFO [main:QuorumPeer@2504] - quorum.cnxn.threads.size set to 20 2022-07-25 15:38:56,653 [myid:3] - INFO [main:FileSnap@85] - Reading snapshot /mnt/disk1/zookeeper/version-2/snapshot.2000c6082 2022-07-25 15:38:56,900 [myid:3] - INFO [main:DataTree@1730] - The digest in the snapshot has digest version of 2, , with zxid as 0x2000c6082, and digest value as 117449458451566 2022-07-25 15:38:57,044 [myid:3] - ERROR [main:QuorumPeer@1148] - Unable to load database on disk java.io.IOException: Unreasonable length = 2573749 at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166) at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127) at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:159) at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:768) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:352) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:258) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:303) at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:285) at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1094) at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1079) at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:227) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90) 2022-07-25 15:38:57,046 [myid:3] - ERROR [main:QuorumPeerMain@113] - Unexpected exception, exiting abnormally java.lang.RuntimeException: Unable to run quorum server at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1149) at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:1079) at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:227) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:136) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90) Caused by: java.io.IOException: Unreasonable length = 2573749 at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166) at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127) at org.apache.zookeeper.server.persistence.Util.readTxnBytes(Util.java:159) at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:768) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:352) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.lambda$restore$0(FileTxnSnapLog.java:258) at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:303) at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:285) at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:1094) åŸå› ï¼šå•ä¸ªèŠ‚ç‚¹æ•°æ®è¿‡å¤§ä¼šå¯¼è‡´æ€§èƒ½é—®é¢˜ï¼Œæ‰€ä»¥zookeeperæœåŠ¡ç«¯é»˜è®¤è¯»å–æ•°æ®èŠ‚ç‚¹å¤§å°ä¸º1.01Mï¼Œç”±å‚æ•°ç¨‹åºå‚æ•°jute.maxbufferæ§åˆ¶ã€‚å½“zookeeperå®¢æˆ·ç«¯è¯»å–çš„èŠ‚ç‚¹å¤§å°è¶…è¿‡è¯¥å€¼æ—¶å°±ä¼šæŠ›å‡ºè¯¥å¼‚å¸¸ä¿¡æ¯ã€‚è§£å†³ï¼šåœ¨zkServer.shå¯åŠ¨è„šæœ¬ä¸­æ·»åŠ SERVER_JVMFLAGS=â€-Djute.maxbuffer=4194304â€å†…å®¹ï¼Œé‡å¯zkï¼Œè®©zkå¯åŠ¨æ—¶æ¥æ”¶åˆ°æ–°çš„jute.maxbufferå‚æ•°ï¼Œå…è®¸æ•°æ®èŠ‚ç‚¹å¤§äº1Mï¼Œä½¿å¾—ZkServerå¯ä»¥å¯åŠ¨ã€‚ PrestoServerå¯åŠ¨æŠ¥node.environment: is malformed```error022-08-04T11:39:33.380+0800 ERROR main com.facebook.presto.server.PrestoServer Unable to create injector, see the following errors: Error: Invalid configuration property node.environment: is malformed (for class com.facebook.airlift.node.NodeConfig.environment)1 errorcom.google.inject.CreationException: Unable to create injector, see the following errors: Error: Invalid configuration property node.environment: is malformed (for class com.facebook.airlift.node.NodeConfig.environment)1 error at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:543) at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:159) at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106) at com.google.inject.Guice.createInjector(Guice.java:87) at com.facebook.airlift.bootstrap.Bootstrap.initialize(Bootstrap.java:251) at com.facebook.presto.server.PrestoServer.run(PrestoServer.java:143) at com.facebook.presto.server.PrestoServer.main(PrestoServer.java:85) åŸå› : $PRESTO_HOME/etc/node.propertiesä¸­node.environmentä¸ç¬¦åˆ[a-z0-9][_a-z0-9]*å‘½åè§„èŒƒ è§£å†³æ–¹æ³•ï¼šé‡æ–°ä¿®æ”¹æ‰€æœ‰è¯¥é›†ç¾¤èŠ‚ç‚¹å‘½åè§„èŒƒå³å¯,å¦‚ä¿®æ”¹ä¸ºpresto_cdh TrinoServeræ— æ•…æŒ‚æ‰,æ—¥å¿—ä¸­æ— ERRORæ£€æŸ¥/var/log/messageå‘ç°å¦‚ä¸‹æ—¥å¿— Dec 7 16:56:49 emr-worker-10014 kernel: Out of memory: Kill process 7308 (trino-server) score 359 or sacrifice child Dec 7 16:56:49 emr-worker-10014 kernel: Killed process 7308 (trino-server) total-vm:68996464kB, anon-rss:46582532kB, file-rss:0kB, shmem-rss:0kB Dec 7 16:56:51 emr-worker-10014 kernel: oom_reaper: reaped process 7308 (trino-server), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB åŸå› : å½“ç³»ç»Ÿèµ„æºä¸è¶³æ—¶,ç³»ç»ŸOOM killeræœºåˆ¶(ç›‘æ§é‚£äº›å ç”¨å†…å­˜è¿‡å¤§ï¼Œå°¤å…¶æ˜¯ç¬é—´å ç”¨å†…å­˜å¾ˆå¿«çš„è¿›ç¨‹ï¼Œç„¶åé˜²æ­¢å†…å­˜è€—å°½è€Œè‡ªåŠ¨æŠŠè¯¥è¿›ç¨‹æ€æ‰)ä¸ºä¿è¯ç³»ç»Ÿç¨³å®šè¿è¡Œ,å°†TrinoServerè¿›ç¨‹Killæ‰äº†.æ—¥å¿—ä¼šè®°å½•åœ¨/var/log/message.è§£å†³: è‡ªåŠ¨æ‹‰èµ· Kerberosç›¸å…³-å®¢æˆ·ç«¯è¯·æ±‚æœåŠ¡ç«¯restæ¥å£æŠ¥é”™è¯·æ±‚çš„urlåœ°å€æ˜¯http://172.11.11.11:1111/aa è¯·æ±‚çš„æ˜¯ipåœ°å€ä¸”æä¾›è¯·æ±‚ç”¨çš„principalå’Œkeytabä½¿ç”¨kinitè®¤è¯æ˜¯æ­£å¸¸çš„. Service ticket not found in the subject Caused by: KrbException: Identifier doesn&#39;t match expected value (906) Negotiate support not initiated, will fallback to other scheme if allowed. Reason GSSException: No valid credentials provided (Mechanism level: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)) error code is 7 error Message is Server not found in Kerberos database KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER åŸå› : ä¸ºäº†è¿½æ±‚æè‡´å®‰å…¨æ€§ï¼ŒKeberosè¦æ±‚è®¿é—®çš„æœåŠ¡èƒ½é€šè¿‡dnsåå‘è§£æï¼ŒéªŒè¯principalæ˜¯å¦æ­£ç¡®ï¼Œè‹¥dnsåŸŸåè§£ææœ‰é—®é¢˜,å°±ä¼šæŠ¥æ­¤é”™è¯¯ã€‚æ•…éœ€è¦ipå¯ä»¥åå‘è§£ææˆæ­£ç¡®åŸŸå(æ­£ç¡®è§£æçš„åŸŸåæ˜¯æŒ‡hive/&#95;&#72;&#x4f;&#83;&#84;&#x40;&#82;&#69;&#x41;&#x4c;&#x4d;&#x2e;&#67;&#x4f;&#77;ä¸­çš„_HOSTé€šé…ç¬¦éƒ¨åˆ†ï¼ŒåŸŸåå¯ä»¥ä»keytabä¸­å¯»æ‰¾)è§£å†³: æ”¹restè¯·æ±‚urlçš„åœ°å€ä¸ºåŸŸå;æˆ–è€…ä¿®æ­£ipçš„dnsè§£æ,ä½¿ipå¯ä»¥æ­£ç¡®åå‘è§£æå‡ºåŸŸååå³å¯è®¤è¯æˆåŠŸ.","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®å¹³å°","slug":"å¤§æ•°æ®å¹³å°","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/"},{"name":"å¼‚å¸¸åˆ†æ","slug":"å¼‚å¸¸åˆ†æ","permalink":"https://shmily-qjj.top/tags/%E5%BC%82%E5%B8%B8%E5%88%86%E6%9E%90/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Impala-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“","slug":"Impala-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“","date":"2021-01-17T02:18:30.000Z","updated":"2022-12-11T05:35:07.906Z","comments":true,"path":"1ae37d82/","link":"","permalink":"https://shmily-qjj.top/1ae37d82/","excerpt":"","text":"Impala-åŸºäºå†…å­˜çš„é«˜æ•ˆSQLäº¤äº’æŸ¥è¯¢å¼•æ“Impalaç®€ä»‹&emsp;&emsp;Impalaæ˜¯Clouderaæä¾›çš„ä¸€æ¬¾é«˜æ•ˆç‡çš„SQLå®æ—¶æŸ¥è¯¢å·¥å…·ï¼Œå®˜æ–¹æµ‹è¯•æ€§èƒ½æ¯”Hiveå¿«10åˆ°100å€ï¼ŒSQLæŸ¥è¯¢æ€§èƒ½ç”šè‡³æ¯”SparkSQLè¿˜æ›´åŠ é«˜æ•ˆã€‚Impalaæ˜¯åŸºäºHiveçš„å¤§æ•°æ®åˆ†ææŸ¥è¯¢å¼•æ“ï¼Œç›´æ¥ä½¿ç”¨Hiveçš„å…ƒæ•°æ®åº“ï¼Œæ„å‘³ç€Impalaå…ƒæ•°æ®éƒ½å­˜å‚¨åœ¨Hiveçš„å…ƒæ•°æ®åº“å½“ä¸­ã€‚Impalaå…¼å®¹ç»å¤§å¤šæ•°HiveQLè¯­æ³•ã€‚ ImpalaåŸºäºMPPï¼ˆMassively Parallel Processingï¼‰[å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†]ç†å¿µçš„æŸ¥è¯¢å¼•æ“ï¼Œä»€ä¹ˆæ˜¯MPPï¼Ÿ MPPæ˜¯ä¸€ç§æµ·é‡æ•°æ®å®æ—¶åˆ†ææ¶æ„,MPPç†å¿µæ˜¯å°†ä»»åŠ¡å¹¶è¡Œçš„åˆ†æ•£åˆ°å¤šä¸ªæœåŠ¡å™¨å’ŒèŠ‚ç‚¹ä¸Šï¼Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—å®Œæˆåï¼Œå°†å„è‡ªéƒ¨åˆ†çš„ç»“æœæ±‡æ€»åœ¨ä¸€èµ·å¾—åˆ°æœ€ç»ˆçš„ç»“æœ ç‰¹ç‚¹ï¼š â— Shared Nothingæ¶æ„ï¼ˆæ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œè‡ªç»™çš„ï¼Œåœ¨ç³»ç»Ÿä¸­ä¸å­˜åœ¨å•ç‚¹ç«äº‰ï¼Œæ²¡æœ‰å…±äº«æ•°æ®ï¼‰ï¼Œç§æœ‰èµ„æº â— ä»»åŠ¡åˆ†å¸ƒå¼å¹¶è¡Œæ‰§è¡Œï¼ˆæ•°æ®æ— å…±äº«ï¼Œæ— IOå†²çªï¼Œæ— é”èµ„æºç«äº‰ï¼Œè®¡ç®—é€Ÿåº¦å¿«ï¼‰ â— æ•°æ®åˆ†å¸ƒå¼å­˜å‚¨(æœ¬åœ°åŒ–) â— æ¨ªå‘æ‰©å±•ï¼ˆæ˜“æ‰©å®¹ï¼‰; â— å•ä¸ªèŠ‚ç‚¹æŸ¥è¯¢æ•ˆç‡æ…¢ä¼šå½±å“æ•´ä¸ªæŸ¥è¯¢ï¼ˆå€¾æ–œï¼‰ é™„ï¼š Shared Everthingï¼šå®Œå…¨é€æ˜å…±äº« CPU/Memory/IOï¼Œå¹¶è¡Œå¤„ç†èƒ½åŠ›æ˜¯æœ€å·®çš„ Shared Storageï¼šå„ä¸ªå¤„ç†å•å…ƒä½¿ç”¨è‡ªå·±çš„ç§æœ‰CPU//Memoryï¼Œä½†å…±äº«ç£ç›˜ç³»ç»Ÿ Shared Nothingï¼šå„ä¸ªå¤„ç†å•å…ƒéƒ½æœ‰è‡ªå·±ç§æœ‰çš„CPU/Memory/IO åŸºäºSQLçš„è®¡ç®—å¼•æ“å¯¹æ¯” å¼•æ“ å¼€å‘è¯­è¨€ æ‰§è¡Œæœºåˆ¶ èµ„æºè°ƒåº¦ å†…å­˜åˆ†é… å®¹é”™ åœºæ™¯ Hive(MR) Java SQL-&gt;MR-&gt;Yarn-&gt;HDFS Yarnè°ƒåº¦ å†…å­˜ä¸å¤Ÿåˆ™ç”¨ç£ç›˜ Hadoopå®¹é”™æœºåˆ¶åŒ…æ‹¬é‡è¯•å’Œæ¨æµ‹æ‰§è¡Œ ç¦»çº¿åˆ†æå’Œè·‘æ‰¹ä»»åŠ¡ Spark Scala SQL-&gt;è®¡åˆ’-&gt;Yarn-&gt;HDFS æ”¯æŒå¤šç§è°ƒåº¦ ä¼˜å…ˆä½¿ç”¨å†…å­˜ä¸å¤Ÿåˆ™ç”¨ç£ç›˜ï¼Œå¯æ‰‹åŠ¨ Lineage+Checkpoint+å¤±è´¥é‡è¯• å…¼å®¹å¤šç§åœºæ™¯ Presto Java SQL-&gt;è®¡åˆ’-&gt;Workers-&gt;HDFS è‡ªèº« çº¯å†…å­˜è®¡ç®— æ— å®¹é”™è®¾è®¡ äº¤äº’å¼åˆ†ææŸ¥è¯¢ ClickHouse C++ SQL-&gt;è®¡åˆ’-&gt;è¯»å­˜å‚¨å¼•æ“æ•°æ®å‘é‡åŒ–æ‰§è¡Œ è‡ªèº« å†…å­˜+è¿ç»­IO å¤šä¸»æœºè·¨æ•°æ®ä¸­å¿ƒå¼‚æ­¥å¤åˆ¶ï¼Œä¸æ€•èŠ‚ç‚¹å®•æœº å­˜å‚¨æ•°æ®åº“+OLAPåˆ†æçš„åœºæ™¯ Impala C++ SQL-&gt;è®¡åˆ’-&gt;HDFS è‡ªèº« çº¯å†…å­˜è®¡ç®— æ— å®¹é”™è®¾è®¡ äº¤äº’å¼åˆ†ææŸ¥è¯¢ Impalaä¼˜ç¼ºç‚¹ä¼˜ç‚¹ï¼š åŸºäºå†…å­˜è®¡ç®—ï¼Œä½å»¶è¿Ÿï¼Œé«˜ååï¼ŒæŸ¥è¯¢é€Ÿåº¦å¿«ï¼Œé€‚ç”¨äºç§’çº§å“åº”çš„OLAPäº¤äº’å¼åˆ†ææŸ¥è¯¢ æä¾›çª—å£å‡½æ•°ï¼ˆèšåˆ OVER PARTITION, RANK, LEAD, LAG, NTILEç­‰ç­‰ï¼‰ä»¥æ”¯æŒé«˜çº§åˆ†æåŠŸèƒ½ æ”¯æŒPBçº§æ•°æ®é‡çš„å®æ—¶åˆ†æ æ”¯æŒmapã€structã€arrayç±»å‹ä¸Šçš„å¤æ‚åµŒå¥—æŸ¥è¯¢ï¼Œæ”¯æŒUDFå’ŒUDAF å¯ä»¥ä½¿ç”¨Impalaæ’å…¥æˆ–æ›´æ–°HBaseï¼ˆç±»ä¼¼Phoenixï¼‰ æ”¯æŒParquetã€Avroã€Textã€RCFileã€SequenceFileã€HFileç­‰å¤šç§æ–‡ä»¶æ ¼å¼ï¼Œæ”¯æŒSnappyï¼ˆæœ‰æ•ˆå¹³è¡¡å‹ç¼©ç‡å’Œè§£å‹ç¼©é€Ÿåº¦ï¼‰ã€Gzipï¼ˆæœ€é«˜å‹ç¼©ç‡çš„å½’æ¡£æ•°æ®å‹ç¼©ï¼‰ã€Deflateï¼ˆä¸æ”¯æŒæ–‡æœ¬æ–‡ä»¶ï¼‰ã€Bzip2ã€LZOï¼ˆåªæ”¯æŒæ–‡æœ¬æ–‡ä»¶ï¼‰ç­‰å¤šç§å‹ç¼©ç¼–ç æ ¼å¼ æ”¯æŒå­˜å‚¨åœ¨HDFSã€HBaseã€S3ã€Kuduä¸Šçš„æ•°æ®æ“ä½œ ä¸CDHæ·±åº¦æ•´åˆï¼Œæ”¯æŒæŸ¥çœ‹æŸ¥è¯¢ä»»åŠ¡çš„å„é¡¹æŒ‡æ ‡ æ”¯æŒSentryå’ŒKerberos å±€é™æ€§ï¼š ä¸é€‚ç”¨äºè·‘æ‰¹ æŸ¥è¯¢æ—¶å ç”¨å¤§é‡å†…å­˜ ä¸æ”¯æŒORC â€œAnalysisException: Impala does not support modifying a non-Kudu tableâ€ ä¸æ”¯æŒéKuduè¡¨çš„Updateã€deleteæ“ä½œ ä¸æ”¯æŒDateæ•°æ®ç±»å‹ ImpalaåŸç†Impalaæ¶æ„å›¾ Impala Daemon&emsp;&emsp;Impalaçš„æ ¸å¿ƒè¿›ç¨‹Impaladï¼Œéƒ¨ç½²åœ¨æ‰€æœ‰çš„æ•°æ®èŠ‚ç‚¹ä¸Šï¼Œæ¥æ”¶å®¢æˆ·ç«¯çš„æŸ¥è¯¢è¯·æ±‚ï¼Œè¯»å†™æ•°æ®ï¼Œå¹¶è¡Œæ‰§è¡Œæ¥è‡ªé›†ç¾¤ä¸­å…¶ä»–èŠ‚ç‚¹çš„æŸ¥è¯¢è¯·æ±‚ï¼Œå°†ä¸­é—´ç»“æœè¿”å›ç»™è°ƒåº¦èŠ‚ç‚¹ã€‚è°ƒç”¨èŠ‚ç‚¹å°†ç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ã€‚Impaladè¿›ç¨‹é€šè¿‡æŒç»­ä¸StateStoreé€šä¿¡æ¥ç¡®è®¤è‡ªå·±æ‰€åœ¨çš„èŠ‚ç‚¹æ˜¯å¦å¥åº·ä»¥åŠæ˜¯å¦å¯ä»¥æ¥å—æ–°çš„ä»»åŠ¡è¯·æ±‚ã€‚ ImpaladåŒ…å«ä¸‰ç§è§’è‰²ï¼š Query Coordinatorï¼šç”¨æˆ·åœ¨Impalaé›†ç¾¤ä¸Šçš„æŸä¸ªèŠ‚ç‚¹æäº¤æ•°æ®å¤„ç†è¯·æ±‚ï¼ˆä¾‹å¦‚impala-shellæäº¤SQLï¼‰ï¼Œåˆ™è¯¥ImpaladèŠ‚ç‚¹ç§°ä¸ºCoordinator Nodeï¼ˆåè°ƒèŠ‚ç‚¹ï¼‰,è´Ÿè´£å®šä½æ•°æ®ä½ç½®ï¼Œæ‹†åˆ†è¯·æ±‚ï¼ˆFragmentï¼‰ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå¯å¹¶è¡Œæ‰§è¡Œçš„å°è¯·æ±‚ï¼Œå‘é€è¿™äº›è¯·æ±‚åˆ°å¤šä¸ªQuery Executorï¼Œæ¥æ”¶Query Executorå¤„ç†åè¿”å›çš„æ•°æ®å¹¶æ„å»ºæœ€ç»ˆç»“æœè¿”å›ç»™ç”¨æˆ·ã€‚ Query Plannerï¼šJavaç¼–å†™çš„ï¼Œè§£æSQLç”ŸæˆQueryPlanTreeæ‰§è¡Œè®¡åˆ’æ ‘ã€‚ Query Executorï¼šæ‰§è¡Œæ•°æ®è®¡ç®—ï¼Œæ¯”å¦‚scanï¼ŒAggregationï¼ŒMergeç­‰ï¼Œè¿”å›æ•°æ®ã€‚ Impala StateStore&emsp;&emsp;Statestoredè¿›ç¨‹ï¼ŒçŠ¶æ€ç®¡ç†è¿›ç¨‹ï¼ˆç±»ä¼¼ZKï¼‰ï¼Œå®šæ—¶æ£€æŸ¥Impala Daemonçš„å¥åº·çŠ¶å†µï¼Œåè°ƒå„ä¸ªè¿è¡ŒImpaladè¿›ç¨‹ä¹‹é—´çš„ä¿¡æ¯ï¼ŒImpalaé€šè¿‡è¿™äº›ä¿¡æ¯å»å®šä½æŸ¥è¯¢è¯·æ±‚æ‰€è¦çš„æ•°æ®ï¼Œå¦‚æœImpalaèŠ‚ç‚¹ä¸‹çº¿ï¼ŒStateStoreä¼šé€šçŸ¥å…¶ä»–èŠ‚ç‚¹ï¼Œé¿å…æŸ¥è¯¢ä»»åŠ¡åˆ†å‘åˆ°ä¸å¯ç”¨çš„èŠ‚ç‚¹ä¸Šã€‚(å®šä½Impalaå¡ä½ä»¥åŠå„ç§å¼‚å¸¸çŠ¶å†µï¼Œå¯ä»¥å…ˆä»ImpalaStateStoreä¸‹æ‰‹ï¼ŒæŸ¥çœ‹StateStoreæ—¥å¿—æ–¹ä¾¿å®šä½é—®é¢˜) Impala Catalog Service&emsp;&emsp;Catalogdè¿›ç¨‹ï¼Œå…ƒæ•°æ®ç®¡ç†æœåŠ¡ï¼Œæ”¶é›†Hiveç­‰ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼Œå°†æ•°æ®è¡¨å˜åŒ–çš„ä¿¡æ¯åˆ†å‘ç»™å„ä¸ªè¿›ç¨‹ã€‚æ¥æ”¶æ¥è‡ªStateStoreçš„æ‰€æœ‰è¯·æ±‚ï¼Œæ¯ä¸ªImpalaèŠ‚ç‚¹åœ¨æœ¬åœ°ç¼“å­˜æ‰€æœ‰å…ƒæ•°æ®ã€‚å½“è¡¨åˆ›å»ºã€æ•°æ®æ›´æ–°æˆ–Schemaå‘ç”Ÿå˜åŒ–æ—¶ï¼Œå…¶ä»–Impalaåå°è¿›ç¨‹å¿…é¡»æ›´æ–°å…ƒæ•°æ®ç¼“å­˜ï¼Œæ‰èƒ½æŸ¥è¯¢ã€‚ Schemaå˜åŒ–æ—¶ï¼ˆHiveæ“ä½œcreate table/drop table/alter table add columnsï¼‰ä½¿ç”¨ï¼šinvalidate metadata //é‡æ–°åŠ è½½æ‰€æœ‰åº“ä¸­çš„æ‰€æœ‰è¡¨ï¼ˆä¸æ¨èï¼Œè¿˜ä¸å¦‚é‡å¯Catalogdè¿›ç¨‹ï¼‰invalidate metadata [table] //é‡æ–°åŠ è½½æŒ‡å®šçš„æŸä¸ªè¡¨ æ•°æ®å˜åŒ–æ—¶ï¼ˆHiveæ“ä½œinsert intoã€load dataã€alter table add partitionã€Alter table drop partitionæˆ–HDFSå¢åˆ é‡å‘½åæ–‡ä»¶ï¼‰ä½¿ç”¨ï¼šrefresh [table] //åˆ·æ–°æŸä¸ªè¡¨refresh [table] partition [partition] //åˆ·æ–°æŸä¸ªè¡¨çš„æŸä¸ªåˆ†åŒº æ³¨æ„ï¼šinvalidateä¼šæ¸…é™¤è¡¨çš„ç¼“å­˜å¹¶ä»MetaStoreé‡æ–°åŒæ­¥å…ƒæ•°æ®ï¼Œä»£ä»·è¾ƒå¤§ï¼›refreshä¼šé‡ç”¨ä¹‹å‰çš„å…ƒæ•°æ®ï¼Œä»…ä»…æ‰§è¡Œæ–‡ä»¶åˆ·æ–°æ“ä½œï¼Œå®ƒèƒ½å¤Ÿæ£€æµ‹åˆ°è¡¨ä¸­åˆ†åŒºçš„å¢åŠ å’Œå‡å°‘ï¼Œä»£ä»·ç›¸å¯¹å°äº› Impala joinç®—æ³•1.HashJoinï¼Œç­‰å€¼Joiné‡‡ç”¨Hashç®—æ³•è¿›è¡ŒJoinï¼Œå…·ä½“åˆ†ä¸ºBroadcast Hash Joinå’ŒShuffle Hash Joinã€‚Boradcast Joiné€‚åˆå³è¡¨æ˜¯å°è¡¨çš„æƒ…æ™¯ï¼ŒImpalaä¼šå¹¿æ’­å°è¡¨åˆ°å„ä¸ªèŠ‚ç‚¹ï¼Œå†å…³è”ã€‚Shuffle Joiné€‚åˆå¤§è¡¨ä¸å¤§è¡¨Joinçš„æƒ…æ™¯ï¼ŒImpalaä¼šå°†å¤§è¡¨åˆ’åˆ†æˆå¤šå—ï¼Œç„¶ååˆ†åˆ«è¿›è¡ŒHash Joinã€‚2.Nested Loop Joinï¼Œéç­‰å€¼Joinä½¿ç”¨ï¼Œéç­‰å€¼Joinæ•ˆç‡ä½ï¼Œä¸æ”¯æŒHintã€‚ Impala Query Hintè¯­æ³•ï¼š SELECT STRAIGHT_JOIN select_list FROM join_left_hand_table JOIN [&#123; /* +BROADCAST */ | /* +SHUFFLE */ &#125;] join_right_hand_table remainder_of_query; -- -------------------------------------- INSERT insert_clauses [&#123; /* +SHUFFLE */ | /* +NOSHUFFLE */ &#125;] [/* +CLUSTERED */] SELECT remainder_of_query; -- -------------------------------------- SELECT select_list FROM table_ref /* +&#123;SCHEDULE_CACHE_LOCAL | SCHEDULE_DISK_LOCAL | SCHEDULE_REMOTE&#125; [,RANDOM_REPLICA] */ remainder_of_query; Hintä¼šæ”¹å˜SQLçš„æ‰§è¡Œè®¡åˆ’ï¼Œä½¿ç”¨Hintæ³¨æ„äº‹é¡¹ï¼š æœ‰ä¸¤ä¸ªåœ°æ–¹éœ€è¦åŠ ä¸Šhintå…³é”®å­—ï¼Œselectåé¢åŠ ä¸ŠSTRAIGHT_JOINï¼›joinåé¢åŠ ä¸Š[shuffle]æˆ–è€…/* +shuffle */ å¦‚æœæ˜¯å¤šå±‚åµŒå¥—çš„joinæ–¹å¼ï¼Œä¹Ÿéœ€è¦åœ¨æ¯ä¸€å±‚åŠ ä¸ŠSTRAIGHT_JOINå’Œ[shuffle]æˆ–è€…/* +shuffle */ å¤–å±‚çš„hintå¯¹äºå†…å±‚çš„joinå­è¯­å¥æ˜¯ä¸èµ·ä½œç”¨çš„ å¦‚æœselectåé¢è·Ÿdistinctä¹‹ç±»çš„å…³é”®å­—ï¼ŒSTRAIGHT_JOINéœ€è¦è·Ÿåœ¨å…³é”®å­—åé¢ä¸åŒHintæ ‡ç­¾çš„å…·ä½“å«ä¹‰å’Œåœºæ™¯è§æ–‡æ¡£:impala_hints åœ¨CDHä½¿ç”¨ImpalaImpalaç›¸å…³è¿›ç¨‹ï¼šæ³¨æ„ï¼šè€ƒè™‘é›†ç¾¤æ€§èƒ½ï¼Œä¸€èˆ¬å°†StateStoreä¸CatalogServiceæ”¾åœ¨åŒä¸€èŠ‚ç‚¹ä¸Šï¼Œå› ä¹‹é—´è¦åšé€šä¿¡ åœ¨StateStoreçš„WEBUI http://cdh101:25010/ å¯ä»¥æŸ¥çœ‹Impalaé›†ç¾¤ç›‘æ§çŠ¶æ€å’Œé…ç½®ä¿¡æ¯ï¼š åœ¨Catalogçš„WEBUI http://cdh101:25020/ å¯ä»¥çœ‹åˆ°å„ä¸ªåº“è¡¨å…ƒæ•°æ®ä¿¡æ¯ã€SchemaåŠå ç”¨å†…å­˜å¤§å° åœ¨Impala Daemonçš„WEBUI http://cdh102:25000/ å¯ä»¥çœ‹åˆ°è¯¥è¿›ç¨‹ä¿¡æ¯ åœ¨http://cdh102:25000/queries å¯ä»¥æŸ¥çœ‹è¯¥èŠ‚ç‚¹æ‰§è¡ŒSQLçš„è¯¦æƒ… åœ¨CDH Impalaç»„ä»¶ä¸­å¯ä»¥æŸ¥çœ‹æ‰§è¡ŒSQLä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ ä¸€äº›é‡è¦çš„å¸¸ç”¨çš„impala-shellä½¿ç”¨å‘½ä»¤ impala-shell -i hostæŒ‡å®šCoordinator -d æŒ‡å®šè¿æ¥åˆ°å“ªä¸ªåº“ -q &quot;select ...&quot; ä¸è¿›å…¥impala-shellç›´æ¥æŸ¥è¯¢æŸè¯­å¥è¿”å›ç»“æœåˆ°å‘½ä»¤è¡Œ -f file æ‰§è¡Œæ–‡ä»¶ä¸­çš„sql -p è·å–æ‰§è¡Œè®¡åˆ’ -o ä¿å­˜æ‰§è¡Œç»“æœåˆ°æ–‡ä»¶ -h å¸®åŠ© -r (--refresh_after_connect) å»ºç«‹è¿æ¥ååˆ·æ–° Impala å…ƒæ•°æ® -c æŸ¥è¯¢æ‰§è¡Œå¤±è´¥æ—¶ç»§ç»­æ‰§è¡Œ -Bï¼ˆ--delimitedï¼‰ å»æ ¼å¼åŒ–è¾“å‡º --output_delimiter=character æŒ‡å®šåˆ†éš”ç¬¦ --print_header æ‰“å°åˆ—å è¾“å‡ºå»ºè¡¨è¯­å¥ï¼šimpala-shell -i impalad:21000 -q &quot;show create table db.table&quot; -B --output_delimiter=&quot;\\t&quot; è¿›å…¥Impala-shellåï¼š explain &lt;sql&gt; æ˜¾ç¤ºæ‰§è¡Œè®¡åˆ’ shell &lt;shell&gt; ä¸é€€å‡ºimpala-shellæ‰§è¡Œç³»ç»Ÿå‘½ä»¤ profile åˆ†æä¸Šä¸€æ¡Queryæ‰§è¡Œï¼Œä»¥ä¾¿äºæ€§èƒ½è°ƒä¼˜ï¼ˆæ¯”-pæ›´å¤šçš„ä¿¡æ¯ï¼‰ refresh &lt;tablename&gt; å¢é‡åˆ·æ–°å…ƒæ•°æ®åº“ invalidate metadata å…¨é‡åˆ·æ–°å…ƒæ•°æ®åº“ï¼ˆæ…ç”¨ï¼‰ï¼ˆåŒäº impala-shell -rï¼‰ history å†å²å‘½ä»¤ ------------------------------------------------------------- ç¤ºä¾‹1:è·å–ä¸€å¼ è¡¨çš„å®Œæ•´å»ºè¡¨è¯­å¥ impala-shell -i 192.168.1.101:21000 -q &quot;show create table default.xxxxx&quot; -B --output_delimiter=&quot;\\t&quot; ç¤ºä¾‹2:æ‰§è¡Œå¤šæ¡SQL impala-shell -i 192.168.1.101:21000 -q &quot;drop table if exists default.xxxxx;set PARQUET_FILE_SIZE=128m;create table default.xxxxx_new stored as parquet as select * from default.xxxxx where time &gt; &#39;2021-12-01 00:00:00.000000000&#39; and time &lt;= &#39;2021-12-02 00:00:00.000000000&#39;;select count(1) from default.xxxxx_new&quot; ç¤ºä¾‹3ï¼šåå°æ‰§è¡Œsqlæ–‡ä»¶ä¸­çš„æ‰€æœ‰sql nohup impala-shell -i 192.168.1.101:21000 -f /tmp/export.sql &amp; æœ€ä½³å®è·µ æ–‡ä»¶æ ¼å¼æ¨èparquetï¼ŒæŸ¥è¯¢æ•ˆç‡é«˜ é¿å…ç¢ç‰‡æ–‡ä»¶ï¼Œæ³¨æ„æ–‡ä»¶çš„å¤§å° æ ¹æ®å®é™…çš„æ–‡ä»¶å¤§å°å’Œä¸ªæ•°é€‰æ‹©åˆ†åŒºçš„ç²’åº¦ åˆ†åŒºkeyé€‰æ‹©æœ€å°çš„æ•´æ•°ç±»å‹ä»£æ›¿å­—ç¬¦ä¸²ç±»å‹ï¼Œé™ä½å…ƒæ•°æ®å ç”¨å†…å­˜å¤§å° ä½¿ç”¨COMPUTE STATSå‘½ä»¤è¿›è¡Œè¡¨ã€åˆ†åŒºçš„æ€§èƒ½åˆ†æï¼ˆæ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼‰ï¼Œæé«˜è¡¨çš„æŸ¥è¯¢æ•ˆç‡ COMPUTE STATS [db_name.]table_name COMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)] æœ€å°åŒ–è¿”å›Clientç«¯çš„æ•°æ®é‡ ä½¿ç”¨explain+SQLå‘½ä»¤ç¡®è®¤æ‰§è¡Œè®¡åˆ’æ˜¯å¦é«˜æ•ˆ æ‰§è¡ŒæŸ¥è¯¢åä½¿ç”¨summaryå‘½ä»¤ç¡®è®¤ç¡¬ä»¶æ¶ˆè€—ï¼ˆç‰©ç†æ€§èƒ½ç‰¹æ€§ï¼‰ï¼Œè¾“å‡ºçš„ä¿¡æ¯åŒ…æ‹¬å“ªä¸ªé˜¶æ®µè€—æ—¶æœ€å¤šï¼Œä»¥åŠæ¯ä¸€é˜¶æ®µä¼°ç®—çš„å†…å­˜æ¶ˆè€—ã€è¡Œæ•°ä¸å®é™…çš„å·®å¼‚ æ‰§è¡ŒæŸ¥è¯¢åä½¿ç”¨profileå‘½ä»¤æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½ä¿¡æ¯ï¼Œè¾“å‡ºçš„ä¿¡æ¯åŒ…æ‹¬å†…å­˜ã€CPUã€I/Oä»¥åŠç½‘ç»œæ¶ˆè€—çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯æ ¹æ®è¯¥ä¿¡æ¯è¿›è¡Œè°ƒä¼˜ ä½¿ç”¨profileæŸ¥çœ‹æ˜¯å¦æœ‰hdfså—å€¾æ–œï¼Œåˆç†åˆ†é…blockå¤§å° å……åˆ†åˆ©ç”¨Impala Query Hintä¼˜åŒ–æŸ¥è¯¢æ•ˆç‡ Joinæ—¶å¤§è¡¨æ”¾åœ¨æœ€å·¦é¢ï¼›æ•ˆç‡æœ€é«˜çš„Joinæ”¾åœ¨æœ€å‰é¢ï¼›å®šæœŸå¯¹è¡¨æ”¶é›†ç»Ÿè®¡ä¿¡æ¯, æˆ–è€…åœ¨å¤§é‡DMLæ“ä½œåä¸»åŠ¨æ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼›å•æ¡SQLçš„Joinæ•°å°½é‡ä¸è¶…è¿‡4å¦åˆ™æ•ˆç‡ä½ä¸‹ impaladæ— æ³•å¯åŠ¨EERRORæ˜¯WebServer: Could not start on address 0.0.0.0:25000 è§£å†³ï¼š lsof -i :25000 æ‹¿åˆ°PIDå¹¶killè¿™ä¸ªPID èŠ‚ç‚¹è®¿é—®è´Ÿè½½å‡è¡¡â€“ä¹‹å‰æåˆ°æ¯ä¸ªImpaladéƒ½æ˜¯Coordinatorï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½æäº¤åˆ°ä¸€å°Coordinatorä¼šå­˜åœ¨å•ç‚¹æ€§èƒ½åŠå•ç‚¹æ•…éšœé—®é¢˜ï¼Œæ‰€ä»¥ä¸ºæé«˜ä»»åŠ¡æäº¤ååé‡é¿å…å•ç‚¹é—®é¢˜ï¼Œå¯é…ç½®è´Ÿè½½å‡è¡¡çš„åœ°å€è®¿é—®impalaDaemon,å‚è€ƒUsing Impala through a Proxy for High Availabilityå®˜æ–¹æ–‡æ¡£æˆ–Configuring Impala Load Balanceråšå®¢ï¼Œé…ç½®å¦‚ä¸‹ï¼š yum -y install haproxy cp /etc/haproxy/haproxy.cfg /etc/haproxy/impala_haproxy.cfg vim /etc/haproxy/impala_haproxy.cfg æ³¨é‡Šæ‰main frontend which proxys to the backendsåé¢çš„æ‰€æœ‰å†…å®¹ æ³¨é‡Šæ‰å¦‚ä¸‹è¡Œï¼š timeout http-requests 10s timeout queue 1m timeout http-keep-alive 10s timeout check 10s ä¿®æ”¹å¦‚ä¸‹è¡Œ: timeout connect 5000 timeout client 3600s timeout server 3600s å¢åŠ å¦‚ä¸‹è¡Œï¼š listen stats :25002 balance mode http stats enable stats auth username:password listen impala :21001 mode tcp option tcplog balance leastconn server impalad_1 impalad1_ip:21000 check server impalad_2 impalad2_ip:21000 check server impalad_3 impalad3_ip:21000 check server impalad_4 impalad4_ip:21000 check server impalad_5 impalad5_ip:21000 check # server alias_name impaladIP:21000 check listen impalajdbc :21051 mode tcp option tcplog balance source server impalad_jdbc_01 impalad1_ip:21050 check server impalad_jdbc_02 impalad2_ip:21050 check server impalad_jdbc_03 impalad3_ip:21050 check server impalad_jdbc_04 impalad4_ip:21050 check server impalad_jdbc_05 impalad5_ip:21050 check # server alias_name impaladIP:21050 check ä¿å­˜é€€å‡ºwq è¿è¡Œhaproxy â€“f /etc/haproxy/impala_haproxy.cfg è®©ä»£ç†ç”Ÿæ•ˆ impala-shell -i haproxy_running_ip:21001 å¯è¿æ¥ä»£ç† jdbcç¨‹åºåŸæ¥è¿æ¥21050æ”¹ä¸ºè¿æ¥21051,åœ°å€æ”¹ä¸ºhaproxy_running_ip ä¿è¯haproxyè¿è¡Œç¨³å®šè¦åšçš„ç¾å¤‡å·¥ä½œ 1.å°†ä»£ç†ç¨‹åºåŠ å…¥å¼€æœºè‡ªåŠ¨è¿è¡Œï¼šrootç”¨æˆ·ä¸‹chmod +x /etc/rc.d/rc.localå¹¶å‘è¯¥æ–‡ä»¶æ·»åŠ /sbin/haproxy â€“f /etc/haproxy/impala_haproxy.cfg 2.ä»£ç†ç¨‹åºè‹¥å› æç«¯æƒ…å†µæŒ‚æ‰ï¼Œå†™ä¸ªè‡ªåŠ¨æ‹‰èµ·è„šæœ¬ä»¥ä¿è¯æœåŠ¡ï¼ˆroot@cdh01 crontab */1 * * * * sh /app/impala/auto_impala_haproxy.shï¼‰ é™åˆ¶Impalaç”ŸæˆParquetæ–‡ä»¶å¤§å° æ€»ç»“ï¼š Impalaæ˜¯å…¸å‹çš„MPPæ¶æ„å®æ—¶æŸ¥è¯¢åˆ†æå¼•æ“ï¼Œç±»ä¼¼çš„å¼•æ“è¿˜æœ‰ClickHouse Impalaéå¸¸é€‚åˆå³æ—¶æŠ¥è¡¨å±•ç¤ºçš„åœºæ™¯ ä½¿ç”¨Impalaä¸€å®šè¦æ³¨æ„å…ƒæ•°æ®ç¼“å­˜é—®é¢˜ä»¥åŠæ‰€æŸ¥è¯¢çš„è¡¨æ–‡ä»¶ä¸ªæ•°ã€å€¾æ–œé—®é¢˜ï¼Œå¦åˆ™ä¼šä¸¥é‡æ‹–æ…¢æ•ˆç‡ å¤šå‚è€ƒä»¥ä¸Šæœ€ä½³å®è·µéƒ¨åˆ† ç›¸å…³é“¾æ¥Impala-3.4 PDF DocumentImpalaä»‹ç»ä»¥åŠä¼˜åŠ£Cloudera Impala WikiImpala Github repositoryImpalaæ¶æ„","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"å®æ—¶SQLæŸ¥è¯¢å¼•æ“","slug":"å®æ—¶SQLæŸ¥è¯¢å¼•æ“","permalink":"https://shmily-qjj.top/tags/%E5%AE%9E%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%BC%95%E6%93%8E/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"æˆ‘çš„2020å¹´åº¦æ€»ç»“åšå®¢","slug":"æˆ‘çš„2020å¹´åº¦æ€»ç»“åšå®¢","date":"2021-01-01T00:08:08.000Z","updated":"2022-12-11T05:35:07.920Z","comments":true,"path":"2020Summary/","link":"","permalink":"https://shmily-qjj.top/2020Summary/","excerpt":"","text":"æˆ‘çš„2020å¹´åº¦æ€»ç»“2020æ˜¯å¾ˆç‰¹åˆ«çš„ä¸€å¹´ï¼Œç‹¬ç«‹å®Œæˆäº†æ¯•ä¸šè®¾è®¡ï¼ŒçœŸæ­£ä»å¤§å­¦æ¯•ä¸šäº†ï¼Œæ¥åˆ°äº†ç¦»å®¶å¾ˆè¿œçš„æ·±åœ³å·¥ä½œï¼Œè€Œä¸”åˆè®¤è¯†äº†ä¸€ç¾¤æ–°çš„ä¼™(dou)ä¼´(bi)ï¼Œå¦‚æœè¯´å»å¹´å¤´å‘æ˜¯å°‘äº†å‡ æ ¹ï¼Œé‚£ä»Šå¹´å°±æ˜¯å°‘äº†å‡ æ’®äº†å§â€¦å½“ç„¶2020è¿˜æ˜¯æœ‰äº›è®¸çš„é—æ†¾ä¸ä¸è¶³â€¦æˆ‘çš„2019å¹´åº¦æ€»ç»“åšå®¢ä¸­æœ‰å†™åˆ°å¯¹è‡ªå·±2020å¹´çš„ä¸€äº›æœŸæœ›ï¼Œè€Œ2020çœŸæ­£åšåˆ°äº†å“ªäº›ï¼Œåˆæœ‰å“ªäº›æ²¡åšåˆ°å‘¢ï¼Ÿ(åƒç“œç¾¤ä¼—:å¥½åƒå¥½å¤šä½ éƒ½æ²¡åšåˆ°å§!!!)ä¸ç®¡åšåˆ°å¤šå°‘ï¼Œæ²¡åšåˆ°å¤šå°‘ï¼Œéƒ½è¦æ„Ÿè°¢2020ï¼Œè®©æˆ‘è®¤æ¸…è‡ªå·±ï¼Œæ›´åŠ æˆç†Ÿã€‚2020æœ‰ç€å¾ˆå¤šå€¼å¾—å›å¿†å’Œè®°å½•çš„äº‹ï¼Œè€Œ2021åˆæœ‰å¾ˆå¤šæ–°çš„æœŸå¾…ï¼Œè®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹å§ã€‚è¿˜æœ‰ï¼Œä»Šå¹´ä½³å¢ƒç»ˆäºæ˜¯æœ‰äººé™ªè·¨å¹´çš„ä½³å¢ƒäº†ï¼Œæ„Ÿè°¢åº·å¯ŒåŒå­¦ï¼Œé™ªæˆ‘ä¸€èµ·è·¨å¹´ï¼Œåšæµ·é²œå¤§é¤ç»™æˆ‘åƒï¼åº·è€æ¿666ï¼å¥—è·¯è€æ¿å‘çº¢åŒ…ï¼Œè¿˜æ˜¯ä½ å¼ºï¼ä¸»æŒäºº å›é¡¾2020ä¸»æŒäººï¼šæ¬¢è¿æ¥åˆ°ã€Šå›é¡¾2020ã€‹èŠ‚ç›®çš„ç°åœºï¼Œä½³å¢ƒåŒå­¦ï¼ä½ è¿˜è®°å¾—æˆ‘å—ï¼Ÿå»å¹´æˆ‘ä»¬åœ¨ã€Šå›é¡¾2019ã€‹èŠ‚ç›®ä¸­è§è¿‡ã€‚æˆ‘ï¼šæˆ‘è®°å¾—ä½ ï¼ä¸»æŒäººï¼š2020ä½ ä¸€å®šåƒå¾—å¾ˆå¥½å§ï¼Ÿæˆ‘ï¼šæˆ‘æ€€ç–‘ä½ åœ¨è¯´æˆ‘å˜æœˆåŠäº†ï¼ä¸»æŒäººï¼šä¸€ä¸ªè¯ç®€å•æ€»ç»“ä¸‹2020å§ã€‚æˆ‘ï¼šEmmmï¼Œç”¨è·Œå®•èµ·ä¼æ¥å½¢å®¹æ¯”è¾ƒåˆé€‚ã€‚ä¸»æŒäººï¼šä½ è¯´2020æ˜¯è·Œå®•èµ·ä¼çš„ä¸€å¹´ï¼Œé‚£è®²ä¸€ä¸‹ä½ éƒ½ç»å†äº†ä»€ä¹ˆå§ã€‚æˆ‘ï¼šä¸‰æœˆï¼Œè¾æ‰äº†å®ä¹ å·¥ä½œï¼Œæƒ³æ‰¾ä¸ªæ›´å¥½çš„æœºä¼šï¼Œä½†ç–«æƒ…ä¸‹æ‰¾å·¥ä½œå¾ˆä¸æ–¹ä¾¿ï¼Œåªèƒ½é€šè¿‡è¿œç¨‹â€¦æ‰¾å·¥ä½œçš„è¿‡ç¨‹ä¸­å‘ç°äº†è‡ªå·±çš„è–„å¼±é¡¹ï¼Œä¹Ÿç®—æ˜¯å¥½äº‹å§ã€‚ä½†è£¸è¾çœŸçš„è¿˜æŒºéš¾å—çš„ï¼Œæ„Ÿè§‰ä¼šç¬é—´é™·å…¥è¿·èŒ«ï¼Œå°¤å…¶æ˜¯ç–«æƒ…ä¹‹ä¸‹ï¼Œæ›´åŠ è¿·èŒ«ã€‚å››æœˆï¼Œé¢è¯•ï¼Œæ¯•ä¸šè®¾è®¡ï¼ŒåŒçº¿ç¨‹æ‰§è¡Œï¼Œé‚£æ®µæ—¶é—´æœ‰å‹åŠ›ï¼Œä½†æ˜¯çˆ¸å¦ˆç»™äº†æˆ‘å¾ˆå¤šé¼“åŠ±ï¼Œæ‰è®©æˆ‘åœ¨é‡å‹ä¸‹å¿ƒæƒ…ä¹Ÿèƒ½å¾—åˆ°æ”¾æ¾ï¼Œæ„Ÿè°¢ä»–ä»¬ï¼ä¹°äº†ä¸€ä¸‡å¤šå—é’±çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œè‚‰ç–¼å•Šâ€¦äº”æœˆï¼Œå­¦ä¹ ä¹‹ä½™ï¼Œé™ªçˆ¸å¦ˆçœ‹ç”µè§†å‰§ï¼Œä¸€èµ·å‡ºå»æºœè¾¾ï¼Œå»åº—é‡Œå¸®å¿™ï¼Œä¸€èµ·åšé¥­â€¦æƒ³åˆ°å¾ˆå¿«è¦å»å·¥ä½œäº†ï¼Œé™ªä»–ä»¬çš„æ—¶é—´è¶Šæ¥è¶Šå°‘ï¼Œè™½ç„¶åœ¨å®¶æœ‰æ—¶ä¼šè·Ÿä»–ä»¬åµï¼Œå‘è„¾æ°”ï¼Œä½†ç¦»å¼€äº†åˆåæ‚”è®©ä»–ä»¬ç”Ÿæ°”ã€‚äººéƒ½æ˜¯è¿™æ ·çŸ›ç›¾å§ã€‚è¿™ä¸ªæœˆæœ«ï¼Œæ¯•ä¸šè®¾è®¡ä¹Ÿæ‹‰ä¸‹äº†å¸·å¹•ï¼Œæˆ‘çš„æ¯•è®¾è¿˜è¢«æ¬¢å“¥ï¼ˆæ¯•è®¾å¯¼å¸ˆï¼‰è¯„ä¸ºä¼˜ç§€ç­‰çº§ï¼Œå­¦æ ¡è¿˜å·å·åœ°å¥–åŠ±äº†400å—é’±ï¼Œè¿™æ˜¯æˆ‘å­¦æ ¡å·¥è¡Œå¡çš„æœ€åä¸€ç¬”å…¥è´¦ï¼Œä¸€å¼€å§‹é’±çœ‹æ¥è‡ªå“ˆç†å·¥ï¼Œéƒ½æ‡µäº†ï¼Œå­¦æ ¡ä½•æ—¶å¦‚æ­¤å¤§æ–¹ï¼Œåæ¥æ‰çŸ¥é“æ˜¯æ¯•è®¾å¥–åŠ±â€¦å…­æœˆï¼Œå¯èƒ½æ˜¯è¿™ä¸€å¹´ä¸­æœ€æ¸…é—²çš„ä¸€ä¸ªæœˆäº†ï¼Œæƒ³åˆ°è¦ç¦»å¼€å®¶å»é‚£ä¹ˆè¿œçš„æ·±åœ³ï¼ŒæŠ“ç´§æ—¶é—´å†è·Ÿå°ä¼™ä¼´ä»¬èšä¸€èšï¼Œèšï¼Œæ— å¤–ä¹å°±æ˜¯æ°é¥­ã€ç”µå½±ã€KTVã€æºœè¾¾å’Œæ‰“æ¸¸æˆï¼Œæœ‰ç‚¹å°é—æ†¾å°±æ˜¯æ²¡çæƒœè¿™æ®µæ—¶é—´æ¥ä¸€æ¬¡è¯´èµ°å°±èµ°çš„æ¯•ä¸šæ—…è¡Œï¼Œä½†æ›´å¤§çš„é—æ†¾æ˜¯å› ä¸ºç–«æƒ…ï¼Œç›´åˆ°æ¯•ä¸šä¹Ÿæ²¡èƒ½å›åˆ°å¤§å­¦æ ¡å›­ï¼Œè·Ÿé‚£å‡ ä¸ªç‹æœ‹ç‹—å‹å†èšä¸€ä¸‹ï¼Œå»å¹´æ›¾çº¦å¥½å›å»åè¦è¸©ç€å•¤é…’ç®±å­å–ï¼Œä½ ä»¬è¯´è¦æŠŠæˆ‘å µåœ¨å®¿èˆé—¨å£å–å®Œå†è®©è¿›å»ï¼Œä½†æœ€åå®¿èˆæ˜¯è€å¸ˆæ”¶æ‹¾çš„ï¼Œä¸ªäººç‰©å“ä¹Ÿæ˜¯è€å¸ˆé‚®å¯„çš„ï¼Œæˆ‘ä»¬è°éƒ½æ²¡èƒ½å›å»ï¼Œä¸‹æ¬¡å†ç›¸èšä¸çŸ¥é“æœ‰å¤šéš¾ã€‚20å·ï¼Œåç»¿çš®ç«è½¦æ¥æ·±åœ³ï¼Œ29ä¸ªå°æ—¶çš„ç¡¬å§ï¼Œè¿˜å¥½æ˜¯å¦ˆå¦ˆé€æˆ‘æ¥çš„ï¼Œä¸€è·¯ä¸Šæœ‰é™ªä¼´ï¼Œè¿˜æœ‰å¸¦äº†çˆ¸çˆ¸åšçš„çƒ¤è‚‰ã€‚è¿™æ®µæ¼«é•¿çš„æ—…é€”ä¸Šï¼Œé™¤äº†é™é™èººä¸‹æ€è€ƒäººç”Ÿå’Œçœ‹è½¦çª—å¤–çš„é£æ™¯ï¼Œæ›´å¤šçš„æ˜¯ç¦»å®¶çš„ä¸èˆï¼Œè¦çŸ¥é“æˆ‘ä¹‹å‰çš„äºŒåäºŒå¹´éƒ½æ˜¯ä»åŒ—æ–¹åº¦è¿‡çš„ï¼Œéšç€ç»¿çš®è½¦å’šå’šå£°è¶Šæ¥è¶Šè¿œçš„ï¼Œæ˜¯å®¶é—¨å£é‚£æ¡å¤§å‡Œæ²³ï¼Œæ˜¯ä¸çœ çš„å¤œçª—å¤–çš„é›ªï¼Œæ˜¯æµ·æ²³è·¯é‚£æ¡ç ´çƒ‚çš„å¤§è¡—ï¼Œæ˜¯é‚£ä¸€ç¾¤ç‹æœ‹ç‹—å‹â€¦ä¸ƒæœˆï¼Œå¯¹æ·±åœ³å¾ˆé™Œç”Ÿï¼Œæ„Ÿè§‰åœ¨è¿™è¾¹å¾ˆéš¾æ‰¾åˆ°ä»¥å‰çš„ç©ä¼´äº†ï¼Œæ²¡æƒ³åˆ°å¥½å‡ ä¸ªå¤§å­¦åŒå­¦å’Œå¤§å­¦æ ¡å‹åœ¨è¿™è¾¹ï¼Œå¯ä»¥ä¸€èµ·å»ç©ï¼Œè¿˜å‚åŠ äº†å“ˆç†å·¥æ ¡å‹èšä¼šæ´»åŠ¨ï¼Œè€Œä¸”å…¥èŒåå‘ç°å…¬å¸é‡Œä¹Ÿæœ‰è€ä¹¡ï¼Œé¡¿æ—¶è§‰å¾—å¿ƒé‡Œè¸å®å¤šäº†ã€‚åˆšå…¥èŒæ–°å…¬å¸ä¸ä¹…ï¼Œå¯¹å…¬å¸ä¸šåŠ¡å’Œå„ä¸ªæŠ€æœ¯éƒ¨é—¨ç»„æˆéƒ½ä¸ç†Ÿæ‚‰ï¼Œä¹Ÿæœ‰äº›è¿·èŒ«ï¼Œå¥½åœ¨é˜¿æœå’Œæ°å“¥çœ‹å¥½æˆ‘ï¼Œæˆ‘è§‰å¾—è‡ªå·±æŒºå¹¸è¿çš„ã€‚ç¬¬ä¸€æ¬¡å›¢å»ºåœ¨å…¬å¸é™„è¿‘åƒçš„ï¼Œä¹Ÿç†Ÿæ‚‰äº†æ–°åŒäº‹ã€‚å°±æ˜¯æ²¡æƒ³åˆ°åé¢å‡ æ¬¡å›¢å»ºéƒ½æ˜¯åƒçƒ§çƒ¤â€¦å“ˆå“ˆå“ˆï¼è¿™ä¸ªæœˆï¼Œå­¦ä¹ äº†Kuduå¹¶åœ¨å›¢é˜Ÿå†…éƒ¨åˆ†äº«ï¼Œå¯¹ä¸€è‡´æ€§ç®—æ³•Raftæœ‰äº†æ›´æ·±çš„ç†è§£ï¼Œç»™å¯¹æ¥ä½ éƒ¨é—¨åŸ¹è®­äº†é«˜æ•ˆPythonä»£ç çš„æ–¹æ¡ˆï¼Œç»Ÿä¸€æ•°æ®åˆ†æå¹³å°Linkisåˆ†å¸ƒå¼éƒ¨ç½²ã€è°ƒä¼˜å’Œè§£å†³äº†äº›BUGã€‚å…«æœˆï¼Œå¥½åƒæ˜¯å¾ˆå¹³å¹³æ·¡æ·¡çš„ä¸€ä¸ªæœˆå“¦ï¼Œæƒ³ä¸èµ·æ¥å¯ä»¥è¯´å•¥ã€‚ä¹æœˆï¼Œåšäº†è‡ªå·±ä¹‹å‰æœªå°åšè¿‡çš„é¢†åŸŸï¼Œæ•°æ®æ²»ç†å’Œè¡€ç¼˜å…³ç³»ï¼Œä»0åˆ°1å®ç°äº†ä¸€ä¸ªè¡€ç¼˜æ”¶é›†é¡¹ç›®å¹¶ä¸Šçº¿äº†ï¼Œå¯¹è¡€ç¼˜æ”¶é›†ã€å›¾æ•°æ®åº“æœ‰äº†ä¸€å®šçš„äº†è§£å’Œå®è·µç»éªŒï¼Œå¯¹æ•°æ®æ²»ç†ä¹Ÿæœ‰äº†ä¸€äº›è‡ªå·±çš„è®¤è¯†ã€‚åæœˆï¼Œåä¸€é»„é‡‘å‘¨ï¼Œè¯·å¦ˆå¦ˆå»ä¸‰äºšç©ï¼ˆä¸‰äºšæ˜¯å¥¹ä¸€ç›´å¾ˆæƒ³å»çš„åœ°æ–¹ï¼‰ï¼Œæ‰“å¡èœˆæ”¯æ´²å²›ï¼Œä¸‰äºšåƒå¤æƒ…ï¼Œç«ç‘°è°·ï¼Œæ§Ÿæ¦”è°·ï¼Œå¤©æ¶¯æµ·è§’ï¼Œå¤§å°æ´å¤©ï¼Œåœ¨é¹¿å›å¤´çœ‹é¹¿åŸå…¨è²Œï¼Œåƒæµ·é²œï¼Œå–æœ€å¥½å–çš„å¥¶æ¤°æ±ï¼Œè¿˜æœ‰æ¸…è¡¥å‡‰ï¼Œæµ·å—å››å¤§åç²‰â€¦å¤ªå¼€å¿ƒäº†ï¼Œæ²¡è§è¿‡è¿™ä¹ˆç¾çš„æ™¯è‰²ï¼Œè€Œä¸”æ˜¯å¸¦ç€å¾ˆé‡è¦çš„äººâ€“å¦ˆå¦ˆä¸€èµ·ï¼Œå¦ˆå¦ˆä¹Ÿå¾ˆå¼€å¿ƒï¼Œåªæ˜¯è¿™ä¸€è¶Ÿæ—…é€”ä¸­ï¼Œæˆ‘æ˜æ˜¾æ„Ÿè§‰å¦ˆå¦ˆçœŸçš„è€äº†ï¼Œå¥½æƒ³å¤šå¸¦çˆ¸å¦ˆå‡ºå»æ—…æ¸¸ï¼Œä»–ä»¬ä¸ºæˆ‘æ“åŠ³å¤ªå¤šï¼Œä»˜å‡ºå¤ªå¤šï¼Œä¸€ç›´åœ¨è€å®¶é‚£è¾¹ï¼Œæ²¡æ—¶é—´å‡ºæ¥çœ‹çœ‹å¤–é¢çš„ä¸–ç•Œï¼Œè¿™ä¸ªæ˜å¹´è¦æèµ·æ¥ï¼é€å¦ˆå¦ˆå›è€å®¶ï¼Œå›å»ä¹Ÿæ˜¯åäº†ç»¿çš®è½¦ï¼Œä¹Ÿæ˜¯29å°æ—¶ï¼Œåªæ˜¯è¿™è¶Ÿæ²¡äººé™ªäº†ï¼Œé€å¥¹åˆ°è½¦ç«™çš„è·¯ä¸Šä¸¤ä¸ªäººéƒ½æ²‰é»˜ä¸è¯­ï¼Œä½†é€å¥¹åˆ·è„¸è¿›ç«™ï¼Œçœ‹åˆ°å¼±å°çš„èƒŒå½±æ…¢æ…¢æ¶ˆå¤±åœ¨èŒ«èŒ«äººæµ·ï¼Œæˆ‘å†…å¿ƒå¾ˆæ²‰é‡ï¼Œæˆ‘åˆ°å®¶ï¼Œç¯é»‘ç€ï¼Œå¾ˆå®‰é™ï¼Œçœ¼çœ¶å°±æ¹¿æ¶¦äº†ï¼Œå¦ˆå¦ˆè®©æˆ‘çœ‹çœ‹æ•å¤´åº•ä¸‹æœ‰å¥¹ç•™çš„å­—æ¡ï¼Œéƒ½æ˜¯é¼“åŠ±çš„è¯ï¼Œè¿˜æ˜¯æ²¡å¿ä½å“­äº†ä¸€ä¸‹ï¼Œè¿˜å¥½æ²¡äººçœ‹åˆ°â€¦â€¦éƒ¨é—¨æ¥äº†ä¸ªæ–°æ€»ç›‘ï¼Œåå“¥ï¼Œæˆ‘è§‰å¾—ä»–äººæŒºå¥½çš„ï¼Œç»™æ•´ä¸ªéƒ¨é—¨å¸¦æ¥äº†å·¨å¤§çš„æ”¹å˜ï¼Œä»–å¾ˆé‡è§†å¯¹å‘˜å·¥çš„åŸ¹å…»ï¼Œå¸Œæœ›æˆ‘ä»¬èƒ½æ›´ä¸“æ³¨äºä¸€äº›æŠ€æœ¯ï¼Œå­¦åˆ°ä¸€äº›æ–°ä¸œè¥¿ã€‚åå“¥ä¹ŸæŒºå™¨é‡æˆ‘çš„ï¼Œè®©æˆ‘åšäº†ä¸€äº›æŠ€æœ¯å«é‡æ¯”è¾ƒé«˜çš„å·¥ä½œï¼Œè¿™ä¸ªæœˆå°±æ˜¯åšäº†BadSQLçš„æ‹¦æˆªï¼Œå¯¹SparkäºŒæ¬¡å¼€å‘ï¼Œå¯¹Sparkçš„logicalPlanæœ‰äº†æ›´æ·±çš„è®¤è¯†ï¼›è¿˜åšäº†Atlaséƒ¨ç½²å’Œæºç ä¿®æ”¹ï¼Œå¯¹Atlasçš„å·¥ä½œåŸç†å’ŒSparkListeneræœ‰äº†ä¸€äº›äº†è§£ã€‚è™½ç„¶è¿™ç±»å·¥ä½œå¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å¯ä»¥æŒ‘æˆ˜å®ƒä¸é¦™å—ï¼Ÿï¼åä¸€æœˆï¼Œå‚åŠ äº†å¾®ä¼—é“¶è¡ŒDssæœ‰å¥–å¾æ–‡æ´»åŠ¨ï¼Œæ‹¿åˆ°äº†æœ€å—æ¬¢è¿å¾æ–‡å¥–ï¼Œæ„Ÿè°¢æ‰€æœ‰æŠ•ç¥¨ç»™æˆ‘æ–‡ç« çš„äººï¼Œå¯æ˜¯å¥–å“å¤§ç¤¼åŒ…åˆ°ç°åœ¨è¿˜æ²¡é‚®åˆ°ï¼Œåç­‰ã€‚ä»Šå¹´çš„åŒåä¸€ï¼ŒèŠ±äº†æœ‰å²ä»¥æ¥æœ€å¤šçš„é’±ï¼Œ1.1wï¼Œè‚‰ç–¼ï¼Œä¹°æ‰‹æœºå ä¸€å¤§ç¬”ï¼ŒæŠ¢Mate40ProæŠ¢åˆ°å¿ƒç´¯ã€‚è½¬æ­£è¯„çº§A+ï¼ŒæŒºå¼€å¿ƒï¼Œæ„Ÿè°¢æ°å“¥ï¼åäºŒæœˆï¼Œåˆæ˜¯åœ¨å·¥ä½œä¸Šæœ‰æŒ‘æˆ˜çš„ä¸€ä¸ªæœˆï¼Œåˆæ˜¯å®Œå…¨é™Œç”Ÿçš„æ–¹å‘ï¼Œåšæ•°æ®è„±æ•ï¼Œåå“¥å’Œç£Šå“¥æŒ‡å¯¼ä¸‹ï¼Œä»æ–¹æ¡ˆè°ƒç ”ï¼Œåˆ°è®¾è®¡ï¼Œè¯„å®¡å†åˆ°å®ç°ï¼Œæ¯ä¸€æ­¥éƒ½å……æ»¡åå·ã€‚è¿™ä¸ªè¿‡ç¨‹è®©æˆ‘ç†Ÿæ‚‰äº†Sparkä»»åŠ¡æ‰§è¡Œæµç¨‹ï¼Œé€»è¾‘è®¡åˆ’ç”Ÿæˆå’Œè½¬æ¢ï¼ŒParquetè¯»å†™ï¼ŒRangerHivePluginåŸç†ï¼Œè®¾è®¡æ€è·¯ä¹Ÿæ›´åŠ å¼€é˜”äº†ï¼Œè™½ç„¶å®ç°ä¸Šæœ‰äº›å°æ¼æ´ï¼Œä½†åé¢ä¼šä¸€æ­¥ä¸€æ­¥å®Œå–„çš„ã€‚è¯´åˆ°è¿™é‡Œï¼Œè¿˜æ˜¯è¦æ„Ÿè°¢åå“¥å™¨é‡ï¼Œè®©æˆ‘æ˜å¹´åšæ•°æ®å¹³å°ç»„ç»„é•¿ï¼Œæˆ‘æ„Ÿè§‰æˆ‘ç¦»å½“ç»„é•¿çš„èƒ½åŠ›è¿˜å·®å¾—è¿œï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§æ¿€åŠ±å§ï¼Œæƒ³æ‹…èµ·è¿™ä¸ªé‡ä»»å°±è¦å…ˆåˆ»è‹¦å……å®è‡ªå·±çš„çŸ¥è¯†æ°´å¹³å’Œèƒ½åŠ›ï¼Œæ–°çš„ä¸€å¹´ç»™è‡ªå·±åŠ æ²¹æ‰“æ°”ï¼åå“¥è¿˜ç»™æˆ‘é¢è¯•ä»–äººçš„æœºä¼šï¼Œæ‰å‘ç°é¢è¯•æœ‰å¥½å¤šå­¦é—®ï¼Œè€Œä¸”é¢è¯•ä¸ä»…æ˜¯è€ƒå¯Ÿé¢è¯•è€…çš„çŸ¥è¯†æ°´å¹³å’Œèƒ½åŠ›ï¼Œæ›´è€ƒéªŒé¢è¯•å®˜çš„èƒ½åŠ›ã€æŠ€å·§ï¼Œé¢è¯•åˆ«äººçš„è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°ä¼˜ç§€çš„äººï¼Œä¼šå‘ç°è‡ªå·±çš„ä¸è¶³ï¼Œè‡ªå·±éœ€è¦åŠ å¼ºçš„åœ°æ–¹å¤ªå¤šäº†ã€‚è¿™ä¸ªæœˆå‚åŠ äº†å…¬å¸ä¿¡æ¯æŠ€æœ¯æ¡çº¿çš„2021å·¥ä½œè§„åˆ’è®¨è®ºï¼Œè·Ÿå¤§ä½¬ä»¬ä¸€èµ·è®¨è®ºæ‰è§‰å¾—è‡ªå·±æ€è·¯å’Œçœ¼ç•Œéƒ½æ¯”è¾ƒçª„ã€‚è·Ÿè€ä¹¡å¹²é¥­å»ï¼Œç¢°è§ä¸ªæˆ·å¤–ç›´æ’­ï¼Œé—®è°ä¼šå”±æ­Œï¼Œè¢«è€ä¹¡æ¨å‡ºå»äº†ï¼Œè¹­äº†ä¸»æ’­çƒ­åº¦å”±äº†é¦–ã€Šå‘Šç™½æ°”çƒã€‹ï¼Œæƒ³é—®ä¸»æ’­é‚£å‡ åˆ†é’Ÿæ‰äº†å¤šå°‘ç²‰ä¸ï¼Ÿè·ŸåŒäº‹å‡ ä¸ªäººä¸€èµ·å»çˆ¬äº†æ¢§æ¡å±±ï¼Œæ‹äº†å¾ˆå¥½çœ‹çš„ç…§ç‰‡ï¼Œä¸€ä¼šç»™å¤§å®¶åˆ†äº«ä¸‹å“ˆï¼å¾ˆä¼¤å¿ƒçš„æ˜¯æˆ‘ä»¬éƒ¨é—¨å”¯ä¸€ä¸œåŒ—è¯è´¼æºœçš„è€ä¹¡ç¦»èŒäº†(æˆ‘æ˜¯ä¸œåŒ—è¯ä¸è¡Œ)ï¼Œå¸Œæœ›å¥¹åé¢ä¸€å¸†é£é¡ºæŠŠã€‚åœ£è¯èŠ‚ç©æ¸¸æˆæ‹¿äº†å¥½å¤šç¤¼ç‰©ï¼Œä¹Ÿæ˜¯å¾ˆå¼€å¿ƒï¼è·¨å¹´æ™šä¸ŠæŠ¢äº†è€æ¿çš„å¤§çº¢åŒ…ï¼Œå¾ˆå·´é€‚ï¼å…ƒæ—¦å‡æœŸè®¡åˆ’äº†è·Ÿå¼ å®‡å’Œåº·å¯Œå»å¹¿å·ä¸‰æ—¥æ¸¸ï¼Œç»ˆäºæœ‹å‹åœˆæœ‰å¾—å†™äº†ï¼Œå“ˆå“ˆï¼ä¸»æŒäººï¼šç»å†è¿‡åˆ†åˆ«ï¼Œé¢å¯¹è¿‡å‹åŠ›ï¼Œé‡åˆ°è¿‡å›°éš¾ï¼Œä¹Ÿå—åˆ°è¿‡é¼“åŠ±ï¼Œ2020å¹´å¯¹ä½ æ¥è¯´æœç„¶æ˜¯è·Œå®•èµ·ä¼å•Šï¼é‚£ä½ è§‰å¾—2020å¹´ä½ æœ€å¤§çš„å˜åŒ–æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ï¼šå­¦ä¼šåšé¥­äº†ï¼Œè¿˜æŒºå¥½åƒçš„ï¼ˆè‡ªæˆ‘æ„Ÿè§‰è‰¯å¥½ï¼Œå…¶å®ä¸å’‹çš„ï¼‰ï¼Œæ›´åŠ ç‹¬ç«‹äº†ï¼Œæ›´é‡è¦çš„æ˜¯æ¸æ¸æ„è¯†åˆ°è¯¥æ‰¾ä¸ªå¯¹è±¡äº†ï¼Œå“ˆå“ˆï¼ä¸»æŒäººï¼šä¼šåšé¥­äº†ï¼Œç¡®å®è¿›æ­¥å¾ˆå¤§å•Šã€‚æœ‰æ‰¾å¯¹è±¡çš„æ„è¯†å¾ˆå¥½ï¼Œå¸Œæœ›2020ä½ èƒ½æ‰¾åˆ°å¯¹çš„äººå§ã€‚è¿˜è®°å¾—å»å¹´çš„å¯¹è‡ªå·±çš„å°ç›®æ ‡å—ï¼Œè®²è®²å®ç°äº†å“ªäº›ï¼Ÿæˆ‘ï¼šæ¡†æ¶åº•å±‚åŸç†æ·±å…¥äº†ä¸€ç‚¹ç‚¹ï¼Œä»£ç èƒ½åŠ›ä¹Ÿæœ‰æé«˜ï¼Œå†™åšå®¢ä¹Ÿåœ¨åšæŒï¼Œåªæ˜¯ä»Šå¹´å†™çš„æ–‡ç« æ¯”è¾ƒå°‘ã€‚äº¤åˆ°äº†å‡ ä¸ªå¥½æœ‹å‹ã€‚æ²¡å®ç°çš„æˆ‘ä¹Ÿè®²ä¸€ä¸‹å§ï¼Œä¸ç†¬å¤œè¿™ä¸ªæ²¡å®ç°ï¼Œå»å¹´çš„æ€»ç»“ï¼Œé—®å¤§å®¶æœ‰æ²¡æœ‰äº’ç›¸ç›‘ç£ç†¬å¤œçš„åœ¨è¯„è®ºåŒºç•™è¨€ï¼Œæ²¡äººç•™è¨€ï¼Œæ²¡æœ‰ç›‘ç£ï¼Œæ˜å¹´å°½é‡å°‘ç†¬å¤œå§ï¼Œèƒ½å‡å°‘å°±è¡Œï¼ç½‘æ˜“äº‘(è¯´é”™äº†æ˜¯ç½‘æŠ‘äº‘)ä½œå“è´¨é‡æ¯”åŸæ¥å¼ºäº†ï¼Œå½“ç„¶åªå‡ºäº†å¾ˆå°‘å‡ ä¸ªä½œå“ï¼šã€Šå¹æ¢¦åˆ°è¥¿æ´²ã€‹ã€ã€Šæœ‰å¹¸ã€‹ğŸ‘ˆç‚¹å‡»å¯ä»¥å¬å“ˆå“ˆå“ˆï¼Œä»Šå¹´è®¤ä¸ºè´¨é‡æœ€å¥½çš„ç¿»å”±ä½œå“å°±æ˜¯è¿™é¦–ã€Šå¹æ¢¦åˆ°è¥¿æ´²ã€‹äº†ã€‚ä¸»æŒäººï¼šé‚£ç”Ÿæ´»æ–¹é¢å’Œåšäº‹æ–¹é¢å‘¢ï¼Ÿæˆ‘ï¼šç”Ÿæ´»æ–¹é¢ï¼Œæ„Ÿè§‰ç”Ÿæ´»è´¨é‡æ¯”å®ä¹ æ—¶å€™æ›´å¥½äº†ï¼Œæ¯•ç«ŸæŒ£é’±æ¯”å®ä¹ æ—¶å¤šäº†ï¼Œæ·±åœ³æŒ£é’±æ·±åœ³èŠ±å˜›ï¼Œä¸è¿‡æˆ‘æ¯”è¾ƒèŠ‚çº¦ï¼Œè¿˜æ˜¯æ”’äº†ä¸€äº›çš„ã€‚å•Šï¼Œæˆ‘çš„å¿ƒé‡Œåªæœ‰ä¸€ä»¶äº‹ï¼Œå°±æ˜¯æé’±ã€‚æˆ‘è§‰å¾—åšäº‹æƒ…æ¯”ä»¥å‰æ›´ç¨³é‡äº†ï¼Œä½†æœ‰æ—¶è¿˜æ˜¯ä¼šæµ®èºï¼Œå·²ç»æ”¹å–„å¾ˆå¤šäº†ã€‚ä¸»æŒäººï¼šæ­¤æ—¶æ­¤åˆ»ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³å¯¹æœ‹å‹å’Œäº²äººè¯´çš„ï¼Ÿæˆ‘ï¼šå¯¹å¥½æœ‹å‹è¯´ï¼Œå¤šæ•°äººéƒ½æ˜¯åŒ†åŒ†è¿‡å®¢ï¼Œè€Œä½ å´ç•™äº†ä¸‹æ¥ï¼Œå³ä½¿å¾ˆä¹…ä¸è”ç³»ï¼Œæœ‰äº‹æƒ…ä¹Ÿèƒ½éšå«éšåˆ°ï¼Œæœ‰ä»€ä¹ˆè¯éƒ½å¯ä»¥è®²ç»™ä½ ï¼Œè®¤è¯†ä½ æ˜¯æˆ‘çš„å¹¸è¿ï¼Œ2021ç»§ç»­æœ‰ä½ ï¼Œè°¢è°¢ä½ çš„é™ªä¼´ï¼å¯¹çˆ¸å¦ˆè¯´ï¼Œè™½ç„¶ç¦»ä½ ä»¬å¾ˆè¿œï¼Œè¿‡å¹´æˆ‘è¿˜æ˜¯ä¼šå›å»çš„ï¼Œæ²¡æœ‰å›¢åœ†çš„å¹´æ˜¯ä¸å®Œæ•´çš„ï¼Œå¾ˆå¿«å°±ä¼šç›¸è§äº†ï¼æˆ‘ä¸åœ¨æ—¶ä½ ä»¬è¦å¤šæ³¨æ„èº«ä½“ï¼ŒåŠ å¼ºé”»ç‚¼ï¼Œæ˜å¹´å¸¦ä½ ä»¬ä¸€èµ·å»æ—…æ¸¸ï¼2021ï¼Œæˆ‘å°½é‡å°‘ç†¬å¤œå“ˆï¼ä¸ç”¨æ‹…å¿ƒæˆ‘ã€‚ä¸»æŒäººï¼šä½ çš„æœ‹å‹å’Œäº²äººæ­¤åˆ»åº”è¯¥ä¹Ÿæ”¶åˆ°äº†ä½ è¯´çš„è¯ã€‚ä½ çš„2021ï¼Œå……æ»¡å¸Œæœ›å’ŒæŒ‘æˆ˜ï¼ŒåŠ æ²¹å§ï¼Œä½ ä¸€å®šèƒ½è¡Œï¼æˆ‘ï¼šè°¢è°¢æ‚¨ï¼ä¸»æŒäººï¼šåˆšæ‰ä½ æåˆ°äº†è¦ç»™å¤§å®¶åˆ†äº«ä¸€äº›ç…§ç‰‡æ˜¯å§ï¼Ÿæˆ‘ï¼šå—¯ï¼Œé‚£ä¸‹é¢æˆ‘æŠŠ2020çš„ç²¾å½©ç¬é—´åˆ†äº«ç»™å¤§å®¶ï¼ å±•æœ›2021æŒ¥æ‰‹å‘Šåˆ«2020ï¼Œ2021æˆ‘æ¥å•¦ï¼2021ä¹Ÿè¦å¥¥åˆ©ç»™å‘€ï¼ å°ç›®æ ‡ æ•°æ®æ²»ç†ã€æ•°æ®æ¹–ã€å®æ—¶è®¡ç®—ç­‰æ–¹é¢è¦æŒæ¡ï¼ŒæŠ€æœ¯è¦ç´§è·Ÿæ½®æµï¼Œä¸èƒ½è½ä¼ æé«˜å¼€å‘æ•ˆç‡ï¼Œèƒ½å¿«é€Ÿç²¾ç¡®å®šä½é—®é¢˜ å­¦ä¹ æ—¶æ›´ä¸“å¿ƒï¼Œä¸æµ®èº å‡å°‘ç†¬å¤œï¼Œæœ‰å¿…è¦å¥èº« å°æ„¿æœ› å®¶äººã€æœ‹å‹å’Œè‡ªå·±éƒ½è¦èº«ä½“å¥åº·ï¼ èƒ½æ‹…å½“èµ·å¹³å°ç»„ç»„é•¿çš„é‡ä»» æ­£åœ¨çœ‹æˆ‘åšå®¢çš„ä½ æ¯å¤©éƒ½å¼€å¿ƒ å°æƒ³æ³•ç¬¬ä¸€ï¼Œä»Šå¹´æ˜¯å˜é©çš„ä¸€å¹´ï¼Œä¸æ–­å­¦ä¹ ï¼Œæ”¹å˜è‡ªå·±ï¼Œä¸ºè‡ªå·±æŠ•èµ„ï¼Œæå‡è‡ªå·±å„æ–¹é¢çš„èƒ½åŠ›ã€‚ç¬¬äºŒï¼Œè¿˜æ˜¯è·Ÿå»å¹´ä¸€æ ·ï¼Œæœ‰æ²¡æœ‰äº’ç›¸ç›‘ç£ä¸ç†¬å¤œçš„ï¼è¯„è®ºåŒºç•™è¨€ï¼","categories":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"ä¸ªäººæ€»ç»“","slug":"ä¸ªäººæ€»ç»“","permalink":"https://shmily-qjj.top/tags/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"},{"name":"2020","slug":"2020","permalink":"https://shmily-qjj.top/tags/2020/"}],"keywords":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"å®ç°åŸºäºSparkçš„æ•°æ®è„±æ•","slug":"å®ç°åŸºäºSparkçš„æ•°æ®è„±æ•","date":"2020-12-11T14:16:00.000Z","updated":"2022-12-11T05:35:07.919Z","comments":true,"path":"4cf161e5/","link":"","permalink":"https://shmily-qjj.top/4cf161e5/","excerpt":"","text":"å®ç°åŸºäºSparkçš„æ•°æ®è„±æ•å‰è¨€&emsp;&emsp;Sparkæ˜¯å½“å‰å¤§æ•°æ®é¢†åŸŸä¸å¯æ›¿ä»£çš„é‡è¦ç»„ä»¶ï¼Œæ‹¥æœ‰æˆç†Ÿçš„ç”Ÿæ€ã€å¼ºå¤§çš„æ€§èƒ½å’Œå¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œä½†åœ¨æ•°æ®å®‰å…¨è¶Šæ¥è¶Šé‡è¦çš„ä»Šå¤©ï¼ŒSparkåœ¨å¯¹æ•°æ®æƒé™çš„ç®¡æ§èƒ½åŠ›æ–¹é¢ä»ç„¶æ²¡æœ‰è¿›å±•ã€‚&emsp;&emsp;ä¸ä»…ä»…æ˜¯Sparkï¼Œå¤§å¤šæ•°å¤§æ•°æ®ç”Ÿæ€åœˆä¸­çš„ç»„ä»¶éƒ½ç¼ºä¹å¯¹æ•°æ®å®‰å…¨çš„ç®¡æ§ï¼Œäºæ˜¯å¾ˆå¤šç¡¬ä»¶èµ„æºè¾ƒå……è£•çš„å…¬å¸ä¼šå°†æ•°æ®å…¨é‡è„±æ•ååˆ†åˆ«å­˜æ”¾ï¼Œç‰ºç‰²å­˜å‚¨ç©ºé—´æ¥è¾¾åˆ°æ•°æ®è„±æ•çš„ç›®çš„ï¼›è€Œæœ‰äº›å…¬å¸é€‰æ‹©Apache Rangerä½œä¸ºæƒé™ç®¡æ§ç»„ä»¶ï¼Œä½†Ranger(ç›®å‰ç‰ˆæœ¬2.2)åœ¨è®¾è®¡ä¸Šå¯¹å„ä¸ªå¤§æ•°æ®ç»„ä»¶ç‰ˆæœ¬æœ‰ç€ä¸¥æ ¼çš„ä¾èµ–ï¼Œä¸”æš‚æ—¶ä¸æ”¯æŒå¯¹Sparkçš„æƒé™ç®¡æ§ã€‚&emsp;&emsp;ç»è¿‡é˜…è¯»Rangeræºç ï¼Œä»¥åŠåœ¨æµ‹è¯•ç¯å¢ƒè¯•ç”¨åï¼Œç¡®å®šäº†Rangeræ–¹æ¡ˆä¸å¯è¡Œï¼Œæˆ‘å†³å®šåœ¨SparkåŸºç¡€ä¸ŠäºŒæ¬¡å¼€å‘æ¥å®ç°æ•°æ®è„±æ•åŠŸèƒ½ã€‚ çŸ¥è¯†å‡†å¤‡ SparkSQLæ‰§è¡Œè¿‡ç¨‹ï¼šSQL-&gt;Parser(Antlr)-&gt;AST-&gt;Catalyst-&gt;UnresolvedLogicalPlan-&gt;Analyzer-&gt;Optimizer-&gt;PhysicalPlan-&gt;æ‰§è¡Œè®¡ç®—å’ŒIO ä»¥ä¸Šè¿‡ç¨‹ç›´åˆ°ç‰©ç†è®¡åˆ’éƒ½æ˜¯ç»§æ‰¿è‡ªLogicalPlanï¼Œå…±å››ç§ï¼šUnresolvedLogicalPlan: ä¹Ÿå«ParsedLogicalPlanï¼Œæ˜¯æ ¹æ®è¯­æ³•æ ‘è§£æSQLåå¾—åˆ°çš„é€»è¾‘è®¡åˆ’ï¼Œæ²¡æœ‰å…³è”catalogï¼Œæ²¡æœ‰è·å–åº•å±‚å­˜å‚¨çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯è¯´SELECT *åœ¨è¿™ä¸ªé˜¶æ®µä¸ä¼šè¢«è§£æä¸ºå…·ä½“å­—æ®µï¼ˆAnalyzedLogicalPlanã€OptimizedLogicalPlanã€PhysicalPlanå¯çœ‹åˆ°å…·ä½“å­—æ®µï¼‰ã€‚AnalyzedLogicalPlan: ç»“åˆè¡¨çš„Catalogï¼Œç»‘å®šå…ƒæ•°æ®ï¼ŒresolveåŒ–LogicalPlanï¼Œæ›¿æ¢æ‰UnresolvedLogicalPlanï¼Œè¿™é‡Œä¼šæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä»¥åŠSchemaå®Œæ•´æ€§ã€‚ç»‘å®šå…ƒæ•°æ®æ˜¯å¦æˆåŠŸä¸»è¦æœ‰ä¸¤ç‚¹ï¼š1.å­èŠ‚ç‚¹æ˜¯å¦æ˜¯resolved 2.è¾“å…¥çš„æ•°æ®ç±»å‹æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œå…·ä½“å¯å‚è€ƒç±»ï¼šExpressionï¼ŒAnalyzerç±»ã€‚OptimizedLogicalPlan:å¯¹AnalyzedLogicalPlanè¿›è¡Œä¼˜åŒ–ï¼Œæœ‰å¾ˆå¤šRuleExecutorï¼Œå¦‚è°“è¯ä¸‹æ¨ï¼ŒFilterè£å‰ªï¼ŒWholeStageCodegen(å¤§é‡ç±»å‹è½¬æ¢å’Œè™šå‡½æ•°è°ƒç”¨è½¬ä¸ºå³æ—¶ç¼–è¯‘)ï¼ŒRemoveLiteralFromGroupExpressionsç§»é™¤groupä¸‹çš„å¸¸é‡ï¼ŒRemoveRepetitionFromGroupExpressionsç§»é™¤é‡å¤çš„groupè¡¨è¾¾å¼ç­‰â€¦PhysicalPlan:å°†OptimizedLogicalPlanè½¬æ¢ä¸ºå®é™…æ‰§è¡Œçš„æ­¥éª¤ï¼Œå…·ä½“å¯å‚è€ƒSparkPlannerç±»ã€‚ // æ‹¿åˆ°å››ç§æ‰§è¡Œè®¡åˆ’çš„æ–¹æ³• val sql:String = &quot;select * from qjj&quot; // å•ç‹¬è·å–UnresolvedLogicalPlanï¼Œä¸ç”¨è¯»å…ƒæ•°æ®ï¼Œæ•ˆç‡æœ€é«˜ val unresolvedLogicalPlan:LogicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(sql) // è·å–QueryExecution val qe:org.apache.spark.sql.execution.QueryExecution = sqlContext.sparkSession.sql(sql).queryExecution // é€šè¿‡QueryExecutionè·å–æ‰€æœ‰è®¡åˆ’åŒ…æ‹¬ParsedLogicalPlanï¼ŒAnalyzedLogicalPlanï¼ŒOptimizedLogicalPlanå’ŒPhysicalPlan val parsedLogicalPlan:LogicalPlan = qe.logical val analyzedLogicalPlan:LogicalPlan = qe.analyzed val optimizedLogicalPlan:LogicalPlan = qe.optimizedPlan val physicalPlan:LogicalPlan = qe.executedPlan val physicalPlan:LogicalPlan = qe.sparkPlan LogicalPlanåŒ…å«ä¸‰ç§å­ç±»å‹UnaryNode,BinaryNodeå’ŒLeafNodeï¼Œæ¯ç§å­ç±»å‹ä¸‹åˆæœ‰å¤šç§å­ç±»å‹ï¼Œå­ç±»å‹ä¸‹åˆåŒ…å«å­ç±»å‹å¦‚ï¼šProject,GlobalLimit,LocalLimit,CreateTable,Distinct,SubqueryAlias,InsertIntoTable,Join,Aggregate,Union,Filterç­‰ã€‚ Datasetå’ŒDataFrameçš„åŒºåˆ«ä¸è”ç³»ï¼šè”ç³»ï¼š 1.APIç»Ÿä¸€ï¼Œä½¿ç”¨ä¸Šæ²¡å·®åˆ« 2.DataFrameç®—æ˜¯ç‰¹æ®Šç±»å‹çš„Datasetï¼Œæ˜¯æ¯ä¸ªå…ƒç´ éƒ½ä¸ºROWç±»å‹çš„Datasetï¼ˆDataFrame = Dataset[Row]ï¼‰åŒºåˆ«ï¼š 1.Datasetæ˜¯å¼ºç±»å‹ï¼Œç¼–è¯‘æ—¶æ£€æŸ¥ç±»å‹ï¼ŒDataFrameæ˜¯å¼±ç±»å‹ï¼Œæ‰§è¡Œæ—¶æ‰æ£€æŸ¥ç±»å‹ 2.Datasetæ˜¯é€šè¿‡Encoderè¿›è¡Œåºåˆ—åŒ–ï¼Œæ”¯æŒåŠ¨æ€çš„ç”Ÿæˆä»£ç ï¼Œç›´æ¥åœ¨bytesçš„å±‚é¢è¿›è¡Œæ’åºè¿‡æ»¤ç­‰çš„æ“ä½œï¼›è€ŒDataFrameæ˜¯é‡‡ç”¨å¯é€‰çš„javaçš„æ ‡å‡†åºåˆ—åŒ–æˆ–æ˜¯kyroè¿›è¡Œåºåˆ—åŒ– Sparkçš„TemporaryViewï¼šSparkçš„å››ç§è§†å›¾åˆ›å»ºæ–¹æ³•ï¼š â‘ df.createGlobalTempView(df.createOrReplaceGlobalTempView) åˆ›å»ºå…¨å±€ä¸´æ—¶è§†å›¾ï¼Œå¤šä¸ªSparkSessionå…±äº« SparkSQLå†™æ³•ï¼šcreate global temporary view view_name(col1,col2â€¦) as select (col1,col2â€¦) from table_name; â‘¡df.createTempView(df.createOrReplaceTempView) åˆ›å»ºSessionçº§åˆ«çš„ä¸´æ—¶è§†å›¾,å¤šä¸ªSparkSessionä¸å…±äº« SparkSQLå†™æ³•ï¼šcreate temporary view view_name(col1,col2â€¦) as select (col1,col2â€¦) from table_name;é€šè¿‡SQLåˆ›å»ºè§†å›¾æ—¶ä¼šæœ‰å‡ ç§å¼‚å¸¸ï¼šIt is not allowed to define a TEMPORARY view with IF NOT EXISTS. ï¼ˆåˆ›å»ºè§†å›¾ä¸æ”¯æŒif not existsï¼‰It is not allowed to add database prefix test for the TEMPORARY view name. ï¼ˆåˆ›å»ºè§†å›¾ä¸æ”¯æŒåº“åå‰ç¼€ï¼‰Not allowed to create a permanent view by referencing a temporary function. ï¼ˆä¸æ”¯æŒç”¨å¸¦ä¸´æ—¶UDFçš„é€»è¾‘åˆ›å»ºæ°¸ä¹…è§†å›¾ï¼‰è§†å›¾åˆ é™¤ï¼šspark.catalog.dropTempView(â€˜view_nameâ€™)spark.catalog.dropGlobalTempView(â€˜global_view_nameâ€™)å…¨å±€è§†å›¾è°ƒç”¨ï¼šspark.sql(â€œselect * from global_temp.view_nameâ€) éœ€è¦åŠ global_tempå‰ç¼€ ä»ä¸€æ¡SQLåˆ°ThriftServerä¸Šçš„ä¸€ä¸ªJobï¼Œå¦‚ä½•ç”Ÿæˆï¼šï¼ˆè¯¥å›¾å¼•è‡ªSparkSQLå¹¶è¡Œæ‰§è¡Œå¤šä¸ªJobçš„æ¢ç´¢ï¼Œæ–‡ç« ä¸é”™ï¼Œæ¨èæœ‰ç©ºçœ‹çœ‹ï¼‰ åŸºäºSparkThriftServerçš„æ•°æ®è„±æ•å·¥ä½œåŸç†æµç¨‹æµç¨‹ï¼šåŸç†ï¼š å®ç°ç»†èŠ‚æ•°æ®åº“å»ºè¡¨ï¼š -- è„±æ•è§„åˆ™è¡¨ create table desensitization_rules( rule_name varchar(100) not null primary key comment &quot;è„±æ•è§„åˆ™åç§°&quot;, rule_type varchar(100) not null comment &quot;ç±»å‹-å¯é€†ã€ä¸å¯é€†ã€åŠ å¯†ã€è§£å¯†&quot;, encrypt_column_type varchar(100) not null comment &quot;å¯åŠ å¯†çš„æ•æ„Ÿæ•°æ®åˆ†ç±»å¦‚phone_numã€id_cardã€bank_accountã€cust_name&quot;, encrypt_udf_name varchar(100) comment &quot;å¯¹åº”è„±æ•UDFåç§°&quot;, decrypt_udf_name varchar(100) comment &quot;è§£å¯†UDFåç§°&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;è§„åˆ™åˆ›å»ºæ—¶é—´&quot; )ENGINE=Innodb comment=&#39;è„±æ•è§„åˆ™åº“-ç”¨äºé…ç½®è„±æ•è§„åˆ™ä¸å¯¹åº”çš„åŠ å¯†UDFã€è§£å¯†UDFå’ŒåŠ å¯†æ•°æ®ç±»å‹çš„æ˜ å°„å…³ç³»&#39;; -- è„±æ•é…ç½®è¡¨ create table desensitization_conf( db_table varchar(255) comment &quot;åº“å.è¡¨åï¼Œä¸º*ä»£è¡¨å¯¹æ‰€æœ‰è¡¨éƒ½ç”Ÿæ•ˆ&quot;, column_name varchar(255) not null comment &quot;å•ä¸ªæ•æ„Ÿå­—æ®µå&quot;, column_type varchar(100) not null comment &quot;æ•æ„Ÿå­—æ®µæ•°æ®åˆ†ç±»&quot;, rule_name varchar(100) not null comment &quot;è„±æ•è§„åˆ™åç§°&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;åˆ›å»ºæ—¶é—´&quot;, PRIMARY KEY (db_table,column_name) )ENGINE=Innodb comment=&#39;è„±æ•é…ç½®è¡¨-å…·ä½“åˆ°å­—æ®µçš„è„±æ•é…ç½®ï¼Œè¯¥è¡¨å†³å®šå¦‚ä½•è„±æ•ï¼Œæœªé…ç½®çš„ é»˜è®¤æ˜¯ç™½åå•&#39;; -- è§’è‰²æƒé™è¡¨ create table desensitization_role_permissions( role varchar(100) not null primary key comment &quot;è§’è‰²&quot;, authorized_dbs varchar(1000) NOT NULL comment &quot;æœ‰æŸ¥æ•æ„Ÿä¿¡æ¯æƒé™çš„åº“ï¼Œé€—å·éš”å¼€ï¼Œallè¡¨ç¤ºå…¨éƒ¨åº“&quot;, authorized_tables text NOT NULL comment &quot;æœ‰æŸ¥æ•æ„Ÿä¿¡æ¯æƒé™çš„è¡¨ï¼Œé€—å·éš”å¼€-åº“å.è¡¨åï¼Œallè¡¨ç¤ºå…¨éƒ¨è¡¨&quot;, authorized_data_type varchar(1000) NOT NULL comment &quot;æœ‰æƒé™çš„æ•æ„Ÿæ•°æ®ç±»å‹å¦‚phone_numã€id_cardã€account_numã€cust_nameï¼Œallè¡¨ç¤ºæ•°æ®ç±»å‹&quot;, authorized_columns text NOT NULL comment &quot;æœ‰æƒé™çš„å­—æ®µåï¼Œå¤šä¸ªå­—æ®µé€—å·éš”å¼€ï¼Œallè¡¨ç¤ºå…¨éƒ¨å­—æ®µ&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;è§’è‰²åˆ›å»ºæ—¶é—´&quot; )ENGINE=Innodb comment=&#39;è§’è‰²æƒé™è¡¨-ç”¨äºé…ç½®æ¯ä¸ªè§’è‰²çš„æƒé™&#39;; -- ç”¨æˆ·æƒé™è¡¨ create table desensitization_user_role( user varchar(100) not null primary key comment &quot;ç”¨æˆ·å&quot;, role varchar(255) not null comment &quot;è§’è‰²ï¼Œå¤šä¸ªè§’è‰²é€—å·éš”å¼€&quot;, create_datetime timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP comment &quot;æ·»åŠ æ—¶é—´&quot; )ENGINE=Innodb comment=&#39;ç”¨æˆ·è§’è‰²æ˜ å°„å…³ç³»è¡¨-é»˜è®¤æ— æŸ¥è¯¢æ•æ„Ÿæ•°æ®çš„æƒé™&#39;; -- ----------é…ç½®æ•°æ®---------- insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_num&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_number&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile_phone&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;mobile_no&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;phone_no&quot;,&quot;phone_num&quot;,&quot;HideLast4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;cust_name&quot;,&quot;cust_name&quot;,&quot;HideUserName&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;*&quot;,&quot;bank_card_no&quot;,&quot;bank_account&quot;,&quot;HideBankCardNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;test_pn&quot;,&quot;phone_num&quot;,&quot;phone_num&quot;,&quot;HideMid4PhoneNumber&quot;); insert into desensitization_conf(db_table,column_name,column_type,rule_name) values (&quot;test_pn&quot;,&quot;phone&quot;,&quot;phone_num&quot;,&quot;HideMid4PhoneNumber&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;admin&quot;,&quot;all&quot;,&quot;all&quot;,&quot;all&quot;,&quot;&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;d_bd&quot;,&quot;d_bd&quot;,&quot;&quot;,&quot;&quot;,&quot;cust_name&quot;); insert into desensitization_role_permissions(role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns) values (&quot;d_qjj&quot;,&quot;&quot;,&quot;d_bd.test_qjj&quot;,&quot;&quot;,&quot;cust_name&quot;); insert into desensitization_user_role(user,role) values (&quot;admin&quot;,&quot;d_bd,d_qjj&quot;); insert into desensitization_user_role(user,role) values (&quot;bd_admin&quot;,&quot;admin&quot;); è¿™æ ·è®¾è®¡æƒé™è€ƒè™‘çš„ç‚¹ï¼š 1.ç”¨æˆ·æƒé™æŒ‰è§’è‰²ç®¡ç† 2.ç”¨æˆ·å¯ä»¥æœ‰å¤šä¸ªè§’è‰² 3.å¯ä»¥æ–¹ä¾¿ç”¨æˆ·ä¸´æ—¶ç”³è¯·æƒé™ 4.æ•æ„Ÿæ•°æ®çš„å­—æ®µå¯ä»¥æ”¯æŒé»˜è®¤è„±æ•é…ç½® 5.å¯ä»¥ç»†ç²’åº¦åœ°æŒ‡å®šæŸä¸ªç”¨æˆ·æŸ¥è¯¢æŸä¸ªè¡¨æŸä¸ªå­—æ®µæ—¶å¦‚ä½•è„±æ• 6.å¯¹æ•°æ®è¿›è¡Œæ•æ„Ÿä¿¡æ¯åˆ†ç±»ï¼Œæ–¹ä¾¿æŒ‰æ•æ„Ÿæ•°æ®ç±»å‹æ§åˆ¶æƒé™ï¼ˆè¡¨è®¾è®¡äº†ä½†å®é™…æ²¡å®ç°è¿™å—ï¼‰ ä»£ç å®ç°SQLAnalyzerç±»ï¼Œå·¥å…·ç±»ï¼Œä¸»è¦æ˜¯åŒ¹é…SQLä¸­çš„è¡¨ä»¥åŠåˆ¤æ–­SQLç±»å‹ï¼Œä¸‹é¢åˆ—ä¸¾ä¸»è¦æ–¹æ³• /** * Get all tables in a select sql * @param sql * @return List(table1,table2) or List() */ def getTablesInSelect(sql:String):List[String] = &quot;(?i)(?:from|join)\\\\s+[a-zA-Z0-9_.]+&quot; .r.findAllIn(sql) .map(x =&gt; x.replaceFirst(&quot;(?i)(?:from|join)\\\\s+&quot;, &quot;&quot;)) .toList /** * Determines SQL is a select statement or not. * @param sql * @return boolean */ def isSelectSQL(sql:String):Boolean = !StringUtils.isBlank(sql) &amp;&amp; sql.trim.replaceAll(&quot;\\r|\\n|\\r\\n&quot;,&quot; &quot;).matches(&quot;(?i)\\\\s*select.*&quot;) /** * Determines SQL is a Data transfer statement or not. * @param sql * @return boolean */ def isSelectInsertSQL(sql:String):Boolean = !StringUtils.isBlank(sql) &amp;&amp; sql.trim.replaceAll(&quot;\\r|\\n|\\r\\n&quot;,&quot; &quot;).matches(&quot;(?i)\\\\s*insert.*select.*&quot;) DesensitizationModuleç±»ï¼Œè„±æ•æ¨¡å—ï¼Œå®ç°äº†ï¼šè¯»å–å¹¶ç¼“å­˜è„±æ•é…ç½®ï¼›ç”¨æˆ·æƒé™èšåˆï¼Œé‰´æƒï¼Œåˆ›å»ºä¸´æ—¶è§†å›¾å¹¶åº”ç”¨è„±æ•è§„åˆ™ï¼Œæ›¿æ¢SQLç”ŸæˆçœŸæ­£æ‰§è¡Œçš„SQLï¼Œå…¶ä¸­getDesensitizedSQLæ˜¯è¿™ä¸ªç±»æä¾›ç»™å¤–éƒ¨çš„æ–¹æ³•ï¼Œè¿”å›çš„æ˜¯å®é™…æ‰§è¡Œçš„SQLï¼Œç”¨äºæ›¿æ¢åŸæ¥çš„é€»è¾‘è®¡åˆ’ã€‚loadDesensitizationMetaä¹Ÿæ˜¯æä¾›ç»™å¤–éƒ¨çš„æ–¹æ³•ï¼Œç”¨äºåŠ è½½è„±æ•é…ç½®å’Œç”¨æˆ·æƒé™ä¿¡æ¯ã€‚ package org.apache.spark.sql.hive.thriftserver.desensitization import com.smy.exceptions.DesensitizationException import org.apache.spark.internal.Logging import org.apache.spark.sql.hive.thriftserver.xxxxx.getDF // è¿æ¥mysqlï¼ŒæŸ¥è¯¢è„±æ•é…ç½®å¾—åˆ°dataframeçš„æ–¹æ³• import org.apache.spark.sql.&#123;SQLContext, SparkSession&#125; import org.apache.spark.sql.hive.thriftserver.desensitization.SQLAnalyzer._ import scala.tools.scalap.scalax.util.StringUtil private[hive] object DesensitizationModule extends Logging&#123; // entities case class DesensitizationConf(columnName: String, columnType: String, ruleName:String) case class Permission(role: String, authorizedDBs: String, authorizedTables:String, authorizedDataTypes:String, authorizedColumns:String) object DesensitizationMeta &#123; var RULE_UDF_MAP: Map[String, String] = _ //desensitization_rules var DBTABLE_DESENSITIZATION_CONF_MAP: Map[String, Set[DesensitizationConf]] = _ //desensitization_conf var USER_PERMISSION_MAP: Map[String, Permission] = _ //desensitization_user_permissions var ROLE_PERMISSION_MAP: Map[String, Permission] = _ //desensitization_role_permissions var DEFAULT_COLUMN_UDF_MAPPING: Map[String,String] = _ &#125; private def getMergedParas(para1: String, para2: String, isRolePara: Boolean=false): String = &#123; if(isRolePara &amp;&amp; (para1.split(&quot;,&quot;).contains(&quot;admin&quot;) || para2.split(&quot;,&quot;).contains(&quot;admin&quot;))) return &quot;admin&quot; if (para1 != null &amp;&amp; para1.nonEmpty &amp;&amp; para2 != null &amp;&amp; para2.nonEmpty) &#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;) || para2.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; s&quot;$para1,$para2&quot; &#125; else if (para1 != null &amp;&amp; para1.nonEmpty) &#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; para1 &#125; else if (para2 != null &amp;&amp; para2.nonEmpty)&#123; if(para1.split(&quot;,&quot;).contains(&quot;all&quot;)) return &quot;all&quot; para2 &#125;else&#123; &quot;&quot; &#125; &#125; private def permissionReduce(p1: Permission, p2: Permission): Permission = Permission( getMergedParas(p1.role,p2.role,true), getMergedParas(p1.authorizedDBs, p2.authorizedDBs), getMergedParas(p1.authorizedTables, p2.authorizedTables), getMergedParas(p1.authorizedDataTypes, p2.authorizedDataTypes), getMergedParas(p1.authorizedColumns, p2.authorizedColumns)) private def getFullTableName(tableName:String):String = if (tableName.isEmpty || tableName.equals(&quot;*&quot;)) &quot;*&quot; else if(tableName.contains(&quot;.&quot;)) tableName else s&quot;default.$tableName&quot; /** * Load desensitization metadata * @param sqlContext * @return true-&gt;succeed false-&gt;failed */ def loadDesensitizationMeta(sqlContext: SQLContext):Boolean = &#123; logWarning(&quot;Loading desensitization meta.&quot;) try &#123; // load RULE_UDF_MAP DesensitizationMeta.RULE_UDF_MAP = getDF(sqlContext, &quot;select rule_name,encrypt_udf_name from desensitization_rules&quot;) .collect() .map(x =&gt; (x.get(&quot;rule_name&quot;), x.get(&quot;encrypt_udf_name&quot;))).toMap // load DBTABLE_DESENSITIZATION_CONF_MAP val desensitizationConfMap = scala.collection.mutable.Map[String,scala.collection.mutable.MutableList[DesensitizationConf]]() getDF(sqlContext,&quot;select db_table,column_name,column_type,rule_name from desensitization_conf&quot;) .collect() .foreach&#123;x =&gt; val desensitizationConf = DesensitizationConf(x.get(&quot;column_name&quot;), x.get(&quot;column_type&quot;), x.get(&quot;rule_name&quot;)) if(desensitizationConfMap.get(x.get(&quot;db_table&quot;)).orNull==null)&#123; desensitizationConfMap.put(x.get(&quot;db_table&quot;),scala.collection.mutable.MutableList(desensitizationConf)) &#125;else&#123; desensitizationConfMap(x.get(&quot;db_table&quot;)) += desensitizationConf &#125; &#125; DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP = desensitizationConfMap.map(x =&gt; (getFullTableName(x._1),x._2.toSet)).toMap // load USER_PERMISSION_MAP and ROLE_PERMISSION_MAP DesensitizationMeta.ROLE_PERMISSION_MAP = getDF(sqlContext, &quot;select role,authorized_dbs,authorized_tables,authorized_data_type,authorized_columns from desensitization_role_permissions&quot;) .collect() .map &#123;x =&gt; (x.get(&quot;role&quot;), Permission(x.get(&quot;role&quot;), x.getOrDefault(&quot;authorized_dbs&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_tables&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_data_type&quot;,&quot;&quot;), x.getOrDefault(&quot;authorized_columns&quot;,&quot;&quot;))) &#125;.toMap DesensitizationMeta.USER_PERMISSION_MAP = getDF(sqlContext, &quot;select user,role from desensitization_user_role where user != &#39;&#39; and role != &#39;&#39;&quot;) .collect() .map&#123; x =&gt; (x.get(&quot;user&quot;), x.get(&quot;role&quot;) .split(&quot;,&quot;) .map(role =&gt; DesensitizationMeta.ROLE_PERMISSION_MAP.getOrElse(role,null)) .filter(x =&gt; x != null) .reduce(permissionReduce) ) &#125;.toMap // load DEFAULT_COLUMN_UDF_MAPPING (db_table is * means default global column desensitization configuration.) DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING = DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP.getOrElse(&quot;*&quot;,Set()) .map &#123; ddc =&gt; val udfName = DesensitizationMeta.RULE_UDF_MAP.getOrElse(ddc.ruleName,null) if (udfName == null) &#123; throw new DesensitizationException(s&quot;The mask rule $&#123;ddc.ruleName&#125; on column $&#123;ddc.columnName&#125; is not right or no Configured UDF.Please check desensitization_conf.&quot;) &#125; (ddc.columnName, udfName) &#125;.toMap logWarning(s&quot;## RULE_UDF_MAP ==&gt; $&#123;DesensitizationMeta.RULE_UDF_MAP&#125;&quot;) logWarning(s&quot;## DBTABLE_DESENSITIZATION_CONF_MAP ==&gt; $&#123;DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP&#125;&quot;) logWarning(s&quot;## USER_PERMISSION_MAP ==&gt; $&#123;DesensitizationMeta.USER_PERMISSION_MAP&#125;&quot;) logWarning(s&quot;## DEFAULT_COLUMN_UDF_MAPPING ==&gt; $&#123;DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING&#125;&quot;) true &#125;catch &#123; case e:Exception =&gt; throw new DesensitizationException(&quot;Exception when reloadAuth&quot;,e) false &#125; &#125; /** * create temporary view and get the view name. * The temporary view will clear when spark session exited. * @param spark SQLContext * @param tableName Full tableName with database prefix. * @param userName */ def createAndGetTempViewName(spark:SQLContext,userName:String,tableName:String):String=&#123; val columns = spark.table(tableName).columns val sourceCols = columns.mkString(&quot;,&quot;) val newViewName = tableName.replace(&quot;.&quot;,&quot;_&quot;) + &quot;_&quot; + System.currentTimeMillis() try&#123; val colUDFMap:Map[String,String] = getTableColUDFMapping(tableName, userName, columns) if(colUDFMap.isEmpty) return tableName logWarning(s&quot;### Desensitization table:$tableName user:$userName columnUDFMapping:$colUDFMap&quot;) val viewCols = columns.map &#123; col =&gt; if (!colUDFMap.contains(col)) &#123; col &#125; else &#123; colUDFMap(col) + s&quot;($col)&quot; &#125; &#125;.mkString(&quot;,&quot;) spark.sql(s&quot;create temporary view $newViewName($sourceCols) as select $viewCols from $tableName&quot;) newViewName &#125;catch &#123; case e:Exception =&gt; logError(s&quot;Failed to create a masked temporary view on table $tableName.&quot;,e) // If there is an exception, return the source table name. tableName &#125; &#125; /** * Determine whether the user has access to the table and get the desensitization strategy of table columns. * @param tableName FullTableName with database prefix * @param userName userName * @param tableColumns è¡¨çš„æ‰€æœ‰å­—æ®µ * @return columnUDFMap */ def getTableColUDFMapping(tableName:String,userName:String,tableColumns:Array[String]):Map[String,String] = &#123; logInfo(s&quot;Begin to get table($tableName) user($userName) auth and col-udf mapping.&quot;) // Judge User permission. val userPermission = DesensitizationMeta.USER_PERMISSION_MAP.getOrElse(userName,Permission(&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;)) if(userPermission.role.equals(&quot;admin&quot;))&#123; // user role is admin. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedDBs) || userPermission.authorizedDBs.split(&quot;,&quot;).contains(tableName.split(&quot;\\\\.&quot;)(0))) &#123; //User has permissions for this database. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedTables) || userPermission.authorizedTables.split(&quot;,&quot;).map(getFullTableName).contains(tableName)) &#123; //User has permissions for this table. return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedDataTypes))&#123; return Map() &#125;else if(&quot;all&quot;.equals(userPermission.authorizedColumns))&#123; // User has permissions for all fields. return Map() &#125; val result:scala.collection.mutable.Map[String,String] = scala.collection.mutable.Map[String,String]() // Apply specified desensitization conf. // TODO: User has access to one dataType? val authorizedCols:Set[String] = userPermission.authorizedColumns.split(&quot;,&quot;).toSet val unauthorizedCols:Set[String] = DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING.keySet -- authorizedCols val tableSpecificDesensitizationConf:Set[DesensitizationConf] = DesensitizationMeta.DBTABLE_DESENSITIZATION_CONF_MAP.getOrElse(tableName,Set()) if(tableSpecificDesensitizationConf.nonEmpty) &#123; logWarning(s&quot;Table $tableName has specific desensitization rules.&quot;) tableSpecificDesensitizationConf.foreach&#123; tsdc =&gt; val udfName = DesensitizationMeta.RULE_UDF_MAP.get(tsdc.ruleName).orNull if (udfName == null) &#123; throw new DesensitizationException(s&quot;The mask rule $&#123;tsdc.ruleName&#125; on table $tableName column $&#123;tsdc.columnName&#125; is not right or no Configured UDF.Please check desensitization_conf.&quot;) &#125; logInfo(s&quot;Apply specific desensitization udf: $&#123;tsdc.columnName&#125; =&gt; $udfName .&quot;) result.put(tsdc.columnName,udfName) &#125; &#125;else&#123; logInfo(s&quot;There is no specific desensitization conf in table $tableName.Use default desensitization conf.&quot;) &#125; // Apply default desensitization conf. unauthorizedCols.foreach&#123; uc =&gt; if(tableColumns.contains(uc) &amp;&amp; !result.contains(uc)) &#123; val udfName = DesensitizationMeta.DEFAULT_COLUMN_UDF_MAPPING.get(uc).orNull // if (udfName == null) &#123; // throw new DesensitizationException(s&quot;There is no desensitization UDF associated with column $uc ,Please check desensitization_conf where db_table=&#39;*&#39;.&quot;) // &#125; logInfo(s&quot;Apply default desensitization udf: $uc =&gt; $udfName .&quot;) result.put(uc,udfName) &#125; &#125; result.toMap &#125; /** * Main method to generate desensitized sql. * @param spark sparkSession * @param userName request user of this sql * @param sql * @return Desensitized sql */ def getDesensitizedSQL(spark:SQLContext,userName:String,sql:String):String = &#123; if(isSelectSQL(sql))&#123; val tableList = getTablesInSelect(sql) if(tableList.isEmpty)&#123; return sql &#125; var outputSQL:String = sql tableList .foreach &#123; table =&gt; val viewName = createAndGetTempViewName(spark, userName,getFullTableName(table)) logInfo(s&quot;tableName: $table =&gt; viewName: $viewName&quot;) outputSQL = outputSQL.replace(table, viewName) &#125; outputSQL &#125;else if(isSelectInsertSQL(sql))&#123; //TODO: There is a vulnerability in the export of sensitive data. // &#39;insert select&#39; operation and &#39;create table as select&#39; operation. sql &#125;else&#123; sql &#125; &#125; &#125; å¯»æ‰¾ä¿®æ”¹åˆ‡å…¥ç‚¹org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperationç±»ï¼Œæ˜¯ThriftServeræä¾›æœåŠ¡çš„å…¥å£ï¼Œå…¶ä¸­æˆ‘ä»¬åªéœ€è¦å°†æ‰§è¡Œæ—¶çš„é€»è¾‘è®¡åˆ’æ›¿æ¢æ‰å³å¯ï¼Œè¯¥ç±»ä¸­executeæ–¹æ³•ä¸­å¯ä»¥æ‰¾åˆ°æºç ï¼š // Always set the session state classloader to `executionHiveClassLoader` even for sync mode if (!runInBackground) &#123; parentSession.getSessionState.getConf.setClassLoader(executionHiveClassLoader) &#125; sqlContext.sparkContext.setJobGroup(statementId, substitutorStatement, forceCancel) result = sqlContext.sql(statement) logDebug(result.queryExecution.toString()) HiveThriftServer2.eventManager.onStatementParsed(statementId, result.queryExecution.toString()) iter = if (sqlContext.getConf(SQLConf.THRIFTSERVER_INCREMENTAL_COLLECT.key).toBoolean) &#123; new IterableFetchIterator[SparkRow](new Iterable[SparkRow] &#123; override def iterator: Iterator[SparkRow] = result.toLocalIterator.asScala &#125;) &#125; else &#123; new ArrayFetchIterator[SparkRow](result.collect()) &#125; æˆ‘ä»¬å¯ä»¥æ˜ç¡®çš„æ˜¯statementæ˜¯ç”¨æˆ·æäº¤è¿è¡Œçš„SQLï¼Œåé¢è§¦å‘è®¡ç®—æ“ä½œæ—¶è°ƒç”¨äº†result.collect()ï¼Œåˆ™resultå°±æ˜¯è¿™æ¡SQLçš„ç»“æœé›†ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹çš„å°±æ˜¯resultè¿™ä¸ªdataframeå¯¹è±¡ã€‚ä¿®æ”¹æ–¹æ³•å¾ˆç®€å•ï¼Œç”¨æˆ‘ä»¬è„±æ•åçš„SQLé‡æ–°ç”ŸæˆParsedLogicalPlanï¼Œå†ç”¨Dataset.ofRowså¾—åˆ°æ–°çš„Datasetï¼š var logicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(statement) // Desensitization var sqlAfterDesensitization:String = statement try&#123; sqlAfterDesensitization = DesensitizationModule.getDesensitizedSQL(sqlContext, parentSession.getUserName, sql) if(!sqlAfterDesensitization.equals(statement))&#123; logWarning(s&quot;### SQL has changed after Desensitization Module. outputSQL: $sqlAfterDesensitization ###&quot;) logicalPlan = sqlContext.sparkSession.sessionState.sqlParser.parsePlan(sqlAfterDesensitization) &#125;else&#123; logInfo(s&quot;###ç»è¿‡è„±æ•æ¨¡å—å¤„ç†åçš„SQLä¸º: $statement æœªå‘ç”Ÿæ”¹å˜###&quot;) &#125; &#125;catch &#123; case e:Exception =&gt; logError(&quot;***There may be some errors in DesensitizationModule.getDesensitizedSQL ***&quot;, e) // throw new DesensitizationException(&quot;Desensitization failed.&quot;,e) // Table not found and sql syntax error also throw this. &#125; result=Dataset.ofRows(sqlContext.sparkSession, logicalPlan) è¿™æ ·åœ¨éœ€è¦è„±æ•æ—¶é€»è¾‘è®¡åˆ’å°±å¯ä»¥è¢«æ›¿æ¢å¹¶æ‰§è¡Œåç»­çš„æ“ä½œäº†ï¼Œè¿”å›ç»™ç”¨æˆ·çš„æ•°æ®ä¹Ÿæ˜¯è„±æ•åçš„æ•°æ®ã€‚ UDFç¼–å†™å’Œæ³¨å†ŒDesensitizationUDFsç±»ï¼Œæ³¨å†Œè„±æ•UDFçš„ç»Ÿä¸€å…¥å£ï¼Œåœ¨org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManagerçš„OpenSessionæ–¹æ³•ä¸­è°ƒç”¨ï¼šDesensitizationUDFs.register(ctx,username)ï¼Œä¸ºæ­£åœ¨ç™»é™†çš„ç”¨æˆ·è°ƒç”¨æ³¨å†ŒUDFï¼Œä¿è¯UDFå¯ç”¨ã€‚ä½†å¦‚æœæœ‰ç”¨æˆ·æ¶æ„é¢‘ç¹ç™»é™†ä¼šè§¦å‘é¢‘ç¹UDFæ³¨å†Œï¼Œå¯¼è‡´Thriftserverè´Ÿè½½é«˜ï¼Œæ•…å¯è®¾ç½®å…UDFåŠ è½½çš„ç™½åå•ç”¨æˆ·å‚æ•°ï¼šâ€“conf â€œspark.thrift.desensitization.load.udf.user.whitelist=user1,adminâ€ // Register Desensitization UDFs var loadUDFWhitelist:Array[String] = Array() try&#123; // æäº¤ä»»åŠ¡æ—¶åŠ --conf &quot;spark.thrift.desensitization.load.udf.user.whitelist=user1,admin&quot; è¿™äº›ç”¨æˆ·ä¸åŠ è½½è„±æ•UDF loadUDFWhitelist = ctx.sparkSession.conf.get(&quot;spark.thrift.desensitization.load.udf.user.whitelist&quot;).split(&quot;,&quot;) &#125;catch &#123; case e:Exception =&gt; e.printStackTrace() //å¦‚æœæ²¡é…ç½®è¯¥å‚æ•° java.util.NoSuchElementException &#125; if(!loadUDFWhitelist.contains(username))&#123; DesensitizationUDFs.register(ctx,username) &#125; UDFç±»ï¼š package org.apache.spark.sql.hive.thriftserver.desensitization import org.apache.commons.lang.StringUtils import org.apache.spark.internal.Logging import org.apache.spark.sql.SQLContext import org.apache.spark.sql.hive.thriftserver.UDFUtil /** * Desensitization UDF * Created by Shmily on 2020/11/23. */ object DesensitizationUDFs extends Serializable with Logging&#123; // private val logger: Logger = LoggerFactory.getLogger(Desensitization.getClass) /** * [Irreversible] Hide the mid 4 digits of mobile phone number * @param phoneNumber * @return Encrypted phoneNumber */ def HideMid4PhoneNumber(phoneNumber:String): String = &#123; if(phoneNumber==null)&#123; return phoneNumber &#125; if(StringUtils.isBlank(phoneNumber))&#123; return &quot;&quot; &#125; phoneNumber.length match &#123; case 11 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*(\\\\w&#123;4&#125;)&quot;, &quot;$1****$2&quot;) case 7 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*&quot;, &quot;$1****&quot;) case _ =&gt; phoneNumber &#125; &#125; /** * [Irreversible] Hide the last 4 digits of mobile phone number * @param phoneNumber * @return Encrypted phoneNumber */ def HideLast4PhoneNumber(phoneNumber:String): String = &#123; if(phoneNumber==null)&#123; return phoneNumber &#125; if(StringUtils.isBlank(phoneNumber))&#123; return &quot;&quot; &#125; phoneNumber.length match &#123; case 11 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;7&#125;)\\\\w*&quot;, &quot;$1****&quot;) case 7 =&gt; phoneNumber.replaceAll(&quot;(\\\\w&#123;3&#125;)\\\\w*&quot;, &quot;$1****&quot;) case _ =&gt; phoneNumber &#125; &#125; /** * [Irreversible] Keep first char only. * @param name * @return Encrypted Name */ def HideUserName(name:String): String = &#123; if(name==null)&#123; return name &#125; if(StringUtils.isBlank(name))&#123; return &quot;&quot; &#125; val length = name.length name.substring(0,1).concat(&quot;*&quot; * (length-1)) &#125; /** * [Irreversible] ID Card keep 1-6 and last 3 digits. * @param id * @return Encrypted id */ def HideIDCard(id:String): String = &#123; if(id==null)&#123; return id &#125; if(StringUtils.isBlank(id))&#123; return &quot;&quot; &#125; id.length match &#123; case 15 =&gt; id.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1******$2&quot;) case 18 =&gt; id.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1*********$2&quot;) case _ =&gt; id &#125; &#125; /** * [Irreversible] ID Card keep 1-6 and last 3 digits. * @param bankCardId * @return Encrypted bankCardId */ def HideBankCardNumber(bankCardId:String): String = &#123; if(bankCardId==null)&#123; return bankCardId &#125; if(StringUtils.isBlank(bankCardId))&#123; return &quot;&quot; &#125; bankCardId.length match &#123; case 16 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1*******$2&quot;) case 17 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1********$2&quot;) case 19 =&gt; bankCardId.replaceAll(&quot;(\\\\w&#123;6&#125;)\\\\w*(\\\\w&#123;3&#125;)&quot;, &quot;$1**********$2&quot;) case _ =&gt; bankCardId &#125; &#125; /** * register all desensitization udf * @param ctx SQLContext */ def register(ctx:SQLContext,username:String):Unit = &#123; logWarning(s&quot;Registering desensitization UDFs for session [user: $username]&quot;) ctx.udf.register(&quot;hide_mid_4_phone_number&quot;,HideMid4PhoneNumber _) ctx.udf.register(&quot;hide_last_4_phone_number&quot;,HideLast4PhoneNumber _) ctx.udf.register(&quot;hide_user_name&quot;,HideUserName _) ctx.udf.register(&quot;hide_id_card&quot;,HideIDCard _) ctx.udf.register(&quot;hide_bank_card_number&quot;,HideBankCardNumber _) &#125; &#125; å®ç°å¯åŠ¨æ—¶åŠ è½½è„±æ•é…ç½®org.apache.spark.sql.hive.thriftserver.HiveThriftServer2ç±»ï¼Œå«æœ‰ThriftServerçš„mainæ–¹æ³•ï¼Œåœ¨å¯åŠ¨ThriftServeræœåŠ¡æ—¶è¿è¡Œï¼Œåœ¨DeveloperApiæ³¨è§£ä¸‹çš„startWithContextæ–¹æ³•æ·»åŠ åŠ è½½è„±æ•é…ç½®ä¿¡æ¯çš„æ–¹æ³•loadDesensitizationMetaï¼Œä¿è¯æ¯æ¬¡å¯åŠ¨ThriftServerç”Ÿæ•ˆé…ç½®ã€‚ @DeveloperApi def startWithContext(sqlContext: SQLContext): HiveThriftServer2 = &#123; val executionHive = HiveUtils.newClientForExecution( sqlContext.sparkContext.conf, sqlContext.sessionState.newHadoopConf()) DesensitizationModule.loadDesensitizationMeta(sqlContext) ...... å®ç°æ‰‹åŠ¨åˆ·æ–°è„±æ•é…ç½®org.apache.spark.sql.hive.thriftserver.server.SparkSQLOperationManagerç±»ï¼Œç”¨äºç®¡ç†Sparkçš„Operationï¼Œå…¶ä¸­newExecuteStatementOperationæ˜¯å®é™…æ‰§è¡Œstatementçš„æ–¹æ³•ï¼Œåœ¨è¿™é‡Œåˆ¤æ–­SQLå¦‚æœä¸ºâ€refresh_desensitization_authâ€åˆ™è°ƒç”¨loadDesensitizationMetaæ–¹æ³•åˆ·æ–°è„±æ•é…ç½®ä¿¡æ¯ override def newExecuteStatementOperation( parentSession: HiveSession, statement: String, confOverlay: JMap[String, String], async: Boolean): ExecuteStatementOperation = synchronized &#123; val sqlContext = sessionToContexts.get(parentSession.getSessionHandle) Authentication.checkIp(parentSession) Authentication.checkUser(parentSession) require(sqlContext != null, s&quot;Session handle: $&#123;parentSession.getSessionHandle&#125; has not been&quot; + s&quot; initialized or had already closed.&quot;) val conf = sqlContext.sessionState.conf val hiveSessionState = parentSession.getSessionState setConfMap(conf, hiveSessionState.getOverriddenConfigurations) setConfMap(conf, hiveSessionState.getHiveVariables) val runInBackground = async &amp;&amp; conf.getConf(HiveUtils.HIVE_THRIFT_SERVER_ASYNC) var sql = statement.toLowerCase.trim var statement2 = statement if (sql.startsWith(&quot;create&quot;) &amp;&amp; sql.indexOf(&quot;options(&quot;) != -1) &#123; //remove &quot;\\n&quot; when creating external hbase table statement2 = statement.replaceAll(&quot;\\n&quot;, &quot; &quot;) &#125; // load desensitization if (&quot;refresh_desensitization_auth&quot;.equals(sql)) &#123; if (DesensitizationModule.loadDesensitizationMeta(sqlContext)) &#123; // è¿™é‡Œå¯ä»¥åŠ é™åˆ¶adminæƒé™çš„ç”¨æˆ·æ‰èƒ½åˆ·æ–° statement2 = &quot;select &#39;refresh_desensitization_auth succeed&#39;&quot; &#125; else &#123; statement2 = &quot;select &#39;refresh_desensitization_auth failed&#39;&quot; &#125; &#125; ...... å®ç°æ•ˆæœ æ€»ç»“ä¼˜ç‚¹ï¼š 1.æ— é¢å¤–çš„ç¡¬ä»¶æˆæœ¬å¼€é”€ 2.æ€§èƒ½æŸè€—å° 3.ç”¨æˆ·æ— æ„ŸçŸ¥ 4.æƒé™è®¾è®¡çµæ´»ç¼ºç‚¹ï¼š 1.ç”¨æˆ·å¦‚æœç”¨æ•æ„Ÿå­—æ®µå…³è”ï¼Œç»“æœä¸å‡†ç¡® 2.å¦‚æœç”¨æˆ·å°†æ•æ„Ÿæ•°æ®åˆ›å»ºä¸´æ—¶è¡¨ï¼Œä¸”å­—æ®µåç§°éé€šç”¨æ•æ„Ÿå­—æ®µåç§°ï¼Œå°±æ²¡åŠæ³•è„±æ•äº†æ”¹è¿›ï¼šè®¾ç½®è·‘æ‰¹ç¨‹åºï¼Œéå†æ•°ä»“çš„è¡¨ï¼Œæ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨å‘ç°æ•æ„Ÿå­—æ®µï¼Œå¹¶è‡ªåŠ¨è¿­ä»£è„±æ•é…ç½®åº“ æ‰©å±•&emsp;&emsp;ä¸Šå›¾æ˜¯HiveServer2å’ŒSparkThriftServerçš„æ¶æ„ï¼Œå¯ä»¥çœ‹å‡ºä¸¤è€…æ¶æ„ç›¸è¿‘ã€‚SparkThriftServerå¤§é‡å¤ç”¨äº†HiveServer2çš„ä»£ç ã€‚&emsp;&emsp;HiveServer2çš„æ¶æ„ä¸»è¦æ˜¯é€šè¿‡ThriftCLIServiceç›‘å¬ç«¯å£ï¼Œç„¶åè·å–è¯·æ±‚åå§”æ‰˜ç»™CLIServiceå¤„ç†ã€‚CLIServiceåˆä¸€å±‚å±‚çš„å§”æ‰˜ï¼Œæœ€ç»ˆäº¤ç»™OperationManagerå¤„ç†ã€‚OperationManagerä¼šæ ¹æ®è¯·æ±‚çš„ç±»å‹åˆ›å»ºä¸€ä¸ªOperationçš„å…·ä½“å®ç°å¤„ç†ã€‚æ¯”å¦‚Hiveä¸­æ‰§è¡Œsqlçš„Operationå®ç°æ˜¯SQLOperationã€‚&emsp;&emsp;Spark Thrift Serveråšçš„äº‹æƒ…å°±æ˜¯å®ç°è‡ªå·±çš„CLIServiceâ€”â€”SparkSQLCLIServiceï¼Œæ¥ç€ä¹Ÿå®ç°äº†SparkSQLSessionManagerä»¥åŠSparkSQLOperationManagerã€‚å¦å¤–è¿˜å®ç°äº†ä¸€ä¸ªå¤„ç†sqlçš„Operationâ€”â€”SparkExecuteStatementOperationã€‚è¿™æ ·ï¼Œå½“Spark Thrift Serverå¯åŠ¨åï¼Œå¯¹äºsqlçš„æ‰§è¡Œå°±ä¼šæœ€ç»ˆäº¤ç»™SparkExecuteStatementOperationäº†ã€‚ åŸºäºSparkæ‰§è¡Œè®¡åˆ’è‡ªå®šä¹‰Ruleçš„æ•°æ®è„±æ•æœªå®Œå¾…ç»­â€¦ã€‚ã€‚ã€‚ï¼ï¼ï¼ï¼Ÿï¼Ÿï¼Ÿ","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://shmily-qjj.top/tags/Spark/"},{"name":"æ•°æ®è„±æ•","slug":"æ•°æ®è„±æ•","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E8%84%B1%E6%95%8F/"},{"name":"äºŒæ¬¡å¼€å‘","slug":"äºŒæ¬¡å¼€å‘","permalink":"https://shmily-qjj.top/tags/%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"å¤§æ•°æ®è„±æ•æ–¹æ¡ˆè°ƒç ”","slug":"å¤§æ•°æ®è„±æ•æ–¹æ¡ˆè°ƒç ”","date":"2020-10-20T05:19:00.000Z","updated":"2022-12-11T05:35:07.917Z","comments":true,"path":"f5da73a2/","link":"","permalink":"https://shmily-qjj.top/f5da73a2/","excerpt":"","text":"å¤§æ•°æ®è„±æ•æ–¹æ¡ˆè°ƒç ”èƒŒæ™¯&emsp;&emsp;å¤§æ•°æ®å‘å±•é€Ÿåº¦é£å¿«ï¼Œå¤§æ•°æ®çš„ä»·å€¼ä¹Ÿæœ‰ç›®å…±ç¹ï¼Œåœ¨å¤§æ•°æ®æŠ€æœ¯é¢†åŸŸï¼Œå¯¹äºåˆ†ææ€§èƒ½ï¼Œå®æ—¶æ€§ç­‰æ–¹é¢éƒ½æœ‰äº†å¾ˆå¤§çš„çªç ´ï¼Œä½†æ•°æ®å®‰å…¨é—®é¢˜åœ¨æ•°æ®ä¸šåŠ¡å»ºç«‹åˆæœŸå¾ˆéš¾è¢«é‡è§†ï¼Œè€Œæ•°æ®è§„æ¨¡å£®å¤§åæ‰å¼€å§‹é‡è§†ï¼Œä»¥è‡´å¤§å¤šæ•°ä¼ä¸šå¤§æ•°æ®å¹³å°å®‰å…¨ç®¡æ§èƒ½åŠ›æ™®éç¼ºå¤±ã€‚ã€‚ç°ä»Šæ•°æ®å®‰å…¨é—®é¢˜é¢‘å‘ï¼Œè€Œä¸”ä¸€æ—¦å‘ç”Ÿå°±ä¼šå¯¹å…¬å¸é€ æˆå¾ˆä¸¥é‡çš„åˆ©ç›Šç”šè‡³å£°èª‰æŸå®³ã€‚&emsp;&emsp;æ•°æ®å®‰å…¨å¼•å‘çš„é—®é¢˜ä»£ä»·æé«˜ï¼Œåæœä¸¥é‡ï¼Œè€Œæˆ‘ä»¬åˆä¸èƒ½ä¿è¯æœåŠ¡å™¨æ°¸è¿œä¸ä¼šè¢«æ”»å‡»ï¼Œæ‰€ä»¥ï¼ŒåŠæ—¶æ­¢æŸæ‰æ˜¯å…³é”®ï¼Œå¤§æ•°æ®è„±æ•æ­£æ˜¯è¿™å…³é”®çš„ä¸€ä¸ªç¯èŠ‚ã€‚æœ‰äº†æ•°æ®è„±æ•ï¼Œå°±å¯ä»¥éšæ—¶ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œé˜²æ­¢é‡è¦ä¿¡æ¯æ³„éœ²ï¼Œå³ä½¿æœåŠ¡å™¨è¢«æ”»å‡»æˆ–æœ‰å†…é¬¼ï¼Œä¹Ÿä¸æ‹…å¿ƒæ•æ„Ÿæ•°æ®è¢«å¸¦èµ°ã€‚çœ‹æ¥æ—¢è¦é˜²å¤–è´¼åˆè¦é˜²å†…é¬¼ï¼Œä»»é‡é“è¿œå•Šâ€¦&emsp;&emsp;æ‰€ä»¥æ•°æ®è„±æ•æ˜¯å¤§æ•°æ®å¤„ç†é“¾è·¯ä¸­é‡è¦çš„ä¸€ç¯ï¼Œå»ºç«‹å¤§æ•°æ®è„±æ•ä½“ç³»å¹³å°è¿«åœ¨çœ‰ç«ã€‚ æ•°æ®è„±æ•å®šä¹‰&emsp;&emsp;æ•°æ®è„±æ•(Data Masking),åˆç§°æ•°æ®æ¼‚ç™½ã€æ•°æ®å»éšç§åŒ–æˆ–æ•°æ®å˜å½¢ã€‚å¯¹æ•æ„Ÿä¿¡æ¯é€šè¿‡è„±æ•è§„åˆ™è¿›è¡Œæ•°æ®çš„å˜å½¢ï¼Œæ¨¡ç³ŠåŒ–ï¼Œä¼ªè£…ä»è€Œå®ç°æ•æ„Ÿéšç§æ•°æ® çš„å¯é ä¿æŠ¤ã€‚æ•°æ®è„±æ•åï¼Œå°±å¯ä»¥åœ¨å¼€å‘ã€æµ‹è¯•å’Œå…¶å®ƒéç”Ÿäº§ç¯å¢ƒä»¥åŠå¤–åŒ…ç¯å¢ƒä¸­å®‰å…¨åœ°ä½¿ç”¨è„±æ•åçš„çœŸå®æ•°æ®é›†ã€‚ ç›®æ ‡ é’ˆå¯¹å¤§æ•°æ®æ•æ„Ÿæ•°æ®ä¿¡æ¯ï¼Œè®¾è®¡å¹¶è½å®æ•æ„Ÿæ•°æ®å®‰å…¨è§£å†³æ–¹æ¡ˆï¼Œå®ç°æ•æ„Ÿæ•°æ®çš„æ¨¡ç³ŠåŒ–ï¼Œç¡®ä¿æ•æ„Ÿæ•°æ®ä¿¡æ¯å®‰å…¨å¯é  é€šè¿‡å¤§æ•°æ®å¹³å°å®‰å…¨æ–¹æ¡ˆçš„å»ºè®¾ï¼Œå¡«è¡¥å¤§æ•°æ®å¹³å°æ•°æ®å®‰å…¨é˜²æŠ¤æ–¹é¢çš„ç©ºç¼ºï¼Œæœ‰æ•ˆé™ä½å¤§æ•°æ®å®‰å…¨ç®¡æ§æ–¹é¢çš„é£é™© å‘ç”Ÿæ•°æ®æ³„éœ²æ—¶é£é™©å¯æ§ å¯ç®¡æ§çš„æ•°æ®è„±æ•å¹³å°ï¼Œç»“åˆç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†ä»¥åŠéšç§æ•°æ®çº§åˆ«å®ç°åŸºäºå®¡æ‰¹æ¨¡å¼çš„æ•°æ®è®¿é—® æ•°æ®åˆ†æä¸æ•°æ®è„±æ•æ˜¯çŸ›ç›¾çš„ï¼Œè¦åšåˆ°åŒæ—¶å…¼é¡¾æ•°æ®å®‰å…¨å’Œæ•°æ®ä½¿ç”¨ï¼Œä¿è¯æ•°æ®å®‰å…¨çš„åŒæ—¶æœ€å¤§åŒ–æ•°æ®çš„åˆ†æä»·å€¼ åšåˆ°æ•°æ®å®¡è®¡ï¼Œå‘ç”Ÿæ•°æ®æ³„éœ²æ—¶æ–¹ä¾¿å¿«é€Ÿå®šä½æ³„éœ²åŸå›  éš¾ç‚¹ æµ·é‡å­˜é‡æ•°æ®å·²ç»å½¢æˆ ä¸»è¦æ¶‰åŠç³»ç»Ÿå’Œæ•°ä»“ä¸¤ä¸ªå±‚é¢ï¼Œåº”ç”¨å¤šï¼Œåº”ç”¨ç¯å¢ƒå¤æ‚ ä¸»åŠ¨å‘ç°æ•æ„Ÿæ•°æ®å›°éš¾ åŸåˆ™ è„±æ•é€šå¸¸å¤šæ•°æƒ…å†µæ˜¯ä¸å¯é€†çš„ï¼Œä½†ä¹Ÿæœ‰è¦æ±‚å¯ä»¥æ¢å¤åŸå§‹æ•°æ®çš„åœºæ™¯ è„±æ•åæ•°æ®é€šå¸¸åº”å…·æœ‰åŸæ•°æ®çš„ç‰¹å¾ï¼Œé€‚ç”¨äºå¼€å‘å’Œæµ‹è¯•ç¯å¢ƒï¼Œè€Œä¸æ˜¯æ— æ„ä¹‰çš„å­—ç¬¦ä¸²ï¼Œæ¯”å¦‚é“¶è¡Œå¡å·å‰å››ä½è¡¨ç¤ºé“¶è¡Œåç§°è„±æ•åè¿™å››ä½ä¹Ÿä¿æŒä¸å˜ï¼›æ•°æ®è¦æ±‚é«˜æ—¶ï¼Œå¯èƒ½è¦åšåˆ°è„±æ•åæ•°æ®ä¸åŸå§‹æ•°æ®é¢‘ç‡åˆ†å¸ƒä¸€è‡´ï¼Œå­—æ®µå”¯ä¸€æ€§ç­‰ æ•°æ®å…³è”å…³ç³»è¦ä¿ç•™ï¼Œä¸šåŠ¡è§„åˆ™å…³è”æ€§ä¿è¯ï¼Œå¦‚ä¸»é”®å¤–é”®æ•°æ®è„±æ•ååœ¨å¦ä¸€ä¸ªè¡¨ä»ç„¶èƒ½å…³è”åˆ°ï¼Œå¦‚è´¦æˆ·ç±»æ•°æ®å¾€å¾€ä¼šè´¯ç©¿ä¸»ä½“çš„æ‰€æœ‰å…³ç³»å’Œè¡Œä¸ºä¿¡æ¯éœ€è¦ç‰¹åˆ«æ³¨æ„ä¿è¯æ‰€æœ‰ç›¸å…³ä¸»ä½“ä¿¡æ¯çš„ä¸€è‡´æ€§ æ‰€æœ‰å¯èƒ½ç”Ÿæˆæ•æ„Ÿæ•°æ®çš„éæ•æ„Ÿå­—æ®µåŒæ ·éœ€è¦è„±æ•ï¼Œè¦é¢å¯¹æ ¹æ®éæ•æ„Ÿå­—æ®µèƒ½æ¨å¯¼å‡ºæ•æ„Ÿä¿¡æ¯çš„åœºæ™¯ è„±æ•è¿‡ç¨‹è‡ªåŠ¨åŒ–å¯é‡å¤ï¼Œè„±æ•ç»“æœç¨³å®šå‡†ç¡®ï¼Œå¤šæ¬¡è„±æ•åæ•°æ®å§‹ç»ˆä¸€è‡´ æ•°æ®è„±æ•æµç¨‹åˆ†ä¸ºæ•æ„Ÿæ•°æ®å‘ç°-&gt;æ•æ„Ÿæ•°æ®æ¢³ç†-&gt;è„±æ•æ–¹æ¡ˆåˆ¶å®š-&gt;è„±æ•ä»»åŠ¡æ‰§è¡Œ æ•æ„Ÿæ•°æ®å‘ç°æ•æ„Ÿæ•°æ®å‘ç°åˆ†ä¸ºäººå·¥å’Œè‡ªåŠ¨ä¸¤ç§ï¼Œä¸€èˆ¬æ˜¯ä»¥è‡ªåŠ¨ä¸ºä¸»ç»“åˆäººå·¥è¾…åŠ©ã€‚äººå·¥å¯ä»¥æŒ‡å®šæ•°æ®è„±æ•è§„åˆ™ã€æ•æ„Ÿæ•°æ®ç‰¹å¾å’Œä¸åŒæ•°æ®çš„è„±æ•ç­–ç•¥ã€‚è‡ªåŠ¨è¯†åˆ«æ˜¯æ ¹æ®äººå·¥æŒ‡å®šçš„æ•æ„Ÿæ•°æ®ç‰¹å¾ï¼Œå€ŸåŠ©æ•æ„Ÿæ•°æ®ä¿¡æ¯åº“å’Œåˆ†è¯ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«æ•æ„Ÿä¿¡æ¯ï¼Œç›¸å¯¹äºäººå·¥æ–¹å¼ï¼Œè‡ªåŠ¨è¯†åˆ«å¯ä»¥å‡å°‘å·¥ä½œé‡å’Œé˜²æ­¢ç–æ¼ã€‚æ•æ„Ÿæ•°æ®å‘ç°æ˜¯ä¸€ä¸ªé—­ç¯è¿‡ç¨‹ï¼Œä¸æ–­ä¼˜åŒ–å’Œå®Œå–„æ•æ„Ÿæ•°æ®ä¿¡æ¯åº“ã€‚ æ•æ„Ÿæ•°æ®æ¢³ç†åœ¨æ•æ„Ÿæ•°æ®å‘ç°çš„åŸºç¡€ä¸Šï¼Œæ¢³ç†æ•æ„Ÿæ•°æ®åˆ—ï¼Œæ•æ„Ÿæ•°æ®å…³è”å…³ç³»ï¼Œä¸åŒç±»å‹æ•°æ®çš„ä¸åŒè„±æ•æ–¹å¼ï¼Œä¿è¯æ¸…æ™°çš„è„±æ•åå…³è”å…³ç³»ã€‚æ•æ„Ÿä¿¡æ¯å­—æ®µçš„åç§°ã€æ•æ„Ÿçº§åˆ«ã€å­—æ®µç±»å‹ã€å­—æ®µé•¿åº¦ã€èµ‹å€¼è§„èŒƒç­‰å†…å®¹åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­æ˜ç¡®ï¼Œç”¨äºä¸‹é¢è„±æ•ç­–ç•¥åˆ¶å®šçš„ä¾æ®ã€‚ è„±æ•æ–¹æ¡ˆåˆ¶å®šé’ˆå¯¹ä¸åŒä¸šåŠ¡çš„æ•°æ®è„±æ•éœ€æ±‚ï¼Œåœ¨å·²æœ‰è„±æ•ç®—æ³•åŸºç¡€ä¸Šå®šåˆ¶è„±æ•ç­–ç•¥ã€‚è¯¥æ­¥ä¸»è¦é€šè¿‡è„±æ•ç­–ç•¥å¤ç”¨ä¸åŒè„±æ•ç®—æ³•å®ç°ã€‚ è„±æ•ä»»åŠ¡æ‰§è¡Œå®‰æ’è„±æ•ä»»åŠ¡ï¼Œå®šæ—¶è·‘æ‰¹ï¼Œå¹¶è¡Œå¤„ç†ï¼Œæ–­ç‚¹ç»­å»¶ï¼Œå®¹é”™â€¦ è„±æ•ç®—æ³•æœ‰å‡ ç§é€šç”¨ç®—æ³•ï¼Œä¹Ÿæœ‰è¦æ ¹æ®ä¸šåŠ¡éœ€æ±‚å’Œæ•°æ®æ¥å®šåˆ¶çš„è„±æ•ç®—æ³•å¦‚k-åŒ¿åï¼ŒL-å¤šæ ·æ€§ï¼Œæ•°æ®æŠ‘åˆ¶ï¼Œæ•°æ®æ‰°åŠ¨ï¼Œå·®åˆ†éšç§ç­‰â€¦ è„±æ•è§„åˆ™åˆ†ä¸ºå¯æ¢å¤å’Œä¸å¯æ¢å¤ä¸¤ç§ç±»å‹çš„è„±æ•è§„åˆ™ç›®æ ‡ï¼šå»ºç«‹æ•°æ®è„±æ•è§„åˆ™ç®—æ³•åº“ æ›¿ä»£ç”¨ä¼ªè£…æ•°æ®å®Œå…¨æ›¿æ¢æºæ•°æ®ä¸­çš„æ•æ„Ÿæ•°æ®ï¼Œæ›¿ä»£æ˜¯æœ€å¸¸ç”¨çš„æ•°æ®è„±æ•æ–¹æ³•å…·ä½“æ“ä½œï¼š å¸¸æ•°æ›¿ä»£ï¼ˆæ‰€æœ‰æ•æ„Ÿæ•°æ®éƒ½æ›¿æ¢ä¸ºå”¯ä¸€çš„å¸¸æ•°å€¼ï¼‰ æŸ¥è¡¨æ›¿ä»£ï¼ˆä»ä¸­é—´è¡¨ä¸­éšæœºæˆ–æŒ‰ç…§ç‰¹å®šç®—æ³•é€‰æ‹©æ•°æ®è¿›è¡Œæ›¿ä»£ï¼Œä¸­é—´è¡¨çš„è®¾è®¡éå¸¸å…³é”®ï¼‰ å‚æ•°åŒ–æ›¿ä»£ï¼ˆä»¥æ•æ„Ÿæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç‰¹å®šå‡½æ•°å½¢æˆæ–°çš„æ›¿ä»£æ•°æ®ï¼‰å…·ä½“ä½¿ç”¨å“ªç§æ“ä½œå–å†³äºæ•ˆç‡ã€ä¸šåŠ¡éœ€æ±‚ç­‰å› ç´ é—´çš„å¹³è¡¡ç‰¹ç‚¹ï¼šèƒ½å¤Ÿå½»åº•çš„è„±æ•å•ç±»æ•°æ®ï¼Œä½†ä¹Ÿä¼šä½¿ç›¸å…³å­—æ®µå¤±å»ä¸šåŠ¡å«ä¹‰ éšæœºå˜æ¢å¯¹å¾…è„±æ•æ•°æ®é€šè¿‡éšæœºå‡½æ•°è°ƒæ•´ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨è„±æ•æ–¹æ³•éšæœºå‡½æ•°é€»è¾‘ï¼š æ•°å€¼ç±»å‹éšæœºå¢å‡ç™¾åˆ†æ¯” æ—¥æœŸç±»å‹éšæœºå¢åŠ å¤©æ•° Stringç±»å‹æ•°å­—å˜éšæœºæ•°å­—ï¼Œå­—æ¯å˜éšæœºå­—æ¯ç‰¹ç‚¹ï¼šèƒ½ä¿æŒæ•°æ®ç‰¹å¾å’Œä¸šåŠ¡å«ä¹‰ä¾‹å­ï¼šabc123 -&gt; drh428 æ··æ´—å¯¹æ•æ„Ÿå­—æ®µæ•°æ®è·¨è¡Œéšæœºäº’æ¢æ¥ç ´ååŸæœ‰æ•°æ®å®ç°è„±æ•ç‰¹ç‚¹ï¼šä¿è¯äº†å­—æ®µçš„æ•°æ®èŒƒå›´ï¼Œæ•°æ®ç‰¹å¾å’Œä¸šåŠ¡å«ä¹‰ï¼Œä½†ç‰ºç‰²äº†å®‰å…¨æ€§ï¼Œæœ‰è¢«è¿˜åŸçš„å¯èƒ½ä¾‹å­ï¼š20201024 -&gt; 20180112 åŠ å¯†åŠ å¯†å¾…è„±æ•çš„æ•°æ®ï¼Œä½¿ç”¨æ–¹é€šè¿‡ä¸åŒçš„å¯†é’¥æ¥è§£å¯†å¾—åˆ°åŸå§‹æ•°æ®ï¼Œä½¿ç”¨è¾ƒå°‘ç‰¹ç‚¹ï¼šä¸ä¿è¯æ•°æ®ç‰¹å¾å’Œä¸šåŠ¡å«ä¹‰ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£å¦‚å¯†é’¥æ³„éœ²å’ŒåŠ å¯†å¼ºåº¦ä¸å¤Ÿï¼Œè€—è´¹é›†ç¾¤ç®—åŠ›ä¾‹å­ï¼šabc -&gt; TH3Wwi2wif51ga é®æŒ¡å¯¹æ•æ„Ÿæ•°æ®ç”¨*æˆ–xç­‰å­—ç¬¦é®æŒ¡ä»è€ŒåŠ å¯†æ•°æ®ï¼Œæ˜¯å¸¸ç”¨è„±æ•æ–¹å¼ç‰¹ç‚¹ï¼šä¿æŒæ•°æ®ç‰¹å¾æ ¼å¼ï¼Œè„±æ•æ•ˆæœå¥½ä¾‹å­ï¼š18612346666 -&gt; 186xxxx66xx Hashæ˜ å°„å°†æ•°æ®æ˜ å°„ä¸ºHashå€¼ç‰¹ç‚¹ï¼šä¸èƒ½ä¿è¯æ•°æ®ç‰¹å¾å’Œä¸šåŠ¡å«ä¹‰ï¼Œå¯ä»¥å°†é•¿åº¦ä¸ä¸€çš„æ•°æ®å˜ä¸ºç›¸åŒé•¿åº¦ä¾‹å­ï¼šzwdwf -&gt; 710057965 åç§»ç±»ä¼¼äºåŠ ç›ï¼Œå¯¹æ•°æ®å¢åŠ ä¸€ä¸ªå›ºå®šçš„åç§»é‡ç‰¹ç‚¹ï¼šéšè—æ•°å€¼çš„éƒ¨åˆ†ç‰¹å¾ä¾‹å­ï¼š253 -&gt; 1253 æˆªæ–­åªä¿ç•™æ•°æ®çš„æŸå‡ ä½ï¼Œå…¶ä½™ä½æˆªæ–­ä¾‹å­ï¼š0421-88888 -&gt; 0421 å”¯ä¸€å€¼æ˜ å°„å°†æ•°æ®æ˜ å°„ä¸ºå”¯ä¸€çš„ä¸€ä¸ªå€¼ï¼Œé€šè¿‡æ˜ å°„è¡¨æ‰¾å›åŸæœ‰çš„å€¼ å±€éƒ¨æ··æ·†å‰å‡ ä½ä¸å˜ï¼Œåé¢ä½ç½®æ•°æ®æ··æ·† è„±æ•ç¯å¢ƒæ•°æ®è„±æ•ç¯å¢ƒç»†åˆ†ä¸ºç”Ÿäº§ç¯å¢ƒå’Œéç”Ÿäº§ç¯å¢ƒï¼ˆå¼€å‘ã€æµ‹è¯•ã€é¢„å‘å¸ƒã€å¤–åŒ…ã€æ•°æ®åˆ†æç­‰ï¼‰æ ¹æ®è„±æ•ç¯å¢ƒçš„å…·ä½“åœºæ™¯å°†è„±æ•åˆ†ä¸ºï¼š é™æ€æ•°æ®è„±æ•SDM(Static Data Masking)ä¸€èˆ¬ç”¨åœ¨éç”Ÿäº§ç¯å¢ƒï¼Œæ•°æ®æ˜¯ä»ç”Ÿäº§ç¯å¢ƒç»è¿‡è„±æ•åå†åœ¨éç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚è§£å†³çš„é—®é¢˜ï¼šæµ‹è¯•ç¯å¢ƒå¼€å‘åº“éœ€è¦ç”Ÿäº§åº“çš„æ•°æ®é‡å’Œæ•°æ®æ ¼å¼æ–¹ä¾¿æ’æŸ¥é—®é¢˜æˆ–è¿›è¡Œåˆ†æï¼Œä½†åˆä¸èƒ½å°†æ•æ„Ÿæ•°æ®æ”¾åœ¨éç”Ÿäº§ç¯å¢ƒçš„åœºæ™¯ åŠ¨æ€æ•°æ®è„±æ•DDM(Dynamic Data Masking)ä¸€èˆ¬åŒåœ¨ç”Ÿäº§ç¯å¢ƒï¼Œè®¿é—®æ•æ„Ÿæ•°æ®æ—¶è¿›è¡ŒåŠ¨æ€è„±æ•ã€‚è§£å†³çš„é—®é¢˜ï¼šç”Ÿäº§ç¯å¢ƒæ ¹æ®ä¸åŒåœºæ™¯ä¸åŒç”¨æˆ·å¯¹æ•æ„Ÿæ•°æ®é‡‡ç”¨ä¸åŒè„±æ•çº§åˆ«å’Œè„±æ•ç®—æ³•è¿›è¡Œè„±æ• å¤§æ•°æ®è„±æ•æŠ€æœ¯æ–¹æ¡ˆå…ˆä¸Šä¸€ä¸ªæ€»ä½“çš„è„‘å›¾ï¼šæ•°æ®è„±æ•è„‘å›¾XMIND æ‰¹é‡æ•°æ®è„±æ• å…ˆå°†æ•°æ®åŒæ­¥åˆ°æ•°æ®ä»“åº“çš„ä¸€ä¸ªä¸­é—´è¡¨ï¼Œç„¶åé€šè¿‡ä¸€äº›è‡ªå®šä¹‰çš„è„±æ•å‡½æ•°udfè¿›è¡Œæ•°æ®è„±æ•ï¼Œç„¶åå°†æœªè„±æ•çš„ä¸­é—´è¡¨åˆ é™¤æˆ–é€šè¿‡æƒé™ç®¡æ§èµ·æ¥å³å¯ã€‚å¯¹äºå­˜é‡æ•°æ®ä¹Ÿæ˜¯ç»è¿‡UDFå¤„ç†åå¾—åˆ°è„±æ•æ•°æ®ã€‚æ‰‹æœºå·è„±æ•UDFç¤ºä¾‹: package top.shmily.qjj; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDF; // ä¸Šä¼ udf jaråˆ°é›†ç¾¤ hdfs dfs -put udf-1.0-SNAPSHOT-jar-with-dependencies.jar /tmp/udf_path/ // ä¿®æ”¹æ–‡ä»¶æƒé™ hdfs dfs -chmod -R 777 hdfs:///tmp/udf_path/ // æ³¨å†Œudfå‡½æ•° create function tmp.pul as &#39;top.shmily.qjj.PhoneUnlookUdf&#39; using jar &#39;hdfs:///tmp/udf_path/udf-1.0-SNAPSHOT-jar-with-dependencies.jar public class PhoneUnlookUdf extends UDF &#123; //é‡å†™evaluateæ–¹æ³• public String evaluate(String phone)&#123; if (phone.length() == 11)&#123; String res = phone.substring(0, 3) + &quot;****&quot; + phone.substring(7, phone.length()); return res; &#125; else &#123; return phone; &#125; &#125; &#125; ä½¿ç”¨ApacheRangerè¿›è¡Œæ•°ä»“Hiveè¡¨æ•°æ®è¿›è¡Œè„±æ•Apache Rangerå¯¹Hiveæ•°æ®æ”¯æŒä¸¤ç§è„±æ•æ–¹å¼ï¼šè¡Œè¿‡æ»¤(Row Filter)å’Œåˆ—å±è”½(Column Masking)ã€‚å®ƒå¯å¯¹Selectç»“æœè¿›è¡Œè¡Œåˆ—çº§åˆ«æ•°æ®è„±æ•ï¼Œä»è€Œè¾¾åˆ°å¯¹ç”¨æˆ·å±è”½æ•æ„Ÿä¿¡æ¯çš„ç›®çš„ã€‚è„±æ•æ›´å¤šçš„æ˜¯ç”¨åˆ°Rangerçš„åˆ—å±è”½ï¼Œå¯ç”¨ä¸åŒç­–ç•¥å¯¹ä¸åŒåˆ—è„±æ•ï¼Œåˆ—å±è”½æ”¯æŒçš„ç­–ç•¥ï¼šRedactç­–ç•¥: ç”¨xå±è”½æ‰€æœ‰å­—æ¯å­—ç¬¦ï¼Œç”¨nå±è”½æ‰€æœ‰æ•°å­—å­—ç¬¦ã€‚Partial mask-show last 4 ç­–ç•¥: ä»…æ˜¾ç¤ºæœ€åå››ä¸ªå­—ç¬¦,å…¶ä»–ç”¨xä»£æ›¿ã€‚Partial mask-show first 4 ç­–ç•¥: ä»…æ˜¾ç¤ºå‰å››ä¸ªå­—ç¬¦,å…¶ä»–ç”¨xä»£æ›¿ã€‚Hashç­–ç•¥: ç”¨å€¼çš„å“ˆå¸Œå€¼æ›¿æ¢åŸå€¼ã€‚Nullifyç­–ç•¥: ç”¨NULLå€¼æ›¿æ¢åŸå€¼ã€‚Unmaskedç­–ç•¥: åŸæ ·æ˜¾ç¤ºã€‚Date-show only yearç­–ç•¥: ä»…æ˜¾ç¤ºæ—¥æœŸå­—ç¬¦ä¸²çš„å¹´ä»½éƒ¨åˆ†ï¼Œå¹¶å°†æœˆä»½å’Œæ—¥æœŸé»˜è®¤ä¸º01/01ã€‚Customç­–ç•¥: å¯ä½¿ç”¨ä»»ä½•æœ‰æ•ˆHive UDF(è¿”å›ä¸è¢«å±è”½çš„åˆ—ä¸­çš„æ•°æ®ç±»å‹ç›¸åŒçš„æ•°æ®ç±»å‹)æ¥è‡ªå®šä¹‰ç­–ç•¥ã€‚ä½†Rangerå¯¹ç³»ç»Ÿå„ä¸ªç»„ä»¶ä¾èµ–è¾ƒä¸¥æ ¼ï¼Œç‰ˆæœ¬æœ‰å·®å¼‚éƒ½ä¼šç¼–è¯‘å¤±è´¥ï¼Œå¯ä»¥å‚è€ƒRangerçš„Pluginä¸­æƒé™ç›¸å…³å®ç°ç±»çš„å®ç°æ–¹æ³•ï¼Œåˆ©ç”¨Rangerçš„æ€æƒ³è‡ªå·±ç¼–å†™æƒé™æ§åˆ¶å’Œè„±æ•é€»è¾‘ã€‚Rangerå®ç°è„±æ•çš„æ–¹å¼åœ¨RangerWikiæœ‰è¯¦ç»†è¯´æ˜ï¼šRow level filtering and column-masking using Apache Ranger policies in Apache Hive ä½¿ç”¨Apache ShardingSphereå®ç°æ•°æ®è„±æ•&emsp;&emsp;Apache ShardingSphereæ˜¯ä¸€å¥—å¼€æºçš„åˆ†å¸ƒå¼æ•°æ®åº“ä¸­é—´ä»¶è§£å†³æ–¹æ¡ˆç”±Sharding-JDBCã€Sharding-Proxyå’ŒSharding-Sidecarï¼ˆè§„åˆ’ä¸­ï¼‰è¿™3æ¬¾ç›¸äº’ç‹¬ç«‹ï¼Œå´åˆèƒ½å¤Ÿæ··åˆéƒ¨ç½²é…åˆä½¿ç”¨çš„äº§å“ç»„æˆã€‚å®ƒä½¿ç”¨å®¢æˆ·ç«¯ç›´è¿æ•°æ®åº“ï¼Œä»¥jaråŒ…å½¢å¼æä¾›æœåŠ¡ï¼Œæ— éœ€é¢å¤–éƒ¨ç½²å’Œä¾èµ–ï¼Œå¯ç†è§£ä¸ºå¢å¼ºç‰ˆçš„JDBCé©±åŠ¨ï¼Œå®Œå…¨å…¼å®¹JDBCå’Œå„ç§ORMæ¡†æ¶ã€‚&emsp;&emsp;æ•°æ®è„±æ•æ¨¡å—å±äºShardingSphereåˆ†å¸ƒå¼æ•°æ®æ²»ç†è¿™ä¸€æ ¸å¿ƒåŠŸèƒ½ä¸‹çš„å­åŠŸèƒ½æ¨¡å—ã€‚å®ƒé€šè¿‡å¯¹ç”¨æˆ·è¾“å…¥çš„SQLè¿›è¡Œè§£æï¼Œå¹¶ä¾æ®ç”¨æˆ·æä¾›çš„è„±æ•é…ç½®å¯¹SQLè¿›è¡Œæ”¹å†™ï¼Œä»è€Œå®ç°å¯¹åŸæ–‡æ•°æ®è¿›è¡ŒåŠ å¯†ï¼Œå¹¶å°†åŸæ–‡æ•°æ®(å¯é€‰)åŠå¯†æ–‡æ•°æ®åŒæ—¶å­˜å‚¨åˆ°åº•å±‚æ•°æ®åº“ã€‚åœ¨ç”¨æˆ·æŸ¥è¯¢æ•°æ®æ—¶ï¼Œå®ƒåˆä»æ•°æ®åº“ä¸­å–å‡ºå¯†æ–‡æ•°æ®ï¼Œå¹¶å¯¹å…¶è§£å¯†ï¼Œæœ€ç»ˆå°†è§£å¯†åçš„åŸå§‹æ•°æ®è¿”å›ç»™ç”¨æˆ·ã€‚Apache ShardingSphereåˆ†å¸ƒå¼æ•°æ®åº“ä¸­é—´ä»¶è‡ªåŠ¨åŒ–&amp;é€æ˜åŒ–äº†æ•°æ®è„±æ•è¿‡ç¨‹ï¼Œè®©ç”¨æˆ·æ— éœ€å…³æ³¨æ•°æ®è„±æ•çš„å®ç°ç»†èŠ‚ï¼Œåƒä½¿ç”¨æ™®é€šæ•°æ®é‚£æ ·ä½¿ç”¨è„±æ•æ•°æ®ã€‚æ­¤å¤–ï¼Œæ— è®ºæ˜¯å·²åœ¨çº¿ä¸šåŠ¡è¿›è¡Œè„±æ•æ”¹é€ ï¼Œè¿˜æ˜¯æ–°ä¸Šçº¿ä¸šåŠ¡ä½¿ç”¨è„±æ•åŠŸèƒ½ï¼ŒShardingSphereéƒ½å¯ä»¥æä¾›ä¸€å¥—ç›¸å¯¹å®Œå–„çš„è§£å†³æ–¹æ¡ˆã€‚&emsp;&emsp;å…·ä½“å¯å‚è€ƒè¯¦ç»†æ–‡æ¡£å’Œå®˜ç½‘ã€‚&emsp;&emsp; ä¿®æ”¹ThriftServeræºç ï¼Œåœ¨ThriftServerç«¯ä½¿ç”¨Antlr4è§£æSQLï¼ŒåŒ¹é…è„±æ•è§„åˆ™åº“åé’ˆå¯¹æ•æ„Ÿå­—æ®µè‡ªåŠ¨å¥—ç”¨UDFå¹¶æäº¤æ‰§è¡Œã€‚ï¼ˆJDBCç«¯ä¿®æ”¹ä¸çŸ¥é“æ˜¯ä¸æ˜¯ä¹Ÿå¯è¡Œï¼Œæ„Ÿè§‰ThriftServeræºç æ”¹èµ·æ¥æ›´ç®€å•ç‚¹ï¼‰é€šè¿‡ä¿®æ”¹SparkThriftServeræºç å®ç°æ•°æ®è„±æ•çš„æ–¹æ³•è§æˆ‘çš„å¦ä¸€ç¯‡åšå®¢ï¼šå®ç°åŸºäºSparkçš„æ•°æ®è„±æ• å®æ—¶æ•°æ®è„±æ• å®æ—¶æ•°æ®ç»“åˆå†å²æ•°æ®é€šè¿‡ä¸­é—´ä»¶è¿›å…¥Flinkæˆ–Stormç¨‹åºï¼Œè¿™æ—¶åªæœ‰å®æ—¶æ•°æ®ï¼Œä¸é€‚ç”¨åŸºäºå…¨é‡æ•°æ®çš„è„±æ•ç®—æ³•ï¼Œæ‰€ä»¥ç»“åˆå†å²æ•°æ®å’Œç›¸åº”ç®—æ³•è¿›è¡Œå®æ—¶è„±æ•ã€‚ æ•°æ®å®‰å…¨å®¡è®¡æ•°æ®å®‰å…¨å®¡è®¡ï¼Œç›®å‰ä¸šç•Œå¸¸ç”¨çš„å°±æ˜¯ELK(Es+LogStash+Kibana)ï¼Œä¸‹é¢æ˜¯ç”¨äºå¤§æ•°æ®å®¡è®¡çš„æµç¨‹ æ€»ç»“ è„±æ•çš„è¿‡ç¨‹å°±æ˜¯ä¸€ä¸ªåœ¨æ•°æ®å®‰å…¨æ€§å’Œæ•°æ®å¯ç”¨æ€§ä¹‹é—´å¹³è¡¡çš„è¿‡ç¨‹ é€‰æ‹©æˆ–è®¾è®¡ä¸€ç§æ—¢èƒ½æ»¡è¶³å¼€å‘æµ‹è¯•å¤–åŒ…çš„è¦æ±‚ï¼Œåˆèƒ½ä¿è¯å®‰å…¨æ€§çš„è„±æ•ç®—æ³•ç‰¹åˆ«é‡è¦ è„±æ•åæ•°æ®çš„å…³è”å…³ç³»å’Œä¸šåŠ¡å…³ç³»ä¸èƒ½è¢«ç ´å è¦ç»“åˆæ•°æ®ã€ä¸šåŠ¡éœ€æ±‚å®é™…æƒ…å†µåˆ¶å®šé€‚åˆçš„è„±æ•è§„åˆ™ å‚è€ƒç¾å›¢æŠ€æœ¯å›¢é˜Ÿæ•°æ®è„±æ•å¤§æ•°æ®è„±æ•æ–¹æ¡ˆæ•°æ®è„±æ•åŸç†åŠæ–¹æ³•ç®€ææ‰‹æœºå·ç è„±æ•Apache ShardingSphereæ•°æ®è„±æ•å…¨è§£å†³æ–¹æ¡ˆè¯¦è§£","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"æ•°æ®è„±æ•","slug":"æ•°æ®è„±æ•","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E8%84%B1%E6%95%8F/"},{"name":"æ•°æ®å®‰å…¨","slug":"æ•°æ®å®‰å…¨","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"ä½¿ç”¨PySparkä¼˜åŒ–Pandas","slug":"ä½¿ç”¨PySparkä¼˜åŒ–Pandas","date":"2020-08-10T03:26:08.000Z","updated":"2022-12-11T05:35:07.913Z","comments":true,"path":"pyspark_pandas/","link":"","permalink":"https://shmily-qjj.top/pyspark_pandas/","excerpt":"","text":"å‰è¨€&emsp;&emsp;Pandasä¸€ç›´æ˜¯éå¸¸å—æ¬¢è¿çš„æ•°æ®åˆ†æåˆ©å™¨ï¼Œå®ƒåŸºäºNumpyï¼Œä¸“ä¸ºè§£å†³æ•°æ®åˆ†æä»»åŠ¡ã€‚å› å…¶åŸºäºPythonï¼Œåªèƒ½å•èŠ‚ç‚¹å•æ ¸å¿ƒè¿è¡Œï¼Œæ‰€ä»¥åœ¨å¤§æ•°æ®åˆ†æåœºæ™¯ä¸‹ï¼Œç“¶é¢ˆå¾ˆæ˜æ˜¾ã€‚PySparkæ˜¯åŸºäºSpark JavaClientçš„ä¸Šå±‚æ¥å£ï¼Œå¯ä»¥ç»“åˆPythonè¯­è¨€ä»¥åŠSparkåˆ†å¸ƒå¼è¿è¡Œçš„ç‰¹ç‚¹ï¼Œæ¥è§£å†³Pandasåœ¨å¤§æ•°æ®ä¸‹çš„ç“¶é¢ˆã€‚æœ¬ç¯‡æ–‡ç« ä¸»è¦å¯¹æ¯”Pandas APIä¸PySparkAPIï¼Œæ€»ç»“ä¸€äº›Pandasåº”ç”¨åœºæ™¯ä¸‹ä½¿ç”¨PySparkæé«˜æ•ˆç‡çš„æ–¹æ¡ˆã€‚&emsp;&emsp;æœ¬ç¯‡ä¸»è¦æ˜¯å¯¹æ¯”Pandaså’ŒPySparkçš„APIä½¿ç”¨ï¼Œä½†ä¸èƒ½å¯¹å®ƒä»¬ä¼—å¤šAPIåšä¸€ä¸€å¯¹æ¯”ä»‹ç»ï¼Œæ‰€ä»¥å¯¹äºPySparkçš„æ›´å¤šAPIä½¿ç”¨è¯·å‚è€ƒï¼š**pyspark.sqlå®˜æ–¹ä½¿ç”¨æ–‡æ¡£** å¯¹æ¯” ç‰¹ç‚¹ Pandas PySpark è¿è¡Œæ–¹å¼ å•æœºå•æ ¸ åˆ†å¸ƒå¼ å¹¶è¡Œæœºåˆ¶ ä¸æ”¯æŒ æ”¯æŒ æ•°æ®ä½ç½® å•æœºå†…å­˜ å¤šèŠ‚ç‚¹å†…å­˜å’Œç£ç›˜ å¤§æ•°æ®æ”¯æŒ å·® ä¼˜ æ•°æ®å¤„ç†æ–¹å¼ æ— æ‡’åŠ è½½ æ‡’åŠ è½½+ä¼˜åŒ–æ— ç”¨æ“ä½œ DataFrame å¯å˜ ä¸å¯å˜ åŸºæœ¬åŸåˆ™ éœ€è¦å¯¹å¤§é‡æ•°æ®è¿›è¡Œåˆ†æçš„åœºæ™¯ä¸‹ï¼Œåœ¨å¤§æ•°æ®å¤„ç†çš„æºå¤´å¿…é¡»ä½¿ç”¨PySpark æ•°æ®ç»è¿‡ä¸€ç³»åˆ—æ“ä½œã€èšåˆåæ•°æ®é‡å‡å°‘ï¼Œä¸”è¿«ä¸å¾—å·²ç”¨Pandasçš„æƒ…å†µä¸‹å†ä½¿ç”¨Pandas(ç”¨Pandaså¤„ç†çš„æ•°æ®å°½é‡æ›´å°‘) å¦‚æœå¯ä»¥ï¼Œå°½é‡å…¨ç¨‹ä½¿ç”¨PySparkè¿›è¡Œåˆ†ææ“ä½œ éœ€è¦å¯¹è®¡ç®—å¤æ‚ä¸”è€—æ—¶çš„Sparkdataframeè¿›è¡Œcacheé¿å…é‡ç®—æé«˜æ•ˆç‡ å°½å¯èƒ½å°†ä¸€æ®µå¤„ç†é€»è¾‘å†™åˆ°ä¸€æ®µSQLä¸­ï¼Œè€Œéå¾—åˆ°å¤šä¸ªDataframeç„¶åè¿›è¡Œjoin æ•°æ®åˆ›å»ºæ–‡ä¸­æ‰€æœ‰Spark Dataframeå¯¹è±¡ç®€ç§°df,Pandasçš„Dataframeå¯¹è±¡ç®€ç§°pd_dfã€‚ Pandas pd_df = pd.read_csv(&#39;/datas/root/csv_data/csv_file.csv&#39;) # 1.è¯»æœ¬åœ°csvæ•°æ®æº pd_df = spark.sql(&quot;select col1,col2 from table&quot;).to_pandas # 2.è¯»Hiveæ•°æ®æº pd_df = spark.sql(&quot;select * from table&quot;).to_pandas # 3.è¯»Hiveæ•´ä¸ªè¡¨ # 4.è¯»MySQLè¡¨æ•°æ® pd_df = pd.read_sql(&#39;select * from table&#39;, con=pymysql.connect(host=&quot;localhost&quot;,user=username,passwd=password,db=database_name,charset=&quot;utf8&quot;)) # 5.ä»listï¼Œsetï¼Œdictåˆ›å»ºdataftame pd_df = pd.DataFrame(&#123;&quot;id&quot;:[1,2,3,4,5],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;,np.nan]&#125;) # 6.è¯»json pd_df = pd.read_json(&#39;/datas/root/csv_data/json_file&#39;) # zerosåˆ›å»ºæŒ‡å®šshapeçš„å¸¦0çš„ndarray pd_df = np.zeros((5,3), dtype=&#39;int64&#39;) #5 è¡Œ 3 åˆ— PySpark df = spark.read.option(&#39;inferSchema&#39;,&quot;true&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&#39;/data/data_test/csv_file.csv&#39;) # 1.è¯»HDFSä¸Šcsvæ•°æ®æº df = spark.read.csv(&quot;file:///a.csv&quot;) # è¯»æœ¬åœ°csv è·¯å¾„/a.csv df = spark.sql(&quot;select col1,col2 from table&quot;) # 2.è¯»Hiveæ•°æ®æº df = spark.table(&#39;table&#39;) # 3.è¯»Hiveæ•´ä¸ªè¡¨ # 4.è¯»MySQLè¡¨æ•°æ® conf = &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh101:3306/&quot;, &quot;dbtable&quot;: &#39;test.a&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, &#125; df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() # 5.ä»listï¼Œsetï¼Œdictåˆ›å»ºdataftame df = spark.createDataFrame(pd.DataFrame(&#123;&quot;id&quot;:[1,2,3,4,5],&quot;name&quot;:[&#39;qjj&#39;,&#39;zxw&#39;,&#39;zzz&#39;,&#39;abc&#39;,None]&#125;)) æˆ– df = spark.createDataFrame([(1,&#39;qjj&#39;),(2,&#39;zxw&#39;),(3,&#39;zzz&#39;),(4,&#39;abc&#39;)], [&#39;id&#39;, &#39;name&#39;]) # 6.è¯»jsonæ–‡ä»¶ df = spark.read.json(&#39;/datas/root/csv_data/json_file&#39;) # 7.ä»Parquetåˆ›å»ºæ•°æ® df = spark.read.parquet(&quot;...&quot;) df = spark.read.format(&#39;parquet&#39;).load(&#39;parquet_file&#39;),opt...) # 8.ä»ORCåˆ›å»ºæ•°æ® df = spark.read.orc(&#39;...&#39;) # 9.ä»textåˆ›å»ºæ•°æ® df = spark.read.text(&#39;...&#39;) # 10.åˆ›å»ºæŒ‡å®šshapeçš„å¸¦0çš„dataframe df = spark.createDataFrame([[0 for i in range(3)] for i in range(5)]) #5 è¡Œ 3 åˆ— # åˆ›å»ºæ•°æ®å¹¶æŒ‡å®šå­—æ®µå(Schema) from pyspark.sql.types import * schema = StructType().add(&#39;col1&#39;, StringType(), True).add(&#39;col2&#39;, IntegerType()) # Trueæ˜¯å¦å¯ä»¥ä¸ºç©º df = spark.createDataFrame([(&#39;aaa&#39;, 1),(&#39;bbb&#39;, 2)], schema=schema) æ•°æ®ç»“æ„ Pandasindexç´¢å¼•ï¼šè‡ªåŠ¨åˆ›å»ºè¡Œç»“æ„ï¼šSeriesç»“æ„ï¼Œå±äºPandas DataFrameåˆ—ç»“æ„ï¼šColumnç»“æ„ï¼Œå±äºPandas DataFrame pd_df[&#39;col&#39;] = 0 # åˆ—æ·»åŠ  pd_df[&#39;col&#39;] = 1 # åˆ—ä¿®æ”¹ pd_df.rename(columns=&#123;&#39;col&#39;:&#39;new_col&#39;,&#39;xx&#39;:&#39;xxx&#39;&#125;) # é‡å‘½ååˆ—å pd_df.columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;] # é‡å‘½ååˆ—å pd_df.dtypes # æŸ¥çœ‹å­—æ®µå’Œç±»å‹ pd_df.drop(columns=[&#39;col&#39;, &#39;name&#39;]) # åˆ é™¤å­—æ®µcol PySparkindexç´¢å¼•ï¼šæ— è¡Œç»“æ„ï¼šRowå¯¹è±¡ï¼Œå±äºSpark DataFrameåˆ—ç»“æ„ï¼šColumnå¯¹è±¡ï¼Œå±äºSpark DataFrame from pyspark.sql.functions import lit df = df.withColumn(&quot;col&quot;, lit(0)) # åˆ—æ·»åŠ  df = df.withColumn(&quot;col&quot;, lit(1)) # åˆ—ä¿®æ”¹ df = df.withColumnRenamed(&#39;col&#39;, &#39;new_col&#39;).withColumnRenamed(&#39;col1&#39;, &#39;new_col1&#39;) # é‡å‘½ååˆ—å df.dtypes # æŸ¥çœ‹å­—æ®µå’Œç±»å‹ df.printSchema() # æ‰“å°å­—æ®µå’Œç±»å‹-æ ‘å½¢ df.drop(&#39;col&#39;, &#39;name&#39;) # åˆ é™¤å­—æ®µcol æ•°æ®æ˜¾ç¤º Pandas pd.set_option(&#39;max_rows&#39;,1024) # æœ€å¤šæ˜¾ç¤º1024è¡Œä¸éšè— pd.set_option(&#39;max_columns&#39;,1024) # æœ€å¤šæ˜¾ç¤º1024åˆ—ä¸éšè— pd_dfæˆ–print(pd_df) PySpark df.show() # æ‰“å°å‰20è¡Œä¸”æ¯ä¸ªå­—æ®µæ‰“å°ä¸è¶…è¿‡20å­—ç¬¦ df.show(30) # æ‰“å°å‰30è¡Œä¸”æ¯ä¸ªå­—æ®µæ‰“å°ä¸è¶…è¿‡20å­—ç¬¦ df.show(100,False) # æ‰“å°å‰100è¡Œä¸”æ¯ä¸ªå­—æ®µæ‰“å°å­—ç¬¦æ•°ä¸é™ï¼ˆä¸éšè—ï¼‰ æ•°æ®æ’åº Pandas pd_df.sort_index(by=&#39;score&#39;, ascending=False) # æŒ‰è½´ï¼ˆå­—æ®µscoreï¼‰è¿›è¡Œå€’åºæ’åº pd_df.sort_index(by=&#39;score&#39;, ascending=False).reset_index() # æŒ‰è½´ï¼ˆå­—æ®µscoreï¼‰è¿›è¡Œå€’åºæ’åº,æ’åºåindexä¼šä¹±åºï¼Œé‡è®¾indexä¸ºé¡ºåº pd_df.sort_values(by=&#39;score&#39;) # åœ¨åˆ—ä¸­æŒ‰å€¼è¿›è¡Œæ’åº PySpark df.sort(&#39;score&#39;, ascending=False) # æŒ‰åˆ—ï¼ˆscoreå­—æ®µï¼‰å€’åºæ’åº df.orderBy(&#39;score&#39;) # æŒ‰åˆ—ï¼ˆscoreå­—æ®µï¼‰é¡ºåºæ’åº äº¤é›†å¹¶é›†å·®é›† Pandas pd.merge(pd_df1, pd_df2, on=[&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;]) # äº¤é›† pd.merge(pd_df1,pd_df2,on=[&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;], how=&#39;outer&#39;) # å¹¶é›† pd_df1=pd_df1.append(pd_df2);pd_df1=pd_df1.drop_duplicates(subset=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;],keep=False);pd_df1 # å·®é›† PySpark df = df1.intersect(df2) # äº¤é›† df = df1.union(df2) # å¹¶é›† df = df1.subtract(df2) # å·®é›† æ•°æ®é€‰æ‹©æˆ–åˆ‡ç‰‡ Pandas # 1.å–ä¸€åˆ— pd_df.col_name # 2.å–å¤šåˆ— pd_df[[&#39;id&#39;,&#39;score&#39;]] # 3.å–ç¬¬ä¸€è¡Œ pd_df.ix[0] # 4.å–å‰ä¸¤è¡Œ pd_df.head(2) # 5.æŒ‰æ¡ä»¶å–æ•°æ® pd_df.loc[pd_df.name==&#39;qjj&#39;] # å–pd_dfçš„nameå­—æ®µå€¼ä¸ºqjjè®°å½• pd_df.loc[pd_df.name==&#39;qjj&#39;, &#39;col&#39;] # å–pd_dfçš„nameå­—æ®µå€¼ä¸ºqjjçš„è®°å½•ä¸­nameå­—æ®µå’Œcolå­—æ®µçš„å€¼ # 6.æ•°æ®éšæœºæŠ½æ · pd_df.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) # nè¡Œæ•° fracæŠ½å–æ¯”ä¾‹ replace=Falseæ— æ”¾å› ... PySpark # 1.å–ä¸€åˆ— df.select(&#39;score&#39;).show() # 2.å–å¤šåˆ— df.select(&#39;id&#39;,&#39;score&#39;).show() df.select(df[&#39;id&#39;],df[&#39;score&#39;]).show() # 2.å–å¤šåˆ— æ¯ä¸ªå€¼åŠ 20 df.select(df[&#39;id&#39;] + 20,df[&#39;score&#39;]).show() # 3.å–ç¬¬ä¸€è¡Œ df.first() # 4.å–å‰ä¸¤è¡Œ df.head(2) æˆ– df.take(2) # 5.æŒ‰æ¡ä»¶å–æ•°æ® df.filter(&quot;name=&#39;qjj&#39;&quot;) # å–dfçš„nameå­—æ®µå€¼ä¸ºqjjè®°å½• df.filter(&quot;name=&#39;qjj&#39;&quot;).select(&#39;name&#39;, &#39;col&#39;) # å–dfçš„nameå­—æ®µå€¼ä¸ºqjjçš„è®°å½•ä¸­nameå­—æ®µå’Œcolå­—æ®µçš„å€¼ # 6.æ•°æ®éšæœºæŠ½æ · df=df.sample(withReplacement=False, fraction=0.01) # withReplacementä¸ºFalseæŠ½å‡ºæ•°æ®ä¸æ”¾å›ï¼Œfractionä¸ºæŠ½å–æ¯”ä¾‹èŒƒå›´0-1ï¼Œseedå‚æ•°ä¸ºéšæœºæ•°ç§å­ï¼Œé»˜è®¤å³å¯ æ•°æ®è¿‡æ»¤ Pandas pd_df[pd_df[&#39;score&#39;]&gt;=60] pd_df[pd_df[&#39;score&#39;]&gt;=60][pd_df[&#39;id&#39;]&gt;=5] pd_df.query(&#39;score &gt;= 60&#39;) PySpark df.filter(&#39;score&gt;=60&#39;) æˆ– df.where(&#39;score&gt;=60&#39;) df.filter(&#39;score&gt;=60 and id&gt;=5&#39;) æˆ– df.where(&#39;score&gt;=60 and id&gt;=5&#39;) æ•°æ®å»é‡ Pandas pd_df.drop_duplicates(&#39;col&#39;) PySpark df.drop_duplicates() # dataä¸­ä¸€è¡Œå…ƒç´ å…¨éƒ¨ç›¸åŒæ—¶æ‰å»é™¤ df.drop_duplicates([&#39;a&#39;,&#39;b&#39;]) # dataæ ¹æ®â€™a&#39;,&#39;b&#39;ç»„åˆåˆ—åˆ é™¤é‡å¤é¡¹ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„å€¼ç»„åˆï¼ˆfirstï¼‰ã€‚ å–å”¯ä¸€å€¼ Pandas pd_df[&#39;col&#39;].unique() PySpark df.select(&#39;col&#39;).distinct().count() æˆ–df.drop_duplicates([&#39;col&#39;]).count() åˆ†ç»„èšåˆ Pandas pd_df.groupby(&#39;col&#39;).mean() PySpark df.groupBy(&#39;col&#39;).mean().show() df.groupBy(&#39;col&#39;).avg(&#39;score&#39;).show() from pyspark.sql import functions df.groupBy(&#39;col&#39;).agg(functions.avg(&#39;score&#39;), functions.min(&#39;score&#39;), functions.max(&#39;score&#39;)).show() # ä½¿ç”¨SQLåˆ†ç»„èšåˆ spark.sql(&quot;select name,first(col) as col,sum(score) from table group by name&quot;).show() æ•°æ®è®¡ç®— Pandaspd_df[&#39;col&#39;].apply(lambda x: round(math.log(7,2),2)) # è®¡ç®—2ä¸ºåº•7çš„logï¼Œç²¾ç¡®å°æ•°ç‚¹å2ä½ pd_df[&#39;col&#39;].apply(lambda x: sum(x)) # æ±‚å’Œ PySparkspark.sql(&quot;select round(log(2,7),2) as r&quot;).show() # è®¡ç®—2ä¸ºåº•7çš„logï¼Œç²¾ç¡®å°æ•°ç‚¹å2ä½ spark.sql(&quot;select sum(col) from df&quot;).show() # æ±‚å’Œ æ•°æ®ç»Ÿè®¡ Pandas pd_df.count() # è¾“å‡ºæ¯ä¸€åˆ—çš„éç©ºè¡Œæ•° pd_df.describe() # æè¿°æŸäº›åˆ—çš„count, mean, std, min, 25%, 50%, 75%, max pd_df[&#39;col&#39;].value_counts() # ç»Ÿè®¡æŸåˆ—çš„æ•°æ®é‡ PySpark df.count() # è¾“å‡ºæ€»è¡Œæ•° df.describe().show() # æè¿°æŸäº›åˆ—çš„count, mean, stddev, min, max df.select(&#39;col&#39;).filter(&#39;col is null&#39;).count() # ç»Ÿè®¡æŸåˆ—çš„æ•°æ®é‡ æ•°æ®åˆå¹¶TODO:å¾…å®Œå–„æµ‹è¯• Pandas pd.concat([pd_df,pd_df1], axis=0) # æ•°æ®æ¨ªå‘åˆå¹¶axis=0 çºµå‘åˆå¹¶axis=1 Pandasä¸‹æœ‰mergeæ–¹æ³•ï¼Œæ”¯æŒå¤šåˆ—åˆå¹¶ åŒååˆ—è‡ªåŠ¨æ·»åŠ åç¼€ï¼Œå¯¹åº”é”®ä»…ä¿ç•™ä¸€ä»½å‰¯æœ¬ pd_df.join() æ”¯æŒå¤šåˆ—åˆå¹¶ pd_df.append() æ”¯æŒå¤šè¡Œåˆå¹¶ # æ ¹æ®ä¸€å®šè®¡ç®—è§„åˆ™è®¡ç®—å¾—åˆ°æ–°å¢åˆ— PySpark df.withColumn(æ–°åˆ—åï¼Œdf[åˆ—å]**2) # æ•°æ®ç®€å•æ“ä½œåæ¨ªå‘åˆå¹¶ df.union(df1) # æ•°æ®çºµå‘åˆå¹¶-è‡ªåŠ¨å»é™¤é‡å¤æ•°æ® df.unionAll(df1) # æ•°æ®çºµå‘åˆå¹¶-ä¸å»é™¤é‡å¤æ•°æ® # å¯ä»¥ä½¿ç”¨sqlå®ç°concatã€mergeåŠŸèƒ½ df.join(df1,df.id==df1.id) # inner join df.join(df1,df.id==df1.id, &#39;left&#39;) # left join df.join(df1,df.id==df1.id, &#39;left&#39;) # right join df.join(df1,df.id==df1.id, &#39;outer&#39;) # full outer join ä»»ä½•ä¸€è¾¹ä¸å­˜åœ¨å¡«å……null # æ ¹æ®UDFè®¡ç®—å¾—åˆ°æ–°å¢åˆ— udf+withColumn+é—­åŒ… from pyspark.sql.functions import udf from pyspark.sql.types import IntegerType l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] for i in l: my_udf = udf(lambda x: x.count(i) if x else 0, IntegerType()) df = df.withColumn(&#39;col_&#39; + i, my_udf(&#39;array_type_col&#39;)) æ•°æ®ä¿®æ”¹å¯¹åº”pd.apply(f)æ–¹æ³• å³ç»™dfçš„æ¯ä¸€åˆ—åº”ç”¨å‡½æ•°f Pandas pd_df.apply(f) # å¯ä½œç”¨äºSeriesæˆ–æ•´ä¸ªDataframeï¼Œå¹¶å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å‡½æ•°f pd_df.apply(f, axis=1) # axis=0 è¡¨ç¤ºæŒ‰åˆ—ï¼Œaxis=1 è¡¨ç¤ºæŒ‰è¡Œ pd_df.replace(&#123;1:10, 2:20&#125;) # å°†dataframeä¸­å€¼ä¸º1çš„éƒ½æ›¿æ¢æˆ10,2æ›¿æ¢æˆ20 pandasæ”¯æŒæ›¿æ¢ä¸ºä¸åŒç±»å‹ PySpark df.foreach(f) æˆ–è€… df.rdd.foreach(f) # å°†dfçš„æ¯ä¸€åˆ—åº”ç”¨å‡½æ•°f df.foreachPartition(f) æˆ–è€… df.rdd.foreachPartition(f) # å°†dfçš„æ¯ä¸€åˆ†åŒºæ•°æ®åº”ç”¨å‡½æ•°f pd_df.replace(&#123;1:10, 2:20&#125;) # å°†dataframeä¸­å€¼ä¸º1çš„éƒ½æ›¿æ¢æˆ10,2æ›¿æ¢æˆ20 sparkä¸æ”¯æŒæ›¿æ¢ä¸ºä¸åŒç±»å‹ æ³¨æ„ï¼šSparkçš„applyæ–¹æ³•ä¼šè§¦å‘å…¨é‡æ•°æ®Shuffleï¼Œå¦‚æœæ•°æ®é‡è¿‡å¤§ä¼šæœ‰shuffleå¼‚å¸¸å’ŒExecutorOOMç­‰é”™è¯¯ï¼Œä»»åŠ¡å¤±è´¥æ¦‚ç‡ä¼šå¢åŠ ï¼Œè€Œä¸”éœ€è¦æ¶ˆè€—æ›´å¤šè®¡ç®—èµ„æº ç©ºå€¼å¤„ç† Pandas # å¯¹ç¼ºå¤±æ•°æ®è‡ªåŠ¨æ·»åŠ NaNs pd_df.fillna(1) # fillnaå‡½æ•° å°†NaNçš„åœ°æ–¹æ›¿æ¢ä¸º1.0 pd_df.dropna() # dropnaå‡½æ•° å°†å«æœ‰NaNçš„è¡Œåˆ é™¤ pd_df[&#39;col&#39;]=np.where(pd.isnull(pd_df[&#39;col&#39;], &quot;unknown&quot;, pd_df[&#39;col&#39;])) # æŸä¸ªå­—æ®µå‡ºç°ç©ºæ—¶æ›¿æ¢ä¸ºunknown pd_df[&#39;col&#39;]=np.where(pd_df[&#39;col&#39;]==&#39;&#39;, &quot;unknown&quot;, pd_df[&#39;col&#39;]) # æŸä¸ªå­—æ®µå‡ºç°ç©ºå­—ç¬¦ä¸²æ—¶æ›¿æ¢ä¸ºunknown pd_df.isna() # éç©ºå€¼å˜ä¸ºFalseï¼Œæœ‰ç©ºå€¼å˜ä¸ºTrue PySpark ä¸è‡ªåŠ¨æ·»åŠ NaNsï¼Œä¸”ä¸æŠ›å‡ºé”™è¯¯ df.na.fill(1).show() # fillnaå‡½æ•° å°†nullçš„åœ°æ–¹æ›¿æ¢ä¸º1.0 df.na.drop().show() # dropnaå‡½æ•° å°†å«æœ‰nullå€¼å­—æ®µçš„è¡Œåˆ é™¤ df.dropna(subset=[&#39;col1&#39;, &#39;col2&#39;]) # æ‰”æ‰col1æˆ–col2ä¸­ä»»ä¸€ä¸€åˆ—åŒ…å«nullçš„è¡Œ df=df.na.fill(subset=&#39;col&#39;, value=&#39;unknown&#39;) # æŸä¸ªå­—æ®µå‡ºç°ç©ºæ—¶æ›¿æ¢ä¸ºunknown select if(col=&#39;&#39;,&#39;unknown&#39;,col) as col # æŸä¸ªå­—æ®µå‡ºç°ç©ºå­—ç¬¦ä¸²æ—¶æ›¿æ¢ä¸ºunknown df.fillna(&#39;True&#39;) # æœ‰ç©ºå€¼å˜ä¸ºTrue è¿˜å¯ä½¿ç”¨case whenæˆ–ifå¤„ç†ç©ºå€¼ SQLæ”¯æŒ Pandas import pymysql con = pymysql.connect(host=&quot;localhost&quot;, user=&quot;root&quot;, password=&quot;123456&quot;, database=&quot;test&quot;, charset=&#39;utf8&#39;, use_unicode=True) sql_cmd = &quot;SELECT * FROM a&quot; # aæ˜¯teståº“ä¸‹çš„è¡¨å pd_df = pd.read_sql(sql_cmd, con) PySpark # sqlæ“ä½œ df.registerTempTable(&#39;score_table&#39;) # å°†å·²æœ‰æ•°æ®æ³¨å†Œæˆä¸´æ—¶è¡¨ï¼ˆå…³é—­SparkSessionè¿™ä¸ªè¡¨å°±ä¼šæ¶ˆå¤±ï¼‰ df.createOrReplaceTempView(&#39;score_table&#39;) # ä¸registerTempTableåŠŸèƒ½ç›¸åŒï¼Œæ˜¯è¾ƒæ–°çš„API df.createOrReplaceGlobalTempView(&#39;score_table&#39;) # ä¸Šé¢ä¸¤ä¸ªæ˜¯åˆ›å»ºSparkSessionçº§åˆ«çš„ä¸´æ—¶è¡¨ è¿™ä¸ªæ˜¯Applicationçº§åˆ«çš„ä¸´æ—¶è¡¨ spark.sql(&quot;desc score_table&quot;).show() spark.sql(&quot;&quot;&quot;select count(1) as count from score_table&quot;&quot;&quot;).show() # UDFé«˜çº§åŠŸèƒ½å‡½æ•°æ³¨å†Œæ“ä½œ from pyspark.sql.types import StringType # å¼•å…¥è¿”å›å€¼ç±»å‹ spark.udf.register(&quot;get_length&quot;, lambda x: len(x), StringType()) # æ³¨å†ŒUDFå‡½æ•° spark.sql(&quot;select get_length(&#39;name&#39;) from score_table&quot;).show() # ä½¿ç”¨UDFå‡½æ•° # å¯¹ç‰¹å¾è¿›è¡Œæ“ä½œ df.selectExpr(&quot;a*2+b as a&quot;,&quot;b*3 as b&quot;) # aå­—æ®µå€¼æ”¹ä¸ºåŸå§‹å€¼*2åŠ bå­—æ®µå€¼ å¯ä»¥æœ‰å¤šä¸ªè¿ç®—æ“ä½œ df = df.selectExpr(&quot;*&quot;,&quot;b*3 as b_3&quot;) # åŸå§‹å­—æ®µä¸å˜ï¼Œæ–°å¢b_3å­—æ®µå€¼ä¸ºbå­—æ®µ*3 äº’ç›¸è½¬æ¢ Pandas df = spark.createDataFrame(pandas_df) # Pandasè½¬Spark df df = spark.createDataFrame(pandas_df[[&#39;col1&#39;, &#39;col2&#39;]]) # PandasæŸå‡ ä¸ªå­—æ®µçš„dfè½¬Spark df PySpark pandas_df = spark_df.toPandas() # Sparkè½¬Pandas df pandas_df = spark_df.select(&#39;col1&#39;, &#39;col2&#39;).toPandas() # SparkæŸå‡ ä¸ªå­—æ®µçš„dfè½¬Pandas df æ³¨ï¼šSparkè½¬Pandas dfä¼šå°†Spark dfå…¨éƒ¨æ•°æ®æ‹‰åˆ°Driverç«¯å•æœºå•èŠ‚ç‚¹è¿è¡Œï¼Œæ€§èƒ½å·®ä¸”ç½‘ç»œIOå ç”¨é«˜ï¼Œå°½é‡é¿å…å°†å¤§é‡æ•°æ®è½¬æˆPandas DataFrameã€‚ é€è§†è¡¨é€è§†è¡¨ä¸é€†é€è§†è¡¨ï¼šé€è§†Pivotï¼šæŒ‰ä¸éœ€è¦è½¬æ¢çš„å­—æ®µåˆ†ç»„ï¼ˆgroupByï¼‰ -&gt; pivotå‡½æ•°è¿›è¡Œé€è§†ï¼Œå¯é€‰ç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šè¾“å‡ºå­—æ®µæ•°æ®é¡¹ -&gt; èšåˆæ±‡æ€»æ•°æ®é¡¹å¾—åˆ°ç»“æœé€†é€è§†unpivotï¼šåˆ—å½¢å¼ä¸”æ— é‡å¤å€¼çš„æ•°æ®è½¬æˆè¡Œå½¢å¼ä¸”æœ‰é‡å¤å€¼å¾—æ•°æ® Pandas l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] for tag in l: pivot_table = pd.pivot_table(pd_df, index=[&#39;col1&#39;, &#39;col2&#39;], values=&#39;list_type_col&#39;, aggfunc=lambda x: sum(tag==j for i in x for j in i)) # ç»Ÿè®¡æ•°ç»„å€¼ç­‰äºtagè®¡æ•°Trueä¸ªæ•° pivot_table.columns=[tag] PySpark # æ³¨æ„ï¼špivotåªèƒ½è·Ÿåœ¨groupByä¹‹å l = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] pivot_table = df.selectExpr(&#39;*&#39;, &#39;explode(list_type_col)&#39;, &#39;1 as tmp&#39;).groupBy(&#39;col1&#39;, &#39;col2&#39;).pivot(&quot;list_type_col&quot;, l).sum(&quot;tmp&quot;).fillna(0) # æ³¨æ„ï¼šä¸æŒ‡å®špivotçš„ç¬¬äºŒä¸ªå‚æ•°æ‰€éœ€å­—æ®µä¼šé™ä½æ•ˆç‡ # ç›¸å…³é€»è¾‘å¯ä»¥ç›´æ¥ä½¿ç”¨spark sqlç¼–å†™ diffæ“ä½œ Pandas pd_df.diff() # diffå‡½æ•°æ˜¯ç”¨æ¥å°†æ•°æ®è¿›è¡ŒæŸç§ç§»åŠ¨ä¹‹åä¸åŸæ•°æ®è¿›è¡Œæ¯”è¾ƒå¾—å‡ºçš„å·®å¼‚æ•°æ® PySpark æ²¡æœ‰diffæ“ä½œï¼ˆSparkçš„ä¸Šä¸‹è¡Œæ˜¯ç›¸äº’ç‹¬ç«‹ï¼Œåˆ†å¸ƒå¼å­˜å‚¨çš„ï¼‰ æ•°æ®ä¿å­˜pd_df.to_csv(&quot;/data/path_to_file&quot;) # å†™æœ¬åœ°csvæ–‡ä»¶ PySparkdf.write.csv(&quot;file:///data/path&quot;) # æ•°æ®å†™æœ¬åœ°csvï¼Œå¯èƒ½å†™å¤šä¸ªæ–‡ä»¶ df.coalesce(1).write.csv(&quot;file:///data/path&quot;) # æ•°æ®å†™æœ¬åœ°ï¼Œå†™1ä¸ªcsvæ–‡ä»¶ df.coalesce(1).mode(&quot;overwrite&quot;).option(header=True).csv(&#39;/data/hdfs_path&#39;,sep=&#39;\\t&#39;) # å†™ä¸€ä¸ªcsvæ–‡ä»¶åˆ°hdfsï¼Œå¸¦headerï¼Œé»˜è®¤è¦†ç›–ï¼Œåˆ†éš”ç¬¦ä¸º\\t df.write.insertInto(&#39;exist_hive_table&#39;) # è¿½åŠ å†™æ•°æ®åˆ°å·²å­˜åœ¨çš„hiveè¡¨ å­—æ®µä¸dfä¸­å­—æ®µåç§°é¡ºåºç±»å‹è¦å¯¹åº” df.write.insertInto(&#39;exist_hive_table&#39;, overwrite=True) # è¦†ç›–å†™æ•°æ®åˆ°å·²å­˜åœ¨çš„hiveè¡¨ å­—æ®µä¸dfä¸­å­—æ®µåç§°é¡ºåºç±»å‹è¦å¯¹åº” df.write.jdbc(url=&quot;jdbc:mysql://xxx.xxx.xxx.xxx:3306/db_name&quot;, table=&quot;table_name&quot;, mode=&quot;overwrite&quot;, properties=&#123;&quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;123456&quot;&#125;) # å°†æ•°æ®overwriteåˆ°mysql æ³¨æ„æ•°æ®é‡ä¸èƒ½å¤ªå¤§ä¸”å¹¶è¡Œåº¦ä¸èƒ½å¤ªé«˜ï¼Œå¯èƒ½ä¼šæŠŠmysqlæå®ï¼Œå»ºè®®å¹¶è¡Œåº¦ä¸è¶…è¿‡10==&gt;NumExecutors*ExecutorCores &lt;= 10 å†™è¡¨æ—¶è§‚å¯Ÿmysqlç«¯çš„è´Ÿè½½å’Œå‹åŠ›:show status;å’Œshow processlist; df.write.saveAsTable(â€œhive_tableâ€, mode=â€appendâ€) # ç›´æ¥å†™æ•°æ®åˆ°hiveè¡¨ æ— è®ºè¡¨æ˜¯å¦å·²ç»å­˜åœ¨éƒ½å¯ä»¥ è¿˜æœ‰optionsï¼ŒpartitionByï¼Œformatç­‰å‚æ•°å½±å“è¡¨ç»“æ„df.write.format(â€˜parquetâ€™).bucketBy(100,â€™yearâ€™,â€™monthâ€™).sortBy(â€˜dayâ€™).mode(â€˜overwriteâ€™).saveAsTable(â€˜sorted_bucketed_tableâ€™) # æ•°æ®æ’åºåˆ†åŒºå­˜å‚¨æˆparquetdf.coalesce(1).write.save(path,format,mode,partitionBy,**Options) # å­˜å‚¨æ•°æ®df.coalesce(1).write.json(â€œfile:///data/pathâ€,mode=â€™overwriteâ€™,) # å†™æ•°æ®åˆ°å•ä¸ªjsonæ–‡ä»¶ æ³¨ï¼šæ–‡ä»¶å†™åˆ°hdfsä¹Ÿä¸è¦ç´§ï¼Œå¯ä»¥é€šè¿‡æŒ‚è½½NFSæˆ–è€…FUSEç­‰æ–¹å¼å°†hdfsç›®å½•æŒ‚è½½åˆ°æœ¬åœ°ï¼ŒåŒæ ·æ–¹ä¾¿åç»­å¤„ç† ## é«˜çº§ç”¨æ³•ï¼ˆä¼˜åŒ–ï¼‰ * PySparkè¿ç»­ç¼–å†™è½¬æ¢å‡½æ•° ```python spark.table(&#39;ods_test.test&#39;).filter(&#39;age=22&#39;).where(&#39;dt=&quot;20200524&quot;&#39;).groupBy(&#39;id&#39;).avg(&#39;age&#39;).registerTempTable(&#39;tmp&#39;) for i in spark.sql(&quot;select id,&#39;avg(age)&#39; as avg_age from tmp&quot;).collect(): print(i[0], i[1]) è¯»å–MySQLå¤§è¡¨ä¼˜åŒ–partitionColumnï¼šåˆ†åŒºå­—æ®µï¼Œéœ€è¦æ˜¯æ•°å€¼ç±»çš„ï¼ˆpartitionColumn must be a numeric column from the table in question.ï¼‰ï¼Œç»æµ‹è¯•ï¼Œé™¤æ•´å‹å¤–ï¼Œfloatã€doubleã€decimaléƒ½æ˜¯å¯ä»¥çš„lowerBoundï¼šä¸‹ç•Œï¼Œå¿…é¡»ä¸ºæ•´æ•°ï¼Œä¸èƒ½å¤§äºupperBoundå¦åˆ™æŠ¥é”™upperBoundï¼šä¸Šç•Œï¼Œå¿…é¡»ä¸ºæ•´æ•°ï¼Œä¸lowerBoundä¸€èµ·ç¡®å®šåˆ†åŒºæ•°æ®é‡æ­¥é•¿ï¼ŒlowerBoundå’ŒupperBoundå¹¶ä¸ä¼šè¿‡æ»¤æ•°æ®ã€‚numPartitionsï¼šæœ€å¤§åˆ†åŒºæ•°é‡ï¼Œå¿…é¡»ä¸ºæ•´æ•°ï¼Œå½“ä¸º0æˆ–è´Ÿæ•´æ•°æ—¶ï¼Œå®é™…çš„åˆ†åŒºæ•°ä¸º1ï¼›å¹¶ä¸ä¸€å®šæ˜¯æœ€ç»ˆçš„åˆ†åŒºæ•°é‡ï¼Œä¾‹å¦‚â€œupperBound - lowerBound&lt; numPartitionsâ€æ—¶ï¼Œå®é™…çš„åˆ†åŒºæ•°é‡æ˜¯â€œupperBound - lowerBoundâ€ï¼›ä»¥ä¸Šå››ä¸ªå‚æ•°å¿…é¡»åŒæ—¶åˆ¶å®šå¦åˆ™æŠ¥é”™ã€‚åœ¨åˆ†åŒºç»“æœä¸­ï¼Œåˆ†åŒºæ˜¯è¿ç»­çš„ï¼Œè™½ç„¶æŸ¥çœ‹æ¯æ¡è®°å½•çš„åˆ†åŒºï¼Œä¸æ˜¯é¡ºåºçš„ï¼Œä½†æ˜¯å°†rddä¿å­˜ä¸ºæ–‡ä»¶åï¼Œå¯ä»¥çœ‹å‡ºæ˜¯é¡ºåºçš„ã€‚ conf = &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://cdh102:3306/&quot;, &quot;dbtable&quot;: &#39;db_users.tb_user_records&#39;, &quot;user&quot;: &#39;root&#39;, &quot;password&quot;: &#39;123456&#39;, &quot;partitionColumn&quot;: &quot;duration&quot;, # è¿™ä¸ªå­—æ®µä¸ºintç±»å‹ &quot;lowerBound&quot;: &quot;0&quot;, &quot;upperBound&quot;: &quot;10000&quot;, &quot;numPartitions&quot;: &quot;5&quot; &#125; df = spark.read.format(&quot;jdbc&quot;).options(**conf).load() df1.rdd.getNumPartitions() # ä¼šå¾—åˆ°5ä¸ªåˆ†åŒº è¯¥æ“ä½œçš„ç›®çš„æ˜¯å¢åŠ å¹¶è¡ŒJDBCè¿æ¥æ•°ï¼Œå¢åŠ è¯»å–é€Ÿåº¦ä»¥åŠå¢åŠ DataFrameçš„åˆ†åŒºæ•°ä»è€Œå¢åŠ è®¡ç®—çš„å¹¶å‘åº¦ã€‚å¹¶å‘åº¦å³ä¸ºSparkçš„Taskæ•°ï¼Œè¿™ä¸ªæ•°é‡ä¸€èˆ¬æ ¹æ®æ€»coreæ•°ï¼ˆexecutor_coresnum_executorsï¼‰æ¥è®¡ç®—ï¼šTaskæ•°â‰ˆæ€»coreæ•°ï¼ˆ2~3å€ï¼‰å¦‚æœæ•°æ®é‡è¾ƒå°‘ï¼Œåˆ™ä¸éœ€è¦ä»¥è¿™ç§æ–¹å¼è¯»å–ï¼Œå¦åˆ™å¯èƒ½é™ä½æ•ˆç‡ä¼ªä»£ç ï¼Œå¸®åŠ©ç†è§£åŸç†ï¼š # æƒ…å†µä¸€ï¼š if partitionColumn || lowerBound || upperBound || numPartitions æœ‰ä»»æ„é€‰é¡¹æœªæŒ‡å®šï¼ŒæŠ¥é”™ # æƒ…å†µäºŒï¼š if numPartitions == 1 å¿½ç•¥è¿™äº›é€‰é¡¹ï¼Œç›´æ¥è¯»å–ï¼Œè¿”å›ä¸€ä¸ªåˆ†åŒº # æƒ…å†µä¸‰ï¼š if numPartitions &gt; 1 &amp;&amp; lowerBound &gt; upperBound æŠ¥é”™ # æƒ…å†µå››ï¼š numPartitions = min(upperBound - lowerBound, numPartitions) if numPartitions == 1 åŒæƒ…å†µäºŒ else è¿”å›numPartitionsä¸ªåˆ†åŒº delta = (upperBound - lowerBound) / numPartitions åˆ†åŒº1æ•°æ®æ¡ä»¶ï¼špartitionColumn &lt;= lowerBound + delta || partitionColumn is null åˆ†åŒº2æ•°æ®æ¡ä»¶ï¼špartitionColumn &gt; lowerBound + delta &amp;&amp; partitionColumn &lt;= lowerBound + 2 * delta ... æœ€ååˆ†åŒºæ•°æ®æ¡ä»¶ï¼špartitionColumn &gt; lowerBound + n*delta ä¹Ÿå°±æ˜¯è¯´ï¼Œéœ€è¦åˆç†è®¾ç½®numPartitionså’ŒupperBoundå’ŒupperBoundçš„å€¼ï¼Œé¿å…æŸä¸ªåˆ†åŒºæ•°æ®é‡è¿‡å¤§ã€‚å°½é‡ä½¿ç”¨èŒƒå›´åŸºæœ¬ç¡®å®šä¸”åˆ†åŒºå­—æ®µå€¼åˆ†å¸ƒç›¸å¯¹å‡åŒ€çš„Intç±»å‹å­—æ®µåšåˆ†åŒºå­—æ®µã€‚ å¤šä¸ªUDFä½œç”¨äºåŒä¸€åˆ—æ•°æ®Demo:multi_udf_one_col.py å…¶ä»–Pythonä¸‰æ–¹åº“ï¼šSparklingPandasSparklingPandas å‚è€ƒPySpark.sql modulepandasä¸pysparkå¯¹æ¯”Sparkï¼šä½¿ç”¨partitionColumné€‰é¡¹è¯»å–æ•°æ®åº“åŸç†PySpark-DataFrameæ“ä½œæŒ‡å—","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://shmily-qjj.top/tags/pandas/"},{"name":"pyspark","slug":"pyspark","permalink":"https://shmily-qjj.top/tags/pyspark/"},{"name":"ä¼˜åŒ–","slug":"ä¼˜åŒ–","permalink":"https://shmily-qjj.top/tags/%E4%BC%98%E5%8C%96/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"é«˜æ•ˆè¿è¡ŒPythonæ–¹æ¡ˆ","slug":"é«˜æ•ˆè¿è¡ŒPythonæ–¹æ¡ˆ","date":"2020-07-26T03:16:00.000Z","updated":"2022-12-11T05:35:07.927Z","comments":true,"path":"2ed52290/","link":"","permalink":"https://shmily-qjj.top/2ed52290/","excerpt":"","text":"é«˜æ•ˆè¿è¡ŒPython&emsp;&emsp;Pythonä»¥å…¶ç®€æ´çš„è¯­æ³•ï¼Œä¸°å¯Œçš„ä¸‰æ–¹åº“ï¼Œå¼ºå¤§çš„åŠŸèƒ½è€Œå—åˆ°è¶Šæ¥è¶Šå¤šäººçš„æ¬¢è¿ï¼Œä½†æ²¡æœ‰åå…¨åç¾çš„ç¼–ç¨‹è¯­è¨€ï¼ŒPythonçš„è¿è¡Œæ•ˆç‡ä¸€ç›´è¢«äººä»¬è¯Ÿç—…ã€‚åœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›Pythonä¹Ÿèƒ½å¤Ÿé«˜æ•ˆç‡è¿è¡Œï¼Œå……åˆ†åˆ©ç”¨ç³»ç»Ÿèµ„æºï¼Œæ‰€ä»¥è¿™ç¯‡æ–‡ç« è®°å½•ä¸€äº›åŠ å¿«Pythonç¨‹åºè¿è¡Œæ•ˆç‡çš„æ–¹æ³•ï¼Œè®©æˆ‘ä»¬çš„Pythonæ›´é«˜æ•ˆï¼ åŠ é€Ÿå·²æœ‰ä»£ç &emsp;&emsp;è¿™éƒ¨åˆ†ä»‹ç»çš„æ–¹æ¡ˆä¸»è¦é’ˆå¯¹å·²æœ‰Pythonä»£ç åœ¨ä¸æƒ³åšå¤ªå¤§æ”¹åŠ¨çš„æƒ…å†µä¸‹çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚ ä½¿ç”¨numbaåŠ é€Ÿnumbaå®˜æ–¹ç½‘ç«™ ä¼˜ç‚¹ï¼š æ— å­¦ä¹ æˆæœ¬ï¼ŒåªåŠ ä¸€è¡Œä»£ç ï¼ˆé«˜çº§ç”¨æ³•å’Œè°ƒä¼˜é™¤å¤–ï¼‰ åŠ¨æ€ç¼–è¯‘ï¼Œç›´æ¥ç¿»è¯‘æœºå™¨ç ï¼Œä¸èµ°Pythonè™šæ‹Ÿæœºï¼Œæ€§èƒ½è¾¾åˆ°Cè¯­è¨€æ°´å¹³ æ”¯æŒGPUåŠ é€Ÿ å…¼å®¹å¸¸ç”¨çš„ç§‘å­¦è®¡ç®—åº“ å±€é™ï¼š æˆ‘æµ‹è¯•æ—¶æœ‰äº›åœºæ™¯ä¼šæŠ¥WARNï¼Œéœ€è¦è°ƒä¸€ä¸‹å‚æ•°ï¼Œä¹Ÿå¯èƒ½ç¯å¢ƒåŸå›  å¯¹éƒ¨åˆ†ç¬¬ä¸‰æ–¹åº“æœ‰å…¼å®¹æ€§é—®é¢˜ æµ‹è¯•ï¼š pip install numba æ‰©å±•ï¼š from numba import jit @jit åœ¨æ–¹æ³•å‰åŠ è£…é¥°å™¨-å¸¸ç”¨åšæ³•ï¼Œobjectæ¨¡å¼ï¼šé»˜è®¤nopythonæ¨¡å¼ï¼Œä½†å¦‚æœé‡åˆ°ä¸å…¼å®¹çš„ç¬¬ä¸‰æ–¹åº“ä¼šé€€åŒ–æˆpythonæ¨¡å¼ï¼Œä¿è¯èƒ½è¿è¡Œä½†ä¸èƒ½æé€Ÿã€‚ @jit(nopython=True,fastmath=True) ç‰ºç‰²ä¸€ç‚¹æ•°å­¦ç²¾åº¦æ¥æé«˜é€Ÿåº¦ï¼ˆé»˜è®¤ç²¾åº¦é«˜ï¼‰ @jit(nopython=True,parallel=True) è‡ªåŠ¨è¿›è¡Œå¹¶è¡Œè®¡ç®— åŸç†ï¼šnumbaåŠ é€ŸPythonä»£ç çš„åŸç†æ˜¯ä½¿ç”¨jitå³æ—¶ç¼–è¯‘ç›´æ¥å°†Pythonä»£ç ç¿»è¯‘æˆæœºå™¨ç ï¼ˆä¸Šå›¾å·¦ä¾§æµç¨‹ï¼‰ï¼Œé¿å…äº†ç¼–è¯‘æˆPythonå­—èŠ‚ç pycå†èµ°Pythonè™šæ‹Ÿæœºï¼ˆä¸Šå›¾å³ä¾§æµç¨‹ï¼‰ï¼Œç›´æ¥æé«˜äº†è¿è¡Œæ•ˆç‡ã€‚ ç»“è®ºï¼šä»ä¸Šé¢çš„æµ‹è¯•ç»“æœå¯ä»¥çœ‹åˆ°æœ‰å°†è¿‘300å€çš„æ•ˆç‡æå‡ï¼Œèƒ½å¤§å¹…åŠ é€ŸPythonè„šæœ¬çš„æ‰§è¡Œæ•ˆç‡ï¼Œå¯¹å¤§é‡æ•°æ®å‹å¥½ï¼Œå¯¹å¾ªç¯å‹å¥½ã€‚æˆ‘æµ‹è¯•å³ä½¿åœ¨ä½ç«¯å¤„ç†å™¨ç¯å¢ƒè¿è¡Œï¼Œä¹Ÿèƒ½æœ‰100+å€çš„æ€§èƒ½æå‡ã€‚è¿™ä¸ªæ–¹æ¡ˆæé€Ÿæ•ˆæœç›¸å½“æ˜æ˜¾ï¼Œè€Œä¸”å¯¹åŸæœ‰ä»£ç å’Œç¯å¢ƒæ”¹åŠ¨å¾ˆå°ï¼Œæ¨èå“¦ï¼ ä½¿ç”¨modinåŠ é€Ÿpandasmodinå®˜æ–¹ç½‘ç«™pandasæ˜¯å¾ˆå¸¸ç”¨çš„æ•°æ®åˆ†æåº“ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œä½†å®ƒæœ‰ä¸ªç¼ºç‚¹å°±æ˜¯å¯¹å¤§æ•°æ®çš„æ”¯æŒå¹¶ä¸å¥½ï¼Œä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®ã€‚ä¼˜ç‚¹ï¼š æ— å­¦ä¹ æˆæœ¬ï¼Œåªæ”¹ä¸€è¡Œä»£ç  å¯ä»¥åˆ†å¸ƒå¼è·‘ï¼ŒåŸºäºray æ”¯æŒGPUåŠ é€Ÿ å±€é™æ€§ï¼š ç›®å‰æ”¯æŒ93%çš„Pandas API åˆ†å¸ƒå¼è¿è¡ŒåŠŸèƒ½ä¸ºä¸ºå®éªŒæ€§åŠŸèƒ½ éšç€è¿è¡Œæ ¸å¿ƒæ•°å¢åŠ ï¼Œä¼šå ç”¨æ›´å¤šå†…å­˜ å®‰è£…æ—¶å¯èƒ½ä¼šæ›´æ”¹åŸæœ‰pandasç‰ˆæœ¬ï¼Œéœ€ç•™æ„ éœ€è¦å®‰è£…rayæˆ–daskä¾èµ–åŒ…ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–ä¾èµ–åŒ… æµ‹è¯•ï¼š def pandas_test(): import pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x: x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x: 1 if x.a%2==0 else 0, axis=1) print(&#39;pandas_df.apply Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg(&#123;&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]&#125;) print(&#39;pandas_df.groupby Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;pandas_df.read_csv Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) def modin_pandas_test(): import modin.pandas as pd from time import time df = pd.DataFrame(zip(range(1000000),range(1000000,2000000)),columns=[&#39;a&#39;,&#39;b&#39;]) start = time() df[&#39;c&#39;] = df.apply(lambda x:x.a+x.b ,axis=1) df[&#39;d&#39;] = df.apply(lambda x:1 if x.a%2==0 else 0, axis=1) print(&#39;modin_pandas_df.apply Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) start = time() group_df = df[[&#39;d&#39;,&#39;a&#39;]].groupby(&#39;d&#39;,as_index=False).agg(&#123;&quot;a&quot;:[&#39;sum&#39;,&#39;max&#39;,&#39;min&#39;,&#39;mean&#39;]&#125;) print(&#39;modin_pandas_df.groupby Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) # start = time() # data = pd.read_csv(&#39;test_modin.csv&#39;) # print(&#39;modin_pandas_df.read_csv Time: &#123;:5.2f&#125;s&#39;.format(time() - start)) if __name__ == &#39;__main__&#39;: pandas_test() modin_pandas_test() å•æœºè·‘Apply APIé€Ÿåº¦å¤§æ¦‚å¿«äº†3.5å€å¤šã€‚åˆ†å¸ƒå¼è¿˜æ²¡æµ‹è¯•ã€‚ ç»“è®ºï¼šä½¿ç”¨modinæ¨¡å—çš„pandasä»£æ›¿æ™®é€šçš„pandasï¼Œæœ¬è´¨æ˜¯å°†å•æœºå•æ ¸è·‘çš„ä»»åŠ¡è´Ÿè½½åˆ†æ•£åˆ°å¤šæ ¸å¿ƒç”šè‡³å¤šæœºå™¨æ¥åŠ é€Ÿè¿ç®—ã€‚åŸºæœ¬å¯ä»¥æ»¡è¶³ä½¿ç”¨pandasçš„ä¸šåŠ¡éœ€æ±‚åœºæ™¯ï¼Œè€Œä¸”æ ¸å¿ƒæ•°è¶Šå¤šï¼Œæœºå™¨æ•°è¶Šå¤šï¼Œè¿è¡Œæ•ˆç‡æå‡è¶Šé«˜ï¼Œä½†ç›¸åº”éœ€è¦æ›´å¤§çš„å†…å­˜ã€‚é€‚åˆå¯¹å¤§é‡æ•°æ®æ“ä½œçš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œpandaså®˜ç½‘ç»™å‡ºäº†ä¸€äº›ä¼˜åŒ–æ•ˆç‡çš„å»ºè®®ï¼Œå‚è€ƒï¼šEnhancing performance ä½¿ç”¨pandarallelåŠ é€Ÿpandaspandarallelå®˜æ–¹ç½‘ç«™ä¼˜ç‚¹ï¼š æ— å­¦ä¹ æˆæœ¬ï¼Œåªæ·»åŠ 1-2è¡Œä»£ç  å……åˆ†åˆ©ç”¨CPU å±€é™æ€§ï¼š ç†è®ºä¸Šåªæé€Ÿç‰©ç†æ ¸å¿ƒæ•°å€çš„æ•ˆç‡ã€‚ æœ‰ä½¿ç”¨æˆæœ¬ï¼ˆå®ç°æ–°è¿›ç¨‹ï¼Œé€šè¿‡å…±äº«å†…å­˜å‘é€æ•°æ®ç­‰ç­‰ï¼‰ï¼Œå› æ­¤åªæœ‰è®¡ç®—é‡è¶³å¤Ÿé«˜æ—¶ï¼Œæ‰æ›´æœ‰æ•ˆã€‚ ä½¿ç”¨ï¼špandarallel-example ç»“è®ºï¼šå¯¹äºéå¸¸å°‘é‡çš„æ•°æ®ï¼Œä¸å€¼å¾—ä½¿ç”¨ã€‚å¯¹å¤§é‡æ•°æ®ï¼Œå¯ä»¥å°è¯•è¯¥æ–¹æ¡ˆï¼Œä¸ä¼šåƒmodinä¸€æ ·ä¾èµ–pandasç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨åŸæœ‰pandasç‰ˆæœ¬ä¸Šæ“ä½œã€‚ ç¼–å†™é«˜æ•ˆä»£ç &emsp;&emsp;é™¤äº†ä¸Šé¢å·²ç»æåˆ°çš„æ–¹æ¡ˆï¼Œåœ¨æˆ‘ä»¬å¹³æ—¶ç¼–ç æ—¶ä¹Ÿè¦æ³¨æ„ç¼–ç æ•ˆç‡ï¼Œè¿™éƒ¨åˆ†ä¸»è¦ä»‹ç»ç¼–å†™Pythonä»£ç æ—¶ä¸€äº›æé«˜è¿è¡Œæ•ˆç‡çš„æ–¹æ³•ã€æŠ€å·§å’Œå·¥å…·ã€‚ ä½¿ç”¨PySparkä¼˜ç‚¹ï¼š ä½¿ç”¨Pysparkçš„dataframeè¿›è¡Œæ•°æ®æ“ä½œæ•°æ®åˆ†æç®€å•é«˜æ•ˆï¼Œæœ‰è¾ƒä½çš„å­¦ä¹ æˆæœ¬ã€‚ åªéœ€è¦ä¸€è¡Œä»£ç å³å¯å®ç°pyspark dataframeå’Œpandas dataframeäº’ç›¸è½¬æ¢ã€‚ Pyspark dataframeå¯ä»¥ç›´æ¥registerTempTableï¼Œç„¶åå¯ä»¥å¾ˆå®¹æ˜“åœ°ä½¿ç”¨pyspark.sqlå¯¹è¿™ä¸ªè¡¨åšsqlåˆ†æã€‚ åˆ†å¸ƒå¼è¿è¡Œï¼Œåˆ†ææ•ˆç‡æ•ˆç‡é«˜ï¼Œå¯¹å¤§é‡æ•°æ®å¾ˆå‹å¥½ã€‚ åŠŸèƒ½å¼ºå¤§ï¼Œæ”¯æŒudfã€‚ å±€é™ï¼š å†™ä»£ç è¦æ³¨æ„ï¼Œé¿å…å°æ–‡ä»¶ï¼Œå‡å°‘driverResultSetï¼ˆæ³¨æ„å°½é‡é¿å…è®©driverå•ç‚¹è¿ç®—å…¨éƒ¨æ•°æ®ï¼‰ éœ€è¦æ›´å¤šå†…å­˜åšè®¡ç®— ä½¿ç”¨ï¼š # ä¾‹å¦‚ä»¥å‰çš„pandasåˆ†æä½œä¸šï¼Œå¯ä»¥ç§»æ¤åˆ°pyspark # â‘ pandas dataframeè½¬pyspark dataframeï¼š df = spark.createDataFrame(pandas_dataframe) # â‘¡pyspark dataframeè½¬pandas dataframe: pandas_dataframe = spark_dataframe.toPandas() # â‘¢ä»£ç ä¸­å°†spark dataframeæ³¨å†Œæˆä¸´æ—¶è¡¨ï¼ˆéšsparkSessioné”€æ¯ï¼Œä¸å ç©ºé—´ï¼‰ df.registerTempTable(â€˜tmpâ€™) # â‘£å¯¹æ•°æ®åšSQLåˆ†æ df = spark.sql(â€œâ€â€select * from tmp limit 10â€â€â€) ç»“æœä¸ºæ–°çš„dataframe # â‘¤ç»“æœè¾“å‡º df.show() / df.writeInsertInto(table_name) / df.write.option(â€˜headerâ€™,True),csv(file) # â€¦â€¦ å¾ˆå¤šç§è¾“å‡ºæ–¹å¼ï¼Œä¹Ÿå¯ä»¥ç»§ç»­è½¬å›pandas dataframeåšåç»­æ“ä½œ PySparkä½¿ç”¨æ–‡æ¡£ ç»“è®ºï¼šåœ¨æ•°æ®é‡ç‰¹åˆ«å¤§çš„æƒ…å†µä¸‹ï¼Œåˆ†å¸ƒå¼è®¡ç®—æ˜¯é¦–é€‰ï¼Œæ‰€ä»¥å¯¹äºå¤§è§„æ¨¡æ•°æ®åˆ†æï¼Œç›®å‰PySparkæ˜¯æ¯”è¾ƒæ¨èçš„æ–¹å¼ã€‚ ä½¿ç”¨DaskDaskå®˜æ–¹ç½‘ç«™ä¼˜ç‚¹ï¼š é«˜æ•ˆå¤„ç†å¤§é‡æ•°æ® æ”¯æŒåˆ†å¸ƒå¼ å±€é™ï¼š åªæœ‰æ¥è‡ªpandasçš„æŸäº›åŠŸèƒ½æ‰èƒ½ç§»æ¤åˆ°Daskä¸Šæ‰§è¡Œ ä»…åœ¨ä¸é€‚åˆä¸»å­˜å‚¨å™¨çš„æ•°æ®é›†ä¸Šï¼Œæ‰å»ºè®®ä½¿ç”¨Dask ç¤ºä¾‹ï¼š # ä½é€Ÿï¼š import numpy as np import pandas as pd df = pd.Dataframe(np.random.randint(0, 6, size=(100000000, 5)), columns = list(&#39;abcde&#39;) df.groupby(&#39;a&#39;).mean() # é«˜é€Ÿï¼š import dask.dataframe as dd df_dask = dd.from_pandas(df, npartitions=50) df_dask.groupby(&#39;a&#39;).mean().compute() è¯¦ç»†äº†è§£Dask ä½¿ç”¨å¤šçº¿ç¨‹ä¼˜ç‚¹ï¼šèƒ½æé«˜IOå¯†é›†å‹Pythonç¨‹åºæ•ˆç‡ã€‚å› ä¸ºåœ¨ä¸€ä¸ªçº¿ç¨‹å› IOé˜»å¡ç­‰å¾…æ—¶ï¼ŒCPUåˆ‡æ¢åˆ°å…¶ä»–çº¿ç¨‹ï¼ŒCPUåˆ©ç”¨ç‡é«˜ã€‚å±€é™ï¼šç”±äºGIL(Global Interpreter Lock)æœºåˆ¶é™åˆ¶Pythonè§£é‡Šå™¨ä»»ä½•æ—¶åˆ»éƒ½åªèƒ½æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹ï¼Œåœ¨è®¡ç®—å¯†é›†å‹Pythonç¨‹åºå¹¶ä¸èƒ½æé«˜æ‰§è¡Œæ•ˆç‡ï¼Œåè€Œå¯èƒ½å› çº¿ç¨‹åˆ‡æ¢é™ä½æ•ˆç‡ã€‚ä½¿ç”¨ï¼š # ç”¨æ³•1 import threading import time class myThread(threading.Thread): def __init__(self,threadID,name,counter): threading.Thread.__init__(self) self.threadId = threadID self.name = name self.counter = counter def run(self): # çº¿ç¨‹åˆ›å»ºæ‰§è¡Œrunå‡½æ•° while self.counter &lt; 8: time.sleep(2) self.counter += 1 print(self.threadId,self.name,self.counter,time.ctime(time.time())) print(&quot;Thread Stop&quot;) thread1 = myThread(1, &quot;Thread-1&quot;, 1) thread2 = myThread(2, &quot;Thread-2&quot;, 2) thread1.start() thread2.start() # ç”¨æ³•2 import threading from queue import Queue import time def testThread(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(5): t = threading.Thread(target=testThread, arg=(i, )) t.start() GILï¼šGILæ˜¯CPythonè§£é‡Šå™¨å¼•å…¥çš„é”ï¼ŒGILåœ¨è§£é‡Šå™¨å±‚é¢é˜»æ­¢äº†çœŸæ­£çš„å¹¶è¡Œè¿è¡Œã€‚è§£é‡Šå™¨åœ¨æ‰§è¡Œä»»ä½•çº¿ç¨‹ä¹‹å‰ï¼Œå¿…é¡»ç­‰å¾…å½“å‰æ­£åœ¨è¿è¡Œçš„çº¿ç¨‹é‡Šæ”¾GILï¼Œäº‹å®ä¸Šï¼Œè§£é‡Šå™¨ä¼šå¼ºè¿«æƒ³è¦è¿è¡Œçš„çº¿ç¨‹å¿…é¡»æ‹¿åˆ°GILæ‰èƒ½è®¿é—®è§£é‡Šå™¨çš„ä»»ä½•èµ„æºï¼Œä¾‹å¦‚æ ˆæˆ–Pythonå¯¹è±¡ç­‰ï¼Œè¿™ä¹Ÿæ­£æ˜¯GILçš„ç›®çš„ï¼Œä¸ºäº†é˜»æ­¢ä¸åŒçš„çº¿ç¨‹å¹¶å‘è®¿é—®Pythonå¯¹è±¡ã€‚è¿™æ ·GILå¯ä»¥ä¿æŠ¤è§£é‡Šå™¨çš„å†…å­˜ï¼Œè®©åƒåœ¾å›æ”¶å·¥ä½œæ­£å¸¸ï¼Œä¸ä¼šå‡ºç°è¿è¡Œæ­»é”ã€‚ä½†äº‹å®ä¸Šï¼Œè¿™å´é€ æˆäº†ç¨‹åºå‘˜æ— æ³•é€šè¿‡å¹¶è¡Œæ‰§è¡Œå¤šçº¿ç¨‹æ¥æé«˜ç¨‹åºçš„æ€§èƒ½ã€‚å¦‚æœæˆ‘ä»¬å»æ‰GILï¼Œå°±å¯ä»¥å®ç°çœŸæ­£çš„å¹¶è¡Œã€‚GILå¹¶æ²¡æœ‰å½±å“å¤šå¤„ç†å™¨å¹¶è¡Œçš„çº¿ç¨‹ï¼Œåªæ˜¯é™åˆ¶äº†ä¸€ä¸ªè§£é‡Šå™¨åªèƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨è¿è¡Œã€‚ç»“è®ºï¼šIOåŒ…æ‹¬ç£ç›˜IOå’Œç½‘ç»œIOï¼Œæ‰€ä»¥å¯ä»¥åœ¨ç£ç›˜IOå¯†é›†å‹Pythonä»»åŠ¡æˆ–ç½‘ç»œå»¶è¿Ÿæ˜¯ç“¶é¢ˆçš„Pythonä»»åŠ¡ä¸­ä½¿ç”¨Pythonå¤šçº¿ç¨‹ã€‚ ä½¿ç”¨å¤šè¿›ç¨‹ä¼˜ç‚¹ï¼šå¯ä»¥æé«˜è®¡ç®—å¯†é›†å‹Pythonç¨‹åºæ‰§è¡Œæ•ˆç‡ã€‚ä¼šç”¨åˆ°å¤šä¸ªCPUæ ¸å¿ƒã€‚ç»•è¿‡GILæœºåˆ¶ï¼Œå……åˆ†åˆ©ç”¨CPUã€‚æ ¸å¿ƒåŸç†æ˜¯ä»¥å­è¿›ç¨‹çš„å½¢å¼ï¼Œå¹³è¡Œçš„è¿è¡Œå¤šä¸ªpythonè§£é‡Šå™¨ï¼Œä»è€Œä»¤pythonç¨‹åºå¯ä»¥åˆ©ç”¨å¤šæ ¸CPUæ¥æå‡æ‰§è¡Œé€Ÿåº¦ã€‚ç”±äºå­è¿›ç¨‹ä¸ä¸»è§£é‡Šå™¨ç›¸åˆ†ç¦»ï¼Œæ‰€ä»¥ä»–ä»¬çš„å…¨å±€è§£é‡Šå™¨é”ä¹Ÿæ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚æ¯ä¸ªå­è¿›ç¨‹éƒ½èƒ½å¤Ÿå®Œæ•´ä½¿ç”¨ä¸€ä¸ªCPUå†…æ ¸ã€‚ å±€é™ï¼š è¿›ç¨‹é—´è¿›è¡Œæ•°æ®çš„äº¤äº’ä¼šäº§ç”Ÿé¢å¤–çš„I/Oå¼€é”€ã€‚ æ•´ä¸ªå†…å­˜ç©ºé—´è¢«å¤åˆ¶åˆ°æ¯ä¸ªå­è¿›ç¨‹ä¸­ï¼Œè¿™æ ·å¯¹äºæ¯”è¾ƒå¤æ‚çš„ç¨‹åºé€ æˆçš„é¢å¤–å¼€é”€ä¹Ÿå¾ˆå¤§ã€‚ ä½¿ç”¨ï¼š # ç”¨æ³•1 import multiprocessing def method(num): print(num) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() # ç”¨æ³•2 from multiprocessing.pool import ThreadPool # å¯ä»¥æä¾›æŒ‡å®šæ•°é‡çš„è¿›ç¨‹ä¾›ç”¨æˆ·è°ƒç”¨ï¼Œå½“æœ‰æ–°çš„è¯·æ±‚æäº¤åˆ°Poolä¸­æ—¶ï¼Œå¦‚æœæ± è¿˜æ²¡æœ‰æ»¡ï¼Œå°±ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„è¿›ç¨‹æ¥æ‰§è¡Œè¯·æ±‚ã€‚ # å¦‚æœæ± æ»¡ï¼Œè¯·æ±‚å°±ä¼šå‘ŠçŸ¥å…ˆç­‰å¾…ï¼Œç›´åˆ°æ± ä¸­æœ‰è¿›ç¨‹ç»“æŸï¼Œæ‰ä¼šåˆ›å»ºæ–°çš„è¿›ç¨‹æ¥æ‰§è¡Œè¿™äº›è¯·æ±‚ã€‚ def my_print(item): print(item[0]+item[1]) pool_size = 10 # è¿›ç¨‹æ± å¤§å° items = [(1,2),(2,3),(3,4),(4,5)] pool = ThreadPool(pool_size) # åˆ›å»ºä¸€ä¸ªè¿›ç¨‹æ±  pool.map(my_print, items) # å¾€è¿›ç¨‹æ± ä¸­å¡«è¿›ç¨‹ pool.close() # å…³é—­è¿›ç¨‹æ± ï¼Œä¸å†æ¥å—è¿›ç¨‹ pool.join() # ç­‰å¾…å­è¿›ç¨‹ç»“æŸä»¥åå†ç»§ç»­å¾€ä¸‹è¿è¡Œï¼Œé€šå¸¸ç”¨äºè¿›ç¨‹é—´çš„åŒæ­¥ ç­‰å¾…è¿›ç¨‹æ± ä¸­è¿›ç¨‹å…¨éƒ¨æ‰§è¡Œå®Œ # å…±äº«å†…å­˜-å…±äº«å˜é‡ import multiprocessing from ctypes import c_char_p import time int_val = multiprocessing.Value(&#39;i&#39;, 0) # intç±»å‹å…±äº«å˜é‡ s = (c_char_p, &#39;str&#39;) # strç±»å‹å…±äº«å˜é‡ def method(num): for i in range(10): time.sleep(0.1) with int_val.get_lock(): # ä»ç„¶éœ€è¦ä½¿ç”¨ get_lock æ–¹æ³•æ¥è·å–é”å¯¹è±¡ int_val.value += num print(int_val.value) if __name__ == &#39;__main__&#39;: for i in range(100): p = multiprocessing.Process(target=method, args=(i,)) p.start() ç»“è®ºï¼šå¦‚æœPythonç¨‹åºç“¶é¢ˆåœ¨CPUæ•°é‡æˆ–æ˜¯CPUå¯†é›†å‹ï¼Œéƒ½å¯é‡‡ç”¨å¤šè¿›ç¨‹ã€‚ ä½¿ç”¨Cythonä¼˜ç‚¹ï¼š Pythonä»£ç å¯é€šè¿‡ä¸€å®šå·¥å…·è½¬Cythonä»£ç  æ€§èƒ½è¾¾åˆ°Cè¯­è¨€æ°´å¹³ å±€é™ï¼š éœ€è¦ä¿®æ”¹è½¬æ¢å·¥å…· é«˜çº§ç”¨æ³•å­¦ä¹ æˆæœ¬é«˜ ä½¿ç”¨ï¼šå­¦ä¹ Cythonï¼šcython-book pip install cython ä½¿ç”¨concurrent.futuresä»‹ç»ï¼šå¯¹threadingå’Œmultiprocessingè¿›ä¸€æ­¥å°è£…çš„åŒ…ï¼Œæ–¹ä¾¿å®ç°çº¿ç¨‹æ± å’Œè¿›ç¨‹æ± ã€‚ä½¿ç”¨ï¼š # çº¿ç¨‹æ±  import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ThreadPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) # è¿›ç¨‹æ±  import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, Executor start = time.time() pool = ProcessPoolExecutor(max_workers=2) results = list(pool.map(gcd, numbers)) end = time.time() print &#39;Took %.3f seconds.&#39; % (end - start) æ‰©å±•ï¼š åœ¨ä¸¤ä¸ªCPUæ ¸å¿ƒçš„æœºå™¨ä¸Šè¿è¡Œå¤šè¿›ç¨‹ç¨‹åºï¼Œæ¯”å…¶ä»–ä¸¤ä¸ªç‰ˆæœ¬éƒ½å¿«ã€‚ è¿™æ˜¯å› ä¸ºï¼ŒProcessPoolExecutorç±»ä¼šåˆ©ç”¨multiprocessingæ¨¡å—æ‰€æä¾›çš„åº•å±‚æœºåˆ¶ï¼Œå®Œæˆä¸‹åˆ—æ“ä½œï¼š 1. æŠŠnumbersåˆ—è¡¨ä¸­çš„æ¯ä¸€é¡¹è¾“å…¥æ•°æ®éƒ½ä¼ ç»™mapã€‚ 2. ç”¨pickleæ¨¡å—å¯¹æ•°æ®è¿›è¡Œåºåˆ—åŒ–ï¼Œå°†å…¶å˜æˆäºŒè¿›åˆ¶å½¢å¼ã€‚ 3. é€šè¿‡æœ¬åœ°å¥—æ¥å­—ï¼Œå°†åºåˆ—åŒ–ä¹‹åçš„æ•°æ®ä»ç…®è§£é‡Šå™¨æ‰€åœ¨çš„è¿›ç¨‹ï¼Œå‘é€åˆ°å­è§£é‡Šå™¨æ‰€åœ¨çš„è¿›ç¨‹ã€‚ 4. åœ¨å­è¿›ç¨‹ä¸­ï¼Œç”¨pickleå¯¹äºŒè¿›åˆ¶æ•°æ®è¿›è¡Œååºåˆ—åŒ–ï¼Œå°†å…¶è¿˜åŸæˆpythonå¯¹è±¡ã€‚ 5. å¼•å…¥åŒ…å«gcdå‡½æ•°çš„pythonæ¨¡å—ã€‚ 6. å„ä¸ªå­è¿›ç¨‹å¹¶è¡Œçš„å¯¹å„è‡ªçš„è¾“å…¥æ•°æ®è¿›è¡Œè®¡ç®—ã€‚ 7. å¯¹è¿è¡Œçš„ç»“æœè¿›è¡Œåºåˆ—åŒ–æ“ä½œï¼Œå°†å…¶è½¬å˜æˆå­—èŠ‚ã€‚ 8. å°†è¿™äº›å­—èŠ‚é€šè¿‡socketå¤åˆ¶åˆ°ä¸»è¿›ç¨‹ä¹‹ä¸­ã€‚ 9. ä¸»è¿›ç¨‹å¯¹è¿™äº›å­—èŠ‚æ‰§è¡Œååºåˆ—åŒ–æ“ä½œï¼Œå°†å…¶è¿˜åŸæˆpythonå¯¹è±¡ 10.æœ€åï¼ŒæŠŠæ¯ä¸ªå­è¿›ç¨‹æ‰€æ±‚å‡ºçš„è®¡ç®—ç»“æœåˆå¹¶åˆ°ä¸€ä»½åˆ—è¡¨ä¹‹ä¸­ï¼Œå¹¶è¿”å›ç»™è°ƒç”¨è€…ã€‚ multiprocessingå¼€é”€æ¯”è¾ƒå¤§ï¼ŒåŸå› å°±åœ¨äºï¼šä¸»è¿›ç¨‹å’Œå­è¿›ç¨‹ä¹‹é—´é€šä¿¡ï¼Œå¿…é¡»è¿›è¡Œåºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„æ“ä½œã€‚ è¯¦ç»†å‚è€ƒï¼š**python concurrent.futures** å¸¸è§ä»£ç ä¼˜åŒ– åœ¨setä¸­æŸ¥æ‰¾æ¯”åœ¨listæŸ¥æ‰¾å¿« list_data = list(data) set_data = set(data) # ä½é€Ÿï¼š 789 in list_data # é«˜é€Ÿï¼š 789 in set_data ç”¨dictè€Œéä¸¤ä¸ªlistè¿›è¡ŒåŒ¹é…æŸ¥æ‰¾ # å·²çŸ¥list_a,list_b # ä½é€Ÿï¼š list_b[list_a.index(123)] # é«˜é€Ÿï¼š dict(zip(list_a,list_b)).get(123,None) ä¼˜å…ˆç”¨forå¾ªç¯ï¼Œæ¯”whileç•¥å¿« åœ¨å¾ªç¯ä½“ä¸­é¿å…é‡å¤è®¡ç®— ç”¨å¾ªç¯æœºåˆ¶ä»£æ›¿é€’å½’å‡½æ•° # ä½é€Ÿï¼š def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # é«˜é€Ÿï¼š def fib(n): if n in (1,2): return 1 a, b = 1, 1 for i in range(2,n): a,b = b, a+b return b ä½¿ç”¨ç¼“å­˜æœºåˆ¶åŠ é€Ÿé€’å½’å‡½æ•° # ä½é€Ÿï¼š def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) # é«˜é€Ÿï¼š from functools import lru_cache @lru_cache(100) def fib(): return (1 if n in (1,2) else fib(n-1)+fib(n-2)) ä½¿ç”¨collections.CounteråŠ é€Ÿè®¡æ•° import time data = [x**2 % 1989 for x in range(2000000)] # ä½é€Ÿ st = time.time() values_count = &#123;&#125; for i in data: i_cnt = values_count.get(i, 0) values_count[i] = i_cnt + 1 print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) # é«˜é€Ÿ st = time.time() from collections import Counter values_count = Counter(data) print(values_count.get(4, 0)) print(&quot;time: %s&quot; % (time.time() - st)) ä½¿ç”¨collections.ChainMapåŠ é€Ÿå­—å…¸åˆå¹¶ # ä½é€Ÿ dict_a = &#123;i: i + 1 for i in range(1, 1000000, 2)&#125; dict_b = &#123;i: i * 2 + 1 for i in range(1, 1000000, 3)&#125; dict_c = &#123;i: i * 3 + 1 for i in range(1, 1000000, 5)&#125; dict_d = &#123;i: i * 4 + 1 for i in range(1, 1000000, 7)&#125; result = dict_a.copy() result.update(dict_b) result.update(dict_c) result.update(dict_d) print(result.get(9999)) # é«˜é€Ÿ from collections import ChainMap chain = ChainMap(dict_a, dict_b, dict_c, dict_d) print(chain.get(9999)) ä½¿ç”¨mapä»£æ›¿æ¨å¯¼å¼è¿›è¡ŒåŠ é€Ÿ a = [x**2 for x in range(1, 1000000, 3)] # ä½é€Ÿ a = map(lambda x: x**2, range(1, 1000000, 3)) # é«˜é€Ÿ ä½¿ç”¨filterä»£æ›¿æ¨å¯¼å¼è¿›è¡ŒåŠ é€Ÿ a = [x for x in range(1, 1000000, 3) if x % 7 == 0] # ä½é€Ÿ a = filter(lambda x: x % 7 == 0, range(1, 1000000, 3)) # é«˜é€Ÿ numpyå‘é‡åŒ–åŠ é€Ÿ-ä½¿ç”¨np.arrayä»£æ›¿listé›†åˆ a = range(1, 1000000, 3) b = range(1, 1000000, -3) c = [3 * a[i] - 2 * b[i] for i in range(0, len(a)] # ä½é€Ÿ import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.arange(1, 1000000, -3) array_c = 3 * array_a - 2 * array_b # é«˜é€Ÿ ä½¿ç”¨np.ufuncä»£æ›¿math.func # ä½é€Ÿ import math a = range(1, 1000000, 3) b = [math.log(x) for x in a] # é«˜é€Ÿ import numpy as np array_a = np.arange(1, 1000000, 3) array_b = np.log(array_a) pandas df.to_excelæ•ˆç‡ä½äºdf.to_csv æŸ¥çœ‹Pythonæ€§èƒ½æ—¥å¿—ä½¿ç”¨profilerpythonä¸­çš„profilerå¯ä»¥å¸®åŠ©æˆ‘ä»¬æµ‹é‡ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­è¯¦ç»†çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦ã€‚ä½¿ç”¨æ—¶é€šè¿‡-oå‚æ•°ä¼ å…¥å¯é€‰è¾“å‡ºæ–‡ä»¶ä»¥ä¿ç•™æ€§èƒ½æ—¥å¿—ã€‚ python -m cProfile [-o output_file] my_python_file.py ä½¿ç”¨profileå¯¼å…¥profileç›‘æ§pythonç¨‹åºæ•´ä½“æ‰§è¡Œè€—æ—¶ã€‚ import profile profile.run(&#39;main()&#39;) ä½¿ç”¨line_profilerç›‘æ§æ–¹æ³•è€—æ—¶ã€‚ # pip install line_profiler def a(): pass def main(): a() from line_profiler import LineProfiler lp = LineProfiler(a,main) lp.run(&#39;main()&#39;) lp.print_stats() åœ¨ipythonä¸­è·å–ä»£ç è€—æ—¶%time code è·å–æ‰§è¡Œcodeè¿™ä¸€è¡Œä»£ç çš„è€—æ—¶ %%time è·å–è€—æ—¶ %%timeit -n 10 è·å–æ‰§è¡Œ10æ¬¡çš„å¹³å‡è€—æ—¶ %prun method() è·å–æ‰§è¡Œmethodæ–¹æ³•çš„è€—æ—¶è¯¦æƒ…ï¼Œè¾“å‡ºä¸profilerä¸€æ ·","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://shmily-qjj.top/tags/Python/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Apache Kuduæ€»ç»“","slug":"Apache Kuduæ€»ç»“","date":"2020-07-05T04:26:08.000Z","updated":"2022-12-11T05:35:07.901Z","comments":true,"path":"5f26355/","link":"","permalink":"https://shmily-qjj.top/5f26355/","excerpt":"","text":"Apache Kuduå‰è¨€&emsp;&emsp;åœ¨Kuduå‡ºç°å‰ï¼Œç”±äºä¼ ç»Ÿå­˜å‚¨ç³»ç»Ÿçš„å±€é™æ€§ï¼Œå¯¹äºæ•°æ®çš„å¿«é€Ÿè¾“å…¥å’Œåˆ†æè¿˜æ²¡æœ‰ä¸€ä¸ªå®Œç¾çš„è§£å†³æ–¹æ¡ˆï¼Œè¦ä¹ˆä»¥ç¼“æ…¢çš„æ•°æ®è¾“å…¥ä¸ºä»£ä»·å®ç°å¿«é€Ÿåˆ†æï¼Œè¦ä¹ˆä»¥ç¼“æ…¢çš„åˆ†æä¸ºä»£ä»·å®ç°æ•°æ®å¿«é€Ÿè¾“å…¥ã€‚éšç€å¿«é€Ÿè¾“å…¥å’Œåˆ†æåœºæ™¯è¶Šæ¥è¶Šå¤šï¼Œä¼ ç»Ÿå­˜å‚¨å±‚çš„å±€é™æ€§è¶Šæ¥è¶Šæ˜æ˜¾ï¼ŒKuduåº”è¿è€Œç”Ÿï¼Œå®ƒçš„å®šä½ä»‹äºHDFSå’ŒHBaseä¹‹é—´ï¼Œå°†ä½å»¶è¿Ÿéšæœºè®¿é—®ï¼Œé€è¡Œæ’å…¥ã€æ›´æ–°å’Œå¿«é€Ÿåˆ†ææ‰«æèåˆåˆ°ä¸€ä¸ªå­˜å‚¨å±‚ä¸­ï¼Œæ˜¯ä¸€ä¸ªæ—¢æ”¯æŒéšæœºè¯»å†™åˆæ”¯æŒOLAPåˆ†æçš„å­˜å‚¨å¼•æ“ã€‚æœ¬ç¯‡æ–‡ç« ç ”ç©¶ä¸€ä¸‹Kuduï¼Œå¯¹å…¶åº”ç”¨åœºæ™¯ï¼Œæ¶æ„åŸç†åŠåŸºæœ¬ä½¿ç”¨åšä¸€ä¸ªæ€»ç»“ã€‚ Kuduä»‹ç» åœ¨Kuduå‡ºç°å‰ï¼Œæ— æ³•å¯¹å®æ—¶å˜åŒ–çš„æ•°æ®åšå¿«é€Ÿåˆ†æï¼š ä»¥ä¸Šè®¾è®¡æ–¹æ¡ˆçš„ç¼ºé™·ï¼š 1.æ•°æ®å­˜å‚¨å¤šä»½é€ æˆå†—ä½™ï¼Œå­˜å‚¨èµ„æºæµªè´¹ã€‚ 2.æ¶æ„å¤æ‚ï¼Œè¿ç»´æˆæœ¬é«˜ï¼Œæ’æŸ¥é—®é¢˜å›°éš¾ã€‚ è€ŒKuduå°±èåˆäº†åŠ¨æ€æ•°æ®ä¸é™æ€æ•°æ®çš„å¤„ç†ï¼ŒåŒæ—¶æ”¯æŒéšæœºè¯»å†™å’ŒOLAPåˆ†æã€‚ Kuduä¸HDFS,HBaseçš„å¯¹æ¯”ï¼š é€‚ç”¨åœºæ™¯ æ—¢æœ‰éšæœºè¯»å†™éšæœºè®¿é—®ï¼Œåˆæœ‰æ‰¹é‡æ‰«æåˆ†æçš„åœºæ™¯(OLAP) HTAPï¼ˆHybrid Transactional Analytical Processingï¼‰æ··åˆäº‹åŠ¡åˆ†æå¤„ç†åœºæ™¯ è¦æ±‚åˆ†æç»“æœå®æ—¶æ€§é«˜ï¼ˆå¦‚å®æ—¶å†³ç­–ï¼Œå®æ—¶æ›´æ–°ï¼‰çš„åœºæ™¯ å®æ—¶æ•°ä»“ æ”¯æŒæ•°æ®é€è¡Œæ’å…¥ã€æ›´æ–°æ“ä½œ åŒæ—¶é«˜æ•ˆè¿è¡Œé¡ºåºè¯»å†™å’Œéšæœºè¯»å†™ä»»åŠ¡çš„åœºæ™¯ Kuduä½œä¸ºæŒä¹…å±‚ä¸Impalaç´§å¯†é›†æˆçš„åœºæ™¯ è§£å†³HBase(Phoenix)å¤§æ‰¹é‡æ•°æ®SQLåˆ†ææ€§èƒ½ä¸ä½³çš„åœºæ™¯ è·¨å¤§é‡å†å²æ•°æ®çš„æŸ¥è¯¢åˆ†æåœºæ™¯ï¼ˆTime-seriesåœºæ™¯ï¼‰ ç‰¹ç‚¹åŠç¼ºç‚¹ ç‰¹ç‚¹ åŸºäºåˆ—å¼å­˜å‚¨ å¿«é€Ÿé¡ºåºè¯»å†™ ä½¿ç”¨ LSMæ ‘ ä»¥æ”¯æŒé«˜æ•ˆéšæœºè¯»å†™ æŸ¥è¯¢æ€§èƒ½å’Œè€—æ—¶è¾ƒç¨³å®š ä¸ä¾èµ–Zookeeper æœ‰è¡¨ç»“æ„ï¼Œéœ€è¦å®šä¹‰Schemaï¼Œéœ€è¦å®šä¹‰å”¯ä¸€é”®ï¼Œæ”¯æŒSQLåˆ†æï¼ˆä¾èµ–Impalaï¼ŒSparkç­‰å¼•æ“ï¼‰ æ”¯æŒå¢åˆ åˆ—,å•è¡Œçº§ACIDï¼ˆä¸æ”¯æŒå¤šè¡Œäº‹åŠ¡-ä¸æ»¡è¶³åŸå­æ€§ï¼‰ æŸ¥è¯¢æ—¶å…ˆæŸ¥è¯¢å†…å­˜å†æŸ¥è¯¢ç£ç›˜ æ•°æ®å­˜å‚¨åœ¨Linuxæ–‡ä»¶ç³»ç»Ÿï¼Œä¸ä¾èµ–HDFSå­˜å‚¨ ç¼ºç‚¹ æš‚ä¸æ”¯æŒé™¤PKå¤–çš„äºŒçº§ç´¢å¼•å’Œå”¯ä¸€æ€§é™åˆ¶ ä¸æ”¯æŒå¤šè¡Œäº‹åŠ¡ ä¸æ”¯æŒBloomFilterä¼˜åŒ–join ä¸æ”¯æŒæ•°æ®å›æ»š ä¸èƒ½ä¿®æ”¹PKï¼Œä¸æ”¯æŒAUTO INCREMENT PK æ¯è¡¨æœ€å¤šä¸èƒ½æœ‰300åˆ—ï¼Œæ¯ä¸ªTServeræ•°æ®å‹ç¼©åä¸è¶…8TB æ•°æ®ç±»å‹å°‘ï¼Œä¸æ”¯æŒMapï¼ŒARRAYï¼ŒStructç­‰å¤æ‚ç±»å‹ ä¸ç›¸ä¼¼ç±»å‹å­˜å‚¨å¼•æ“å¯¹æ¯”&emsp;&emsp;æœ¬æ–‡é‡ç‚¹è¯´Kuduï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦äº†è§£å…¶ä»–ç±»ä¼¼ç»„ä»¶ï¼Œäº†è§£å®ƒä»¬å„è‡ªæ“…é•¿çš„åœ°æ–¹ï¼Œæ‰èƒ½æ›´å¥½åœ°åšæŠ€æœ¯é€‰å‹ã€‚è¿™é‡Œç®€å•å¯¹æ¯”ä¸€ä¸‹Kuduï¼ŒHudiå’ŒDeltaLakeè¿™ä¸‰ç§å­˜å‚¨æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬éƒ½å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ï¼Œèƒ½è§£å†³ç±»ä¼¼çš„é—®é¢˜ã€‚ ç‰¹æ€§ Kudu Hudi Delta Lake è¡Œçº§åˆ«æ›´æ–° æ”¯æŒ æ”¯æŒ æ”¯æŒ schemaä¿®æ”¹ æ”¯æŒ æ”¯æŒ æ”¯æŒ æ‰¹æµå…±äº« æ”¯æŒ æ”¯æŒ æ”¯æŒ å¯ç”¨ç´¢å¼• æ˜¯ æ˜¯ å¦ å¤šå¹¶å‘å†™ æ”¯æŒ ä¸æ”¯æŒ æ”¯æŒ ç‰ˆæœ¬å›æ»š ä¸æ”¯æŒ æ”¯æŒ æ”¯æŒ å®æ—¶æ€§ é«˜ è¿‘å®æ—¶ å·® ä½¿ç”¨HDFS ä¸æ”¯æŒ æ”¯æŒ æ”¯æŒ ç©ºå€¼å¤„ç† é»˜è®¤null error é»˜è®¤null å¹¶å‘è¯»å†™ æ”¯æŒ ä¸æ”¯æŒå¹¶å‘å†™ æ”¯æŒ äº‘å­˜å‚¨ ä¸æ”¯æŒ æ”¯æŒ æ”¯æŒ å…¼å®¹æ€§ Sparkï¼ŒImpalaï¼ŒPresto Sparkï¼ŒPrestoï¼ŒHiveï¼ŒMR è¾ƒå¥½ ä¾èµ–Sparkï¼Œæœ‰é™æ”¯æŒHiveï¼ŒPresto é€‰æ‹©å»ºè®®ï¼šè€ƒè™‘å®æ—¶æ•°ä»“æ–¹æ¡ˆä»¥åŠSQLæ”¯æŒæ–¹é¢å¯é€‰Kuduï¼Œæ•°æ®æ¹–æ–¹æ¡ˆåŠå¯å›æ»šå¯é€‰DeltaLakeå’ŒHudiï¼Œè€ƒè™‘å…¼å®¹æ€§é«˜ä¸”åº”å¯¹è¯»å¤šå†™å°‘è¯»å°‘å†™å¤šéƒ½æœ‰å¾ˆå¥½çš„æ–¹æ¡ˆé€‰Hudiï¼Œè€ƒè™‘å¹¶å‘å†™èƒ½åŠ›è¯»å¤šå†™å°‘ä¸”ä¸Sparkç´§å¯†ç»“åˆé€‰DeltaLakeã€‚ Kuduæ¶æ„åŸç†Raftç®—æ³•ä»‹ç»&emsp;&emsp;ä¸ºäº†æ›´å¥½åœ°ç†è§£Kuduï¼Œéœ€è¦ç®€å•äº†è§£ä¸€ä¸‹Raftç®—æ³•ã€‚Raftæ˜¯ä¸€ä¸ªä¸€è‡´æ€§ç®—æ³•ï¼Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ä¸€è‡´æ€§ç®—æ³•å°±æ˜¯è®©å¤šä¸ªèŠ‚ç‚¹åœ¨ç½‘ç»œä¸ç¨³å®šç”šè‡³éƒ¨åˆ†èŠ‚ç‚¹å®•æœºçš„æƒ…å†µä¸‹èƒ½å¯¹æŸä¸ªäº‹ä»¶è¾¾æˆä¸€è‡´ã€‚è€ŒRaftæ˜¯ä¸€ä¸ªç”¨äºç®¡ç†æ—¥å¿—ä¸€è‡´æ€§çš„åè®®ï¼Œå®ƒå°†åˆ†å¸ƒå¼ä¸€è‡´æ€§åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼šLeaderElectionï¼ŒLogReplicationï¼ŒSafetyï¼ŒLogCompactionç­‰ã€‚ &emsp;&emsp;Raftå°†ç³»ç»Ÿä¸­çš„è§’è‰²åˆ†ä¸ºLeaderï¼ŒFollowerå’ŒCandidateã€‚æ­£å¸¸è¿è¡Œæ—¶åªæœ‰Leaderå’ŒFollowerï¼Œé€‰ä¸¾æ—¶æ‰ä¼šæœ‰Candidateã€‚&emsp;&emsp;Leader:æ¥å—å®¢æˆ·ç«¯è¯·æ±‚ï¼Œå¹¶å‘FolloweråŒæ­¥è¯·æ±‚æ—¥å¿—ï¼Œå½“æ—¥å¿—åŒæ­¥åˆ°å¤§å¤šæ•°èŠ‚ç‚¹ä¸Šåå‘Šè¯‰Followeræäº¤æ—¥å¿—ã€‚&emsp;&emsp;Follower:æ¥å—å¹¶æŒä¹…åŒ–LeaderåŒæ­¥çš„æ—¥å¿—ï¼Œåœ¨Leaderå‘Šä¹‹æ—¥å¿—å¯ä»¥æäº¤ä¹‹åï¼Œæäº¤æ—¥å¿—ã€‚å“åº”Candidateçš„é‚€è¯·æŠ•ç¥¨è¯·æ±‚ã€‚æŠŠå®¢æˆ·ç«¯è¯·æ±‚é‡å®šå‘åˆ°Leaderã€‚&emsp;&emsp;Candidate:Leaderé€‰ä¸¾è¿‡ç¨‹ä¸­çš„ä¸´æ—¶è§’è‰²ï¼Œç”±äºFollowerè½¬å˜è€Œæ¥ã€‚ Leaderé€‰ä¸¾å‘ç”Ÿåœ¨Followeræ¥æ”¶ä¸åˆ°Leaderçš„HeartBeatå¯¼è‡´ElectionTimeoutè¶…æ—¶çš„æƒ…å†µä¸‹ã€‚â‘ æ¯ä¸ªFolloweréƒ½æœ‰ä¸€ä¸ªæ—¶é’ŸElectionTimeoutï¼Œæ˜¯ä¸ªéšæœºå€¼ï¼Œè¡¨ç¤ºFollowerç­‰å¾…æˆä¸ºLeaderçš„æ—¶é—´ï¼Œè°çš„æ—¶é’Ÿå…ˆè·‘å®Œåˆ™å…ˆå‘èµ·Leaderé€‰ä¸¾ã€‚ï¼ˆæ”¶åˆ°Leaderå¿ƒè·³æ—¶ä¼šæ¸…é›¶ElectionTimeoutï¼‰â‘¡Followerå°†å…¶ä»»æœŸ(Term)åŠ 1ç„¶åè½¬ä¸ºCandidateçŠ¶æ€ï¼Œå¹¶ä¸”ç»™è‡ªå·±æŠ•ç¥¨ï¼Œç„¶åæºTerm_idå’Œæ—¥å¿—indexç»™å…¶ä»–èŠ‚ç‚¹å‘èµ·é€‰ä¸¾ï¼ˆRequestVote RPCï¼‰ã€‚æœ‰ä¸‰ç§æƒ…å†µï¼šâ‘ èµ¢å¾—åŠæ•°ä»¥ä¸Šé€‰ç¥¨ï¼Œæˆä¸ºLeaderâ‘¡æ”¶åˆ°Leaderæ¶ˆæ¯ï¼ŒLeaderè¢«æŠ¢äº†ï¼Œæˆä¸ºFollowerâ‘¢é€‰ä¸¾è¶…æ—¶æ—¶ï¼Œæ²¡æœ‰èŠ‚ç‚¹èµ¢å¾—å¤šæ•°é€‰ç¥¨ï¼Œé€‰ä¸¾å¤±è´¥ï¼ŒTerm_idè‡ªå¢1ï¼Œè¿›è¡Œä¸‹ä¸€è½®é€‰ä¸¾ â‘¢Raftåè®®æ‰€æœ‰æ—¥å¿—éƒ½åªèƒ½ä»Leaderå†™å…¥Followerï¼ŒLeaderèŠ‚ç‚¹æ—¥å¿—åªä¼šå¢åŠ ï¼ˆindex+1ï¼‰ï¼Œä¸ä¼šåˆ é™¤å’Œè¦†ç›–ã€‚æ‰€ä»¥Leaderå¿…é¡»åŒ…å«å…¨éƒ¨æ—¥å¿—ï¼Œèƒ½è¢«é€‰ä¸¾ä¸ºLeaderçš„èŠ‚ç‚¹ä¸€å®šåŒ…å«äº†æ‰€æœ‰å·²ç»æäº¤çš„æ—¥å¿—ã€‚æ¯ä¸ªèŠ‚ç‚¹æœ€å¤šåªèƒ½ç»™ä¸€ä¸ªå€™é€‰äººæŠ•ç¥¨ï¼Œå…ˆåˆ°å…ˆæœåŠ¡çš„åŸåˆ™ã€‚é€‰ä¸¾èƒœå‡ºè§„åˆ™ï¼šèŠ‚ç‚¹Term_idè¶Šå¤§è¶Šæ–°åˆ™å¯èƒ½èƒœå‡ºï¼Œä½†å¯èƒ½æœ‰Term_idç›¸åŒçš„æƒ…å†µï¼ŒTerm_idç›¸åŒï¼Œæ¯”è¾ƒæ—¥å¿—Indexè¶Šå¤§è¶Šæ–°åˆ™èƒœå‡ºã€‚è¿™ä¸€ç‚¹å¾ˆåƒZookeeperé€‰ä¸¾çš„è§„åˆ™ã€‚è¯¦ç»†è¿‡ç¨‹ï¼šé¦–å…ˆä¼šæœ‰ä¸€ä¸ªCandidateé€‰è‡ªå·±ç„¶åå‘èµ·æŠ•ç¥¨-&gt;Followeræ”¶åˆ°é‚€ç¥¨ï¼Œå¦‚æœè¿™ä¸ªFollowerè¿˜æ²¡ç»™å…¶ä»–èŠ‚ç‚¹æŠ•ç¥¨(ä¸€ä¸ªèŠ‚ç‚¹åªèƒ½ä¸€ç¥¨),ä¸”å¯¹æ¯”Term_idæ¯”è‡ªå·±å¤§ï¼Œå°±æŠŠç¥¨æŠ•ç»™è¿™ä¸ªCandidateï¼Œå¦‚æœTerm_idæ¯”è‡ªå·±å°ä¸”è‡ªå·±è¿˜æ²¡æŠ•ç¥¨ï¼Œå°±æ‹’ç»è¯·æ±‚ï¼Œç»™è‡ªå·±æŠ•ç¥¨ã€‚ æ¦‚æ‹¬ï¼šå¢åŠ ä»»æœŸç¼–å·-&gt;ç»™è‡ªå·±æŠ•ç¥¨-&gt;é‡ç½®ElectionTimeout-&gt;å‘é€æŠ•ç¥¨RPCç»™å…¶ä»–èŠ‚ç‚¹ æ—¥å¿—å¤åˆ¶è¿‡ç¨‹ï¼šLeaderæ¥æ”¶æ¥è‡ªå®¢æˆ·ç«¯çš„è¯·æ±‚å¹¶å°†å…¶ä»¥æ—¥å¿—çš„å½¢å¼å¤åˆ¶åˆ°é›†ç¾¤ä¸­çš„å…¶å®ƒèŠ‚ç‚¹ï¼Œå¹¶ä¸”å¼ºåˆ¶è¦æ±‚å…¶å®ƒèŠ‚ç‚¹çš„æ—¥å¿—å’Œè‡ªå·±ä¿æŒä¸€è‡´ã€‚ RaftåŒæ­¥æ—¥å¿—ç”±ç¼–å·indexã€term_idå’Œå‘½ä»¤ç»„æˆã€‚=&gt;æœ‰åŠ©äºé€‰ä¸¾å’Œæ ¹æ®termæŒä¹…åŒ–æ—¥å¿—æ°¸è¿œåªæœ‰ä¸€ä¸ªæµå‘Leader-&gt;Follower 1.æ—¥å¿—å¤åˆ¶çš„ä¿è¯ï¼š 1.å¦‚æœä¸åŒæ—¥å¿—ä¸­çš„ä¸¤ä¸ªæ¡ç›®æœ‰ç€ç›¸åŒçš„ç´¢å¼•å’Œä»»æœŸå·ï¼Œåˆ™å®ƒä»¬æ‰€å­˜å‚¨çš„å‘½ä»¤æ˜¯ç›¸åŒçš„ï¼ˆåŸå› ï¼šleader æœ€å¤šåœ¨ä¸€ä¸ªä»»æœŸé‡Œçš„ä¸€ä¸ªæ—¥å¿—ç´¢å¼•ä½ç½®åˆ›å»ºä¸€æ¡æ—¥å¿—æ¡ç›®ï¼Œæ—¥å¿—æ¡ç›®åœ¨æ—¥å¿—çš„ä½ç½®ä»æ¥ä¸ä¼šæ”¹å˜ï¼‰ã€‚ 2.å¦‚æœä¸åŒæ—¥å¿—ä¸­çš„ä¸¤ä¸ªæ¡ç›®æœ‰ç€ç›¸åŒçš„ç´¢å¼•å’Œä»»æœŸå·ï¼Œåˆ™å®ƒä»¬ä¹‹å‰çš„æ‰€æœ‰æ¡ç›®éƒ½æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼ˆåŸå› ï¼šæ¯æ¬¡ RPC å‘é€é™„åŠ æ—¥å¿—æ—¶ï¼Œleader ä¼šæŠŠè¿™æ¡æ—¥å¿—æ¡ç›®çš„å‰é¢çš„æ—¥å¿—çš„ä¸‹æ ‡å’Œä»»æœŸå·ä¸€èµ·å‘é€ç»™ followerï¼Œå¦‚æœ follower å‘ç°å’Œè‡ªå·±çš„æ—¥å¿—ä¸åŒ¹é…ï¼Œé‚£ä¹ˆå°±æ‹’ç»æ¥å—è¿™æ¡æ—¥å¿—ï¼Œè¿™ä¸ªç§°ä¹‹ä¸ºä¸€è‡´æ€§æ£€æŸ¥ï¼‰ã€‚ 2.ç½‘ç»œæ•…éšœæˆ–Leaderå´©æºƒæ—¶ä¿è¯ä¸€è‡´æ€§ï¼š ç½‘ç»œå´©æºƒè®²é›†ç¾¤åˆ†ä¸ºä¸¤æ‹¨ï¼Œæ²¡æœ‰Leaderå­˜åœ¨çš„å¦ä¸€æ³¢ä¼šé‡æ–°é€‰ä¸»ï¼Œè¿™æ—¶ç½‘ç»œæ¢å¤ï¼Œä¼šå‡ºç°ä¸¤ä¸ªLeadrçš„æƒ…å†µï¼Œè¿™æ˜¯ä¼šäº§ç”Ÿå†²çªçš„ï¼Œè¿™æ—¶ä¼šæ ¹æ®ä»»æœŸTerm_idå°†ä»»æœŸä½çš„Leaderè‡ªåŠ¨é™çº§ä¸ºFollowerï¼ŒLeaderå’ŒFolloweræ—¥å¿—æœ‰å†²çªçš„æ—¶å€™ï¼ŒLeaderå°†æ ¡éªŒFolloweræœ€åä¸€æ¡æ—¥å¿—æ˜¯å¦å’ŒLeaderåŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…ï¼Œå°†é€’å‡æŸ¥è¯¢ï¼Œç›´åˆ°åŒ¹é…ï¼ŒåŒ¹é…åï¼Œåˆ é™¤å†²çªçš„æ—¥å¿—ã€‚è¿™æ ·å°±å®ç°äº†ä¸»ä»æ—¥å¿—çš„ä¸€è‡´æ€§ã€‚ é€’å‡æŸ¥è¯¢ï¼Œç›´åˆ°åŒ¹é…ï¼Œå¼ºåˆ¶è¦†ç›– =&gt;Leaderä¼šå¼ºåˆ¶Followerå¤åˆ¶å®ƒçš„æ—¥å¿—ï¼ŒLeaderä¼šä»æœ€åçš„LogIndexä»åå¾€å‰è¯•ï¼Œç›´åˆ°æ‰¾åˆ°æ—¥å¿—ä¸€è‡´çš„indexï¼Œç„¶åå¼€å§‹å¤åˆ¶ï¼Œè¦†ç›–è¯¥indexä¹‹åçš„æ—¥å¿—æ¡ç›®ã€‚ åœºæ™¯ï¼šå‘ç”Ÿäº†ç½‘ç»œåˆ†åŒºæˆ–è€…ç½‘ç»œé€šä¿¡æ•…éšœï¼Œä½¿å¾—Leaderä¸èƒ½è®¿é—®å¤§å¤šæ•°Follweräº†ï¼Œé‚£ä¹ˆLeaderåªèƒ½æ­£å¸¸æ›´æ–°å®ƒèƒ½è®¿é—®çš„é‚£äº›Followerï¼Œè€Œå¤§å¤šæ•°çš„Followerå› ä¸ºæ²¡æœ‰äº†Leaderï¼Œä»–ä»¬é‡æ–°é€‰å‡ºä¸€ä¸ªLeaderï¼Œç„¶åè¿™ä¸ª Leaderæ¥æ¥å—å®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œå¦‚æœå®¢æˆ·ç«¯è¦æ±‚å…¶æ·»åŠ æ–°çš„æ—¥å¿—ï¼Œè¿™ä¸ªæ–°çš„Leaderä¼šé€šçŸ¥å¤§å¤šæ•°Followerã€‚å¦‚æœè¿™æ—¶ç½‘ç»œæ•…éšœä¿®å¤ äº†ï¼Œé‚£ä¹ˆåŸå…ˆçš„Leaderå°±å˜æˆFollowerï¼Œåœ¨å¤±è”é˜¶æ®µè¿™ä¸ªè€Leaderçš„ä»»ä½•æ›´æ–°éƒ½ä¸èƒ½ç®—commitï¼Œéƒ½å›æ»šï¼Œæ¥å—æ–°çš„Leaderçš„æ–°çš„æ›´æ–°ï¼ˆé€’å‡æŸ¥è¯¢åŒ¹é…æ—¥å¿—ï¼‰ã€‚ æ—¥å¿—å‹ç¼©æ—¥å¿—ä¸èƒ½æ— é™å¢é•¿ï¼Œå¦åˆ™ä¼šå¯¼è‡´é‡æ’­æ—¥å¿—æ—¶è€—æ—¶å¾ˆé•¿ã€‚æ‰€ä»¥å¯¹æ—¥å¿—è¿›è¡Œå‹ç¼©ï¼Œå®šé‡Snapshotã€‚ &emsp;&emsp;Raftå‚è€ƒï¼šRaftç®—æ³•è¯¦è§£&emsp;&emsp;Raftç®—æ³•åœ¨Kuduä¸­çš„åº”ç”¨ï¼šå¤šä¸ªTMasterä¹‹é—´é€šè¿‡Raftåè®®å®ç°æ•°æ®åŒæ­¥å’Œé«˜å¯ç”¨â€“Raftè´Ÿè´£åœ¨å¤šä¸ªTabletå‰¯æœ¬ä¸­é€‰å‡ºLeaderå’ŒFollowerï¼ŒLeader Tabletè´Ÿè´£å‘é€å†™å…¥æ•°æ®ç»™Follower Tabletï¼Œå¤§å¤šæ•°å‰¯æœ¬éƒ½å®Œæˆäº†å†™æ“ä½œåˆ™ä¼šå‘å®¢æˆ·ç«¯ç¡®è®¤ã€‚ Kuduçš„ä¸€è‡´æ€§æ¨¡å‹ç›¸å…³èµ„æ–™ä¸å¤šï¼Œå¯ä»¥ä¸€èµ·è®¨è®º Kuduä¸ºç”¨æˆ·æä¾›äº†ä¸¤ç§ä¸€è‡´æ€§æ¨¡å‹(snapshot consistencyå’Œexternal consistency)ã€‚é»˜è®¤çš„ä¸€è‡´æ€§æ¨¡å‹æ˜¯snapshot consistencyã€‚è¿™ç§ä¸€è‡´æ€§æ¨¡å‹ä¿è¯ç”¨æˆ·æ¯æ¬¡è¯»å–å‡ºæ¥çš„éƒ½æ˜¯ä¸€ä¸ªå¯ç”¨çš„å¿«ç…§ï¼Œä½†è¿™ç§ä¸€è‡´æ€§æ¨¡å‹åªèƒ½ä¿è¯å•ä¸ªclientå¯ä»¥çœ‹åˆ°æœ€æ–°çš„æ•°æ®ï¼Œä½†ä¸èƒ½ä¿è¯å¤šä¸ªclientæ¯æ¬¡å–å‡ºçš„éƒ½æ˜¯æœ€æ–°çš„æ•°æ®ã€‚å¦ä¸€ç§ä¸€è‡´æ€§æ¨¡å‹external consistency(åå¼€å§‹çš„äº‹åŠ¡ä¸€å®šå¯ä»¥çœ‹åˆ°å…ˆæäº¤çš„äº‹åŠ¡çš„ä¿®æ”¹ã€‚æ‰€æœ‰äº‹åŠ¡çš„è¯»å†™éƒ½åŠ é”å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç¼ºç‚¹æ˜¯æ€§èƒ½è¾ƒå·®ã€‚)å¯ä»¥åœ¨å¤šä¸ªclientä¹‹é—´ä¿è¯æ¯æ¬¡å–åˆ°çš„éƒ½æ˜¯æœ€æ–°æ•°æ®ï¼Œä½†æ˜¯Kuduæ²¡æœ‰æä¾›é»˜è®¤çš„å®ç°ï¼Œéœ€è¦ç”¨æˆ·åšä¸€äº›é¢å¤–å·¥ä½œã€‚ ä¸ºäº†å®ç°external consistencyï¼ŒKuduæä¾›äº†ä¸¤ç§æ–¹å¼ï¼š 1.åœ¨clientä¹‹é—´ä¼ æ’­timestamp tokenã€‚åœ¨ä¸€ä¸ªclientå®Œæˆä¸€æ¬¡å†™å…¥åï¼Œä¼šå¾—åˆ°ä¸€ä¸ªtimestamp tokenï¼Œç„¶åè¿™ä¸ªclientæŠŠè¿™ä¸ªtokenä¼ æ’­åˆ°å…¶ä»–clientï¼Œè¿™æ ·å…¶ä»–clientå°±å¯ä»¥é€šè¿‡tokenå–åˆ°æœ€æ–°æ•°æ®äº†ã€‚ä¸è¿‡è¿™ä¸ªæ–¹å¼çš„å¤æ‚åº¦å¾ˆé«˜ï¼ŒåŸºäºHybridTimeæ–¹æ¡ˆï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆKudué«˜åº¦ä¾èµ–NTPã€‚ 2.é€šè¿‡commit-waitæ–¹å¼ï¼Œè¿™æœ‰äº›ç±»ä¼¼äºGoogleçš„Spannerã€‚ä½†æ˜¯ç›®å‰åŸºäºNTPçš„commit-waitæ–¹å¼å»¶è¿Ÿå®åœ¨æœ‰ç‚¹é«˜ã€‚ä¸è¿‡Kuduç›¸ä¿¡ï¼Œéšç€[åˆ†å¸ƒå¼äº‹åŠ¡å®ç°-Spanner](https://blog.csdn.net/weixin_30650039/article/details/94998723)çš„å‡ºç°ï¼Œæœªæ¥å‡ å¹´å†…åŸºäºreal-time clockçš„æŠ€æœ¯å°†ä¼šé€æ¸æˆç†Ÿã€‚ LSMæ ‘LSMæ ‘(Log-Structured Merge Tree)&emsp;&emsp;Kuduä¸HBaseåœ¨å†™çš„è¿‡ç¨‹ä¸­éƒ½é‡‡ç”¨äº†LSMæ ‘çš„ç»“æ„ï¼ŒLSMæ ‘çš„ä¸»è¦æ€æƒ³å°±æ˜¯éšæœºå†™è½¬æ¢ä¸ºé¡ºåºå†™æ¥æé«˜å†™æ€§èƒ½ï¼Œéšæœºè¯»å†™éœ€è¦ç£ç›˜çš„æœºæ¢°è‡‚ä¸æ–­å¯»é“ï¼Œå»¶è¿Ÿè¾ƒé«˜ï¼Œè€Œè½¬æ¢ä¸ºé¡ºåºå†™åæœºæ¢°è‡‚ä¸ä¼šé¢‘ç¹å¯»å€ï¼Œæ€§èƒ½è¾ƒå¥½ã€‚&emsp;&emsp;LSMæ ‘åŸç†æ˜¯æŠŠä¸€æ£µå¤§çš„æ ‘æ‹†åˆ†æˆNæ£µå°æ ‘ï¼Œå°æ ‘å­˜åœ¨äºå†…å­˜ä¸­ï¼Œéšç€æ›´æ–°å’Œå†™å…¥æ“ä½œï¼Œå°æ ‘å­˜æ”¾æ•°æ®è¾¾åˆ°ä¸€å®šå¤§å°åä¼šå†™å…¥ç£ç›˜ï¼Œå°æ ‘åˆ°äº†ç£ç›˜ä¸­ï¼Œå®šæœŸä¸ç£ç›˜ä¸­çš„å¤§æ ‘åšåˆå¹¶ã€‚&emsp;&emsp;å¤§å®¶éƒ½çŸ¥é“HBaseçš„MemStoreï¼ŒKuduåœ¨å†™å…¥æ–¹é¢çš„è®¾è®¡ä¸ä¹‹ç±»ä¼¼ï¼ŒKuduå…ˆå°†å¯¹æ•°æ®çš„ä¿®æ”¹ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œè¾¾åˆ°ä¸€å®šå¤§å°åå°†è¿™äº›ä¿®æ”¹æ“ä½œæ‰¹é‡å†™å…¥ç£ç›˜ã€‚ä½†è¯»å–çš„æ—¶å€™ç¨å¾®éº»çƒ¦äº›ï¼Œéœ€è¦è¯»å–å†å²æ•°æ®å’Œå†…å­˜ä¸­æœ€è¿‘ä¿®æ”¹æ“ä½œã€‚æ‰€ä»¥å†™å…¥æ€§èƒ½å¤§å¤§æå‡ï¼Œè€Œè¯»å–æ—¶è¦å…ˆå»å†…å­˜è¯»å–ï¼Œå¦‚æœæ²¡å‘½ä¸­ï¼Œåˆ™ä¼šå»ç£ç›˜è¯»å¤šä¸ªæ–‡ä»¶ã€‚ å‹ç¼©å’Œç¼–ç &emsp;&emsp;æˆ‘ä»¬éƒ½çŸ¥é“åˆ—å¼å­˜å‚¨çš„å‹ç¼©æ•ˆæœå¾ˆå¥½ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆåˆ—å¼å­˜å‚¨æ¯”è¡Œå­˜å‚¨å‹ç¼©æ•ˆæœå¥½å‘¢ï¼Ÿ&emsp;&emsp;æ¯”å¦‚ä¸€ä¸ªåˆ—å­˜çš„å›½å®¶åï¼Œé‚£åªèƒ½åŒ…å«â€œç¾å›½â€ï¼Œâ€œæ—¥æœ¬â€ï¼Œâ€œéŸ©å›½â€ï¼Œâ€œåŠ æ‹¿å¤§â€ç­‰å€¼ï¼Œè€Œè¿™äº›å€¼ä¼šå­˜å‚¨åœ¨ä¸€èµ·ï¼Œè€Œä¸æ˜¯åˆ†æ•£åˆ°åŒ…å«å¾ˆå¤šä¸ç›¸å…³çš„å…¶ä»–åˆ—å€¼ä¹‹é—´ã€‚è¿™æ ·åˆ—å¼å­˜å‚¨ä¹Ÿå°±ä¸éœ€è¦å°†æ¯ä¸ªå€¼éƒ½å®Œå®Œæ•´æ•´ä¿å­˜èµ·æ¥ï¼Œæ‰€ä»¥å‹ç¼©æ•ˆæœæ˜¾è‘—ã€‚&emsp;&emsp;ç¼–ç å¯¹äºåˆ—å¼å­˜å‚¨çš„ä¼˜åŒ–æ›´åŠ æ˜æ˜¾ï¼Œç¼–ç å’Œå‹ç¼©ä½œç”¨ç›¸åŒï¼Œæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼Œç¼–ç ä¼šå°†æ•°æ®çš„å€¼è½¬æ¢ä¸ºä¸€ç§æ›´å°çš„è¡¨ç°å½¢å¼ï¼Œæ¯”å¦‚ï¼Œâ€œç¾å›½â€ç¼–ç ä¸º1ï¼Œâ€œæ—¥æœ¬â€ç¼–ç ä¸º2ï¼Œâ€œéŸ©å›½â€ç¼–ç ä¸º3ï¼Œâ€œåŠ æ‹¿å¤§â€ç¼–ç ä¸º4â€¦åˆ™Kuduåªå­˜å‚¨1ï¼Œ2ï¼Œ3ï¼Œ4â€¦è€Œä¸å­˜å‚¨é•¿å­—ç¬¦ä¸²ï¼Œå ç”¨ç©ºé—´å¤§å¤§å‡å°‘ã€‚ Kuduä¸€äº›æ¦‚å¿µ Tableï¼šå…·æœ‰Schemaå’Œå…¨å±€æœ‰åºä¸»é”®çš„è¡¨ã€‚ä¸€å¼ è¡¨æœ‰å¤šä¸ªTabletï¼Œå¤šä¸ªTabletåŒ…å«è¡¨çš„å…¨éƒ¨æ•°æ®ã€‚Tabletï¼šKuduçš„è¡¨Tableè¢«æ°´å¹³åˆ†å‰²ä¸ºå¤šæ®µï¼ŒTabletæ˜¯Kuduè¡¨çš„ä¸€ä¸ªç‰‡æ®µï¼ˆåˆ†åŒºï¼‰ï¼Œæ¯ä¸ªTabletå­˜å‚¨ä¸€æ®µè¿ç»­èŒƒå›´çš„æ•°æ®ï¼ˆä¼šè®°å½•å¼€å§‹Keyå’Œç»“æŸKeyï¼‰ï¼Œä¸”ä¸¤ä¸ªTableté—´ä¸ä¼šæœ‰é‡å¤èŒƒå›´çš„æ•°æ®ã€‚ä¸€ä¸ªTabletä¼šå¤åˆ¶ï¼ˆé€»è¾‘å¤åˆ¶è€Œéç‰©ç†å¤åˆ¶ï¼Œå‰¯æœ¬ä¸­çš„å†…å®¹ä¸æ˜¯å®é™…çš„æ•°æ®ï¼Œè€Œæ˜¯æ“ä½œè¯¥å‰¯æœ¬ä¸Šçš„æ•°æ®æ—¶å¯¹åº”çš„æ›´æ”¹ä¿¡æ¯ï¼‰å¤šä¸ªå‰¯æœ¬åœ¨å¤šå°TServerä¸Šï¼Œå…¶ä¸­ä¸€ä¸ªå‰¯æœ¬ä¸ºLeader Tabletï¼Œå…¶ä»–åˆ™ä¸ºFollower Tabletã€‚åªæœ‰Leader Tabletå“åº”å†™è¯·æ±‚ï¼Œä»»ä½•Tabletå‰¯æœ¬å¯ä»¥å“åº”è¯»è¯·æ±‚ã€‚TabletServerï¼šç®€ç§°TServerï¼Œè´Ÿè´£æ•°æ®å­˜å‚¨Tabletã€æä¾›æ•°æ®è¯»å†™æœåŠ¡ã€ç¼–ç ã€å‹ç¼©ã€åˆå¹¶å’Œå¤åˆ¶ã€‚ä¸€ä¸ªTServerå¯ä»¥æ˜¯æŸäº›Tabletçš„Leaderï¼Œä¹Ÿå¯ä»¥æ˜¯æŸäº›Tabletçš„Followerï¼Œä¸€ä¸ªTabletå¯ä»¥è¢«å¤šä¸ªTServeræœåŠ¡ï¼ˆå¤šå¯¹å¤šå…³ç³»ï¼‰ã€‚TServerä¼šå®šæœŸï¼ˆé»˜è®¤1sï¼‰å‘Masterå‘é€å¿ƒè·³ã€‚Catalog Tableï¼šç›®å½•è¡¨ï¼Œç”¨æˆ·ä¸å¯ç›´æ¥è¯»å–æˆ–å†™å…¥ï¼Œä»…ç”±Masterç»´æŠ¤ï¼Œå­˜å‚¨ä¸¤ç±»å…ƒæ•°æ®ï¼šè¡¨å…ƒæ•°æ®ï¼ˆSchemaä¿¡æ¯ï¼Œä½ç½®å’ŒçŠ¶æ€ï¼‰å’ŒTabletå…ƒæ•°æ®ï¼ˆæ‰€æœ‰TServerçš„åˆ—è¡¨ã€æ¯ä¸ªTServeråŒ…å«å“ªäº›Tabletå‰¯æœ¬ã€Tabletçš„å¼€å§‹Keyå’Œç»“æŸKeyï¼‰ã€‚Catalog Tableåªå­˜å‚¨åœ¨MasterèŠ‚ç‚¹ï¼Œä¹Ÿæ˜¯ä»¥Tabletçš„å½¢å¼ï¼Œæ•°æ®é‡ä¸ä¼šå¾ˆå¤§ï¼Œåªæœ‰ä¸€ä¸ªåˆ†åŒºï¼Œéšç€Masterå¯åŠ¨è€Œè¢«å…¨é‡åŠ è½½åˆ°å†…å­˜ã€‚Masterï¼šè´Ÿè´£é›†ç¾¤ç®¡ç†å’Œå…ƒæ•°æ®ç®¡ç†ã€‚å…·ä½“ï¼šè·Ÿè¸ªæ‰€æœ‰Tabletsã€TServerã€Catalog Tableå’Œå…¶ä»–ç›¸å…³çš„å…ƒæ•°æ®ã€‚åè°ƒå®¢æˆ·ç«¯åšå…ƒæ•°æ®æ“ä½œï¼Œæ¯”å¦‚åˆ›å»ºä¸€ä¸ªæ–°è¡¨ï¼Œå®¢æˆ·ç«¯å‘Masterå‘èµ·è¯·æ±‚ï¼ŒMasterå†™å…¥å…¶WALå¹¶å¾—åˆ°å…¶ä»–MasteråŒæ„åå°†æ–°è¡¨çš„å…ƒæ•°æ®å†™å…¥Catalog Tableï¼Œå¹¶åè°ƒTServeråˆ›å»ºTabletã€‚WALï¼šä¸€ä¸ªä»…æ”¯æŒè¿½åŠ å†™çš„é¢„å†™æ—¥å¿—ï¼Œæ— è®ºMasterè¿˜æ˜¯Tabletéƒ½æœ‰é¢„å†™æ—¥å¿—ï¼Œä»»ä½•å¯¹è¡¨çš„ä¿®æ”¹éƒ½ä¼šåœ¨è¯¥è¡¨å¯¹åº”çš„WALä¸­å†™å…¥æ¡ç›®(entry)ï¼Œå…¶ä»–å‰¯æœ¬åœ¨æ•°æ®ç›¸å¯¹è½åæ—¶å¯ä»¥é€šè¿‡WALèµ¶ä¸Šæ¥ã€‚é€»è¾‘å¤åˆ¶ï¼šKuduåŸºäºRaftåè®®åœ¨é›†ç¾¤ä¸­å¯¹æ¯ä¸ªTabletéƒ½å­˜å‚¨å¤šä¸ªå‰¯æœ¬ï¼Œå‰¯æœ¬ä¸­çš„å†…å®¹ä¸æ˜¯å®é™…çš„æ•°æ®ï¼Œè€Œæ˜¯æ“ä½œè¯¥å‰¯æœ¬ä¸Šçš„æ•°æ®æ—¶å¯¹åº”çš„æ›´æ”¹ä¿¡æ¯ã€‚Insertå’ŒUpdateæ“ä½œä¼šèµ°ç½‘ç»œIOï¼Œä½†Deleteæ“ä½œä¸ä¼šï¼Œå‹ç¼©æ•°æ®ä¹Ÿä¸ä¼šèµ°ç½‘ç»œã€‚ å­˜å‚¨ä¸è¯»å†™Kuduçš„å­˜å‚¨ç»“æ„ï¼š&emsp;&emsp;å¦‚å›¾ï¼ŒTableåˆ†ä¸ºè‹¥å¹²Tabletï¼›TabletåŒ…å«Metadataå’ŒRowSetï¼ŒRowSetåŒ…å«ä¸€ä¸ªMemRowSetåŠè‹¥å¹²ä¸ªDiskRowSetï¼ŒDiskRowSetä¸­åŒ…å«ä¸€ä¸ªBloomFileã€AdhocIndexã€BaseDataã€DeltaMemåŠè‹¥å¹²ä¸ªRedoFileå’ŒUndoFileï¼ˆUndoFileä¸€èˆ¬æƒ…å†µä¸‹åªæœ‰ä¸€ä¸ªï¼‰ã€‚&emsp;&emsp;MemRowSetï¼šæ’å…¥æ–°æ•°æ®åŠæ›´æ–°å·²åœ¨MemRowSetä¸­çš„æ•°æ®ï¼Œæ•°æ®ç»“æ„æ˜¯B+æ ‘ï¼Œä¸»é”®åœ¨éå¶å­èŠ‚ç‚¹ï¼Œæ•°æ®éƒ½åœ¨å¶å­èŠ‚ç‚¹ã€‚MemRowSetå†™æ»¡åä¼šå°†æ•°æ®åˆ·åˆ°ç£ç›˜å½¢æˆè‹¥å¹²ä¸ªDiskRowSetã€‚æ¯æ¬¡è¾¾åˆ°1Gæˆ–è€…120sæ—¶ç”Ÿæˆä¸€ä¸ªDiskRowSetï¼ŒDiskRowSetæŒ‰åˆ—å­˜å‚¨ï¼Œç±»ä¼¼Parquetã€‚&emsp;&emsp;DiskRowSetï¼šDiskRowSetså­˜å‚¨æ–‡ä»¶æ ¼å¼ä¸ºCFileã€‚DiskRowSetåˆ†ä¸ºBaseDataå’ŒDeltaFileã€‚è¿™é‡Œæ¯ä¸ªColumnè¢«å­˜å‚¨åœ¨ä¸€ä¸ªç›¸é‚»çš„æ•°æ®åŒºåŸŸï¼Œè¿™ä¸ªæ•°æ®åŒºåŸŸè¢«åˆ†ä¸ºå¤šä¸ªå°çš„Pageï¼Œæ¯ä¸ªColumn Pageéƒ½å¯ä»¥ä½¿ç”¨ä¸€äº›Encodingä»¥åŠCompressionç®—æ³•ã€‚åå°ä¼šå®šæœŸå¯¹DiskRowSetåšCompactionï¼Œä»¥åˆ é™¤æ²¡ç”¨çš„æ•°æ®åŠåˆå¹¶å†å²æ•°æ®ï¼Œå‡å°‘æŸ¥è¯¢è¿‡ç¨‹ä¸­çš„IOå¼€é”€ã€‚&emsp;&emsp;BaseDataï¼šDiskRowSetåˆ·å†™å®Œæˆçš„æ•°æ®ï¼ŒCFileï¼ŒæŒ‰åˆ—å­˜å‚¨ï¼Œä¸»é”®æœ‰åºã€‚BaseDataä¸å¯å˜ï¼Œç±»ä¼¼Parquetã€‚&emsp;&emsp;BloomFileï¼šæ ¹æ®ä¸€ä¸ªDiskRowSetä¸­çš„Keyç”Ÿæˆä¸€ä¸ªBloomFilterï¼Œç”¨äºå¿«é€Ÿæ¨¡ç³Šå®šä½æŸä¸ªkeyæ˜¯å¦åœ¨DiskRowSetä¸­å­˜åœ¨ã€‚&emsp;&emsp;AdhocIndexï¼šå­˜æ”¾ä¸»é”®çš„ç´¢å¼•ï¼Œç”¨äºå®šä½Keyåœ¨DiskRowSetä¸­çš„å…·ä½“å“ªä¸ªåç§»ä½ç½®ã€‚&emsp;&emsp;DeltaMemStoreï¼šæ¯ä»½DiskRowSetéƒ½å¯¹åº”å†…å­˜ä¸­ä¸€ä¸ªDeltaMemStoreï¼Œè´Ÿè´£è®°å½•è¿™ä¸ªDiskRowSetä¸ŠBaseDataå‘ç”Ÿåç»­å˜æ›´çš„æ•°æ®ï¼Œå…ˆå†™åˆ°å†…å­˜ä¸­ï¼Œå†™æ»¡åFlushåˆ°ç£ç›˜ç”ŸæˆRedoFileã€‚DeltaMemStoreçš„ç»„ç»‡æ–¹å¼ä¸MemRowSetç›¸åŒï¼Œä¹Ÿç»´æŠ¤ä¸€ä¸ªB+æ ‘ã€‚&emsp;&emsp;DeltaFileï¼šDeltaMemStoreåˆ°ä¸€å®šå¤§å°ä¼šå­˜å‚¨åˆ°ç£ç›˜å½¢æˆDeltaFileï¼Œåˆ†ä¸ºUndoFileå’ŒRedoFileã€‚&emsp;&emsp;RedoFileï¼šé‡åšæ–‡ä»¶ï¼Œè®°å½•ä¸Šä¸€æ¬¡Flushç”ŸæˆBaseDataä¹‹åå‘ç”Ÿå˜æ›´æ•°æ®ã€‚DeltaMemStoreå†™æ»¡ä¹‹åï¼Œä¹Ÿä¼šåˆ·æˆCFileï¼Œä¸è¿‡ä¸BaseDataåˆ†å¼€å­˜å‚¨ï¼Œåä¸ºRedoFileã€‚UndoFileå’ŒRedoFileä¸å…³ç³»å‹æ•°æ®åº“ä¸­çš„Undoæ—¥å­å’ŒRedoæ—¥å¿—ç±»ä¼¼ã€‚&emsp;&emsp;UndoFileï¼šæ’¤é”€æ–‡ä»¶ï¼Œè®°å½•ä¸Šä¸€æ¬¡Flushç”ŸæˆBaseDataä¹‹å‰æ—¶é—´çš„å†å²æ•°æ®ï¼ŒKudué€šè¿‡UndoFileå¯ä»¥è¯»åˆ°å†å²æŸä¸ªæ—¶é—´ç‚¹çš„æ•°æ®ã€‚UndoFileä¸€èˆ¬åªæœ‰ä¸€ä»½ã€‚é»˜è®¤UndoFileä¿å­˜15åˆ†é’Ÿï¼ŒKuduå¯ä»¥æŸ¥è¯¢åˆ°15åˆ†é’Ÿå†…æŸåˆ—çš„å†…å®¹ï¼Œè¶…è¿‡15åˆ†é’Ÿåä¼šè¿‡æœŸï¼Œè¯¥UndoFileè¢«åˆ é™¤ã€‚ &emsp;&emsp;DeltaFile(ä¸»è¦æ˜¯RedoFile)ä¼šä¸æ–­å¢åŠ ï¼Œäº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼Œä¸Compactionè‚¯å®šå½±å“æ€§èƒ½ï¼Œæ‰€ä»¥å°±æœ‰äº†ä¸‹é¢ä¸¤ç§åˆå¹¶æ–¹å¼ï¼š Minor Compactionï¼šå¤šä¸ªDeltaFileè¿›è¡Œåˆå¹¶ç”Ÿæˆä¸€ä¸ªå¤§çš„DeltaFileã€‚é»˜è®¤æ˜¯1000ä¸ªDeltaFileè¿›è¡Œåˆå¹¶ä¸€æ¬¡ã€‚ Major Compactionï¼šRedoFileæ–‡ä»¶çš„å¤§å°å’ŒBaseDataçš„æ–‡ä»¶çš„æ¯”ä¾‹ä¸º0.1çš„æ—¶å€™ï¼Œä¼šå°†RedoFileåˆå¹¶è¿›å…¥BaseDataï¼ŒKuduè®°å½•æ‰€æœ‰æ›´æ–°æ“ä½œå¹¶ä¿å­˜ä¸ºUndoFileã€‚è¡¥å……ä¸€ä¸‹ï¼šåˆå¹¶å’Œé‡å†™BaseDataæ˜¯æˆæœ¬å¾ˆé«˜çš„ï¼Œä¼šäº§ç”Ÿå¤§é‡IOæ“ä½œï¼ŒKuduä¸ä¼šå°†å…¨éƒ¨DeltaFileåˆå¹¶è¿›BaseDataã€‚å¦‚æœåªæ›´æ–°å‡ è¡Œæ•°æ®ï¼Œä½†è¦é‡å†™BaseDataï¼Œè´¹åŠ›ä¸è®¨å¥½ï¼Œæ‰€ä»¥Kuduä¼šåœ¨æŸä¸ªç‰¹å®šåˆ—éœ€è¦å¤§é‡æ›´æ–°æ—¶å†æŠŠBaseDataä¸DeltaFileåˆå¹¶ã€‚æœªåˆå¹¶çš„RedoFileä¼šç»§ç»­ä¿ç•™ç­‰å¾…åç»­åˆå¹¶æ“ä½œã€‚ Kuduè¯»æµç¨‹ï¼š Clientå‘é€è¯»è¯·æ±‚ï¼ŒMasteræ ¹æ®ä¸»é”®èŒƒå›´ç¡®å®šåˆ°åŒ…å«æ‰€éœ€æ•°æ®çš„æ‰€æœ‰Tabletä½ç½®å’Œä¿¡æ¯ã€‚ Clientæ‰¾åˆ°æ‰€éœ€Tabletæ‰€åœ¨TServerï¼ŒTServeræ¥å—è¯»è¯·æ±‚ã€‚ å¦‚æœè¦è¯»å–çš„æ•°æ®ä½äºå†…å­˜ï¼Œå…ˆä»å†…å­˜ï¼ˆMemRowSetï¼ŒDeltaMemStoreï¼‰è¯»å–æ•°æ®ï¼Œæ ¹æ®è¯»å–è¯·æ±‚åŒ…å«çš„æ—¶é—´æˆ³å‰æäº¤çš„æ›´æ–°åˆå¹¶æˆæœ€ç»ˆæ•°æ®ã€‚ å¦‚æœè¦è¯»å–çš„æ•°æ®ä½äºç£ç›˜ï¼ˆDiskRowSetï¼ŒDeltaFileï¼‰ï¼Œåœ¨DeltaFileçš„UndoFileã€RedoFileä¸­æ‰¾ç›®æ ‡æ•°æ®ç›¸å…³çš„æ”¹åŠ¨ï¼Œæ ¹æ®è¯»å–è¯·æ±‚åŒ…å«çš„æ—¶é—´æˆ³åˆå¹¶æˆæœ€æ–°æ•°æ®å¹¶è¿”å›ã€‚ Kuduå†™æµç¨‹ï¼š Clientå‘Masterå‘èµ·å†™è¯·æ±‚ï¼ŒMasteræ‰¾åˆ°å¯¹åº”çš„Tabletå…ƒæ•°æ®ä¿¡æ¯ï¼Œæ£€æŸ¥è¯·æ±‚æ•°æ®æ˜¯å¦ç¬¦åˆè¡¨ç»“æ„ã€‚ å› ä¸ºKuduä¸å…è®¸æœ‰ä¸»é”®é‡å¤çš„è®°å½•ï¼Œæ‰€ä»¥éœ€è¦åˆ¤æ–­ä¸»é”®æ˜¯å¦å·²ç»å­˜åœ¨ï¼Œå…ˆæŸ¥è¯¢ä¸»é”®èŒƒå›´ï¼Œå¦‚æœä¸åœ¨èŒƒå›´å†…åˆ™å‡†å¤‡å†™MemRowSetã€‚ å¦‚æœåœ¨ä¸»é”®èŒƒå›´å†…ï¼Œå…ˆé€šè¿‡ä¸»é”®Keyçš„å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œæœªå‘½ä¸­åˆ™å‡†å¤‡å†™MemRowSetã€‚ å¦‚æœBloomFilterå‘½ä¸­ï¼Œåˆ™æŸ¥è¯¢ç´¢å¼•ï¼Œå¦‚æœæ²¡å‘½ä¸­ç´¢å¼•åˆ™å‡†å¤‡å†™MemRowSetï¼Œå¦‚æœå‘½ä¸­äº†ä¸»é”®ç´¢å¼•å°±æŠ¥é”™ï¼šä¸»é”®é‡å¤ã€‚ å†™å…¥MemRowSetå‰å…ˆè¢«æäº¤åˆ°ä¸€ä¸ªTabletçš„WALé¢„å†™æ—¥å¿—ï¼Œå¹¶æ ¹æ®Raftä¸€è‡´æ€§ç®—æ³•å–å¾—Follower Tabletsçš„åŒæ„ï¼Œç„¶åæ‰ä¼šè¢«å†™å…¥åˆ°å…¶ä¸­ä¸€ä¸ªTabletçš„MemRowSetä¸­ã€‚ä¸ºäº†åœ¨MemRowSetä¸­æ”¯æŒå¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶(MVCC)ï¼Œå¯¹æœ€è¿‘æ’å…¥çš„è¡Œ(å³å°šæœªåˆ·æ–°åˆ°ç£ç›˜çš„æ–°çš„è¡Œ)çš„æ›´æ–°å’Œåˆ é™¤æ“ä½œå°†è¢«è¿½åŠ åˆ°MemRowSetä¸­çš„åŸå§‹è¡Œä¹‹åä»¥ç”Ÿæˆé‡åš(REDO)è®°å½•çš„åˆ—è¡¨ã€‚ MemRowSetå†™æ»¡åï¼ŒKuduå°†æ•°æ®æ¯è¡Œç›¸é‚»çš„åˆ—åˆ†ä¸ºä¸åŒçš„åŒºé—´ï¼Œæ¯ä¸ªåˆ—ä¸ºä¸€ä¸ªåŒºé—´ï¼ŒFlushåˆ°DiskRowSetã€‚ Kuduæ›´æ–°æµç¨‹ï¼š Clientå‘é€æ›´æ–°è¯·æ±‚ï¼ŒMasterè·å–è¡¨çš„ç›¸å…³ä¿¡æ¯ï¼Œè¡¨çš„æ‰€æœ‰Tabletä¿¡æ¯ã€‚ Kuduæ£€æŸ¥æ˜¯å¦ç¬¦åˆè¡¨ç»“æ„ã€‚ å¦‚æœéœ€è¦æ›´æ–°çš„æ•°æ®åœ¨MemRowSetï¼ŒB+æ ‘æ‰¾åˆ°å¾…æ›´æ–°æ•°æ®æ‰€åœ¨å¶å­èŠ‚ç‚¹ï¼Œç„¶åå°†æ›´æ–°æ“ä½œè®°å½•åœ¨æ‰€åœ¨è¡Œä¸­ä¸€ä¸ªMutationé“¾è¡¨ä¸­ï¼›Kudué‡‡ç”¨äº†MVCC(å¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶ï¼Œå®ç°è¯»å’Œå†™çš„å¹¶è¡Œï¼Œä»»ä½•å†™éƒ½æ˜¯æ’å…¥)æ€æƒ³ï¼Œå°†æ›´æ”¹çš„æ•°æ®ä»¥é“¾è¡¨å½¢å¼è¿½åŠ åˆ°å¶å­èŠ‚ç‚¹åé¢ï¼Œé¿å…åœ¨æ ‘ä¸Šè¿›è¡Œæ›´æ–°å’Œåˆ é™¤æ“ä½œã€‚ å¦‚æœéœ€è¦æ›´æ–°çš„æ•°æ®åœ¨DiskRowSetï¼Œæ‰¾åˆ°å…¶æ‰€åœ¨çš„DiskRowSetï¼Œå‰é¢æåˆ°æ¯ä¸ªDiskRowSetéƒ½ä¼šåœ¨å†…å­˜ä¸­æœ‰ä¸€ä¸ªDeltaMemStoreï¼Œå°†æ›´æ–°æ“ä½œè®°å½•åœ¨DeltaMemStoreï¼Œè¾¾åˆ°ä¸€å®šå¤§å°æ‰ä¼šç”ŸæˆDeltaFileåˆ°ç£ç›˜ã€‚ åˆ†åŒºæ–¹å¼&emsp;&emsp;Kuduçš„åˆ†åŒºå³ä¸ºTabletï¼Œå¦‚æœä¸»é”®è®¾è®¡ä¸å¥½ä»¥åŠåˆ†åŒºä¸åˆç†éƒ½ä¼šé€ æˆæ•°æ®å‘ç”Ÿå•ç‚¹è¯»å†™é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯çƒ­ç‚¹é—®é¢˜ã€‚Kuduåˆ†åŒºè®¾è®¡æ–¹æ¡ˆéœ€è¦æ ¹æ®åœºæ™¯å’Œè¯»å–å†™å…¥çš„æ–¹å¼æ¥åˆ¶å®šã€‚æœ€å¥½æ˜¯å°†è¯»å†™æ“ä½œéƒ½èƒ½åˆ†æ•£åˆ°å¤§éƒ¨åˆ†èŠ‚ç‚¹ã€‚&emsp;&emsp;åœ¨Kuduä¸­åªæœ‰ä¸»é”®æ‰èƒ½è¢«ç”¨æ¥åˆ†åŒºã€‚åˆ†åŒºæ¨¡å¼æœ‰ä¸‰ç§ï¼š åŸºäºHashåˆ†åŒº(Hash Partitioning):&emsp;&emsp;å“ˆå¸Œåˆ†åŒºé€šè¿‡å“ˆå¸Œå€¼å°†è¡Œåˆ†é…åˆ°è®¸å¤šBuckets(å­˜å‚¨æ¡¶)ä¹‹ä¸€,ä¸€ä¸ªBucketå¯¹åº”ä¸€ä¸ªTabletã€‚&emsp;&emsp;ä¼˜ç‚¹ï¼šæŒ‰IDå“ˆå¸Œåˆ†åŒºå¯ä»¥å°†æ•°æ®å‡åŒ€åˆ†å¸ƒï¼Œå†™æ“ä½œä¼šåˆ†å¸ƒåœ¨å¤šä¸ªèŠ‚ç‚¹ï¼Œå‡è½»çƒ­ç‚¹å’ŒTabletå¤§å°ä¸å‡åŒ€é—®é¢˜ã€‚ä¹Ÿå°±æ˜¯åŸºäºIDå“ˆå¸Œåˆ†åŒºå†™æ•ˆç‡é«˜&emsp;&emsp;ç¼ºç‚¹ï¼šæŒ‰IDæŸ¥è¯¢æ•°æ®ä¼šè¯»å–å•ä¸ªTablet(Bucket)ï¼Œå•ç‚¹è¯»å–æ•ˆç‡ä½ã€‚ åŸºäºRangeåˆ†åŒº(Range Partitioning):&emsp;&emsp;ç”±PKèŒƒå›´åˆ’åˆ†ç»„æˆï¼Œä¸€ä¸ªåŒºé—´å¯¹åº”ä¸€ä¸ªTabletã€‚å°†æ•°æ®æŒ‰ç»™å®šçš„ä¸»é”®èŒƒå›´çš„å­˜å‚¨åˆ°å„ä¸ªTSèŠ‚ç‚¹ä¸Šã€‚&emsp;&emsp;ä¼˜ç‚¹ï¼šå¦‚æœæŒ‰æ—¥æœŸèŒƒå›´åˆ†åŒºï¼Œå•ä¸ªIDçš„è¯»å–ä¼šè·¨å¤šä¸ªèŠ‚ç‚¹å¹¶è¡Œæ‰§è¡Œï¼Œæ•ˆç‡é«˜ã€‚ä¹Ÿå°±æ˜¯åŸºäºæ—¶é—´èŒƒå›´åˆ†åŒºæŸ¥è¯¢æ•ˆç‡é«˜ã€‚&emsp;&emsp;ç¼ºç‚¹ï¼šå¦‚æœæŒ‰æ—¥æœŸèŒƒå›´åˆ†åŒºä¼šæœ‰å†™çƒ­ç‚¹é—®é¢˜ï¼Œè€Œä¸”ä¸€æ—¦æ•°æ®é‡è¶…å‡ºæœ€åä¸€ä¸ªRangeï¼Œæ¥ä¸‹æ¥çš„æ•°æ®å°†å…¨éƒ¨å†™å…¥æœ€åä¸€ä¸ªRangeåˆ†åŒºï¼Œå‘ç”Ÿå€¾æ–œã€‚ å¤šçº§åˆ†åŒº(Multilevel Partitioning):**å¯ä»¥åœ¨å•è¡¨ä¸Šç»„åˆåˆ†åŒºç±»å‹ã€‚&emsp;&emsp;ä¼˜ç‚¹ï¼šç»“åˆä»¥ä¸Šä¸¤ç§åˆ†åŒºæ–¹å¼ï¼Œä¿ç•™ä¸¤ç§åˆ†åŒºç±»å‹çš„ä¼˜ç‚¹â€“æ—¢å¯ä»¥æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼Œåˆå¯ä»¥åœ¨æ¯ä¸ªåˆ†ç‰‡ä¸­ä¿ç•™æŒ‡å®šçš„æ•°æ®ã€‚ä¹Ÿå°±æ˜¯åŸºäºIDå“ˆå¸Œåˆ†åŒºä¸”åŸºäºæ—¶é—´èŒƒå›´åˆ†åŒºç»„åˆæ–¹å¼è¯»å†™æ•ˆç‡éƒ½ä¼šæé«˜**&emsp;&emsp;ç¼ºç‚¹ï¼šä¼˜ç‚¹å¤ªå¤šâ€¦ å¤åˆ¶ç­–ç•¥&emsp;&emsp;å¦‚æœä¸€ä¸ªTServerå‡ºç°æ•…éšœï¼Œå‰¯æœ¬çš„æ•°é‡ç”±3å‡åˆ°2ä¸ªï¼ŒKuduä¼šå°½å¿«æ¢å¤å‰¯æœ¬æ•°ã€‚ä¸¤ç§å¤åˆ¶ç­–ç•¥ï¼š3-4-3ï¼šå¦‚æœä¸€ä¸ªå‰¯æœ¬ä¸¢å¤±ï¼Œå…ˆæ·»åŠ æ›¿æ¢çš„å‰¯æœ¬ï¼Œå†åˆ é™¤å¤±è´¥çš„å‰¯æœ¬ï¼ŒKudué»˜è®¤ä½¿ç”¨è¿™ç§å¤åˆ¶ç­–ç•¥ã€‚3-2-3ï¼šå¦‚æœä¸€ä¸ªå‰¯æœ¬ä¸¢å¤±ï¼Œå…ˆåˆ é™¤å¤±è´¥çš„å‰¯æœ¬ï¼Œå†æ·»åŠ æ›¿æ¢çš„å‰¯æœ¬ã€‚ ä¸€äº›ç»†èŠ‚ ä¸ºä»€ä¹ˆKuduè¦æ¯”HBaseã€Cassandraæ‰«æé€Ÿåº¦æ›´å¿«ï¼Ÿ&emsp;&emsp;HBaseã€Cassandraéƒ½æœ‰åˆ—ç°‡(CF)ï¼Œå¹¶ä¸æ˜¯çº¯æ­£çš„åˆ—å­˜å‚¨ï¼Œé‚£ä¹ˆä¸€ä¸ªåˆ—ç°‡ä¸­æœ‰å‡ ä¸ªåˆ—ï¼Œä½†è¿™å‡ ä¸ªåˆ—ä¸èƒ½ä¸€èµ·ç¼–ç ï¼Œå‹ç¼©æ•ˆæœç›¸å¯¹ä¸å¥½ï¼Œè€Œä¸”åœ¨æ‰«æå…¶ä¸­ä¸€ä¸ªåˆ—çš„æ•°æ®æ—¶ï¼Œå¿…ç„¶ä¼šæ‰«æåŒä¸€åˆ—ç°‡ä¸­çš„å…¶ä»–åˆ—ã€‚Kuduæ²¡æœ‰åˆ—ç°‡çš„æ¦‚å¿µï¼Œå®ƒçš„ä¸åŒåˆ—æ•°æ®éƒ½åœ¨ç›¸é‚»çš„æ•°æ®åŒºåŸŸï¼Œå¯ä»¥åœ¨ä¸€èµ·å‹ç¼©ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸åŒåˆ—ä½¿ç”¨ä¸åŒå‹ç¼©ç®—æ³•ï¼Œå‹ç¼©æ•ˆæœå¾ˆå¥½ï¼›è€Œä¸”éœ€è¦å“ªåˆ—è¯»å“ªåˆ—ä¸ä¼šè¯»å…¶ä»–åˆ—ï¼Œè¯»å–æ—¶ä¸éœ€è¦è¿›è¡ŒMergeæ“ä½œï¼Œæ ¹æ®BaseDataå’ŒDeltaæ•°æ®å¾—åˆ°æœ€ç»ˆæ•°æ®ã€‚Kuduæ‰«ææ€§èƒ½å¯åª²ç¾Parquetã€‚è¿˜æœ‰ï¼ŒKuduçš„è¯»å–æ–¹å¼é¿å…äº†å¾ˆå¤šå­—æ®µçš„æ¯”è¾ƒæ“ä½œï¼ŒCPUåˆ©ç”¨ç‡é«˜ã€‚ Kuduä¸€ä¸ªTabletä¸­å­˜å¾ˆå¤šå¾ˆå¤šDiskRowSetï¼Œæ€ä¹ˆæ‰èƒ½å¿«é€Ÿåˆ¤æ–­Keyåœ¨å“ªä¸ªDiskRowSetï¼Ÿ&emsp;&emsp;é¦–å…ˆè‚¯å®šä¸èƒ½éå†ï¼ŒO(n)çš„å¤æ‚åº¦æ˜¯å¾ˆéš¾å—çš„ã€‚å®ƒä½¿ç”¨äºŒå‰æŸ¥æ‰¾æ ‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹ç»´æŠ¤å¤šä¸ªDiskRowSetçš„æœ€å¤§Keyå’Œæœ€å°Keyï¼Œè¿™æ ·å°±å¯åœ¨O(logn)æ—¶é—´å†…å®šä½Keyæ‰€åœ¨DiskRowSetã€‚ Kuduä¸åŒçš„åˆ—ç±»å‹ä¸åŒï¼Œä½¿ç”¨çš„ç¼–ç å’Œå‹ç¼©æ–¹å¼ï¼Ÿ&emsp;&emsp;Kuduæ¯åˆ—éƒ½æœ‰ç±»å‹ï¼Œç¼–ç æ–¹å¼å’Œå‹ç¼©æ–¹å¼ï¼Œç¼–ç æ–¹å¼æ ¹æ®æ•°æ®ç±»å‹ä¸åŒæœ‰åˆé€‚çš„é»˜è®¤å€¼ï¼Œå‹ç¼©æ–¹å¼é»˜è®¤ä¸å‹ç¼©ã€‚ Kuduçš„éƒ¨ç½²&emsp;&emsp;Kuduæœ‰ä¸¤ç§è¿›ç¨‹Masterå’ŒTServerï¼ŒKuduæœåŠ¡æ˜¯å¯ä»¥å•ç‹¬éƒ¨ç½²åœ¨é›†ç¾¤çš„ï¼Œä½†å¤§å¤šæ•°æƒ…å†µå¯èƒ½æ˜¯ä¸Hadoopé›†ç¾¤å…±ç½®ï¼Œä¸åŒçš„ç¯å¢ƒéœ€è¦ä¸åŒçš„éƒ¨ç½²æ–¹æ¡ˆï¼Œæœ¬èŠ‚ç”¨æ•°æ®åŒ–è¿è¥çš„æ€æƒ³æ¥è¯´KuduæœåŠ¡çš„éƒ¨ç½²ã€‚ Masteréƒ¨ç½²&emsp;&emsp;Masteré«˜å¯ç”¨ï¼Œä¸€èˆ¬é…ç½®3æˆ–5ä¸ªMasteræ¥ä¿è¯HAï¼ŒåŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªMasterå·¥ä½œï¼ŒåŠæ•°ä»¥ä¸ŠMasterå­˜æ´»ï¼ŒæœåŠ¡éƒ½å¯æ­£å¸¸è¿è¡Œã€‚Masterä¹‹é—´éœ€è¦è¾¾æˆå…±è¯†ï¼Œå¤§å¤šæ•°Masterâ€œæŠ•ç¥¨â€å¾—åˆ°Leaderï¼Œå…¶ä»–çš„ä¸ºFollowerã€‚å¦‚æœè¯¥Masterå‡ºç°é—®é¢˜ï¼Œä¹Ÿæ˜¯é€šè¿‡Raftä¸€è‡´æ€§ç®—æ³•æ¥åšé€‰ä¸¾ï¼Œæ—¢å®¹é”™åˆé«˜æ•ˆã€‚&emsp;&emsp;ä¸€èˆ¬é…ç½®3æˆ–5ä¸ªMasterï¼Œ7ä¸ªå°±æœ‰ç‚¹å¤šäº†æ²¡å¿…è¦ã€‚Masteræ•°ç›®å¿…é¡»ä¸ºå¥‡æ•°ä¸ªã€‚ç»™å®šä¸€ç»„éœ€è¦å†™Nä¸ªå‰¯æœ¬ï¼ˆä¸€èˆ¬ä¸º3æˆ–5ï¼‰çš„Tabletï¼Œå¯ä»¥æ¥å—(N-1)/2ä¸ªå†™å…¥é”™è¯¯ã€‚&emsp;&emsp;ç”±äºMaterä¸­åªä¿å­˜å…ƒæ•°æ®ï¼Œæ•°æ®é‡ä¼šä¸€ç›´æ¯”è¾ƒå°ï¼Œå³ä½¿è¢«é¢‘ç¹è¯·æ±‚ï¼Œè¢«å…¨é‡åŠ è½½åˆ°å†…å­˜ï¼Œä¹Ÿä¸éœ€è¦å ç”¨å¤§é‡ç³»ç»Ÿèµ„æºã€‚ TServeréƒ¨ç½²&emsp;&emsp;æ ¹æ®ä¸šåŠ¡é‡ï¼Œäº†è§£å¤§æ¦‚è¦å­˜å‚¨å¤šå°‘æ•°æ®ã€‚å› ä¸ºKuduåˆ—å¼å­˜å‚¨ä¸Parquetç›¸ä¼¼ï¼Œå¯ä»¥æ ¹æ®ç›¸åŒæ•°æ®é‡Parquetå ç”¨ç£ç›˜å¤§å°æ¥ç²—ç•¥ä¼°è®¡éœ€è¦å¤šå°‘å­˜å‚¨ç©ºé—´ã€‚&emsp;&emsp;TServeræ•°é‡å’Œé…ç½®å¤§è‡´ç»™å¤šå°‘å‘¢ï¼Ÿ å‡è®¾: Parquetæ ¼å¼å­˜å‚¨çš„æ•°æ®é›†å¤§å°60TB æ¯ä¸ªTServeræ•°æ®ç£ç›˜æœ€å¤§8T ç»™æ•°æ®ç£ç›˜é¢„ç•™25%çš„ç£ç›˜ç©ºé—´ Tabletå†—ä½™å‰¯æœ¬3 TServeræ•°é‡ = (Parquetæ ¼å¼å­˜å‚¨çš„æ•°æ®é›†å¤§å° * å†—ä½™æ•°) / (TSç£ç›˜å®¹é‡ * ( 1 - ç£ç›˜é¢„ç•™)) TServeræ•°é‡ = (60 * 3) / (8 * (1 - 0.25)) = 30 å­˜å‚¨ä»‹è´¨&emsp;&emsp;åœ¨CMéƒ¨ç½²Masterå’ŒTServeræ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚ä¸‹é…ç½®ï¼š&emsp;&emsp;Kuduè®¾è®¡æ—¶å°±å¯¹æ•°æ®å’ŒWALåˆ†å¼€å­˜å‚¨çš„ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿ&emsp;&emsp;ä¹‹å‰è¯´è¿‡WALä»…æ”¯æŒè¿½åŠ å†™ï¼Œå•ä¸ªæ“ä½œä¹ä¸€çœ‹ä¼šé¡ºåºå†™WALï¼Œä½†åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡æ—¶ï¼Œæ›´åƒæ˜¯éšæœºå†™WALï¼Œè¿™å°±å¾ˆè€ƒéªŒWALåº•å±‚å­˜å‚¨çš„IOPS(IO Per Second)äº†ã€‚ä¼ ç»Ÿæœºæ¢°ç›˜IOPSä¹Ÿå°±å‡ ç™¾ï¼Œè€ŒNVMe-SSDçš„IOPSèƒ½è¾¾åˆ°ä¸‡çº§ç”šè‡³ç™¾ä¸‡çº§ï¼Œæ‰€ä»¥Kudu WALå°½é‡å­˜å‚¨åœ¨SSDä¸­ã€‚&emsp;&emsp;é‚£WALçš„SSDç›˜å¤§æ¦‚è¦é€‰å¤šå¤§å‘¢ï¼Ÿ Kuduçš„WALæ—¥å¿—æ˜¯å¯ä»¥æ§åˆ¶å¤§å°ï¼Œæ—¥å¿—æ®µæ•°é‡çš„ã€‚ é»˜è®¤æ—¥å¿—æ®µå¤§å°8Mï¼Œæ•°é‡1-80ä¸ªï¼ŒæŒ‰é»˜è®¤çš„8Mï¼Œ80ä¸ªç®—ï¼Œå¦‚æœå…±2000ä¸ªTabletï¼Œéœ€è¦WALçš„SSDå¤§å°ï¼š 8 * 80 * 2000 = 1280000MB çº¦1.3TB &emsp;&emsp;Tabletçš„æ•°æ®å¯ä»¥ç”¨æœºæ¢°ç›˜HDDæ¥å­˜å‚¨(SSDæ›´å¥½)ï¼Œå¯ä»¥ä¸DataNodeå¤„äºåŒä¸€å—ç£ç›˜ï¼Œè¿™æ ·æ›´æ–¹ä¾¿ç®¡ç†ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥å……åˆ†åˆ©ç”¨ç£ç›˜è´Ÿè½½å’Œç£ç›˜ç©ºé—´ï¼Œä¸è‡³äºä¸€ä¸ªç›˜çˆ†æ»¡å¦ä¸€ä¸ªç›˜ç©ºä½™å¾ˆå¤šã€‚ Kuduä½¿ç”¨ç¯å¢ƒï¼šå››å°æœºå™¨CDH6.3.1é›†ç¾¤ï¼Œ6æ ¸å¿ƒ12çº¿ç¨‹ï¼Œå†…å­˜åˆ†åˆ«ä¸ºï¼š20GBï¼Œ14GBï¼Œ14GBå’Œ10GBã€‚**OS:**CentOS7;**Impala:**3.2.0-cdh6.3.1;**Kudu:**1.10.0-cdh6.3.1(3Master+3TServer);**Hive:**2.1.1-cdh6.3.1;ä¾æ¬¡å¯åŠ¨HDFSã€hiveã€Kuduã€Impalaã€‚ Kudu + Impala&emsp;&emsp;Impalaå®šä½æ˜¯ä¸€æ¬¾å®æ—¶æŸ¥è¯¢å¼•æ“(ä½å»¶æ—¶SQLäº¤äº’æŸ¥è¯¢)ï¼Œå¿«çš„åŸå› ï¼šåŸºäºå†…å­˜è®¡ç®—ï¼Œæ— éœ€MRï¼ŒC++ç¼–å†™ï¼Œå…¼å®¹HiveQLå’Œæ”¯æŒæ•°æ®æœ¬åœ°åŒ–ã€‚è¿™ä¸Kuduåœºæ™¯ç›¸å»åˆï¼ŒKuduå®˜ç½‘ä¹Ÿè¯´Impalaå’ŒKuduå¯ä»¥æ— ç¼æ•´åˆã€‚&emsp;&emsp;è¿›å…¥Impalaé…ç½®ï¼ŒKuduæœåŠ¡å¤„å‹¾é€‰Kuduå³å¯ã€‚&emsp;&emsp;æ³¨æ„ï¼š=, &lt;=, â€˜&lt;â€˜, â€˜&gt;â€˜, &gt;=, BETWEEN, INç­‰æ“ä½œä¼šä»Impalaè°“è¯ä¸‹æ¨åˆ°Kuduï¼Œæ€§èƒ½é«˜ã€‚è€Œ!=, likeå’Œå…¶ä»–Impalaå…³é”®å­—ä¼šè®©Kuduè¿”å›æ‰€æœ‰ç»“æœå†è®©Impalaè¿‡æ»¤ï¼Œæ•ˆç‡ä½ä¸‹ã€‚å› ä¸ºKuduæ²¡äºŒçº§ç´¢å¼•ï¼Œæ‰€ä»¥æ²¡æœ‰ä¸»é”®çš„è°“è¯ä¹Ÿä¼šé€ æˆå…¨è¡¨æ‰«æã€‚ 1.ä½¿ç”¨Impalaåˆ›å»ºHashåˆ†åŒºçš„Kuduè¡¨ impala-shell -i cdh102:21000 -- Impala Daemonåœ¨cdh102æœºå™¨ CREATE DATABASE IF NOT EXISTS impala_kudu; -- å»ºå†…éƒ¨è¡¨ï¼ŒImpalaå‘ç”Ÿdropæ“ä½œä¼šåˆ é™¤Kuduä¸Šå¯¹åº”æ•°æ® CREATE TABLE impala_kudu.first_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 8 -- ä½¿ç”¨Hashåˆ†åŒº STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- æŸ¥çœ‹Kuduä¸­å·²èƒ½çœ‹åˆ°åˆšåˆ›å»ºçš„è¡¨ kudu table list cdh102:7051,cdh103:7051,cdh104:7051 -- æŸ¥çœ‹è¡¨ä»¥åŠtablets kudu table list cdh102:7051,cdh103:7051,cdh104:7051 --list_tablets åœ¨WebUIä¸Šå¯ä»¥çœ‹åˆ°è¯¥è¡¨å¯¹åº”8ä¸ªTabletä»¥åŠæ¯ä¸ªTabletä¿¡æ¯ã€‚ 2.ä½¿ç”¨Impalaåˆ›å»ºRANGEåˆ†åŒºçš„Kuduè¡¨ impala-shell -i cdh102:21000 -- Impala Daemonåœ¨cdh102æœºå™¨ CREATE TABLE impala_kudu.second_kudu_table( id INT, name String, PRIMARY KEY(id) ) PARTITION BY RANGE (id) ( -- ä½¿ç”¨Rangeåˆ†åŒº åªæœ‰ä¸»é”®å¯ä»¥åšRANGEåˆ†åŒºçš„å­—æ®µ PARTITION 0 &lt;= values &lt;= 3, -- å¦‚æœidèŒƒå›´å–å¤šä¸ªå€¼ï¼Œåˆ™ä¸ºvaluesï¼Œå¦‚æœåˆ†åŒºå­—æ®µæŒ‰æ˜¯å•ä¸ªå€¼ï¼Œåˆ™ä¸ºvalue PARTITION 4 &lt;= values &lt;= 7, PARTITION 8 &lt;= values &lt;= 11, PARTITION 11 &lt; values ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); -- ------------------------------------------------------------------------------------ CREATE TABLE impala_kudu.third_kudu_table( state STRING, name String, PRIMARY KEY(state,name) ) PARTITION BY RANGE (state) ( -- è”åˆä¸»é”®æ—¶ï¼ŒRANGEåˆ†åŒºå¯ä»¥ä½¿ç”¨å…¶ä¸­ä¸€ä¸ªå­—æ®µ PARTITION value = &#39;succeed&#39;, -- å¦‚æœåˆ†åŒºå­—æ®µæŒ‰æ˜¯å•ä¸ªå€¼ï¼Œåˆ™ä¸ºvalue PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 3.ä½¿ç”¨Impalaåˆ›å»ºæ··åˆåˆ†åŒºçš„Kuduè¡¨ impala-shell -i cdh102:21000 -- Impala Daemonåœ¨cdh102æœºå™¨ CREATE TABLE impala_kudu.fourth_kudu_table( id INT, state String, name String, PRIMARY KEY(id,state) ) PARTITION BY HASH (id) PARTITIONS 4, -- æ··åˆåˆ†åŒº HASH+RANGE RANGE (state) ( PARTITION value = &#39;succeed&#39;, PARTITION value = &#39;queued&#39;, PARTITION value = &#39;waiting&#39;, PARTITION value = &#39;failed&#39; -- æœ€ç»ˆTabletæ•°ä¸ºHASHåˆ†åŒºæ•°ä¹˜ä»¥RANGEåˆ†åŒºæ•° ) STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); 4.åœ¨Impalaæ˜ å°„å·²ç»å­˜åœ¨çš„Kuduè¡¨ [kudu@cdh102 /]# kudu table list cdh102:7051,cdh103:7051,cdh104:7051 impala::impala_kudu.first_kudu_table impala::impala_kudu.test impala::impala_kudu.second_kudu_table impala::impala_kudu.fourth_kudu_table impala::impala_kudu.third_kudu_table CREATE EXTERNAL TABLE impala_kudu.fifth_kudu_table STORED AS KUDU -- Kuduè¡¨æ˜ å°„åˆ°Impalaä¸­ï¼Œä¸èƒ½æŒ‡å®šå­—æ®µï¼Œä¸»é”®å’Œåˆ†åŒºæ–¹å¼ï¼Œç”±Kuduå†³å®š TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39;, &#39;kudu.table_name&#39; = &#39;impala::impala_kudu.first_kudu_table&#39; ); drop table impala_kudu.fifth_kudu_table; -- åˆ é™¤ä¸ä¼šå¯¹Kuduä¸­è¡¨æœ‰å½±å“ 5.è½¬å‚¨ä¸€å¼ Hiveè¡¨åˆ°Kudu show create table default.test; CREATE TABLE impala_kudu.test( id INT, name STRING, PRIMARY KEY(id) ) PARTITION BY HASH PARTITIONS 2 -- æ•°æ®é‡å°‘ï¼Œåˆ†2ä¸ªæ¡¶ STORED AS KUDU TBLPROPERTIES ( &#39;kudu.master_addresses&#39; = &#39;cdh102:7051,cdh103:7051,cdh104:7051&#39; ); INSERT INTO impala_kudu.test SELECT * FROM default.test; SELECT * FROM impala_kudu.test; -- è¿™æ—¶ä¼šå‘ç°æ•°æ®é¡ºåºå‘ç”Ÿå˜åŒ–äº†ï¼Œå› ä¸ºHashåˆ†åŒºçš„åŸå›  6.Kuduè¡¨å¤‡ä»½åˆ°Hiveå®æµ‹åœ¨æ•°æ®é‡ä¸å¤§ ç™¾Gçº§åˆ« ä½¿ç”¨Impalaå®ç°Kuduæ•°æ®å…¨é‡åŒæ­¥Hiveï¼Œä¸ä¼šå¯¹KuduæœåŠ¡é€ æˆå½±å“ CREATE TABLE default.impala_kudu_test LIKE impala_kudu.test; INSERT INTO default.impala_kudu_test SELECT * FROM impala_kudu.test; åœ¨å®˜ç½‘äº†è§£æ›´å¤šï¼šUsing Apache Kudu with Apache Impala Kudu + Spark &lt;!-- scala.bin.version: 2.12, kudu.version: 1.10.0 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-spark2_$&#123;scala.bin.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-spark2-tools_$&#123;scala.bin.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt; &lt;/dependency&gt; Sparkè¿æ¥å¹¶æ“ä½œKuduè¡¨ import org.apache.kudu.client._ import collection.JavaConverters._ // Read a table from Kudu val df = spark.read .options(Map(&quot;kudu.master&quot; -&gt; &quot;kudu.master:7051&quot;, &quot;kudu.table&quot; -&gt; &quot;kudu_table&quot;)) .format(&quot;kudu&quot;).load // Query using the Spark API... df.select(&quot;id&quot;).filter(&quot;id &gt;= 5&quot;).show() // ...or register a temporary table and use SQL df.createOrReplaceTempView(&quot;kudu_table&quot;) val filteredDF = spark.sql(&quot;select id from kudu_table where id &gt;= 5&quot;).show() // Use KuduContext to create, delete, or write to Kudu tables val kuduContext = new KuduContext(&quot;kudu.master:7051&quot;, spark.sparkContext) // Create a new Kudu table from a DataFrame schema // NB: No rows from the DataFrame are inserted into the table kuduContext.createTable( &quot;test_table&quot;, df.schema, Seq(&quot;key&quot;), new CreateTableOptions() .setNumReplicas(1) .addHashPartitions(List(&quot;key&quot;).asJava, 3)) // Insert data kuduContext.insertRows(df, &quot;test_table&quot;) // Delete data kuduContext.deleteRows(filteredDF, &quot;test_table&quot;) // Upsert data kuduContext.upsertRows(df, &quot;test_table&quot;) // Update data val alteredDF = df.select(&quot;id&quot;, $&quot;count&quot; + 1) kuduContext.updateRows(filteredRows, &quot;test_table&quot;) // Data can also be inserted into the Kudu table using the data source, though the methods on // KuduContext are preferred // NB: The default is to upsert rows; to perform standard inserts instead, set operation = insert // in the options map // NB: Only mode Append is supported df.write .options(Map(&quot;kudu.master&quot;-&gt; &quot;kudu.master:7051&quot;, &quot;kudu.table&quot;-&gt; &quot;test_table&quot;)) .mode(&quot;append&quot;) .format(&quot;kudu&quot;).save // Check for the existence of a Kudu table kuduContext.tableExists(&quot;another_table&quot;) // Delete a Kudu table kuduContext.deleteTable(&quot;unwanted_table&quot;) Kudu + Hiveä¸Hive MetaStoreé›†æˆå®˜ç½‘å†™çš„å¾ˆè¯¦ç»†ã€‚åœ¨å®˜ç½‘äº†è§£æ›´å¤šï¼šUsing the Hive Metastore with Kudu Kudu APIsKuduå¸¸ç”¨Command LinesKuduå®¢æˆ·ç«¯å‘½ä»¤ kudu cluster ksck master1,master2,master3 æŸ¥çœ‹è¡¨åŠè¡¨çŠ¶æ€ HEALTHYæ­£å¸¸ UNDER_REPLICATEDç¼ºå¤±å‰¯æœ¬ä½†ä¸å½±å“ä½¿ç”¨ UNAVAILABLEè¡¨æ— æ³•ä½¿ç”¨ kudu cluster rebalance master1,master2,master3 -max_moves_per_server 3 -max_run_time_sec 10 Kuduæ•°æ®é‡å¹³è¡¡é¿å…çƒ­ç‚¹TSï¼ŒTSçš„service_queueè¢«å æ»¡ã€å†…å­˜å ç”¨è¿‡å¤§ Kuduæœ‰å¾ˆå¤šå‘½ä»¤ï¼Œå¤§è‡´åˆ†å‡ ç±»ï¼š su - kudu kudu cluster é›†ç¾¤ç®¡ç†ï¼ŒåŒ…æ‹¬å¥åº·çŠ¶æ€æ£€æŸ¥ï¼Œç§»åŠ¨tabletï¼Œrebalanceç­‰æ“ä½œ kudu diagnose é›†ç¾¤è¯Šæ–­å·¥å…· kudu fs åœ¨æœ¬åœ°Kuduæ–‡ä»¶ç³»ç»Ÿåšæ“ä½œï¼Œæ£€æŸ¥ä¸€è‡´æ€§ï¼Œåˆ—å‡ºå…ƒé¡ºæ®ï¼Œæ•°æ®é›†æ›´æ–°ï¼Œæ•°æ®è½¬å‚¨ kudu hms æ“ä½œHiveMetaStoreï¼ŒåŒ…æ‹¬æ£€æŸ¥ä¸Kuduå…ƒæ•°æ®ä¸€è‡´æ€§ï¼Œè‡ªåŠ¨ä¿®å¤å…ƒæ•°æ®ï¼Œåˆ—å‡ºå…ƒæ•°æ® kudu local_replica æ“ä½œæœ¬åœ°å‰¯æœ¬ï¼ŒåŒ…æ‹¬ä»è¿œç¨‹copyå‰¯æœ¬è¿‡æ¥ï¼Œè·å–ç©ºé—´å ç”¨æƒ…å†µï¼Œåˆ é™¤Tabletï¼Œè·å–å‰¯æœ¬åˆ—è¡¨ï¼Œè½¬å‚¨æœ¬åœ°å‰¯æœ¬ç­‰ kudu master æ“ä½œKuduMasterï¼Œå¯ä»¥è¿è¡Œmasterï¼Œè·å–masterçŠ¶æ€ï¼Œæ—¶é—´æˆ³ï¼Œflagç­‰ä¿¡æ¯ï¼Œ kudu pbc protobufå®¹å™¨æ–‡ä»¶æ“ä½œ kudu perf é›†ç¾¤æ€§èƒ½æµ‹è¯•ï¼Œè¿è¡Œè´Ÿè½½ï¼Œæ˜¾ç¤ºæœ¬åœ°Tabletè¡Œæ•°ç­‰ kudu remote_replica æ“ä½œè¿œç¨‹TServerä¸Šçš„å‰¯æœ¬ï¼Œè¿œç¨‹å¤åˆ¶ï¼Œåˆ é™¤ï¼Œè½¬å‚¨ï¼Œåˆ—å‡ºTablet kudu table æ“ä½œKuduè¡¨ï¼ŒåŒ…æ‹¬æ·»åŠ èŒƒå›´åˆ†åŒºï¼Œè®¾ç½®blockSizeï¼Œè®¾ç½®åˆ—çš„å‹ç¼©ç±»å‹ï¼Œç¼–ç ç±»å‹ï¼Œé»˜è®¤å€¼ï¼Œæ³¨é‡Šï¼Œå¤åˆ¶è¡¨æ•°æ®åˆ°å¦ä¸€è¡¨ï¼Œå»ºè¡¨ï¼Œåˆ é™¤åˆ—ï¼Œåˆ è¡¨ï¼Œæè¿°è¡¨ï¼Œåˆ é™¤èŒƒå›´çš„åˆ†åŒºï¼Œè·å–å’Œæ›´æ”¹è¡¨å…¶ä»–é…ç½®ï¼Œåˆ—å‡ºè¡¨ï¼Œæ‰¾åˆ°Rowæ‰€åœ¨Tabletï¼Œåˆ—é‡å‘½åï¼Œè¡¨é‡å‘½åï¼Œscanï¼Œè·å–è¡¨çš„ç»Ÿè®¡ä¿¡æ¯ kudu tablet æ“ä½œKuduçš„Tablet åŒ…æ‹¬æ›´æ¢Tabletçš„Leaderï¼ŒRafté…ç½® kudu test æµ‹è¯• kudu tserver æ“ä½œTabletServeråŒ…æ‹¬è¿è¡Œï¼Œè®¾ç½®Flagï¼Œè·å–çŠ¶æ€ï¼Œæ—¶é—´æˆ³ï¼Œåˆ—å‡ºTServersç­‰ kudu wal æ“ä½œKudu WALï¼Œè½¬å‚¨WALæ—¥å¿—æ–‡ä»¶ Kuduå¸¸ç”¨Java APIï¼š Maven Depï¼š &lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;/dependency&gt; //KuduDDL.java Kuduæ•°æ®å®šä¹‰APIåŒ…æ‹¬ï¼šå»ºè¡¨ï¼Œåˆ è¡¨ï¼Œå¢åŠ å­—æ®µå’Œåˆ é™¤å­—æ®µ package top.qjj.shmily.operations; import org.apache.kudu.ColumnSchema; import org.apache.kudu.Schema; import org.apache.kudu.Type; import org.apache.kudu.client.*; import org.apache.kudu.shaded.com.google.common.collect.ImmutableList; import java.util.LinkedList; public class KuduDDL&#123; public static void main(String[] args) &#123; String kuduMasterAddrs = &quot;cdh102,cdh103,cdh104&quot;; KuduDDLOperations kuduDDLOperations = KuduDDLOperations.getInstance(kuduMasterAddrs); //åˆ›å»ºKuduè¡¨ String tableName = &quot;kudu_table_with_hash&quot;; //1.SchemaæŒ‡å®š LinkedList&lt;ColumnSchema&gt; schemaList = new LinkedList&lt;&gt;(); schemaList.add(kuduDDLOperations.newColumn(&quot;id&quot;, Type.INT32, true)); schemaList.add(kuduDDLOperations.newColumn(&quot;name&quot;, Type.STRING, false)); Schema schema = new Schema(schemaList); //2.è®¾ç½®å»ºè¡¨å‚æ•°-å“ˆå¸Œåˆ†åŒº // CreateTableOptions options = new CreateTableOptions(); // options.setNumReplicas(1); //è®¾ç½®å­˜å‚¨å‰¯æœ¬æ•°-å¿…é¡»ä¸ºå¥‡æ•°å¦åˆ™ä¼šæŠ›å¼‚å¸¸ // List&lt;String&gt; hashKey = new LinkedList&lt;String&gt;(); // hashKey.add(&quot;id&quot;); // options.addHashPartitions(hashKey,2); //å“ˆå¸Œåˆ†åŒº è®¾ç½®å“ˆå¸Œé”®å’Œæ¡¶æ•° //2.è®¾ç½®å»ºè¡¨å‚æ•°-Rangeåˆ†åŒº CreateTableOptions options = new CreateTableOptions(); options.setRangePartitionColumns(ImmutableList.of(&quot;id&quot;)); //è®¾ç½®idä¸ºRange key int temp = 0; for(int i = 0; i &lt; 10; i++)&#123; //id æ¯10ä¸€ä¸ªåŒºé—´ç›´åˆ°100 PartialRow lowLevel = schema.newPartialRow(); //å®šä¹‰ç”¨æ¥åˆ†åŒºçš„åˆ— lowLevel.addInt(&quot;id&quot;, temp); //ä¸å­—æ®µç±»å‹å¯¹åº” INT32åˆ™addInt INT64åˆ™addLong PartialRow highLevel = schema.newPartialRow(); temp += 10; highLevel.addInt(&quot;id&quot;, temp); options.addRangePartition(lowLevel, highLevel); &#125; //3.å¼€å§‹å»ºè¡¨ boolean result = kuduDDLOperations.createTable(tableName, schema, options); System.out.println(result); //æ·»åŠ å­—æ®µ kuduDDLOperations.addColumn(tableName, &quot;test&quot;, Type.INT8); //åˆ é™¤å­—æ®µ kuduDDLOperations.deleteColumn(tableName, &quot;test&quot;); //åˆ é™¤Kuduè¡¨ boolean delResult = kuduDDLOperations.dropTable(tableName); System.out.println(delResult); //å…³é—­è¿æ¥ kuduDDLOperations.closeConnection(); &#125; &#125; class KuduDDLOperations &#123; private static volatile KuduDDLOperations instance; private KuduClient kuduClient = null; private KuduDDLOperations(String masterAddr)&#123; kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); &#125; public static KuduDDLOperations getInstance(String masterAddr)&#123; if(instance == null)&#123; synchronized (KuduDDLOperations.class)&#123; if(instance == null)&#123; instance = new KuduDDLOperations(masterAddr); &#125; &#125; &#125; return instance; &#125; public void closeConnection()&#123; try &#123; kuduClient.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public ColumnSchema newColumn(String name, Type type, boolean isKey)&#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(isKey); return column.build(); &#125; /** * åˆ›å»ºè¡¨ * æ³¨æ„ï¼šImpala DDLå¯¹è¡¨å­—æ®µåå¤§å°å†™ä¸æ•æ„Ÿï¼Œä½†Kuduå±‚å·²ç»è½¬ä¸ºå°å†™ï¼Œä¸”Kudu APIä¸­å­—æ®µåå¿…é¡»å°å†™ï¼› * æ³¨æ„ï¼šImpala DDLå»ºè¡¨è¡¨åå¤§å°å†™æ•æ„Ÿä¸”åˆ°Kuduå±‚è¡¨åä¸ä¼šè¢«è½¬æˆå°å†™ï¼Œä¸”Kudu APIå¯¹è¡¨åå¤§å°å†™æ•æ„Ÿã€‚ * @param tableName è¡¨å * @param schema Schemaä¿¡æ¯ * @param tableOptions å»ºè¡¨å‚æ•° TableOptionså¯¹è±¡ * @return boolean */ public boolean createTable(String tableName, Schema schema, CreateTableOptions tableOptions)&#123; try &#123; kuduClient.createTable(tableName, schema, tableOptions); System.out.println(&quot;Create table successfully!&quot;); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * åˆ åº“è·‘è·¯ * @param tableName è¦åˆ çš„è¡¨å * @return boolean æ˜¯å¦éœ€è¦è·‘è·¯ */ public boolean dropTable(String tableName)&#123; try &#123; kuduClient.deleteTable(tableName); System.out.println(&quot;Drop table successfully!&quot;); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * ç»™Kuduè¡¨æ·»åŠ å­—æ®µ * @param tableName è¡¨å * @param column å­—æ®µå * @param type ç±»å‹ * @return */ public boolean addColumn(String tableName, String column, Type type) &#123; AlterTableOptions alterTableOptions = new AlterTableOptions(); alterTableOptions.addColumn(new ColumnSchema.ColumnSchemaBuilder(column, type).nullable(true).build()); try &#123; kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;æˆåŠŸæ·»åŠ å­—æ®µ&quot; + column + &quot;åˆ°è¡¨&quot; + tableName); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * åˆ é™¤Kuduè¡¨æŒ‡å®šå­—æ®µ * @param tableName è¡¨å * @param column å­—æ®µå * @return */ public boolean deleteColumn(String tableName, String column)&#123; AlterTableOptions alterTableOptions = new AlterTableOptions().dropColumn(column); try &#123; kuduClient.alterTable(tableName, alterTableOptions); System.out.println(&quot;æˆåŠŸåˆ é™¤è¡¨&quot; + tableName + &quot;çš„å­—æ®µ&quot; + column); return true; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; return false; &#125; &#125; // ------------------------------------------------------------------------------------------------------------- //KuduDML.java Kuduæ•°æ®æ“ä½œAPIåŒ…æ‹¬ï¼šCRUD package top.qjj.shmily.operations; import org.apache.kudu.client.SessionConfiguration.FlushMode; import org.apache.kudu.client.*; public class KuduDML &#123; public static void main(String[] args) &#123; String masterAddr = &quot;cdh102,cdh103,cdh104&quot;; KuduDMLOperations kuduDMLOperations = KuduDMLOperations.getInstance(masterAddr); //æ’å…¥æ•°æ® kuduDMLOperations.insertRows(); //æ›´æ–°ä¸€æ¡æ•°æ® kuduDMLOperations.updateRow(); //åˆ é™¤ä¸€æ¡æ•°æ® kuduDMLOperations.deleteRow(); //æŸ¥è¯¢æ•°æ® kuduDMLOperations.selectRows(); //å…³é—­Clientè¿æ¥ kuduDMLOperations.closeConnection(); &#125; &#125; class KuduDMLOperations &#123; private static volatile KuduDMLOperations instance; private KuduClient kuduClient = null; private KuduDMLOperations(String masterAddr)&#123; kuduClient = new KuduClient.KuduClientBuilder(masterAddr).defaultOperationTimeoutMs(6000).build(); &#125; public static KuduDMLOperations getInstance(String masterAddr)&#123; if(instance == null)&#123; synchronized (KuduDMLOperations.class)&#123; if(instance == null)&#123; instance = new KuduDMLOperations(masterAddr); &#125; &#125; &#125; return instance; &#125; public void closeConnection() &#123; try &#123; kuduClient.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; /** * ä»¥kudu_table_with_hashè¡¨(id INT,name STRING)ä¸ºä¾‹ æ’å…¥æ•°æ® * æ³¨æ„ï¼šå†™æ•°æ®æ—¶æ•°æ®ä¸æ”¯æŒä¸ºnullï¼Œéœ€è¦å¯¹è¿›æ¥çš„æ•°æ®åˆ¤ç©º */ public void insertRows()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); //æ‰“å¼€è¡¨ KuduSession kuduSession = kuduClient.newSession(); //åˆ›å»ºä¼šè¯Session kuduSession.setFlushMode(FlushMode.MANUAL_FLUSH); //è®¾ç½®æ•°æ®æäº¤æ–¹å¼ /** * 1.AUTO_FLUSH_SYNCï¼ˆé»˜è®¤ï¼‰ ç›®å‰æ¯”è¾ƒæ…¢ * 2.AUTO_FLUSH_BACKGROUND ç›®å‰æœ‰BUG * 3.MANUAL_FLUSH ç›®å‰æ•ˆç‡æœ€é«˜ è¿œè¿œé«˜äºå…¶ä»– * å…³äºè¿™ä¸‰ä¸ªå‚æ•°æµ‹è¯•è°ƒä¼˜å¯çœ‹è¿™ç¯‡ï¼šhttps://www.cnblogs.com/harrychinese/p/kudu_java_api.html */ int numOps = 3000; kuduSession.setMutationBufferSpace(numOps); //è®¾ç½®MANUAL_FLUSHéœ€è¦è®¾ç½®ç¼“å†²åŒºæ“ä½œæ¬¡æ•°é™åˆ¶ å¦‚æœè¶…é™ä¼šæŠ›å¼‚å¸¸ int nowOps = 0; //è®°å½•å½“å‰æ“ä½œæ•° for(int i = 0; i &lt;= 100; i++)&#123; Insert insert = table.newInsert(); //å­—æ®µæ•°æ® insert.getRow().addInt(&quot;id&quot;, i); insert.getRow().addString(&quot;name&quot;, &quot;å°&quot;+i); nowOps += 1; if(nowOps == numOps / 2)&#123; //æ‰€ä»¥ç¼“å†²åŒºæ“ä½œæ¬¡æ•°è¾¾åˆ°ä¸€åŠæ—¶è¿›è¡Œflushæäº¤æ•°æ®ï¼Œé¿å…æŠ›å¼‚å¸¸ kuduSession.flush(); //æäº¤æ•°æ® nowOps = 0; //è®¡æ•°å™¨å½’é›¶ &#125; kuduSession.apply(insert); &#125; kuduSession.flush(); //ä¿è¯æœ€åéƒ½æäº¤ä¸Šå»äº† kuduSession.close(); System.out.println(&quot;æ•°æ®æˆåŠŸå†™å…¥Kuduè¡¨&quot;); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;æ•°æ®å†™å…¥å¤±è´¥ï¼ŒåŸå› ï¼š&quot; + e.getMessage()); &#125; &#125; /** * ä»¥kudu_table_with_hashè¡¨(id INT,name STRING)ä¸ºä¾‹ æŸ¥è¯¢æ•°æ® */ public void selectRows()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); // æ‰“å¼€è¡¨ KuduScanner scanner = kuduClient.newScannerBuilder(table).build(); //åˆ›å»ºScanner while (scanner.hasMoreRows())&#123; for (RowResult r: scanner.nextRows()) &#123; System.out.println(r.getInt(&quot;id&quot;) + &quot; - &quot; + r.getString(1)); &#125; &#125; scanner.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;æŸ¥è¯¢å¤±è´¥ï¼ŒåŸå› ï¼š&quot; + e.getMessage()); &#125; &#125; /** * ä»¥kudu_table_with_hashè¡¨(id INT,name STRING)ä¸ºä¾‹ æ›´æ–°ä¸€æ¡æ•°æ® */ public void updateRow()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); session.setFlushMode(FlushMode.AUTO_FLUSH_SYNC); Update update = table.newUpdate(); PartialRow row = update.getRow(); //å®šä¹‰ç”¨æ¥åˆ†åŒºçš„åˆ— row.addInt(&quot;id&quot;, 66); row.addString(&quot;name&quot;, &quot;qjj&quot;); session.apply(update); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;æ•°æ®æ›´æ–°å¤±è´¥ï¼ŒåŸå› ï¼š&quot; + e.getMessage()); &#125; &#125; /** * ä»¥kudu_table_with_hashè¡¨(id INT,name STRING)ä¸ºä¾‹ åˆ é™¤ä¸€æ¡æ•°æ® */ public void deleteRow()&#123; try &#123; KuduTable table = kuduClient.openTable(&quot;kudu_table_with_hash&quot;); KuduSession session = kuduClient.newSession(); Delete delete = table.newDelete(); delete.getRow().addInt(&quot;id&quot;,18); //æ ¹æ®ä¸»é”®å”¯ä¸€åˆ é™¤ä¸€æ¡è®°å½• session.flush(); session.apply(delete); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); System.out.println(&quot;æ•°æ®åˆ é™¤å¤±è´¥ï¼ŒåŸå› ï¼š&quot; + e.getMessage()); &#125; &#125; &#125; Kuduå¸¸ç”¨Python APIï¼š import kudu from kudu.client import Partitioning from datetime import datetime # Connect to Kudu master server client = kudu.connect(host=&#39;cdh102,cdh103,cdh104&#39;, port=7051) # Define a schema for a new table builder = kudu.schema_builder() builder.add_column(&#39;key&#39;).type(kudu.int64).nullable(False).primary_key() builder.add_column(&#39;ts_val&#39;, type_=kudu.unixtime_micros, nullable=False, compression=&#39;lz4&#39;) schema = builder.build() # Define partitioning schema partitioning = Partitioning().add_hash_partitions(column_names=[&#39;key&#39;], num_buckets=3) # Create new table client.create_table(&#39;python-example&#39;, schema, partitioning) # Open a table table = client.table(&#39;python_example&#39;) # Create a new session so that we can apply write operations session = client.new_session() # Insert a row op = table.new_insert(&#123;&#39;key&#39;: 1, &#39;ts_val&#39;: datetime.utcnow()&#125;) session.apply(op) # Upsert a row op = table.new_upsert(&#123;&#39;key&#39;: 2, &#39;ts_val&#39;: &quot;2020-01-01T00:00:00.000000&quot;&#125;) session.apply(op) # Updating a row op = table.new_update(&#123;&#39;key&#39;: 1, &#39;ts_val&#39;: (&quot;2020-07-12&quot;, &quot;%Y-%m-%d&quot;)&#125;) session.apply(op) # Delete a row op = table.new_delete(&#123;&#39;key&#39;: 2&#125;) session.apply(op) # Flush write operations, if failures occur, capture print them. try: session.flush() except kudu.KuduBadStatus as e: print(session.get_pending_errors()) # Create a scanner and add a predicate scanner = table.scanner() scanner.add_predicate(table[&#39;ts_val&#39;] == datetime(2020, 7, 12)) # Open Scanner and read all tuples # Note: This doesn&#39;t scale for large scans result = scanner.open().read_all_tuples() Kuduä¼˜åŒ–1.ä½¿ç”¨SSDä¼šæ˜¾è‘—æé«˜Kuduæ€§èƒ½ã€‚ï¼ˆå› ä¸ºå¦‚æœå–å¤šä¸ªå­—æ®µï¼Œåˆ—å¼å­˜å‚¨åœ¨ä¼ ç»Ÿç£ç›˜ä¸Šä¼šå¤šæ¬¡å¯»å€ï¼Œè€Œä½¿ç”¨SSDä¸ä¼šæœ‰å¯»å€é—®é¢˜ï¼‰2.kuduæ€§èƒ½è°ƒä¼˜3.memory_limit_hard_bytes è¯¥å‚æ•°æ˜¯å•ä¸ªTServerèƒ½å¤Ÿä½¿ç”¨çš„æœ€å¤§å†…å­˜é‡ã€‚å¦‚æœå†™å…¥é‡å¾ˆå¤§è€Œå†…å­˜å¤ªå°ï¼Œä¼šé€ æˆå†™å…¥æ€§èƒ½ä¸‹é™ã€‚å¦‚æœé›†ç¾¤èµ„æºå……è£•ï¼Œå¯ä»¥å°†å®ƒè®¾å¾—æ¯”è¾ƒå¤§ï¼Œæ¯”å¦‚è®¾ç½®ä¸ºå•å°æœåŠ¡å™¨å†…å­˜æ€»é‡çš„ä¸€åŠã€‚å®˜æ–¹ä¹Ÿæä¾›äº†ä¸€ä¸ªè¿‘ä¼¼ä¼°è®¡çš„æ–¹æ³•ï¼Œå³ï¼šæ¯1TBå®é™…å­˜å‚¨çš„æ•°æ®çº¦å ç”¨1.5GBå†…å­˜ï¼Œæ¯ä¸ªå‰¯æœ¬çš„MemRowSetå’ŒDeltaMemStoreçº¦å ç”¨128MBå†…å­˜ï¼Œï¼ˆå¯¹å¤šè¯»å°‘å†™çš„è¡¨è€Œè¨€ï¼‰æ¯åˆ—æ¯CPUæ ¸å¿ƒçº¦å ç”¨256KBå†…å­˜ï¼Œå¦å¤–å†åŠ ä¸Šå—ç¼“å­˜ï¼Œæœ€ååœ¨è¿™äº›åŸºç¡€ä¸Šç•™å‡ºçº¦25%çš„ä½™é‡ã€‚4.block_cache_capacity_mb Kuduä¸­ä¹Ÿè®¾è®¡äº†BlockCacheï¼Œä¸ç®¡åç§°è¿˜æ˜¯ä½œç”¨éƒ½ä¸HBaseä¸­çš„å¯¹åº”è§’è‰²ç›¸åŒã€‚é»˜è®¤å€¼512MBï¼Œç»éªŒå€¼æ˜¯è®¾ç½®1~4GBä¹‹é—´ï¼Œæˆ‘ä»¬è®¾äº†4GBã€‚5.memory.soft_limit_in_bytes/memory.limit_in_bytesè¿™æ˜¯Kuduè¿›ç¨‹ç»„ï¼ˆå³Linux cgroupï¼‰çš„å†…å­˜è½¯é™åˆ¶å’Œç¡¬é™åˆ¶ã€‚å½“ç³»ç»Ÿå†…å­˜ä¸è¶³æ—¶ï¼Œä¼šä¼˜å…ˆå›æ”¶è¶…è¿‡è½¯é™åˆ¶çš„è¿›ç¨‹å ç”¨çš„å†…å­˜ï¼Œä½¿ä¹‹å°½é‡ä½äºé˜ˆå€¼ã€‚å½“è¿›ç¨‹å ç”¨çš„å†…å­˜è¶…è¿‡äº†ç¡¬é™åˆ¶ï¼Œä¼šç›´æ¥è§¦å‘OOMå¯¼è‡´Kuduè¿›ç¨‹è¢«æ€æ‰ã€‚æˆ‘ä»¬è®¾ä¸º-1ï¼Œå³ä¸é™åˆ¶ã€‚6.maintenance_manager_num_threadså•ä¸ªTServerç”¨äºåœ¨åå°æ‰§è¡ŒFlushã€Compactionç­‰åå°æ“ä½œçš„çº¿ç¨‹æ•°ï¼Œé»˜è®¤æ˜¯1ã€‚å¦‚æœæ˜¯é‡‡ç”¨æ™®é€šç¡¬ç›˜ä½œä¸ºå­˜å‚¨çš„è¯ï¼Œè¯¥å€¼åº”ä¸æ‰€é‡‡ç”¨çš„ç¡¬ç›˜æ•°ç›¸åŒã€‚7.max_create_tablets_per_tsåˆ›å»ºè¡¨æ—¶èƒ½å¤ŸæŒ‡å®šçš„æœ€å¤§åˆ†åŒºæ•°ç›®ï¼ˆhash partition * range partitionï¼‰ï¼Œé»˜è®¤ä¸º60ã€‚å¦‚æœä¸èƒ½æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥è°ƒå¤§ã€‚8.follower_unavailable_considered_failed_secå½“Followerä¸Leaderå¤±å»è”ç³»åï¼ŒLeaderå°†Followeråˆ¤å®šä¸ºå¤±è´¥çš„çª—å£æ—¶é—´ï¼Œé»˜è®¤å€¼300s,åˆ¤å®šä¸ºå¤±è´¥åˆ™è®¤ä¸ºæ•°æ®ä¸¢å¤±ã€‚9.max_clock_sync_error_usec NTPæ—¶é—´åŒæ­¥çš„æœ€å¤§å…è®¸è¯¯å·®ï¼Œå•ä½ä¸ºå¾®ç§’ï¼Œé»˜è®¤å€¼10sã€‚å¦‚æœKudué¢‘ç¹æŠ¥æ—¶é—´ä¸åŒæ­¥çš„é”™è¯¯ï¼Œå¯ä»¥é€‚å½“è°ƒå¤§ï¼Œæ¯”å¦‚15sã€‚ Kuduå¼‚å¸¸å¤„ç†Apache Kudu Troubleshooting1.æŠ¥é”™â€Remote error:Service unavailable: Scan request on kudu.tserver.TabletServerService from xx.xx.xx.xx:xx dropped due to backpressure.The service queue is full;it has 50 ites.â€ åŸå› ï¼šé«˜å³°æœŸå•ä¸ªTabletçš„rpcè¯·æ±‚é˜Ÿåˆ—è¾¾åˆ°ä¸Šé™ï¼Œå¯¼è‡´TabletServeræ— æ³•æä¾›æœåŠ¡ï¼Œä¸´æ—¶è§£å†³æ–¹æ¡ˆæ˜¯é‡å¯è¯¥TabletServerã€‚ å¯åœ¨gflagfileå¢åŠ å‚æ•°ï¼šâ€“rpc_service_queue_length=120 (é€‚å½“è°ƒå¤§ï¼Œé»˜è®¤å€¼100) 2.TSç»´æŠ¤å‰éœ€è¦å¥åº·æ£€æŸ¥ï¼Œå¦‚æœæœ‰ä»»ä½•å‰¯æœ¬ä¸è¶³çš„æƒ…å†µï¼Œéœ€ç­‰å¾…å‰¯æœ¬æ‹·è´å®Œæˆåå†ç»´æŠ¤ã€‚å¯åœ¨gflagfileå¢åŠ å‚æ•°ï¼šâ€“rpc_service_queue_length=3600 follower_unavailable_considered_failed_secé»˜è®¤ä¸º300sï¼Œtabletå¤±å»è”ç³»è¶…è¿‡300såï¼Œè¯¥èŠ‚ç‚¹çš„æ•°æ®å°±ä¼šåœ¨å…¶ä»–èŠ‚ç‚¹é‡å»ºï¼Œä¸ºäº†é¿å…ç»´æŠ¤é€ æˆçš„ä¸å¿…è¦çš„æ•°æ®ç§»åŠ¨å’Œæ‹·è´ï¼Œå¯ä»¥ä¸´æ—¶è®¾ç½®æ­¤æ—¶é—´ä¸ºæ›´é•¿çš„æ—¶é—´ï¼ˆé‡å¯ç»´æŠ¤åŠ ä¸Štableté‡å¯ååˆå§‹åŒ–éœ€è¦çš„æ—¶é—´ï¼‰ KuduTSé‡å¯æ¢å¤é€Ÿåº¦æ›´å¿«ã€‚ 3.Kudu JavaAPIå®¢æˆ·ç«¯è¯·æ±‚è¿æ¥æœåŠ¡å™¨Materæ—¶æŠ¥é”™ï¼šâ‘ Caused by: org.apache.kudu.client.RecoverableException: connection disconnectedâ‘¡ä½ çš„ä¸»æœºä¸­çš„è½¯ä»¶ç»ˆæ­¢äº†ä¸€ä¸ªå·²å»ºç«‹çš„é“¾æ¥ unexpected exception from downstream onâ‘¢Caused by: org.apache.kudu.client.NoLeaderFoundException: Master config (master1_ipaddr:7051,master2_ipaddr:7051,master3_ipaddr:7051) has no leader. Exceptions received: org.apache.kudu.client.RecoverableException: connection disconnected,org.apache.kudu.client.RecoverableException: connection disconnected,org.apache.kudu.client.RecoverableException: connection disconnectedâ‘£org.apache.kudu.client.NonRecoverableException: cannot complete before timeout:KuduRpc(method=GetTableSchema,tablet=null,attemtp=7â€¦,Sent:(master-master1_ip:7051,[ConnectToMaster,7 ])â€¦Received(master-master1_ip:7051,[NERWORK_ERROR, 6]))åœ¨ç½‘ä¸Šæœäº†ä¸€å¤§å †ï¼šè®¾ç½®å®Œå¦‚ä¸‹çš„é…ç½®ï¼Œä¹Ÿä¸èµ·ä½œç”¨ã€‚â€“rpc_encryption=disabledâ€“rpc_authentication=disabledâ€“trusted_subnets=0.0.0.0/0åŸå› ï¼škuduå®¢æˆ·ç«¯è¿æ¥kuduæœåŠ¡å™¨æ—¶,æœåŠ¡å™¨è¿”å›masterçš„ä¸»æœºåè€ŒéIPï¼Œå‘Šè¯‰å®¢æˆ·ç«¯è°æ˜¯master,ç„¶åé€šä¿¡ï¼Œä½†æ˜¯ä¸»æœºåä¸æ˜¯ä¸»æœºçš„ipï¼Œæ‰€ä»¥å®¢æˆ·ç«¯ä¼šåœ¨æœ¬åœ°hostsæ–‡ä»¶æ‰¾è¿™ä¸ªä¸»æœºåï¼Œä½†æ˜¯æœ¬æœºæ²¡æœ‰é…ç½®ï¼Œæ‰€ä»¥ä¼šå¤±è´¥ï¼Œç›´åˆ°è¶…æ—¶ã€‚è§£å†³ï¼šéœ€è¦æœ¬åœ°è§£æipå¯¹åº”çš„hostï¼Œä¿®æ”¹æœ¬åœ°hostï¼Œå¢åŠ MasterèŠ‚ç‚¹çš„hostæ˜ å°„å³å¯è§£å†³ã€‚ 4.Impala æŸ¥è¯¢KuduæŠ¥Error loading metadata for kudu table xxx å¦‚ä¸‹ï¼šåŸå› ï¼šImpalaæ“ä½œKuduè¶…æ—¶è§£å†³ï¼šåœ¨Impalaè®¾ç½®kudu_operation_timeout_ms = 1800000 5.Kuduå®¢æˆ·ç«¯å¦‚ä¸‹æŠ¥é”™ï¼šRPC can not complete before timeout: KuduRpc(method=CreateTable, tablet=null, attempt=26, DeadlineTracker(timeout=30000, elapsed=29427)è§£å†³: session.setTimeoutMillis(60000); new KuduClient.KuduClientBuilder(&quot;cdh101&quot;).defaultAdminOperationTimeoutMs(600000).build(); 6.KuduæœåŠ¡éƒ¨åˆ†æˆ–å…¨éƒ¨æŒ‚ï¼ŒæŠ¥é”™æ—¥å¿—å¦‚ä¸‹Check failed: _s.ok() Bad status: Service unavailable: Cannot initialize clock: Timed out waiting for clock sync: Error reading clock. Clock considered unsynchronizedåŸå› ï¼šntpåŒæ­¥é—®é¢˜ï¼Œå¯èƒ½æ˜¯ntpdè¿›ç¨‹æœªå¯åŠ¨ã€æˆ–/etc/sysconfig/ntpdå¸¦æœ‰-xå‚æ•°ã€æˆ–æ‰€æœ‰èŠ‚ç‚¹éƒ½åŒæ­¥åˆ°ä¸€å°å…¬ç”¨ntpæœåŠ¡å™¨ä½†è¯¥ntpæœåŠ¡å™¨æœ‰é—®é¢˜è§£å†³ï¼š1.æ£€æŸ¥ntpdè¿›ç¨‹æ˜¯å¦å­˜åœ¨ 2.å»æ‰/etc/sysconfig/ntpdä¸­-xå‚æ•°(é»˜è®¤åªæœ‰-g) 3.é…ç½®/etc/ntp.confå°†é›†ç¾¤å†…æ‰€æœ‰èŠ‚ç‚¹æ—¶é’ŸåŒæ­¥åˆ°é›†ç¾¤å†…çš„æŸä¸€å°èŠ‚ç‚¹ å†é‡å¯Kuduå³å¯ 7.Kuduå®¢æˆ·ç«¯æŠ¥é”™æç¤ºæœåŠ¡ç«¯éœ€è¦è®¤è¯å…·ä½“å †æ ˆå¦‚ä¸‹ï¼š org.apache.kudu.client.NonRecoverableException: cannot re-acquire authentication token after 5 attempts (Couldn&#39;t find a valid master in (master1:7051,master2:7051,master3:7051). Exceptions received: [org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials, org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials, org.apache.kudu.client.NonRecoverableException: server requires authentication, but client Kerberos credentials (TGT) have expired. Authentication tokens were not used because this connection will be used to acquire a new token and therefore requires primary credentials]) at org.apache.kudu.client.KuduException.transformException(KuduException.java:110) at org.apache.kudu.client.KuduClient.joinAndHandleException(KuduClient.java:413) at org.apache.kudu.client.KuduScanner.nextRows(KuduScanner.java:72) at com.smy.crm.common.kudu.KuduApiHelperNew.select(KuduApiHelperNew.java:182) at com.smy.crm.common.kudu.KuduApiHelper.select(KuduApiHelper.java:161) at com.smy.crm.tag.util.TagKuduUtil.findCustAllTagFromNarrow(TagKuduUtil.java:302) at com.smy.crm.tag.manager.TagKuduManager.findCustAllTag(TagKuduManager.java:332) at com.smy.crm.tag.rule.TagRealTimeExecute.execute(TagRealTimeExecute.java:105) at com.smy.crm.tag.mq.AutoTagMqConsumer.lambda$startNewConsumer$0(AutoTagMqConsumer.java:78) at org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService$ConsumeRequest.run(ConsumeMessageConcurrentlyService.java:411) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) æŸ¥çœ‹kudué…ç½®å‘ç°æœåŠ¡ç«¯â€“rpc_authenticationå‚æ•°å€¼ä¸ºrequiredè€ŒæœªæŒ‰å®˜ç½‘è¯´çš„é»˜è®¤æ˜¯optionalæ‰€ä»¥æœªè®¤è¯æˆ–è®¤è¯è¿‡æœŸçš„é•¿è¿æ¥å°±ä¼šè¿æ¥å¤±è´¥å¯¼è‡´ç¨‹åºæŒ‚æ‰å‚è€ƒKuduå®˜ç½‘ä»‹ç»å¦‚ä¸‹ï¼š Kudu authentication with Kerberos Kudu can be configured to enforce secure authentication among servers, and between clients and servers. Authentication prevents untrusted actors from gaining access to Kudu, and securely identifies connecting users or services for authorization checks. Authentication in Kudu is designed to interoperate with other secure Hadoop components by utilizing Kerberos. Configure authentication on Kudu servers using the --rpc_authentication flag, which can be set to one of the following options: required - Kudu will reject connections from clients and servers who lack authentication credentials. optional - Kudu will attempt to use strong authentication, but will allow unauthenticated connections. disabled - Kudu will only allow unauthenticated connections. By default, the flag is set to optional. To secure your cluster, set --rpc_authentication to required. è§£å†³ï¼šåœ¨gflagfileå¢åŠ å‚æ•°ï¼šâ€“rpc_authentication=optional 8.Kuduæ–°å¢èŠ‚ç‚¹æ— æ³•è¿æ¥å®¢æˆ·ç«¯é”™è¯¯æ—¥å¿— org.apache.kudu.client.NonRecoverableException: cannot complete before timeout: KuduRpc(method=GetTableSchema, tablet=null, attempt=10, TimeoutTracker(timeout=30000, elapsed=30020), Trace Summary(27441 ms): Sent(30), Received(27), Delayed(9), MasterRefresh(10), AuthRefresh(0), Truncated: false Sent: (master-xxx1:7051, [ ConnectToMaster, 10 ]), (master-xxx2:7051, [ ConnectToMaster, 10 ]), (master-xxx3:7051, [ ConnectToMaster, 10 ]) Received: (master-xxx1:7051, [ NETWORK_ERROR, 9 ]), (master-xxx2:7051, [ NETWORK_ERROR, 9 ]), (master-xxx3:7051, [ NETWORK_ERROR, 9 ]) Delayed: (UNKNOWN, [ GetTableSchema, 9 ])) Masteræ—¥å¿— Failed RPC negotiation. Trace: 1203 17:18:29.304059 (+ 0us) reactor.cc:583] Submitting negotiation task for server connection from kudu_client_ip:41573 1203 17:18:29.304166 (+ 107us) server_negotiation.cc:184] Beginning negotiation 1203 17:18:29.304168 (+ 2us) server_negotiation.cc:373] Waiting for connection header 1203 17:18:29.367151 (+ 62983us) server_negotiation.cc:381] Connection header received 1203 17:18:29.368056 (+ 905us) server_negotiation.cc:337] Received NEGOTIATE NegotiatePB request 1203 17:18:29.368057 (+ 1us) server_negotiation.cc:420] Received NEGOTIATE request from client 1203 17:18:29.368073 (+ 16us) server_negotiation.cc:349] Sending NEGOTIATE NegotiatePB response 1203 17:18:29.368090 (+ 17us) server_negotiation.cc:205] Negotiated authn=SASL 1203 17:18:29.527892 (+159802us) server_negotiation.cc:337] Received TLS_HANDSHAKE NegotiatePB request 1203 17:18:29.528938 (+ 1046us) server_negotiation.cc:349] Sending TLS_HANDSHAKE NegotiatePB response 1203 17:18:29.555620 (+ 26682us) server_negotiation.cc:337] Received TLS_HANDSHAKE NegotiatePB request 1203 17:18:29.555806 (+ 186us) server_negotiation.cc:349] Sending TLS_HANDSHAKE NegotiatePB response 1203 17:18:29.555827 (+ 21us) server_negotiation.cc:589] Negotiated TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(256) Mac=AEAD 1203 17:18:29.649572 (+ 93745us) negotiation.cc:304] Negotiation complete: Network error: Server connection negotiation failed: server connection from kudu_client_ip:41573: BlockingRecv error: failed to read from TLS socket (remote: kudu_client_ip:41573): Cannot send after transport endpoint shutdown (error 108) Metrics: &#123;&quot;server-negotiator.queue_time_us&quot;:79,&quot;thread_start_us&quot;:42,&quot;threads_started&quot;:1&#125; åŸå› åŠè§£å†³kudu-master_trusted_subnetskudu-tserver_trusted_subnetså®¢æˆ·ç«¯æ‰€åœ¨æœºå™¨ä¸åœ¨é»˜è®¤ä¿¡ä»»å­ç½‘å†…ï¼ˆç™½åå•ï¼‰ï¼Œå¯¼è‡´è¿æ¥æ— æƒé™ã€‚å¦‚æœä¸è€ƒè™‘å®‰å…¨æ€§å¯ä»¥è®¾ç½®â€“trusted_subnets=0.0.0.0/0ï¼Œå¦‚è‹¥è€ƒè™‘å®‰å…¨æ€§ï¼Œå¯ä»¥åœ¨é»˜è®¤å€¼åŸºç¡€ä¸Šå¢åŠ æ–°èŠ‚ç‚¹æ‰€åœ¨å­ç½‘åœ°å€ã€‚ä¾‹ï¼šâ€“trusted_subnets=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16,172.17.0.0/8æ³¨ï¼šéœ€è¦è®¾ç½®åœ¨â€œgflagfile çš„ Kudu æœåŠ¡é«˜çº§é…ç½®ä»£ç æ®µï¼ˆå®‰å…¨é˜€ï¼‰â€ HTAPæ··åˆäº‹åŠ¡åˆ†æå¤„ç†HTAPï¼Œå³Hybrid Transactional Analytical Processingï¼Œæˆ‘ä»¬çŸ¥é“OLAPã€OLTPï¼Œè€ŒHTAPå°±æ˜¯ç»“åˆä¸¤è€…åœºæ™¯ï¼Œæ—¢éœ€è¦è”æœºäº‹åŠ¡å¤„ç†æœ‰éœ€è¦è”æœºåˆ†æå¤„ç†ï¼Œè¿™ä¹Ÿæ˜¯Kuduçš„åœºæ™¯ã€‚HTAPçš„åœºæ™¯ä¸¾ä¾‹ï¼š ç®¡ç†å±‚å¸Œæœ›çœ‹åˆ°å®æ—¶çš„æ•°æ®æ±‡æ€»æŠ¥è¡¨ å®¢æœäººå‘˜å¸Œæœ›èƒ½å¤Ÿå°½å¿«è®¿é—®æŸè®¾å¤‡çš„æœ€æ–°æ•°æ®ä»¥ä¾¿å°½å¿«æ’é™¤æ•…éšœ ä¹˜è½¦çº¿è·¯æ‹¥å µç«‹åˆ»æ„ŸçŸ¥å¹¶ç«‹åˆ»è§„åˆ’çº¿è·¯ è½¦è”ç½‘ï¼Œç‰©è”ç½‘ æ€»ç»“&emsp;&emsp;Kuduâ€“Fast Analytics on Fast Data.ä¸€ä¸ªKuduå®ç°äº†æ•´ä¸ªå¤§æ•°æ®æŠ€æœ¯æ ˆä¸­è¯¸å¤šç»„ä»¶çš„åŠŸèƒ½ï¼Œæœ‰åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆå¥½æ¯”HDFSï¼‰ï¼Œæœ‰ä¸€è‡´æ€§ç®—æ³•ï¼ˆå¥½æ¯”Zookeeperï¼‰ï¼Œæœ‰Tableï¼ˆå¥½æ¯”Hiveè¡¨ï¼‰ï¼Œæœ‰Tabletï¼ˆå¥½æ¯”Hiveåˆ†åŒºï¼‰ï¼Œæœ‰åˆ—å¼å­˜å‚¨ï¼ˆå¦‚Parquetï¼‰ï¼Œæœ‰é¡ºåºå’Œéšæœºè¯»å–ï¼ˆå¦‚HBaseï¼‰ï¼Œæ‰€ä»¥çœ‹èµ·æ¥kuduåƒä¸€ä¸ªè½»é‡çº§çš„ï¼Œç»“åˆäº†HDFS+Zookeeper+Hive+Parquet+HBaseç­‰ç»„ä»¶åŠŸèƒ½å¹¶åœ¨æ€§èƒ½ä¸Šè¿›è¡Œå¹³è¡¡çš„ç»„ä»¶ã€‚å®ƒè½»æ¾åœ°è§£å†³äº†éšæœºè¯»å†™+å¿«é€Ÿåˆ†æçš„ä¸šåŠ¡åœºæ™¯ï¼Œè§£å†³äº†å®æ—¶æ•°ä»“çš„è¯¸å¤šéš¾ç‚¹ï¼ŒåŒæ—¶é™ä½äº†å­˜å‚¨æˆæœ¬å’Œè¿ç»´æˆæœ¬ã€‚&emsp;&emsp;å­¦Kuduæ—¶è®©æˆ‘æƒ³åˆ°æ›¾ç»çœ‹è¿‡çš„ç»ˆç»“è€…ç³»åˆ—ç”µå½±ï¼Œä¸‡ç‰©äº’è”ï¼Œä¸»è§’ä¸å°å¿ƒè¢«è¡—è¾¹ä¸€ä¸ªä¸èµ·çœ¼çš„ç›‘æ§æ¢å¤´æ‹åˆ°ï¼Œå°±ä¼šç«‹åˆ»å¼•æ¥ç»ˆç»“è€…çš„è¿½æ€ï¼Œä»»ä½•æœ‰ç½‘ç»œçš„åœ°æ–¹ç•™ä¸‹ä»»ä½•ç—•è¿¹éƒ½ä¼šç«‹åˆ»è¢«ç»ˆç»“è€…æ„ŸçŸ¥â€¦å¾ˆæ˜æ˜¾ï¼Œè¿™å°±æ˜¯åœ¨å¿«é€Ÿå˜åŒ–çš„æ•°æ®ä¸Šè¿›è¡Œå¿«é€Ÿåˆ†æï¼Œå¦‚æœæ²¡æœ‰Kuduï¼Œå¤§é‡çš„ç‰©è”ç½‘æ•°æ®å°±åªèƒ½æ‰¹å¤„ç†äº†ï¼Œå°±æ²¡äº†æ—¶æ•ˆæ€§ï¼Œä¸»è§’å°±å¯ä»¥éšä¾¿æµªäº†ã€‚æ²¡å‡†â€å¤©ç½‘â€ç³»ç»Ÿé‡Œå°±éƒ¨ç½²äº†KuduèŠ‚ç‚¹å‘¢ï¼Ÿï¼å“ˆå“ˆå“ˆï¼&emsp;&emsp;åœ¨å®æ—¶æ•°ä»“ã€å®æ—¶è®¡ç®—å’Œç‰©è”ç½‘è“¬å‹ƒå‘å±•çš„ä»Šå¤©ï¼Œä½ ç¡®å®šä¸å­¦ä¸€ä¸‹Kuduå—ï¼Ÿ å‚è€ƒèµ„æ–™1.ã€ŠKudu:æ„å»ºé«˜æ€§èƒ½å®æ—¶æ•°æ®åˆ†æå­˜å‚¨ç³»ç»Ÿã€‹2.Apache Kudu - Fast Analytics on Fast Data3.Kuduä¸“æ³¨äºå¤§è§„æ¨¡æ•°æ®å¿«é€Ÿè¯»å†™ï¼ŒåŒæ—¶è¿›è¡Œå¿«é€Ÿåˆ†æçš„åˆ©å™¨4.KuduåŸºç¡€å…¥é—¨5.Kuduã€Hudiå’ŒDelta Lakeçš„æ¯”è¾ƒ6.è¿Ÿåˆ°çš„Kuduè®¾è®¡è¦ç‚¹é¢é¢è§‚7.è¿Ÿåˆ°çš„Kuduè®¾è®¡è¦ç‚¹é¢é¢è§‚-å‰ç¯‡8.kudu-åˆ—å¼å­˜å‚¨ç®¡ç†å™¨-ç¬¬å››ç¯‡ï¼ˆåŸç†ç¯‡ï¼‰9.Kudu configuration reference","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Kudu","slug":"Kudu","permalink":"https://shmily-qjj.top/tags/Kudu/"},{"name":"å®æ—¶","slug":"å®æ—¶","permalink":"https://shmily-qjj.top/tags/%E5%AE%9E%E6%97%B6/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"åŸºç¡€æ•°æ®ç»“æ„ä¸ç®—æ³•","slug":"åŸºç¡€ç®—æ³•å­¦ä¹ ","date":"2020-04-27T07:45:00.000Z","updated":"2022-12-11T05:35:07.915Z","comments":true,"path":"6a894937/","link":"","permalink":"https://shmily-qjj.top/6a894937/","excerpt":"","text":"å‰è¨€ç®—æ³•ä¸æ•°æ®ç»“æ„ä»ç°åœ¨å¼€å§‹é‡è§†è¿˜æ¥å¾—åŠï¼ä¸ºä½•è¦é‡è§†ç®—æ³•ä¸æ•°æ®ç»“æ„ï¼Ÿ ç®—æ³•èƒ½åŠ›ä»£è¡¨äº†åŸºæœ¬åŠŸæ°´å¹³ï¼Œèƒ½ä»£è¡¨ä¸€ä¸ªç¨‹åºå‘˜åŠŸåº•æ˜¯å¦æ‰å®ã€‚ ç®—æ³•å’Œæ•°æ®ç»“æ„çš„æ€æƒ³ä»¥åŠæ—¶é—´å¤æ‚åº¦ç©ºé—´å¤æ‚åº¦çš„æ¦‚å¿µæ˜¯æé«˜å·¥ä½œæ•ˆç‡ï¼Œå­¦ä¹ èƒ½åŠ›å’Œæˆé•¿æ½œåŠ›çš„é‡è¦é€”å¾„ã€‚ ç®—æ³•èƒ½åŠ›æ˜¯è®¾è®¡ä¸€ä¸ªç³»ç»Ÿ(é€ è½®å­)çš„é‡è¦åŸºç¡€ã€‚ ä¸‹å±‚åŸºç¡€å†³å®šä¸Šå±‚å»ºç­‘ï¼ç®—æ³•ä¸æ•°æ®ç»“æ„æ˜¯æˆ‘çš„è–„å¼±é¡¹ï¼Œéœ€è¦å¤šåšæ€»ç»“ï¼å°½é‡å¯¹æ¯ä¸ªæ•°æ®ç»“æ„çš„ç»å…¸é¢˜ç›®åšæ€»ç»“ï¼Œå°½é‡ä½¿ç”¨LeetCodeåŸé¢˜æ¥å¸®åŠ©ç†è§£ã€‚LeetCodeä¸»é¡µå…ˆè´´è¿™é‡Œ:**Shmilyqjjçš„åŠ›æ‰£ä¸»é¡µ**ï¼Œå¤§å®¶äº’ç›¸ç›‘ç£å­¦ä¹ å‘¦ï¼ åŸºæœ¬æ¦‚å¿µå­¦ä¹ ç®—æ³•æœ‰å¸®åŠ©çš„ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚ é¡ºåºå­˜å‚¨ä¸é“¾å¼å­˜å‚¨ç‰©ç†ç»“æ„ï¼šæ˜¯æŒ‡æ•°æ®çš„é€»è¾‘ç»“æ„åœ¨è®¡ç®—æœºä¸­çš„å­˜å‚¨å½¢å¼ã€‚é€»è¾‘ç»“æ„ï¼šæ˜¯æŒ‡æ•°æ®å¯¹è±¡ä¸­æ•°æ®å…ƒç´ ä¹‹é—´çš„äº’ç›¸å…³ç³»ã€‚ åŸºç¡€æ•°æ®ç»“æ„æ•°æ®ç»“æ„æ˜¯ç®—æ³•çš„åŸºçŸ³ã€‚è¦åšåˆ°å¯¹ç®—æ³•çš„èä¼šè´¯é€šå’Œä¸¾ä¸€åä¸‰ï¼Œç†Ÿæ‚‰å„ç§æ•°æ®ç»“æ„æ˜¯å¿…å¤‡çš„ã€‚ä¼˜ç§€çš„ç®—æ³•å–å†³äºé‡‡ç”¨å“ªç§æ•°æ®ç»“æ„ã€‚é‡ç‚¹ï¼šç†Ÿæ‚‰æ¯ç§æ•°æ®ç»“æ„ä¼˜ç¼ºç‚¹ï¼Œåº”ç”¨åœºæ™¯ï¼Œç†Ÿç»ƒæŒæ¡å…¶æ€æƒ³å¹¶çµæ´»è¿ç”¨ã€‚ æ•°ç»„å’Œå­—ç¬¦ä¸²ç‰¹ç‚¹ï¼šé€»è¾‘ç»“æ„ä¸ç‰©ç†ç»“æ„ä¸€è‡´ä¼˜ç‚¹ï¼šæ„å»ºç®€å•ï¼Œç´¢å¼•æŸä¸ªå…ƒç´ çš„å¤æ‚åº¦O(1)ç¼ºç‚¹ï¼šå¿…é¡»è¿ç»­åˆ†é…ä¸€æ®µå†…å­˜ï¼ŒæŸ¥è¯¢è¿‡ç¨‹éå†æ—¶é—´å¤æ‚åº¦O(n)ï¼Œåˆ é™¤è¿‡ç¨‹æ—¶é—´å¤æ‚åº¦O(n)åœºæ™¯ï¼šå…ƒç´ ä¸ªæ•°ç¡®å®šï¼Œç»å¸¸æŒ‰ç´¢å¼•æŸ¥è¯¢ï¼Œä¸ç»å¸¸æ’å…¥å’Œåˆ é™¤ ç»å…¸é¢˜ç›®ï¼šç¿»è½¬å­—ç¬¦ä¸²å­—æ¯å¼‚ä½è¯ é“¾è¡¨åˆ†ä¸ºå•å‘é“¾è¡¨å’ŒåŒå‘é“¾è¡¨ä¼˜ç‚¹ï¼šèƒ½çµæ´»åˆ†é…å†…å­˜ç©ºé—´ï¼Œæ’å…¥å’Œåˆ é™¤å…ƒç´ æ•ˆç‡é«˜O(1)ç¼ºç‚¹ï¼šä¸èƒ½é€šè¿‡ä¸‹æ ‡è¯»å–ï¼Œè¯»å–ç¬¬nä¸ªä½ç½®å…ƒç´ å¤æ‚åº¦O(n)åœºæ™¯ï¼šå…ƒç´ ä¸ªæ•°ä¸ç¡®å®šï¼Œç»å¸¸æ’å…¥å’Œåˆ é™¤ï¼Œä¸ç»å¸¸æŸ¥è¯¢ ç»å…¸é¢˜ç›®ï¼šå¿«æ…¢æŒ‡é’ˆ-&gt;åˆ¤æ–­æ˜¯å¦æœ‰ç¯å¿«æ…¢æŒ‡é’ˆ-&gt;ç¿»è½¬é“¾è¡¨å¿«æ…¢æŒ‡é’ˆ-&gt;å¯»æ‰¾å€’æ•°ç¬¬Kå…ƒç´ å¿«æ…¢æŒ‡é’ˆ-&gt;æ‰¾é“¾è¡¨ä¸­é—´ä½ç½®æ„å»ºè™šå‡é“¾è¡¨å¤´ æ ˆåè¿›å…ˆå‡º(LIFO),å¯ä»¥é‡‡ç”¨å•é“¾è¡¨å®ç°å¤æ‚åº¦O(1)ï¼Œè™½ç„¶ä¹Ÿå¯ä»¥ç”¨æ•°ç»„å®ç°ä½†æ—¶é—´å¤æ‚åº¦å¯èƒ½è¾ƒå¤§åœºæ™¯ï¼šè§£å†³é—®é¢˜æ—¶åªå…³å¿ƒæœ€è¿‘ä¸€æ¬¡æ“ä½œï¼Œè§£å†³å®Œåè¦æ‰¾ä¹‹å‰çš„æ“ä½œã€‚ ç»å…¸é¢˜ç›®ï¼šåŒ¹é…æ‹¬å·ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰æ•ˆ é˜Ÿåˆ—å…ˆè¿›å…ˆå‡º(FIFO),å¯ä»¥é‡‡ç”¨åŒé“¾è¡¨å®ç°ã€‚åœºæ™¯ï¼šæŒ‰ä¸€å®šé¡ºåºå¤„ç†è¿‡æ¥çš„æ•°æ®ã€‚ ç»å…¸é¢˜ç›®ï¼šå¹¿åº¦ä¼˜å…ˆæœç´¢ åŒç«¯é˜Ÿåˆ—å’Œæ™®é€šé˜Ÿåˆ—çš„åŒºåˆ«æ˜¯ï¼šå¯ä»¥åœ¨é˜Ÿå¤´é˜Ÿå°¾éƒ½å¯ä»¥æŸ¥çœ‹ï¼Œæ·»åŠ å’Œåˆ é™¤æ•°æ®ï¼Œå¤æ‚åº¦éƒ½ä¸ºO(1)ï¼Œå¯ä»¥é‡‡ç”¨åŒé“¾è¡¨å®ç°ã€‚åœºæ™¯ï¼šå®ç°é•¿åº¦åŠ¨æ€å˜åŒ–çš„çª—å£æˆ–è¿ç»­åŒºé—´ï¼ŒåŠ¨æ€çª—å£ã€‚ ç»å…¸é¢˜ç›®ï¼šè¿”å›æ»‘åŠ¨çª—å£æœ€å¤§å€¼ æ ‘å……åˆ†åº”ç”¨é€’å½’ï¼Œæ ‘çš„é—®é¢˜åŸºæœ¬éƒ½ä¸é€’å½’æœ‰å…³é¢è¯•å¸¸è€ƒï¼šæ™®é€šäºŒå‰æ ‘ï¼Œå¹³è¡¡äºŒå‰æ ‘ï¼Œå®Œå…¨äºŒå‰æ ‘ï¼ŒäºŒå‰æœç´¢æ ‘ï¼Œå››å‰æ ‘ï¼Œå¤šå‰æ ‘éœ€è¦äº†è§£ï¼šçº¢é»‘æ ‘ ï¼ŒAVLè‡ªå¹³è¡¡äºŒå‰æœç´¢æ ‘ éå†æ–¹æ³•ï¼šå‰åºéå†ï¼šæ ¹-&gt;å·¦å­æ ‘-&gt;å³å­æ ‘ä¸­åºéå†ï¼šå·¦å­æ ‘-&gt;æ ¹-&gt;å³å­æ ‘ååºéå†ï¼šå·¦å­æ ‘-&gt;å³å­æ ‘-&gt;æ ¹æ·±åº¦ä¼˜å…ˆéå†DFSï¼šåŒ…å«ä»¥ä¸Šä¸‰ç§å¹¿åº¦ä¼˜å…ˆéå†BFSï¼šå³å±‚æ¬¡éå†ï¼Œæ¯ä¸€å±‚ä»å·¦å‘å³è¾“å‡º ç»å…¸é¢˜ç›®ï¼šäºŒå‰æœç´¢æ ‘çš„ç¬¬Kä¸ªæœ€å°å…ƒç´  å¦‚ä½•å­¦ä¹ æ•°æ®ç»“æ„ä¸ç®—æ³•å¥½çš„å­¦ä¹ æ–¹æ³•å¾€å¾€èƒ½èµ·åˆ°äº‹åŠåŠŸå€çš„æ•ˆæœï¼Œè¿™é‡Œæ ¹æ®ç®—æ³•å¤§ä½¬ä»¬çš„å­¦ä¹ ç»å†æ€»ç»“ä¸€ä¸‹ç®—æ³•å­¦ä¹ æ–¹æ³•ï¼Œè­¦ç¤ºè‡ªå·±ç£ä¿ƒè‡ªå·±å…»æˆæ­£ç¡®å­¦ä¹ æ–¹æ³•å§ã€‚ å‚è€ƒé“¾æ¥æˆ‘æ˜¯å¦‚ä½•å­¦ä¹ æ•°æ®ç»“æ„ä¸ç®—æ³•çš„ æ–œä½“æ–‡æœ¬æ–œä½“æ–‡æœ¬ç²—ä½“æ–‡æœ¬ç²—ä½“æ–‡æœ¬ç²—æ–œä½“æ–‡æœ¬ç²—æ–œä½“æ–‡æœ¬å¸¦ä¸‹åˆ’çº¿æ–‡æœ¬","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"ç®—æ³•","slug":"ç®—æ³•","permalink":"https://shmily-qjj.top/tags/%E7%AE%97%E6%B3%95/"},{"name":"æ•°æ®ç»“æ„","slug":"æ•°æ®ç»“æ„","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"MySQLç´¢å¼•åŸç†æ·±å…¥","slug":"MySQLç´¢å¼•åŸç†æ·±å…¥","date":"2020-03-24T02:16:00.000Z","updated":"2022-12-11T05:35:07.908Z","comments":true,"path":"7c15e85/","link":"","permalink":"https://shmily-qjj.top/7c15e85/","excerpt":"","text":"MySQLç´¢å¼•åŸç†æ·±å…¥ç´¢å¼•å¯ä»¥å¤§å¤§æé«˜Mysqlæ£€ç´¢é€Ÿåº¦ï¼Œä¸ºä»€ä¹ˆèƒ½æé«˜ï¼Œæ€ä¹ˆåšåˆ°çš„ï¼Ÿè¿™äº›ç»†èŠ‚å¿…é¡»æ·±å…¥å­¦ä¹ å’Œåˆ†æï¼Œæ‰èƒ½å¯¹æŠ€æœ¯è¿ç”¨äº†å¦‚æŒ‡æŒã€‚ä»Šå¤©æ¥å­¦ä¹ ä¸€ä¸‹Mysqlçš„ç´¢å¼•åŸç†ä¸åº•å±‚å­˜å‚¨é€‰å‹ï¼Œä¸ºäº†èƒ½å¤Ÿå¯¹Mysqlæœ‰æ›´æ·±å…¥çš„äº†è§£ã€‚ ç´¢å¼•å®šä¹‰ç´¢å¼•æ˜¯å¯¹æ•°æ®åº“è¡¨ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„å€¼è¿›è¡Œæ’åºçš„ä¸€ç§ç»“æ„ï¼Œä½¿ç”¨ç´¢å¼•å¯å¿«é€Ÿè®¿é—®æ•°æ®åº“è¡¨ä¸­çš„ç‰¹å®šä¿¡æ¯ã€‚ç´¢å¼•ç›¸å½“äºæˆ‘ä»¬çœ‹ä¹¦çš„ç›®å½•ã€‚ ä¼˜ç‚¹1.å¿«é€Ÿæ£€ç´¢æ•°æ®2.ä¿è¯æ•°æ®è®°å½•å”¯ä¸€æ€§3.å®ç°è¡¨ä¸è¡¨ä¹‹é—´çš„å‚ç…§å®Œæ•´æ€§4.ä½¿ç”¨ORDER BY/GROUP BYå­å¥è¿›è¡Œæ•°æ®æ£€ç´¢æ—¶ï¼Œåˆ©ç”¨ç´¢å¼•å¯ä»¥å‡å°‘æ’åºå’Œåˆ†ç»„çš„æ—¶é—´ ç¼ºç‚¹1.ç´¢å¼•éœ€è¦å ç£ç›˜ç‰©ç†ç©ºé—´2.å¢åˆ æ”¹æ“ä½œæ—¶ç´¢å¼•ä¹Ÿè¦åŠ¨æ€åœ°ç»´æŠ¤ï¼Œæœ‰æ€§èƒ½å¼€é”€3.åˆ›å»ºç´¢å¼•è€—æ—¶ï¼Œæ•°æ®é‡è¶Šå¤§è€—æ—¶ä¹Ÿè¶Šå¤§ åˆ†ç±»1.æ™®é€šç´¢å¼•ï¼šæ— å”¯ä¸€æ€§é™åˆ¶2.å”¯ä¸€ç´¢å¼•ï¼šUNIQUEï¼Œæœ‰å”¯ä¸€æ€§é™åˆ¶3.ä¸»é”®ç´¢å¼•ï¼šå”¯ä¸€ç´¢å¼•çš„ç‰¹æ®Šç±»å‹ï¼Œåœ¨ä¸»é”®ä¸Šåˆ›å»ºç´¢å¼•4.å€™é€‰ç´¢å¼•ï¼šå”¯ä¸€æ€§ï¼Œåˆ‡å†³å®šè®°å½•çš„å¤„ç†é¡ºåº5.èšé›†ç´¢å¼•ï¼šClustered Indexèšç°‡ç´¢å¼•ï¼Œç´¢å¼•åˆ—çš„é”®å€¼çš„ç‰©ç†é¡ºåºä¸é€»è¾‘é¡ºåºç›¸åŒ6.éèšé›†ç´¢å¼•ï¼šNon-Clustered Indexéèšç°‡ç´¢å¼•ï¼Œç´¢å¼•åˆ—çš„é”®å€¼çš„ç‰©ç†é¡ºåºä¸é€»è¾‘é¡ºåºæ— å…³7.å…¨æ–‡ç´¢å¼•ï¼šä¸»è¦é’ˆå¯¹æ–‡æœ¬çš„å†…å®¹è¿›è¡Œåˆ†è¯ï¼ŒåŠ å¿«æŸ¥è¯¢é€Ÿåº¦8.è”åˆç´¢å¼•ï¼šå¤šåˆ—ç»„æˆçš„ç´¢å¼•ï¼ŒæŸ¥è¯¢æ•ˆç‡æå‡é«˜äºå¤šä¸ªå•åˆ—ç´¢å¼•åˆå¹¶çš„æ•ˆç‡ åº”ç”¨åœºæ™¯1.åœ¨ç»å¸¸æœç´¢çš„åˆ—ä¸Šåˆ›å»ºç´¢å¼•2.åœ¨ä¸»é”®ä¸Šåˆ›å»ºç´¢å¼•3.åœ¨ç”¨æ¥JOINçš„åˆ—ä¸Šåˆ›å»ºç´¢å¼•4.åœ¨ç»å¸¸é€šè¿‡WHEREæ ¹æ®èŒƒå›´æ£€ç´¢çš„åˆ—ä¸Šåˆ›å»ºç´¢å¼•5.åœ¨ç»å¸¸GROUP BY/ORDER BYçš„åˆ—ä¸Šåˆ›å»ºç´¢å¼•6.åœ¨ç»å¸¸DISTINCTçš„åˆ—ä¸Šåˆ›å»ºç´¢å¼• ç´¢å¼•å­—æ®µè¦æ±‚1.åˆ—å€¼çš„å”¯ä¸€æ€§å¤ªå°ä¸é€‚åˆå»ºç´¢å¼•2.åˆ—å€¼å¤ªé•¿ä¸é€‚åˆå»ºç´¢å¼•3.æ›´æ–°é¢‘ç¹çš„åˆ—ä¸é€‚åˆå»ºç´¢å¼• ç´¢å¼•å¤±æ•ˆ1.LIKEçš„ä½¿ç”¨ï¼ˆLIKE XXX%å¯ä»¥ç”¨ç´¢å¼•ï¼Œä½†LIKE %xxxä¸èƒ½ï¼‰2.éƒ¨åˆ†æ“ä½œç¬¦ï¼ˆ**&lt;,&lt;=,=,&gt;,&gt;=,BETWEEN,INå¯ä»¥ç”¨ç´¢å¼•ï¼Œä½†&lt;&gt;,not in,!=ä¸èƒ½**ï¼‰3.åˆ¤ç©ºæ“ä½œï¼ˆis nullæˆ–is not nullï¼‰4.intç±»å‹å­—æ®µï¼ˆå¦‚æ‰‹æœºå·æ²¡ç”¨varcharå­˜ï¼ŒæŸ¥186å¼€å¤´çš„ï¼Œä¸èƒ½ï¼‰5.è”åˆç´¢å¼•ï¼ˆè®¾ç½®äº†col1å’Œcol2ä¸¤ä¸ªå­—æ®µè”åˆç´¢å¼•ï¼ŒWHERE col1=â€™xxxâ€™æˆ–WHERE col1=â€™xxxâ€™ AND col2=â€™xxxâ€™æˆ–WHERE col2=â€™xxxâ€™ AND col1=â€™xxxâ€™éƒ½å¯ç”¨ç´¢å¼•ï¼Œä½†WHERE col2=â€™xxxâ€™ä¸èƒ½ï¼‰6.å¯¹ç´¢å¼•åˆ—æ“ä½œï¼ˆè®¡ç®—ã€å‡½æ•°ã€è‡ªåŠ¨ç±»å‹è½¬æ¢ã€æ‰‹åŠ¨ç±»å‹è½¬æ¢éƒ½ä¼šä½¿ç´¢å¼•å¤±æ•ˆï¼‰7.SELECT *ï¼ˆå°½é‡ä½¿ç”¨è¦†ç›–ç´¢å¼•ï¼Œå°½é‡å–ç”¨åˆ°çš„å­—æ®µå€¼è€Œéä½¿ç”¨æ˜Ÿå·,è¿™æ ·WHEREçš„æ—¶å€™è¦†ç›–ç´¢å¼•æ•ˆç‡é«˜ï¼‰8.å­—ç¬¦ä¸²ä¸åŠ å•å¼•å·å¼•èµ·ç´¢å¼•å¤±æ•ˆ9.å°½é‡é¿å…ç´¢å¼•åˆ—æœ‰nullå€¼ Mysqlç´¢å¼•æ•°æ®ç»“æ„é€‰å‹è¯¥éƒ¨åˆ†ä¼šè¯¦ç»†è¯´æ˜ï¼šç´¢å¼•æ•°æ®ç»“æ„çš„é€‰å‹è¿‡ç¨‹ä»¥åŠå„è‡ªçš„ä¼˜ç¼ºç‚¹Bæ ‘ä¸B+æ ‘çš„åŒºåˆ«ä¸ºä»€ä¹ˆä»¥B+æ ‘ä½œä¸ºMysqlçš„ç´¢å¼•æ•°æ®ç»“æ„Innodbå¼•æ“å’ŒMyISAMå¼•æ“çš„åŒºåˆ«ä»¥åŠç´¢å¼•å®ç°å’Œå­˜å‚¨åŒºåˆ«èšç°‡ç´¢å¼•ä¸éèšç°‡ç´¢å¼•åŒºåˆ« æ•°æ®ç»“æ„é€‰å‹è¿‡ç¨‹è¿‡ç¨‹ï¼šå“ˆå¸Œè¡¨-&gt;äºŒå‰æŸ¥æ‰¾æ ‘-&gt;çº¢é»‘æ ‘-&gt;äºŒå‰å¹³è¡¡æ ‘AVL-&gt;Bæ ‘-&gt;B+æ ‘ å“ˆå¸Œè¡¨å“ˆå¸Œç®—æ³•æŠŠä»»æ„çš„keyå˜æ¢æˆå›ºå®šé•¿åº¦çš„keyåœ°å€ã€‚æ€§èƒ½ä¸é”™ï¼Œä½†æ˜¯æœ‰å“ˆå¸Œå†²çªé—®é¢˜ï¼Œä¸€èˆ¬ç”¨é“¾åœ°å€æ³•æ¥è§£å†³ï¼ˆç±»ä¼¼HashMapï¼‰ã€‚è¿™æ ·æŸ¥æ•°æ®çš„æ—¶å€™å…ˆè®¡ç®—Hashï¼Œç„¶åéå†é“¾è¡¨ï¼Œç›´åˆ°æ‹¿åˆ°keyã€‚æ—¶é—´å¤æ‚åº¦O(1),çœ‹æ¥å¾ˆç†æƒ³ã€‚ä½†æ˜¯ä¸ºä»€ä¹ˆæ²¡ç”¨å“ˆå¸Œï¼Ÿå¦‚æœç”¨å“ˆå¸Œè¡¨åšç´¢å¼•çš„æ•°æ®ç»“æ„ï¼Œselect * from tb where id &gt; 1è¿™æ ·çš„èŒƒå›´æŸ¥æ‰¾åœºæ™¯ï¼Œå°±è¦æŠŠç´¢å¼•æ•°æ®å…¨éƒ¨åŠ è½½åˆ°å†…å­˜å†ç­›é€‰ï¼Œå¤ªæ…¢ã€‚è™½ç„¶Hashåšç´¢å¼•çš„æ•°æ®ç»“æ„å¯ä»¥å¿«é€Ÿå®šä½keyï¼Œä½†æ²¡æ³•åšåˆ°é«˜æ•ˆçš„èŒƒå›´æŸ¥æ‰¾ã€‚ps:å½“ç„¶ï¼Œå¦‚æœä¸šåŠ¡æ˜¯ç»å¸¸ä½¿ç”¨whereæ¡ä»¶å•æ¡æŸ¥è¯¢æ•°æ®ï¼ŒHashç´¢å¼•æ•ˆç‡æ›´é«˜å¤æ‚åº¦O(1)ã€‚ä½¿ç”¨Hashç´¢å¼•ï¼ŒInnoDBå’ŒMyISAMä¸æ”¯æŒï¼Œå¯ç”¨MEMORYï¼ŒNDBå¼•æ“ã€‚ äºŒå‰æŸ¥æ‰¾æ ‘BinarySearchTreeï¼ˆBSTï¼‰æ˜¯æ”¯æŒæ•°æ®å¿«é€ŸæŸ¥æ‰¾çš„æ•°æ®ç»“æ„ï¼Œå¤æ‚åº¦O(log2n)~O(n)ä¹‹é—´,ä¹Ÿå¯ä»¥é«˜é€Ÿæ£€ç´¢æ•°æ®èƒ½ä¸èƒ½è§£å†³èŒƒå›´æŸ¥æ‰¾å‘¢ï¼Ÿèƒ½ï¼æ¯”å¦‚æˆ‘æ‰¾id &gt; 3ï¼Œæˆ‘åªè¦æ‰¾åˆ°æ¯”å®ƒå¤§çš„æ ¹èŠ‚ç‚¹å’Œå³å­æ ‘å³å¯ã€‚é‚£ä¸ºå•¥äºŒå‰æŸ¥æ‰¾æ ‘ä¸èƒ½åšç´¢å¼•çš„æ•°æ®ç»“æ„ï¼Ÿå› ä¸ºå¦‚æœäºŒå‰æ’åºæ ‘æ˜¯å¹³è¡¡çš„ï¼Œåˆ™nä¸ªèŠ‚ç‚¹çš„äºŒå‰æ’åºæ ‘çš„é«˜åº¦ä¸ºlog2(n+1),å…¶æŸ¥æ‰¾æ•ˆç‡ä¸ºO(log2n)ï¼Œè¿‘ä¼¼äºæŠ˜åŠæŸ¥æ‰¾ã€‚å¦‚æœäºŒå‰æ’åºæ ‘å®Œå…¨ä¸å¹³è¡¡ï¼Œåˆ™å…¶æ·±åº¦å¯è¾¾åˆ°nï¼ŒæŸ¥æ‰¾æ•ˆç‡ä¸ºO(n)ï¼Œé€€åŒ–ä¸ºé¡ºåºæŸ¥æ‰¾ã€‚è€Œæ•°æ®åº“ä¸­ç»å¸¸æœ‰ä»¥è‡ªå¢idä¸ºä¸»é”®ç´¢å¼•çš„åœºæ™¯ï¼Œå¿…ç„¶ä¼šçº¿æ€§æŸ¥æ‰¾ï¼Œæ€§èƒ½å¤ªä½ã€‚ çº¢é»‘æ ‘é€šè¿‡è‡ªåŠ¨è°ƒæ•´æ ‘å½¢æ€è®©äºŒå‰æ ‘ä¿æŒåŸºæœ¬å¹³è¡¡ï¼Œå¤æ‚åº¦O(logn)ï¼Œå› ä¸ºåŸºæœ¬å¹³è¡¡ï¼ŒæŸ¥è¯¢æ•ˆç‡ä¸ä¼šæ˜æ˜¾é™ä½ï¼Œä¸å­˜åœ¨O(n)çš„æƒ…å†µã€‚åƒç“œç¾¤ä¼—ï¼šé‚£å°±ç”¨è¿™ä¸ªåšç´¢å¼•æ•°æ®ç»“æ„å§ï¼ä¸‡ä¸‡ä¸å¯ç”¨çº¢é»‘æ ‘åšç´¢å¼•çš„æ•°æ®ç»“æ„ï¼è¿˜æ˜¯è‡ªå¢idä¸ºä¸»é”®ç´¢å¼•çš„æƒ…å†µï¼Œå¦‚æœçº¢é»‘æ ‘æŒ‰é¡ºåºæ’å…¥æ•°æ®ï¼Œæ•´ä¸ªçº¢é»‘æ ‘ä¼šæ˜æ˜¾å³å€¾ï¼ŒæŸ¥è¯¢æ•ˆç‡ä¼šæ˜æ˜¾é™ä½ã€‚åƒæˆ‘è¿™ç§æ•°æ®ç»“æ„æ¸£æ¸£ï¼Œé€è‡ªå·±ä¸€ä¸ªå®è´ï¼šçº¢é»‘æ ‘ç®—æ³•å›¾å½¢æ¨¡æ‹Ÿ AVLæ ‘AVLæ ‘ï¼Œä¹Ÿæ˜¯é€šè¿‡è°ƒæ•´å½¢æ€ä¿æŒäºŒå‰æ ‘å¹³è¡¡ï¼Œå®ƒè™½ç„¶åœ¨è°ƒæ•´å½¢æ€æ—¶ä¼šæœ‰æ›´å¤šæ€§èƒ½å¼€é”€ï¼Œä½†å®ƒç»å¯¹å¹³è¡¡ã€‚å®ƒèƒ½æ ¹æœ¬è§£å†³çº¢é»‘æ ‘çš„å³å€¾é—®é¢˜ã€‚å¤æ‚åº¦O(logn)ã€‚ é‚£AVLæ ‘è¿™ä¹ˆå¥½ï¼Œä¸ºå•¥è¿˜æ˜¯ä¸èƒ½ç”¨äºç´¢å¼•æ•°æ®ç»“æ„ï¼ŸAVLæ ‘æ¯ä¸ªèŠ‚ç‚¹åªèƒ½å­˜ä¸€ä¸ªæ•°æ®ï¼Œæ¯æ¬¡æ¯”è¾ƒåªèƒ½åŠ è½½ä¸€ä¸ªæ•°æ®åˆ°å†…å­˜ï¼ŒæŸ¥è¯¢è¾ƒæ·±çš„AVLæ ‘èŠ‚ç‚¹ï¼Œå°±è¦æœ‰å¤šæ¬¡çš„ç£ç›˜IOå¼€é”€ï¼Œç£ç›˜IOæ˜¯æ•°æ®åº“ç“¶é¢ˆï¼Œè¿™æ ·è‚¯å®šä¸åˆç†çš„å‘€ï¼å¯¹ç£ç›˜IOçš„ä¼˜åŒ–æ–¹æ¡ˆå°±æ˜¯ä¸€æ¬¡å°½å¯èƒ½å¤šè¯»æˆ–è€…å¤šå†™æ•°æ®ï¼Œè¯»1bå’Œè¯»1kbé€Ÿåº¦åŸºæœ¬ä¸€æ ·çš„ï¼Œå¸Œæœ›ç£ç›˜èƒ½ä¸€æ¬¡åŠ è½½æ›´å¤šæ•°æ®è¿›å†…å­˜ï¼Œè¿™å°±æ˜¯Bæ ‘ï¼ŒB+æ ‘åŸç†äº†ã€‚ Bæ ‘ä¸B+æ ‘Bæ ‘ï¼šåˆåå¤šè·¯æŸ¥æ‰¾æ ‘ï¼Œç‰¹ç‚¹ï¼šâ‘ èŠ‚ç‚¹æ•°æ®é€’å¢ï¼Œéµå¾ªå·¦å°å³å¤§â‘¡Mé˜¶Bæ ‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šå¯ä»¥æœ‰Mä¸ªå­èŠ‚ç‚¹â‘¢æ ¹èŠ‚ç‚¹è‡³å°‘æœ‰ä¸¤ä¸ªå„¿å­â‘£é™¤æ ¹ç»“ç‚¹ä¹‹å¤–çš„æ‰€æœ‰éå¶å­ç»“ç‚¹è‡³å°‘æœ‰ceil(M/2)ä¸ªå­èŠ‚ç‚¹(ceil(2.1) = 3)â‘¤æ‰€æœ‰å¶å­èŠ‚ç‚¹éƒ½åœ¨åŒä¸€å±‚æ¬¡Bæ ‘ä»£æ›¿AVLæ ‘ï¼šè®©æ¯ä¸ªèŠ‚ç‚¹å­˜çš„keyæ•°ç›®é€‚å½“å¢åŠ ï¼Œå³å¢åŠ Mï¼ˆBæ ‘çš„é˜¶æ•°ï¼‰ï¼Œç£ç›˜è¯»å–æ¬¡æ•°å¤§å¤§é™ä½ï¼Œå°½å¯èƒ½å‡å°‘ç£ç›˜IOï¼ŒåŠ å¿«æ£€ç´¢é€Ÿåº¦ï¼Œè¿˜èƒ½æ”¯æŒèŒƒå›´æŸ¥æ‰¾ã€‚B+æ ‘ï¼šä¸Bæ ‘åŒºåˆ«ï¼šä¸€æ˜¯Bæ ‘æ¯ä¸ªèŠ‚ç‚¹ï¼ˆåŒ…æ‹¬éå¶å­èŠ‚ç‚¹ï¼‰éƒ½å­˜æ•°æ®ï¼ŒB+æ ‘éå¶å­èŠ‚ç‚¹æœ‰ç´¢å¼•ä½œç”¨ï¼Œè€Œæ•°æ®å­˜åœ¨å¶å­èŠ‚ç‚¹ â€“&gt; Bæ ‘çš„æ¯ä¸ªèŠ‚ç‚¹å­˜ä¸äº†å¤ªå¤šæ•°æ®ï¼ŒB+æ ‘æ¯ä¸ªå¶å­èŠ‚ç‚¹èƒ½å­˜å¾ˆå¤šç´¢å¼•ï¼ˆåœ°å€ï¼‰ã€‚æ‰€ä»¥B+æ ‘é«˜åº¦æ›´ä½ï¼Œå‡å°‘äº†ç£ç›˜IOã€‚äºŒæ˜¯Bæ ‘èŠ‚ç‚¹ä¹‹é—´æ²¡ç´¢å¼•ï¼ŒB+ç›¸é‚»å¶å­èŠ‚ç‚¹ä¹‹é—´æœ‰ç´¢å¼•æŒ‡é’ˆ â€“&gt; WHEREèŒƒå›´æŸ¥è¯¢æ€§èƒ½å¾ˆå¥½ã€‚ä¸‰æ˜¯B+æ ‘çš„æŸ¥è¯¢æ•ˆç‡æ›´ç¨³å®šï¼Œå› ä¸ºæ•°æ®éƒ½å­˜åœ¨å¶å­èŠ‚ç‚¹ï¼ŒæŸ¥æ•°æ®çš„æ“ä½œæ¬¡æ•°ç›¸åŒã€‚ ç»¼ä¸Šï¼ŒMysqlåŸºäºB+æ ‘å®ç°çš„ç´¢å¼•ã€‚ InnoDBä¸MyISAMçš„åŒºåˆ« InnoDB MyISAM é»˜è®¤æ”¯æŒACID ä¸æ”¯æŒACID æ”¯æŒå¤–é”® ä¸æ”¯æŒå¤–é”® æ€§èƒ½å¥½ æ€§èƒ½å¥½äºInnodb å¿…é¡»æœ‰ä¸»é”® å¯æ²¡æœ‰ä¸»é”® æ•°æ®ä¸ç´¢å¼•å­˜æ”¾åœ¨ä¸€èµ· æ•°æ®ä¸ç´¢å¼•åˆ†å¼€å­˜æ”¾ èšé›†ç´¢å¼•æ–¹å¼ éèšé›†ç´¢å¼•æ–¹å¼ æ”¯æŒè¡¨çº§ã€è¡Œçº§(é»˜è®¤)é” æ”¯æŒè¡¨çº§é” å´©æºƒæ˜“æ¢å¤ å´©æºƒéš¾æ¢å¤ èšé›†ç´¢å¼•ä¸éèšé›†ç´¢å¼•(InnoDBä¸MyISAMå®ç°ç´¢å¼•çš„åŒºåˆ«)mysql&gt; show global variables like &quot;%datadir%&quot;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------+ mysql&gt; create table innodb_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=InnoDB DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.02 sec) mysql&gt; create table myisam_table(id varchar(255) not null primary key,name varchar(255) not null) -&gt; ENGINE=myisam DEFAULT CHARSET=utf8; Query OK, 0 rows affected (0.00 sec) ll /var/lib/mysql/db_name/ æ€»ç”¨é‡ 128 -rw-rw----. 1 mysql mysql 65 3æœˆ 24 04:30 db.opt -rw-rw----. 1 mysql mysql 8586 3æœˆ 24 04:31 innodb_table.frm -rw-rw----. 1 mysql mysql 98304 3æœˆ 24 04:31 innodb_table.ibd -rw-rw----. 1 mysql mysql 8586 3æœˆ 24 04:33 myisam_table.frm -rw-rw----. 1 mysql mysql 0 3æœˆ 24 04:33 myisam_table.MYD -rw-rw----. 1 mysql mysql 4096 3æœˆ 24 04:33 myisam_table.MYI ä»å»ºè¡¨åç”Ÿæˆçš„æ–‡ä»¶å¯çœ‹å‡ºï¼ŒInnoDBç”Ÿæˆfrm(å»ºè¡¨è¯­å¥)å’Œibd(æ•°æ®+ç´¢å¼•)ï¼Œè€ŒMyISAMç”Ÿæˆfrm(å»ºè¡¨è¯­å¥),MYD(æ•°æ®æ–‡ä»¶)å’ŒMYI(ç´¢å¼•æ–‡ä»¶)ã€‚MyISAMå¼•æ“æŠŠæ•°æ®å’Œç´¢å¼•åˆ†å¼€æˆæ•°æ®æ–‡ä»¶å’Œç´¢å¼•æ–‡ä»¶ä¸¤ä¸ªæ–‡ä»¶ï¼Œè¿™å«åšéèšé›†ç´¢å¼•æ–¹å¼ã€‚Innodb å¼•æ“æŠŠæ•°æ®å’Œç´¢å¼•æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶é‡Œäº†ï¼Œè¿™å«åšèšé›†ç´¢å¼•æ–¹å¼ã€‚ æ›´è¯¦ç»†ä¸€ç‚¹çš„è§£é‡Šï¼šèšé›†ç´¢å¼•ç‰©ç†é¡ºåºä¸é€»è¾‘é¡ºåºä¸€è‡´ï¼Œéèšé›†ç´¢å¼•çš„ç‰©ç†é¡ºåºä¸é€»è¾‘é¡ºåºä¸ä¸€è‡´ã€‚éèšé›†ç´¢å¼•çš„å¶å­èŠ‚ç‚¹å­˜keyå’Œå¯¹åº”åœ°å€ï¼Œä¸å­˜æ•°æ®ï¼›èšé›†ç´¢å¼•çš„å¶å­èŠ‚ç‚¹å­˜keyå’Œå¯¹åº”çš„valueï¼Œvalueå†…å­˜åœ°å€æ˜¯è¿ç»­çš„ ä¸ºä»€ä¹ˆMyISAMæ¯”InnoDBå¿«ï¼Ÿä¸Šé¢æˆ‘ä»¬å·²ç»æåˆ°InnoDBä½¿ç”¨èšé›†ç´¢å¼•ï¼ŒMyISAMä½¿ç”¨éèšé›†ç´¢å¼•ã€‚ä¸¤ç§å¼•æ“çš„æ•°æ®ç»„ç»‡æ–¹å¼ä¸åŒã€‚å¦‚å›¾æ˜¯ä¸¤ç§å¼•æ“ç»„ç»‡æ•°æ®çš„æ–¹å¼æŸ¥è¯¢æ—¶InnoDBéœ€è¦é€šè¿‡ä¸»é”®ç´¢å¼•æ ‘å…ˆæ‹¿åˆ°ä¸»é”®å€¼åå†å»è¾…åŠ©ç´¢å¼•æ ‘æ‹¿åˆ°æ•´æ¡è®°å½•ï¼ˆå»ºè¡¨çš„æ—¶å€™InnoDBå°±ä¼šè‡ªåŠ¨å»ºç«‹å¥½ä¸»é”®ç´¢å¼•æ ‘ï¼‰ï¼Œè€ŒMyISAMæ‹¿åˆ°æ•°æ®çš„ç´¢å¼•å³å¯ç›´æ¥ä»¥Offsetå½¢å¼ç›´æ¥åœ¨æ•°æ®æ–‡ä»¶ä¸­å®šä½æ•°æ®ã€‚è€Œä¸”InnoDBå› ä¸ºæ”¯æŒACIDï¼Œè¿˜è¦æ£€æŸ¥MVCCå¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶ï¼Œè€ŒMyISAMä¸æ”¯æŒäº‹åŠ¡ï¼Œä¹Ÿæ˜¯å…¶å¿«çš„åŸå› ã€‚è¿˜æœ‰MyISAMç»´æŠ¤äº†ä¸€ä¸ªä¿å­˜æ•´è¡¨è¡Œæ•°çš„å˜é‡ï¼Œcount(*)å¾ˆå¿«ã€‚ å…¶ä»–æ•°æ®åº“ç´¢å¼•æ•°æ®ç»“æ„è¯¥éƒ¨åˆ†ä¼šè¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆMysqlç”¨B+æ ‘ç´¢å¼•è€ŒMongoDBç”¨Bæ ‘ã€‚1.MongoDBæœ¬èº«å¾ˆå°‘æœ‰èŒƒå›´æœç´¢æ“ä½œï¼Œåšå•ä¸€æŸ¥è¯¢æ¯”è¾ƒå¤š2.Mysqlå…³ç³»å‹æ•°æ®åº“ï¼ŒåšèŒƒå›´æ£€ç´¢çš„æ“ä½œå¾ˆå¤šï¼Œå¦‚joinï¼Œwhere x&gt;1ç­‰ï¼ŒB+æ ‘å¶å­èŠ‚ç‚¹æœ‰æŒ‡é’ˆï¼Œéå†æ•ˆç‡é«˜ å‚è€ƒèµ„æ–™ä¸‹é¢åˆ—å‡ºå‚è€ƒèµ„æ–™ï¼Œæˆ‘è®¤ä¸ºå†™å¾—å¥½çš„å·²åŠ ç²—æ·±å…¥ç†è§£ Mysql ç´¢å¼•åº•å±‚åŸç†ä¸ºä»€ä¹ˆMongodbç´¢å¼•ç”¨Bæ ‘ï¼Œè€ŒMysqlç”¨B+æ ‘?Mysqlâ€”ç´¢å¼•å¤±æ•ˆç£ç›˜IOæ¦‚å¿µåŠä¼˜åŒ–å…¥é—¨çŸ¥è¯†MVCCå¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶Mysqlç´¢å¼•Mysqlç´¢å¼•å¿…é¡»äº†è§£çš„å‡ ä¸ªé‡è¦é—®é¢˜ å…¶ä»–ç»†èŠ‚1.ORDER BYä¸ç´¢å¼•å¤±æ•ˆï¼šå½“order byçš„å­—æ®µå‡ºç°åœ¨whereæ¡ä»¶ä¸­æ—¶ï¼Œæ‰ä¼šåˆ©ç”¨ç´¢å¼•è€Œä¸æ’åºï¼Œæ›´å‡†ç¡®çš„è¯´ï¼Œorder byä¸­çš„å­—æ®µåœ¨æ‰§è¡Œè®¡åˆ’ä¸­åˆ©ç”¨äº†ç´¢å¼•æ—¶ï¼Œä¸ç”¨æ’åºæ“ä½œã€‚è¿™ä¸ªç»“è®ºä¸ä»…å¯¹order byæœ‰æ•ˆï¼Œå¯¹å…¶ä»–éœ€è¦æ’åºçš„æ“ä½œä¹Ÿæœ‰æ•ˆã€‚æ¯”å¦‚group by ã€union ã€distinctç­‰ã€‚ï¼ˆå‡ºç°åœ¨Order by åçš„ç´¢å¼•åˆ—éƒ½æ˜¯ç”¨äºæ’åºçš„ï¼Œä¸ä¼šç”¨äºæŸ¥æ‰¾ï¼Œæ‰€ä»¥ç´¢å¼•æ— æ•ˆï¼‰2.innodbå¼•æ“çš„4å¤§ç‰¹æ€§ï¼šæ’å…¥ç¼“å†²ï¼ˆinsert buffer),äºŒæ¬¡å†™(double write),è‡ªé€‚åº”å“ˆå¸Œç´¢å¼•(ahi),é¢„è¯»(read ahead)3.ä¸»é”®å’Œå”¯ä¸€ç´¢å¼•çš„åŒºåˆ«ï¼šâ‘ å”¯ä¸€ç´¢å¼•åˆ—å…è®¸ç©ºå€¼ï¼Œä¸»é”®ä¸å…è®¸ç©ºå€¼ â‘¡ä¸»é”®å¯ä»¥è¢«å…¶ä»–è¡¨å¼•ç”¨ä¸ºå¤–é”®ï¼Œå”¯ä¸€ç´¢å¼•ä¸èƒ½ â‘¢ä¸€ä¸ªè¡¨åªèƒ½ä¸€ä¸ªä¸»é”®ä½†å¯æœ‰å¤šä¸ªå”¯ä¸€ç´¢å¼•","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"ç³»ç»Ÿå­¦ä¹ JVM","slug":"ç³»ç»Ÿå­¦ä¹ JVM","date":"2020-03-21T04:19:00.000Z","updated":"2023-06-06T15:26:38.111Z","comments":true,"path":"508b5c7/","link":"","permalink":"https://shmily-qjj.top/508b5c7/","excerpt":"","text":"ç³»ç»Ÿå­¦ä¹ JVMJavaè·¨å¹³å°ï¼Œä¸€æ¬¡ç¼–è¯‘åˆ°å¤„è¿è¡Œï¼Œåƒåœ¾å›æ”¶ç­‰ç‰¹æ€§ç¦»ä¸å¼€JVMï¼Œå­¦ä¹ JVMçš„åŸç†å¯ä»¥è®©æˆ‘ä»¬åœ¨å·¥ä½œä¸­æ›´å¿«é€Ÿå®šä½é—®é¢˜ã€‚å†™è¿™ç¯‡çš„ç›®çš„å°±æ˜¯é¿å…é›¶é›¶æ•£æ•£åœ°å­¦ä¹ JVMï¼Œé‚£æ ·æ•ˆç‡å¾ˆä½ï¼Œä¹Ÿæ–¹ä¾¿ä»¥åå›é¡¾å’Œå¤ä¹ ã€‚ å­—èŠ‚ç å­¦ä¹ ä¹‹å‰å…ˆè¦å­¦ä¼šç®€å•åˆ†æå­—èŠ‚ç ã€‚ç”¨æˆ·Javaä»£ç ä¸JVMäº¤äº’æ²Ÿé€šçš„æ¡¥æ¢ã€‚ä»£ç ç¼–è¯‘ä¸º.classå­—èŠ‚ç ç»™JVMè¿è¡Œã€‚ $ javac Hello.java $ javap -c Hello.class # javapå¯æŸ¥çœ‹å­—èŠ‚ç çš„æ“ä½œæ•° $ javap -p -v Hello # -pæ‰“å°ç§æœ‰å­—æ®µå’Œæ–¹æ³• -vå°½é‡å¤šæ‰“å°ä¸€äº›ä¿¡æ¯ å½“åœ¨javaä»£ç ä¸­æ·»åŠ ä¸€äº›æ³¨é‡Šä¿¡æ¯åï¼Œ.classçš„MD5ä¸ä¸€æ ·äº†ã€‚å› ä¸ºjavacå¯ä»¥æŒ‡å®šè¾“å‡ºä¸€äº›é¢å¤–å†…å®¹åˆ°.class javac -g:lines å¼ºåˆ¶ç”ŸæˆLineNumberTable | javac -g:vars å¼ºåˆ¶ç”ŸæˆLocalVariableTable | javac -g ç”Ÿæˆæ‰€æœ‰debugä¿¡æ¯ å½“ç„¶å¦‚æœä½¿ç”¨IDEAï¼Œå¯ä»¥ä½¿ç”¨jclasslib Bytecode viewerbæ’ä»¶ï¼ˆæ’ä»¶å•†åº—æœç´¢å³å¯ï¼‰ JVMçš„ç¨‹åºè¿è¡Œæ˜¯åœ¨æ ˆä¸Šå®Œæˆçš„ï¼Œè¿è¡Œmainæ–¹æ³•è‡ªåŠ¨åˆ†é…ä¸€ä¸ªæ ˆå¸§ï¼Œé€€å‡ºæ–¹æ³•ä½“æ—¶å€™å†å¼¹å‡ºç›¸åº”æ ˆå¸§ã€‚ä»javapå¾—åˆ°çš„ç»“æœçœ‹ï¼Œå¤§å¤šæ•°å­—èŠ‚ç æŒ‡ä»¤æ˜¯ä¸æ–­æ“ä½œæ ˆå¸§ã€‚æ•´ä¸ªè¿‡ç¨‹ï¼šJava æ–‡ä»¶-&gt;ç¼–è¯‘å™¨-&gt;å­—èŠ‚ç -&gt;JVM-&gt;æœºå™¨ç æ•´ä¸ªè¿‡ç¨‹ï¼šHello.java -&gt; Hello.class -&gt; Javaç±»åŠ è½½å™¨(JVMä¸­) -&gt; æ‰§è¡Œå¼•æ“(JVMä¸­) -&gt; é€šè¿‡æ“ä½œç³»ç»Ÿæ¥å£è§£é‡Šæ‰§è¡Œ+JIT å¦‚ä¸‹æœ‰ä¸¤æ®µä»£ç ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡å­—èŠ‚ç æ–‡ä»¶åˆ¤æ–­å®ƒä»¬çš„æ‰§è¡Œç»“æœ public class A&#123; # ç¬¬ä¸€æ®µ static int a = 0; static &#123; a = 1; b = 1; &#125; static int b = 0; public static void main(String[] args) &#123; System.out.println(a); System.out.println(b); &#125; &#125; //æ‰§è¡Œç»“æœï¼š1 0 //å­—èŠ‚ç å¦‚ä¸‹ï¼š 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: iconst_0 13: putstatic #5 // Field b:I 16: return -------------------------------------------------------------------------------------------------- public class A&#123; # ç¬¬äºŒæ®µ static int a = 0; static &#123; a = 1; b = 1; &#125; static int b; public static void main(String[] args) &#123; System.out.println(a); System.out.println(b); &#125; &#125; //æ‰§è¡Œç»“æœï¼š1 1 //å­—èŠ‚ç å¦‚ä¸‹ï¼š 0: iconst_0 1: putstatic #3 // Field a:I 4: iconst_1 5: putstatic #3 // Field a:I 8: iconst_1 9: putstatic #5 // Field b:I 12: return å…¶ä»–ä¿¡æ¯ï¼šstack=1, locals=0, args_size=0ä¸­stackè¡¨ç¤ºè¯¥æ–¹æ³•æœ€å¤§æ“ä½œæ•°æ ˆæ·±åº¦ä¸º4ï¼ŒJVMæ ¹æ®è¿™ä¸ªåˆ†é…æ ˆå¸§ä¸­æ“ä½œæ ˆæ·±åº¦ï¼Œlocalså˜é‡å­˜å‚¨äº†å±€éƒ¨å˜é‡çš„å­˜å‚¨ç©ºé—´ï¼Œå•ä½æ˜¯Slot(æ§½)ï¼Œargs_sizeæŒ‡æ–¹æ³•å‚æ•°ä¸ªæ•°å…¶ä»–å­—èŠ‚ç æŒ‡ä»¤è¡¨å¯å‚ç…§ï¼šhttps://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html JVMå®šä¹‰JVMï¼ˆJAVAè™šæ‹Ÿæœºï¼‰æ˜¯ä¸€ä¸ªè§„èŒƒï¼Œå®šä¹‰äº†.classæ–‡ä»¶çš„ç»“æ„ï¼ŒåŠ è½½æœºåˆ¶ï¼Œæ•°æ®å­˜å‚¨ï¼Œè¿è¡Œæ—¶æ ˆç­‰å†…å®¹ã€‚JDK8ä»¥åJavaæ˜¯ç¼–è¯‘ä¸è§£é‡Šæ··åˆæ‰§è¡Œæ¨¡å¼ã€‚JDK8ä»¥åJVMçš„æŠ€æœ¯å®ç°æ˜¯HotSpot(åŒ…å«ä¸€ä¸ªè§£é‡Šå™¨å’Œä¸¤ä¸ªç¼–è¯‘å™¨)ã€‚ä¸¤ä¸ªç¼–è¯‘å™¨ï¼šå¯ä»¥åŠ¨æ€ç¼–è¯‘ï¼Œå«serveræ¨¡å¼å’Œclientæ¨¡å¼ã€‚ clientæ¨¡å¼æ˜¯ä¸€ç§è½»é‡çº§ç¼–è¯‘å™¨ï¼Œä¹Ÿå«C1ç¼–è¯‘å™¨ï¼Œå ç”¨å†…å­˜å°ï¼Œå¯åŠ¨å¿«ï¼Œä½†æ˜¯æ‰§è¡Œæ•ˆç‡æ²¡æœ‰serveræ¨¡å¼é«˜ï¼Œé»˜è®¤çŠ¶æ€ä¸‹ä¸è¿›è¡ŒåŠ¨æ€ç¼–è¯‘ï¼Œé€‚ç”¨äºæ¡Œé¢åº”ç”¨ç¨‹åºã€‚ serveræ¨¡å¼æ˜¯ä¸€ç§é‡é‡çº§ç¼–è¯‘å™¨ï¼Œä¹Ÿå«C2ç¼–è¯‘å™¨ï¼Œå¯åŠ¨æ…¢ï¼Œå ç”¨å†…å­˜å¤§ï¼Œæ‰§è¡Œæ•ˆç‡é«˜ï¼Œé»˜è®¤æ˜¯å¼€å¯åŠ¨æ€ç¼–è¯‘çš„ï¼Œé€‚åˆæœåŠ¡å™¨åº”ç”¨ã€‚ -XX:RewriteFrequentPairs ç”¨äºå¼€å¯åŠ¨æ€ç¼–è¯‘ã€‚ -Xint:ç¦ç”¨JITç¼–è¯‘ï¼ŒUYZNGSUYZNGSå³ç¦ç”¨ä¸¤ä¸ªç¼–è¯‘å™¨ï¼Œçº¯è§£é‡Šæ‰§è¡Œã€‚ -Xcomp:çº¯ç¼–è¯‘æ‰§è¡Œï¼Œå¦‚æœæ–¹æ³•æ— æ³•ç¼–è¯‘ï¼Œåˆ™å›é€€åˆ°è§£é‡Šæ‰§è¡Œæ¨¡å¼è§£é‡Šæ— æ³•ç¼–è¯‘çš„ä»£ç ã€‚ å†…å­˜ç®¡ç† JVMå†…å­˜åŒºåŸŸå¦‚ä½•åˆ’åˆ†ï¼ŸJavaå†…å­˜å¸ƒå±€ä¸€ç›´åœ¨è°ƒæ•´ï¼ŒJava8å¼€å§‹å½»åº•ç§»é™¤äº†æŒä¹…ä»£ï¼Œä½¿ç”¨MetaSpace(å…ƒç©ºé—´)æ¥ä»£æ›¿ã€‚ =&gt; -XX:PermSizeå’Œ-XX:MaxPermSizeå¤±æ•ˆ Javaçš„è¿è¡Œæ—¶æ•°æ®åŒºå¯ä»¥åˆ†æˆå †ã€å…ƒç©ºé—´(å«æ–¹æ³•åŒº)ã€è™šæ‹Ÿæœºæ ˆã€æœ¬åœ°æ–¹æ³•æ ˆå’Œç¨‹åºè®¡æ•°å™¨ å †ï¼šå­˜æ”¾ç»å¤§å¤šæ•°Javaå¯¹è±¡ï¼Œæ˜¯JVMä¸­æœ€å¤§çš„ä¸€å—å†…å­˜ï¼Œéšç€é¢‘ç¹åˆ›å»ºå¯¹è±¡ï¼Œå †ç©ºé—´å ç”¨è¶Šæ¥è¶Šå¤§ï¼Œéœ€è¦ä¸å®šæœŸçš„GCã€‚ï¼ˆJVMä¸»è¦GCåŒºåŸŸï¼šå †å’Œå…ƒç©ºé—´ï¼‰ã€‚æ˜¯çº¿ç¨‹å…±äº«çš„ã€‚ å¯¹è±¡æ˜¯å¦è¢«åˆ†é…åœ¨å †ä¸­å–å†³äºå¯¹è±¡çš„åŸºæœ¬ç±»å‹å’ŒJavaç±»ä¸­å­˜åœ¨çš„ä½ç½®ï¼š åŸºæœ¬æ•°æ®ç±»å‹ï¼ˆbyte,short,int,long,float,double,charï¼‰å¦‚æœåœ¨æ–¹æ³•ä½“å†…å£°æ˜åˆ™åœ¨æ ˆä¸Š(æ ˆå¸§çš„å±€éƒ¨å˜é‡è¡¨)ç›´æ¥åˆ†é…ï¼Œå…¶ä»–æƒ…å†µåœ¨å †ä¸Šåˆ†é…ã€‚ int[]è¿™æ ·çš„æ•°ç»„ç±»å‹ä¸å±äºåŸºæœ¬æ•°æ®ç±»å‹ï¼Œåœ¨å †ä¸Šåˆ†é…ã€‚ æ ˆï¼šåˆ†è™šæ‹Ÿæœºæ ˆå’Œæœ¬åœ°æ–¹æ³•æ ˆã€‚ è™šæ‹Ÿæœºæ ˆï¼šJavaä¸­æ¯ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ ˆå¸§ï¼Œæ‰§è¡Œå®Œåå†å‡ºæ ˆï¼Œæ‰€æœ‰æ ˆå¸§éƒ½å‡ºæ ˆåçº¿ç¨‹ç»“æŸã€‚æ¯ä¸€ä¸ªæ–¹æ³•å¯¹åº”ä¸€ä¸ªæ ˆå¸§ï¼Œæ¯ä¸€ä¸ªçº¿ç¨‹å¯¹åº”ä¸€ä¸ªæ ˆã€‚æ ˆå¸§ä¸­åŒ…æ‹¬ï¼šå±€éƒ¨å˜é‡è¡¨ï¼Œæ“ä½œæ•°ï¼ŒåŠ¨æ€é“¾æ¥ï¼Œè¿”å›åœ°å€ï¼Œè¿™äº›ä¸æ˜¯çº¿ç¨‹å…±äº«çš„ã€‚ æœ¬åœ°æ–¹æ³•æ ˆï¼šä¸è™šæ‹Ÿæœºæ ˆç›¸ä¼¼ï¼Œä½†å®ƒä¸»è¦åŒ…å«Nativeå¯¹è±¡ã€‚æœ¬åœ°æ–¹æ³•æ ˆæœ‰ä¸€ä¸ªå«returnAddressçš„æ•°æ®ç±»å‹ã€‚ å…ƒç©ºé—´ï¼šå­˜æ”¾ç±»åä¸å­—æ®µ(ç±»çš„å…ƒæ•°æ®)ï¼Œè¿è¡Œæ—¶å¸¸é‡æ± ï¼ŒJITä¼˜åŒ–ã€‚ å…ˆå¯¹æ¯”ä¸€ä¸‹JDK8å’Œä»¥å‰ç‰ˆæœ¬çš„æ–¹æ³•åŒº PermåŒº(æ°¸ä¹…ä»£)åœ¨JDK8åºŸé™¤ï¼Œç”¨å…ƒç©ºé—´æ¥å–ä»£ã€‚å¥½å¤„ï¼šå…ƒç©ºé—´çš„å‡ºç°è§£å†³äº†ç±»å’Œç±»åŠ è½½å™¨å…ƒæ•°æ®è¿‡å¤šå¯¼è‡´çš„OOMé—®é¢˜ï¼Œå®ƒæ˜¯éå †åŒºï¼Œä½¿ç”¨æ“ä½œç³»ç»Ÿå†…å­˜ï¼Œä¸ä¼šå‡ºç°æ–¹æ³•åŒºå†…å­˜æº¢å‡ºï¼Œçœå»äº†GCæ‰«æå‹ç¼©çš„å¼€é”€ï¼Œæ¯ä¸ªåŠ è½½å™¨æœ‰ä¸“é—¨çš„å­˜å‚¨ç©ºé—´ï¼›åå¤„ï¼šæ— é™åˆ¶ä½¿ç”¨æ“ä½œç³»ç»Ÿå†…å­˜ä¼šå¯¼è‡´æ“ä½œç³»ç»Ÿå´©æºƒï¼Œæ‰€ä»¥ä¸€èˆ¬è¦åŠ -XX:MaxMetaspaceSizeå‚æ•°æ¥æ§åˆ¶å¤§å°ã€‚å…ƒç©ºé—´ä¸æ”¯æŒå‹ç¼©ï¼Œæœ‰å†…å­˜ç¢ç‰‡é—®é¢˜ã€‚ æ–¹æ³•åŒºï¼šåŒ…å«åœ¨å…ƒç©ºé—´ä¸­ã€‚æ–¹æ³•åŒºå­˜å‚¨ï¼šç±»ä¿¡æ¯ã€é™æ€ï¼ˆstaticï¼‰å˜é‡ï¼Œå¸¸é‡ï¼ˆfinalï¼‰ï¼Œç¼–è¯‘åçš„ä»£ç ç­‰æ•°æ®ã€‚æ˜¯çº¿ç¨‹å…±äº«çš„ã€‚ å…ƒç©ºé—´å†…å­˜ç®¡ç†ç”±å…ƒç©ºé—´è™šæ‹Ÿæœºå®Œæˆã€‚ ç¨‹åºè®¡æ•°å™¨ï¼š**[JVMä¸­å”¯ä¸€ä¸ä¼šOOMçš„åŒºåŸŸ]åœ¨å¤šçº¿ç¨‹åˆ‡æ¢çš„æƒ…å†µä¸‹ï¼ŒJavaé€šè¿‡ç¨‹åºè®¡æ•°å™¨æ¥è®°å½•å­—èŠ‚ç æ‰§è¡Œåˆ°ä»€ä¹ˆåœ°æ–¹ï¼Œè¿™æ ·èƒ½ä¿è¯åˆ‡æ¢å›æ¥æ—¶èƒ½å¤Ÿä»åŸæ¥çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œã€‚ï¼ˆç›¸å½“äºå­—èŠ‚ç çš„è¡Œå·æŒ‡ç¤ºå™¨ï¼‰ã€‚ç¨‹åºè®¡æ•°å™¨å®ç°äº†å¼‚å¸¸å¤„ç†ï¼Œè·³è½¬ï¼Œå¾ªç¯åˆ†æ”¯**çš„åŠŸèƒ½ã€‚å› ä¸ºæ¯ä¸ªçº¿ç¨‹éƒ½æœ‰å…¶ç‹¬ç«‹çš„ç¨‹åºè®¡æ•°å™¨ï¼Œæ‰€ä»¥æ˜¯çº¿ç¨‹ç§æœ‰çš„ã€‚ JVMç±»åŠ è½½æœºåˆ¶ç±»åŠ è½½è¿‡ç¨‹ï¼šåŠ è½½-&gt;éªŒè¯-&gt;å‡†å¤‡-&gt;è§£æ-&gt;åˆå§‹åŒ– å¤§å¤šæ•°æƒ…å†µæŒ‰è¿™ä¸ªæµç¨‹åŠ è½½ã€‚åŠ è½½ï¼šå°†ç±»çš„åŒå.classæ–‡ä»¶åŠ è½½åˆ°æ–¹æ³•åŒºéªŒè¯ï¼šæ£€æŸ¥.classæ˜¯å¦åˆè§„ã€‚å¦‚æœ.classä¸åˆè§„ï¼ŒæŠ›å¼‚å¸¸ã€‚å¦‚æœä»»ä½•.classéƒ½èƒ½åŠ è½½å°±ä¸å®‰å…¨äº†ã€‚å‡†å¤‡ï¼šä¸ºä¸€äº›ç±»å˜é‡åˆ†é…å†…å­˜ï¼Œå¹¶åˆå§‹åŒ–ä¸ºé»˜è®¤å€¼ã€‚æ­¤æ—¶ï¼Œå®ä¾‹å¯¹è±¡è¿˜æ²¡æœ‰åˆ†é…å†…å­˜ï¼Œæ‰€ä»¥è¿™äº›åŠ¨ä½œæ˜¯åœ¨æ–¹æ³•åŒºä¸Šè¿›è¡Œçš„ã€‚ ç±»åŠ è½½çš„å‡†å¤‡é˜¶æ®µä¼šç»™ç±»å˜é‡åˆ†é…å†…å­˜å’Œåˆå§‹åŒ–é»˜è®¤å€¼ã€‚æ‰€ä»¥ä¸‹é¢è¿™æ®µï¼Œæˆ‘ä»¬ä¸æ‰‹åŠ¨ç»™aèµ‹å€¼ä¹Ÿèƒ½ç¼–è¯‘é€šè¿‡ã€‚ public class test_java &#123; static int a; public static void main(String[] args) &#123; System.out.println(a); // output:0 &#125; &#125; ç±»å˜é‡æœ‰ä¸¤ä¸ªé˜¶æ®µå¯ä»¥è¢«èµ‹å€¼ï¼Œä¸€æ˜¯ç±»åŠ è½½å‡†å¤‡é˜¶æ®µï¼ŒäºŒæ˜¯åˆå§‹åŒ–é˜¶æ®µã€‚è€Œå±€éƒ¨å˜é‡åªæœ‰ä¸€æ¬¡åˆå§‹åŒ–ï¼Œå¦‚æœæ²¡èµ‹åˆå€¼ï¼Œä¸èƒ½ä½¿ç”¨ï¼Œä¸‹é¢ä»£ç ç¼–è¯‘ä¸é€šè¿‡ã€‚ public class test_java &#123; public static void main(String[] args) &#123; int a; System.out.println(a); &#125; &#125; è§£æï¼šä¿è¯å¼•ç”¨çš„å®Œæ•´æ€§ã€‚åšäº†ï¼šç±»æˆ–æ¥å£è§£æï¼Œç±»æ–¹æ³•è§£æï¼Œæ¥å£æ–¹æ³•è§£æï¼Œå­—æ®µè§£æã€‚ è¿™ä¸ªé˜¶æ®µç›¸å…³çš„æŠ¥é”™ä¿¡æ¯ï¼š java.lang.NoSuchFieldError æ ¹æ®ç»§æ‰¿å…³ç³»ä»ä¸Šå¾€ä¸‹æ²¡æ‰¾åˆ°ç›¸å…³å­—æ®µæ—¶æŠ¥é”™ java.lang.IllegalAccessError ä¸å…·å¤‡è®¿é—®æƒé™æ—¶æŠ¥é”™ java.lang.NoSuchMethodError æ‰¾ä¸åˆ°ç›¸å…³æ–¹æ³•æ—¶æŠ¥é”™ åˆå§‹åŒ–ï¼šåˆå§‹åŒ–æˆå‘˜å˜é‡ï¼Œè¿™ä¸€æ­¥æ‰å¼€å§‹æ‰§è¡Œå­—èŠ‚ç ã€‚ public class A &#123; static&#123; System.out.println(1); &#125; public A()&#123; System.out.println(&quot;A&quot;); &#125; public static void main(String[] args) &#123; A ab = new B(); ab = new B(); &#125; &#125; class B extends A&#123; static &#123; System.out.println(&quot;2&quot;); &#125; public B()&#123; System.out.println(&quot;B&quot;); &#125; //æ‰§è¡Œç»“æœ: 1 2 A B A B åŸå› :åˆå§‹åŒ–å­ç±»å…ˆè°ƒç”¨çˆ¶ç±»æ— å‚æ„é€ ï¼Œstaticåœ¨ç±»åŠ è½½çš„å‡†å¤‡é˜¶æ®µæ‰§è¡Œä¸€æ¬¡ï¼Œä¸é‡å¤æ‰§è¡Œã€‚ //staticåªä¼šæ‰§è¡Œä¸€æ¬¡ï¼Œå¯¹åº”cintæ–¹æ³• //å¯¹è±¡åˆå§‹åŒ–è°ƒç”¨æ„é€ æ–¹æ³•ï¼Œæ¯æ¬¡æ–°å»ºå¯¹è±¡éƒ½ä¼šæ‰§è¡Œï¼Œå¯¹åº”initæ–¹æ³• å¦‚æœä½ è‡ªå·±å†™ä¸€ä¸ªjava.langåŒ…ï¼Œæ”¹å†™äº†Stringç±»ï¼Œç¼–è¯‘åå‘ç°æ²¡èµ·ä½œç”¨ã€‚JREä¸èƒ½è¢«è½»æ˜“ç¯¡æ”¹ï¼Œå¦åˆ™å¯èƒ½ä¼šæœ‰å®‰å…¨é—®é¢˜ã€‚è¿™å°±æ˜¯ç±»åŠ è½½æœºåˆ¶åœ¨èµ·ä½œç”¨ã€‚ç±»åŠ è½½æœºåˆ¶æµç¨‹ï¼š åŒäº²å§”æ´¾æœºåˆ¶ï¼šå½“æŸä¸ªç±»åŠ è½½å™¨éœ€è¦åŠ è½½æŸä¸ª.classæ–‡ä»¶æ—¶ï¼Œå®ƒé¦–å…ˆæŠŠè¿™ä¸ªä»»åŠ¡å§”æ‰˜ç»™ä»–çš„ä¸Šçº§ç±»åŠ è½½å™¨ï¼Œé€’å½’è¿™ä¸ªæ“ä½œï¼Œå¦‚æœä¸Šçº§çš„ç±»åŠ è½½å™¨æ²¡æœ‰åŠ è½½ï¼Œæ‰ä¼šå»çœŸæ­£åŠ è½½è¿™ä¸ªç±»ã€‚æ¯”å¦‚Objectç±»ï¼Œæ¯«æ— ç–‘é—®ä¼šäº¤ç»™æœ€ä¸Šå±‚çš„ç±»åŠ è½½å™¨åŠ è½½ï¼Œä¿è¯åªæœ‰ä¸€ä¸ªè¢«åŠ è½½çš„Objectç±»ã€‚å¦‚æœæ²¡æœ‰åŒäº²å§”æ´¾æœºåˆ¶ï¼Œä¼šæœ‰å¤šä¸ªObjectç±»ï¼Œå¾ˆæ··ä¹±ã€‚ç±»åŠ è½½å™¨è¿è¡Œæœ‰å…ˆåé¡ºåºçš„ï¼Œä¸‹é¢æ˜¯ç±»åŠ è½½å™¨çš„ç§ç±»ï¼š BootstrapClassLoaderï¼ˆå¯åŠ¨ç±»åŠ è½½å™¨ï¼‰ï¼šc++ç¼–å†™ï¼ŒåŠ è½½javaæ ¸å¿ƒåº“ java.*,æ„é€ ExtClassLoaderå’ŒAppClassLoaderã€‚ç”±äºå¼•å¯¼ç±»åŠ è½½å™¨æ¶‰åŠåˆ°è™šæ‹Ÿæœºæœ¬åœ°å®ç°ç»†èŠ‚ï¼Œå¼€å‘è€…æ— æ³•ç›´æ¥è·å–åˆ°å¯åŠ¨ç±»åŠ è½½å™¨çš„å¼•ç”¨ï¼Œæ‰€ä»¥ä¸å…è®¸ç›´æ¥é€šè¿‡å¼•ç”¨è¿›è¡Œæ“ä½œ ExtentionClassLoader ï¼ˆæ ‡å‡†æ‰©å±•ç±»åŠ è½½å™¨ï¼‰ï¼šjavaç¼–å†™ï¼ŒåŠ è½½æ‰©å±•åº“ï¼Œå¦‚classpathä¸­çš„jre(lib/extä¸‹jaråŒ…å’Œ.class)ï¼Œjavax.*å’Œjava.ext.dirsæŒ‡å®šä½ç½®ä¸­çš„ç±»ï¼Œå¼€å‘è€…å¯ä»¥ç›´æ¥ä½¿ç”¨æ ‡å‡†æ‰©å±•ç±»åŠ è½½å™¨ã€‚ AppClassLoaderï¼ˆç³»ç»Ÿç±»åŠ è½½å™¨ï¼‰ï¼šjavaç¼–å†™ï¼ŒåŠ è½½ç¨‹åºæ‰€åœ¨çš„ç›®å½•ï¼Œclasspathä½ç½®ä¸‹å…¶ä»–æ‰€æœ‰jarå’Œ.classã€‚æˆ‘ä»¬å†™çš„ä»£ç æœ€å…ˆå°è¯•ä½¿ç”¨è¿™ä¸ªè¿›è¡ŒåŠ è½½ï¼Œå†é€šè¿‡åŒäº²å§”æ´¾æœºåˆ¶é€’å½’å§”æ‰˜ä¸Šçº§ç±»åŠ è½½å™¨ã€‚ CustomClassLoaderï¼ˆç”¨æˆ·è‡ªå®šä¹‰ç±»åŠ è½½å™¨ï¼‰ï¼šjavaç¼–å†™,ç”¨æˆ·è‡ªå®šä¹‰çš„ç±»åŠ è½½å™¨,å¯åŠ è½½æŒ‡å®šè·¯å¾„çš„classæ–‡ä»¶ã€‚æ”¯æŒè‡ªå®šä¹‰æ‰©å±•åŠŸèƒ½ã€‚ åŒäº²å§”æ´¾æœºåˆ¶ä½œç”¨ï¼š1ã€é˜²æ­¢ä¸€ä¸ª.classè¢«é‡å¤åŠ è½½ï¼Œä¸€ä¸ªä¸€ä¸ªå»ä¸Šé¢é—®ï¼ŒåŠ è½½è¿‡äº†å°±ä¸åŠ è½½äº†ã€‚ä¿è¯æ•°æ®å®‰å…¨ã€‚2ã€ä¿è¯æ ¸å¿ƒ.classä¸è¢«ç¯¡æ”¹ï¼Œå³ä½¿ç¯¡æ”¹ä¹Ÿä¸ä¼šåŠ è½½ï¼Œå³ä½¿åŠ è½½ä¹Ÿä¸ä¼šæ˜¯åŒä¸€ä¸ª.classå¯¹è±¡ã€‚ï¼ˆä¸åŒçš„ç±»åŠ è½½å™¨åŠ è½½åŒä¸€ä¸ª.classå¾—åˆ°çš„æ˜¯ä¸åŒçš„å¯¹è±¡ï¼‰ã€‚ä¿è¯.classæ‰§è¡Œæ²¡é—®é¢˜ã€‚ å¯ä»¥è¦†ç›–HashMapç±»çš„å®ç°å—ï¼Ÿå¯ä»¥ï¼Œç”¨åˆ°Javaçš„endorsedæŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè‡ªå·±çš„HashMapç±»æ‰“æˆjaræ”¾åœ¨-Djava.endorsed.dirsæŒ‡å®šçš„ç›®å½•ï¼Œç±»åå’ŒåŒ…ååº”è¯¥ä¸jdkåŸç”Ÿçš„ä¸€è‡´ã€‚è¿™ä¸ªç›®å½•ä¸‹çš„jarä¼šè¢«ä¼˜å…ˆåŠ è½½ï¼Œæ¯”rt.jarä¼˜å…ˆçº§æ›´é«˜ã€‚ å“ªäº›åœ°æ–¹æ‰“ç ´äº†Javaçš„ç±»åŠ è½½æœºåˆ¶?ä¸¾ä¾‹å­ï¼š 1.tomcatä½¿ç”¨waråŒ…å‘å¸ƒåº”ç”¨ï¼Œç”±WebAppClassLoaderç±»åŠ è½½å™¨ä¼˜å…ˆåŠ è½½ï¼Œå®ƒåŠ è½½è‡ªå·±ç›®å½•çš„.classä½†ä¸ä¼ é€’ç»™çˆ¶ç±»åŠ è½½å™¨ï¼Œä½†å®ƒå¯ä»¥é€šè¿‡SharedClassLoaderå®ç°å…±äº«å’Œåˆ†ç¦»ã€‚2.Javaçš„SPIæœºåˆ¶ï¼Œä¾‹å­ï¼šMysqlçš„JDBCã€‚ä½¿ç”¨JDBC Driverå‰ä½¿ç”¨Class.forName(â€œcom.mysql.jdbc.driver)ï¼Œä½†å¦‚æœåˆ é™¤è¿™è¡Œä»£ç ä¹Ÿèƒ½æ­£ç¡®åŠ è½½åˆ°é©±åŠ¨ç±»ï¼Œå› ä¸ºä½¿ç”¨ServiceLoaderæ¥åŠ¨æ€è£…è½½ã€‚ å¦‚ä½•åŠ è½½è¿œç¨‹.classæ–‡ä»¶ï¼Œæ€ä¹ˆåŠ å¯†.classæ–‡ä»¶ï¼Ÿé€šè¿‡å®ç°ä¸€ä¸ªæ–°çš„è‡ªå®šä¹‰ç±»åŠ è½½å™¨ã€‚ JVMçš„GCGC Rootsï¼šå¯è¾¾æ€§åˆ†ææ³•ï¼Œæ˜¯GCå®ç°çš„ä¸€ç§æ–¹æ³•(å¦ä¸€ç§æ˜¯å¼•ç”¨è®¡æ•°æ³•)ï¼ŒGC Rootsæ˜¯ä¸€ç»„æ´»è·ƒçš„å¼•ç”¨ï¼Œç¨‹åºåœ¨æ¥ä¸‹æ¥çš„è¿è¡Œä¸­èƒ½ç›´æ¥æˆ–é—´æ¥å¼•ç”¨æˆ–èƒ½è¢«å¼•ç”¨çš„å¯¹è±¡ã€‚ä»GC Rootsä¸æ–­å‘ä¸‹è¿½æº¯éå†ï¼Œä¼šäº§ç”ŸReference Chainå¼•ç”¨é“¾ã€‚GC Rootséå†è¿‡ç¨‹æ˜¯æ‰¾å‡ºæ‰€æœ‰æ´»å¯¹è±¡ï¼Œå¹¶æŠŠå…¶ä½™ç©ºé—´è®¤å®šä¸ºæ— ç”¨ï¼Œè€Œä¸æ˜¯æ‰¾åˆ°æ­»å¯¹è±¡ã€‚å¦‚æœä¸€ä¸ªå¯¹è±¡è¿ç»­ä¸¤æ¬¡éå†è¿‡ç¨‹ä¸­è·ŸGC Rootsæ²¡æœ‰ä»»ä½•ç›´æ¥æˆ–é—´æ¥å¼•ç”¨ï¼Œåˆ™ä¼šè¢«GCæ‰ã€‚GC RootsåŒ…æ‹¬ï¼š æ´»åŠ¨çº¿ç¨‹ç›¸å…³çš„å„ç§å¼•ç”¨ ç±»çš„é™æ€å˜é‡çš„å¼•ç”¨ JNIå¼•ç”¨ GC Rootsæ˜¯å¼•ç”¨ä¸æ˜¯å¯¹è±¡ *å¼•ç”¨çº§åˆ«ï¼ˆå¼•ç”¨é“¾çš„è¡¨ç°ï¼‰**ï¼š å¼ºå¼•ç”¨ï¼š[æœ‰ç”¨ä¸”å¿…é¡»]å†…å­˜ä¸è¶³ç›´åˆ°æŠ›OOMï¼Œè¿™ç§å¼ºå¼•ç”¨çš„å¯¹è±¡ä¹Ÿä¸ä¼šè¢«å›æ”¶ã€‚ - å®¹æ˜“é€ æˆå†…å­˜æ³„éœ²(æ¯”å¦‚ä¸€ä¸ªUserç±»æ²¡æœ‰å­—æ®µinfoï¼Œç”¨HashMap&lt;User,String&gt;å­˜ï¼Œç”¨å®ŒUserä½†å› ä¸ºè¢«HashMapä½¿ç”¨è€Œæœªèƒ½å›æ”¶ï¼Œå°±é€ æˆå†…å­˜æ³„éœ²) è½¯å¼•ç”¨ï¼š[æœ‰ç”¨éå¿…é¡»]ç»´æŠ¤ä¸€äº›å¯æœ‰å¯æ— çš„å¯¹è±¡ï¼Œå†…å­˜è¶³å¤Ÿçš„æ—¶å€™ä¸ä¼šè¢«å›æ”¶ï¼Œå†…å­˜ä¸è¶³ä¼šå›æ”¶ã€‚å¦‚æœå›æ”¶äº†è½¯å¼•ç”¨å¯¹è±¡åå†…å­˜è¿˜ä¸å¤Ÿåˆ™æŠ›å‡ºOOMã€‚ å¼±å¼•ç”¨ï¼š[å¯èƒ½æœ‰ç”¨éå¿…é¡»]å¼•ç”¨çš„å¯¹è±¡ç›¸æ¯”è½¯å¼•ç”¨ï¼Œè¦æ›´åŠ æ— ç”¨ä¸€äº›ï¼Œç”Ÿå‘½å‘¨æœŸæ›´çŸ­ã€‚GCæ—¶æ— è®ºå†…å­˜æ˜¯å¦å……è¶³éƒ½ä¼šå›æ”¶å¼±å¼•ç”¨å…³è”çš„å¯¹è±¡ã€‚ä¸è¿‡ç”±äºåƒåœ¾å›æ”¶å™¨æ˜¯ä¸€ä¸ªä¼˜å…ˆçº§è¾ƒä½çš„çº¿ç¨‹ï¼Œæ‰€ä»¥å¹¶ä¸ä¸€å®šèƒ½è¿…é€Ÿå‘ç°å¼±å¼•ç”¨å¯¹è±¡ã€‚ è™šå¼•ç”¨ï¼š[æ— ç”¨]å½¢åŒè™šè®¾çš„å¼•ç”¨ï¼Œä»»ä½•æ—¶å€™éƒ½å¯è¢«å›æ”¶ã€‚ WeakReferenceä¸SoftReferenceçš„åŒºåˆ«?è™½ç„¶WeakReferenceä¸SoftReferenceéƒ½æœ‰åˆ©äºæé«˜GCå’Œå†…å­˜çš„æ•ˆç‡ï¼Œä½†æ˜¯WeakReferenceä¸€æ—¦å¤±å»æœ€åä¸€ä¸ªå¼ºå¼•ç”¨ï¼Œå°±ä¼šè¢«GCå›æ”¶ï¼Œè€Œè½¯å¼•ç”¨è™½ç„¶ä¸èƒ½é˜»æ­¢è¢«å›æ”¶ï¼Œä½†æ˜¯å¯ä»¥æ‹–å»¶åˆ°JVMå†…å­˜ä¸è¶³çš„æ—¶å€™å†è¢«å›æ”¶ã€‚ ä¸ºä½•è¦æœ‰å¤šç§ä¸åŒå¼•ç”¨çº§åˆ«?åˆ©ç”¨è½¯å¼•ç”¨å’Œå¼±å¼•ç”¨è§£å†³OOMé—®é¢˜_ï¼šç”¨ä¸€ä¸ªHashMapæ¥ä¿å­˜å›¾ç‰‡çš„è·¯å¾„å’Œç›¸åº”å›¾ç‰‡å¯¹è±¡å…³è”çš„è½¯å¼•ç”¨ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œåœ¨å†…å­˜ä¸è¶³æ—¶ï¼ŒJVMä¼šè‡ªåŠ¨å›æ”¶è¿™äº›ç¼“å­˜å›¾ç‰‡å¯¹è±¡æ‰€å ç”¨çš„ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåœ°é¿å…äº†OOMçš„é—®é¢˜._é€šè¿‡è½¯å¼•ç”¨å®ç°Javaå¯¹è±¡çš„é«˜é€Ÿç¼“å­˜:æ¯”å¦‚æˆ‘ä»¬åˆ›å»ºäº†Personç±»ï¼Œå¦‚æœæ¯æ¬¡éœ€è¦æŸ¥è¯¢ä¸€ä¸ªäººçš„ä¿¡æ¯,å“ªæ€•æ˜¯å‡ ç§’ä¸­ä¹‹å‰åˆšåˆšæŸ¥è¯¢è¿‡çš„ï¼Œéƒ½è¦é‡æ–°æ„å»ºä¸€ä¸ªå®ä¾‹ï¼Œè¿™å°†å¼•èµ·å¤§é‡Personå¯¹è±¡çš„æ¶ˆè€—,å¹¶ä¸”ç”±äºè¿™äº›å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç›¸å¯¹è¾ƒçŸ­,ä¼šå¼•èµ·å¤šæ¬¡GCå½±å“æ€§èƒ½ã€‚æ­¤æ—¶,é€šè¿‡è½¯å¼•ç”¨å’ŒHashMapçš„ç»“åˆå¯ä»¥æ„å»ºé«˜é€Ÿç¼“å­˜,æé«˜æ€§èƒ½ã€‚ //å¼ºå¼•ç”¨ Shmily shmily = new Shmily(); //è½¯å¼•ç”¨ SoftReference&lt;Shmily&gt; softReference = new SoftReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = softReference.get(); //å¼±å¼•ç”¨ WeakReference&lt;Shmily&gt; weakReference = new WeakReference&lt;Shmily&gt;(new Shmily()); Shmily shmily = weakReference.get(); //è™šå¼•ç”¨ è™šå¼•ç”¨çš„ä½¿ç”¨å¿…é¡»å’Œå¼•ç”¨é˜Ÿåˆ—ï¼ˆReference Queueï¼‰è”åˆä½¿ç”¨ ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(new Shmily(), referenceQueue); Shmily shmily = phantomReference.get(); //æ‰€æœ‰ä»¥ä¸Šå¯¹è±¡å‡ºäº†å¼ºå¼•ç”¨ä¹‹å¤–ï¼Œä¸€æ—¦è¢«å›æ”¶ï¼Œgetæ–¹æ³•è¿”å›nullã€‚ //ä»¥ä¸Šåˆ›å»ºè½¯å¼•ç”¨ï¼Œå¼±å¼•ç”¨çš„å¯¹è±¡softReferenceå’ŒweakReferenceè¿˜éƒ½å±äºå¼ºå¼•ç”¨ï¼Œç”¨å®Œä¹Ÿéœ€è¦å›æ”¶é¿å…å†…å­˜æº¢å‡ºï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š ReferenceQueue referenceQueue = new ReferenceQueue(); PhantomReference&lt;Shmily&gt; phantomReference = new PhantomReference&lt;Shmily&gt;(softReference, referenceQueue); å¯èƒ½å‘ç”ŸOOMçš„å†…å­˜åŒºåŸŸï¼šé™¤äº†ç¨‹åºè®¡æ•°å™¨ï¼Œéƒ½æœ‰å¯èƒ½ã€‚ä½†ä¸»è¦æ˜¯å‘ç”Ÿåœ¨å †ä¸Šã€‚OOMå‘ç”ŸåŸå› ï¼š å†…å­˜ä¸è¶³éœ€è¦æ‰©å®¹ã€‚ é”™è¯¯çš„å¼•ç”¨æ–¹å¼ï¼Œæ²¡æœ‰åŠæ—¶åˆ‡æ–­GC Rootsçš„å¼•ç”¨ï¼Œå¯¼è‡´å†…å­˜æ³„æ¼ã€‚(å…¸å‹) æ²¡æœ‰è¿›è¡Œæ•°æ®èŒƒå›´æ£€æŸ¥ï¼Œæ¯”å¦‚å…¨é‡æŸ¥è¯¢äº†æŸä¸ªæ•°æ®åº“ã€‚ æ— é™åˆ¶æ— èŠ‚åˆ¶ä½¿ç”¨MemoryOverHead JVMçš„åƒåœ¾å›æ”¶ç®—æ³•GCçš„æ ‡è®°è¿‡ç¨‹ï¼šä»GC Rootséå†æ‰€æœ‰å¯è¾¾çš„æ´»è·ƒå¯¹è±¡å¹¶æ ‡è®°ã€‚GCè§¦å‘æ¡ä»¶ï¼š1.è€å¹´ä»£ä¸è¶³ 2.è°ƒç”¨äº†System.gc() 3.é€šè¿‡MinorGCè¿›å…¥è€å¹´ä»£çš„å¯¹è±¡å¤§å°æ€»å’Œå¤§äºè€å¹´ä»£çš„å¤§å°ï¼ˆæ‹…ä¿å¤±è´¥ï¼‰ 4.EdenåŒºä¸å¤Ÿå­˜æ”¾æ–°åˆ›å»ºçš„å¯¹è±¡GCç®—æ³•ï¼š æ ‡è®°æ¸…é™¤ç®—æ³•ï¼šæ ‡è®°-æ ‡è®°å·²ç”¨å¯¹è±¡ï¼Œæ¸…é™¤-æ¸…é™¤æœªè¢«æ ‡è®°çš„å¯¹è±¡ã€‚ ç¼ºç‚¹ï¼šäº§ç”Ÿå†…å­˜ç¢ç‰‡ åœºæ™¯ï¼šé€‚åˆåœ¨æ”¶é›†é¢‘ç‡ä½çš„è€å¹´ä»£ä½¿ç”¨ å¤åˆ¶ç®—æ³•ï¼šå†…å­˜ç©ºé—´åˆ†ç­‰å¤§ä¸¤å—ï¼Œä¸€å—æ»¡äº†ï¼Œæœªè¢«æ ‡è®°çš„å¯¹è±¡å¤åˆ¶åˆ°å¦ä¸€å—ã€‚ ä¼˜ç‚¹ï¼šè§£å†³äº†å†…å­˜ç¢ç‰‡é—®é¢˜ï¼Œæ•ˆç‡æœ€é«˜ ç¼ºç‚¹ï¼šä¼šæœ‰ä¸€åŠçš„å†…å­˜ç©ºé—´æµªè´¹ åœºæ™¯ï¼šé€‚åˆæ”¶é›†é¢‘ç‡é«˜ä¸”è¿½æ±‚æ”¶é›†æ•ˆç‡çš„å¹´è½»ä»£ä½¿ç”¨ æ ‡è®°æ•´ç†ç®—æ³•ï¼šç§»åŠ¨æ‰€æœ‰å­˜æ´»çš„å¯¹è±¡ï¼Œä¸”æŒ‰å†…å­˜åœ°å€é¡ºåºä¾æ¬¡æ’åˆ—ï¼Œç„¶åå°†æœ«ç«¯å†…å­˜å…¨éƒ¨å›æ”¶ã€‚ ä¼˜ç‚¹ï¼šè§£å†³äº†å†…å­˜ç¢ç‰‡é—®é¢˜ï¼ŒåŒæ—¶è§£å†³æ ‡è®°å¤åˆ¶ç®—æ³•çš„å†…å­˜ç©ºé—´æµªè´¹é—®é¢˜ ç¼ºç‚¹ï¼šæ•ˆç‡ä½äºå¤åˆ¶ç®—æ³•å’Œæ ‡è®°æ¸…é™¤ç®—æ³• åœºæ™¯ï¼šé€‚åˆåœ¨æ”¶é›†é¢‘ç‡ä½çš„è€å¹´ä»£ä½¿ç”¨ åˆ†ä»£æ”¶é›†ç®—æ³•: JVMé‡‡ç”¨åˆ†ä»£æ”¶é›†ç®—æ³•ï¼Œå¯¹ä¸åŒçš„åŒºåŸŸé‡‡ç”¨ä¸ç”¨çš„æ”¶é›†ç®—æ³•ã€‚ åˆ†ä»£æ”¶é›†ç®—æ³•æ˜¯èåˆä¸Šè¿°3ç§åŸºç¡€çš„ç®—æ³•æ€æƒ³ï¼Œè€Œäº§ç”Ÿçš„é’ˆå¯¹ä¸åŒæƒ…å†µæ‰€é‡‡ç”¨ä¸åŒç®—æ³•çš„ä¸€å¥—ç»„åˆã€‚æ ¹æ®å¯¹è±¡å­˜æ´»å‘¨æœŸçš„ä¸åŒå°†å†…å­˜åˆ’åˆ†ä¸ºå‡ å—ã€‚ä¸€èˆ¬æ˜¯æŠŠ Java å †åˆ†ä¸ºæ–°ç”Ÿä»£å’Œè€å¹´ä»£ï¼Œè¿™æ ·å°±å¯ä»¥æ ¹æ®å„ä¸ªå¹´ä»£çš„ç‰¹ç‚¹é‡‡ç”¨æœ€é€‚å½“çš„æ”¶é›†ç®—æ³•ã€‚åœ¨æ–°ç”Ÿä»£ä¸­ï¼Œæ¯æ¬¡åƒåœ¾æ”¶é›†æ—¶éƒ½å‘ç°æœ‰å¤§æ‰¹å¯¹è±¡æ˜¯å¾…å›æ”¶çš„ï¼Œåªæœ‰å°‘é‡å­˜æ´»ï¼Œé‚£å°±é€‰ç”¨å¤åˆ¶ç®—æ³•ï¼Œåªéœ€è¦ä»˜å‡ºå°‘é‡å­˜æ´»å¯¹è±¡çš„å¤åˆ¶æˆæœ¬å°±å¯ä»¥å®Œæˆæ”¶é›†ã€‚è€Œè€å¹´ä»£ä¸­å› ä¸ºå¯¹è±¡å­˜æ´»ç‡é«˜ï¼Œå°±ä½¿ç”¨æ ‡è®°-æ¸…é™¤æˆ–æ ‡è®°-æ•´ç†ç®—æ³•æ¥è¿›è¡Œå›æ”¶ã€‚ GCç§ç±»ï¼š MinorGC å‘ç”Ÿåœ¨å¹´è½»ä»£çš„GCè§¦å‘æ¡ä»¶ï¼šEdenåŒºä¸å¤Ÿå­˜æ”¾æ–°åˆ›å»ºçš„å¯¹è±¡ MajorGC å‘ç”Ÿåœ¨è€å¹´ä»£çš„GC ä¸FullGCåŒºåˆ«æ˜¯åªæ¸…ç†è€å¹´ä»£è€Œä¸æ¸…ç†å¹´è½»ä»£è§¦å‘æ¡ä»¶ï¼šâ‘  FullGC å…¨å †åƒåœ¾å›æ”¶ï¼ˆå¦‚å…ƒç©ºé—´å¼•èµ·çš„å¹´è½»ä»£å’Œè€å¹´ä»£å›æ”¶ï¼‰è§¦å‘æ¡ä»¶ï¼šâ‘ è°ƒç”¨System.gc â‘¡è€å¹´ä»£ç©ºé—´ä¸è¶³(å¯èƒ½æ— è¶³å¤Ÿè¿ç»­ç©ºé—´) â‘¢æ‹…ä¿æœºåˆ¶å¤±è´¥(Edenå¤§å¯¹è±¡æ— æ³•å­˜å…¥è€å¹´ä»£ï¼Œå› ä¸ºæ£€æµ‹åˆ°è€å¹´ä»£æ— è¶³å¤Ÿè¿ç»­å†…å­˜ç©ºé—´) â‘£Minor GCåè¿›å…¥è€å¹´ä»£çš„å¹³å‡å¤§å°å¤§äºè€å¹´ä»£å¯ç”¨å†…å­˜ Mixed GC[G1æ”¶é›†å™¨ç‰¹æœ‰] æ”¶é›†æ•´ä¸ªYoungGenerationå’Œéƒ¨åˆ†OldGeneration Javaçš„å¤§éƒ¨åˆ†å¯¹è±¡ç”Ÿå‘½å‘¨æœŸéƒ½ä¸é•¿ï¼Œå®ƒä»¬ä½äºå¹´è½»ä»£(Young Generation)ï¼Œè€Œç”Ÿå‘½å‘¨æœŸè¾ƒé•¿çš„ä½äºè€å¹´ä»£(Old Generation)ã€‚å¹´è½»ä»£çš„GCï¼šå¹´è½»ä»£ä½¿ç”¨å¤åˆ¶ç®—æ³•ï¼Œå› ä¸ºå¹´è½»ä»£å¤§éƒ¨åˆ†å¯¹è±¡ç”Ÿå‘½å‘¨æœŸçŸ­ï¼Œå¦‚æœå‘ç”ŸGCåªä¼šæœ‰å°‘é‡å¯¹è±¡å­˜æ´»ï¼Œå¤åˆ¶è¿™éƒ¨åˆ†å¯¹è±¡æ˜¯é«˜æ•ˆçš„ã€‚å¹´è½»ä»£åˆ†ä¸ºEden:From Survivor:To Survivor = 8:1:1ä¸‰ä¸ªç©ºé—´ã€‚å¯¹è±¡é¦–å…ˆåœ¨EdenåŒºï¼Œå¦‚æœEdenåŒºæ»¡äº†å°±ä¼šè§¦å‘MinorGCã€‚å•æ•°æ¬¡MinorGC:åœ¨MinorGCåï¼Œå­˜æ´»çš„å¯¹è±¡è¿›å…¥Form SurvivoråŒºã€‚åŒæ•°æ¬¡MinorGCï¼ŒEdenå’ŒFrom SurvivoråŒºä¸€èµ·æ¸…ç†ï¼Œå­˜æ´»å¯¹è±¡è¢«å¤åˆ¶åˆ°ToåŒºï¼Œå¹¶æ¸…ç©ºFromåŒºã€‚ä»ä¸Šé¢å¯ä»¥å¾—çŸ¥æ¯æ¬¡GCéƒ½æœ‰ä¸€ä¸ªSurvivoråŒºç©ºé—²ï¼Œç”±äºEden:From Survivor:To Survivor = 8:1:1ï¼Œå¹´è½»ä»£GCå¤åˆ¶ç®—æ³•åªæµªè´¹äº†10%çš„å†…å­˜ç©ºé—´ï¼ŒåŒæ—¶åšåˆ°äº†é«˜æ•ˆï¼Œæ— ç¢ç‰‡å’ŒèŠ‚çº¦ç©ºé—´ã€‚æ‰©å±•ï¼šTLAB(Thread Local Allocation Buffer)ï¼Œæ˜¯JVMç»™æ¯ä¸ªçº¿ç¨‹å•ç‹¬å¼€è¾Ÿçš„åŒºåŸŸï¼Œç”¨æ¥åŠ é€Ÿå¯¹è±¡åˆ†é…ã€‚åœ¨EdenåŒºåˆ†å¤šä¸ªTLABï¼ŒTLABé€šå¸¸æ¯”è¾ƒå°ï¼Œå¯¹è±¡ä¼˜å…ˆåˆ†é…åœ¨TLABä¸Šï¼Œå¯¹è±¡è¾ƒå¤§æ‰ä¼šåœ¨EdenåŒºåˆ†é…ã€‚TLABæ˜¯ä¸€ç§ä¼˜åŒ–ï¼Œç±»ä¼¼äºé€ƒé€¸åˆ†æçš„å¯¹è±¡åœ¨æ ˆä¸Šåˆ†é…çš„ä¼˜åŒ–ã€‚ è€å¹´ä»£çš„GCï¼šè€å¹´ä»£ä¸€èˆ¬ä½¿ç”¨æ ‡è®°æ•´ç†å’Œæ ‡è®°æ¸…é™¤ç®—æ³•ã€‚å› ä¸ºè€å¹´ä»£å¾ˆå¤šå¯¹è±¡å­˜æ´»ç‡é«˜ï¼Œå ç”¨è¾ƒå¤§ï¼Œä¸æ–¹ä¾¿å¤åˆ¶ã€‚å¯¹è±¡æ€ä¹ˆè¿›å…¥è€å¹´ä»£ï¼š è¾¾åˆ°ä¸€å®šå¹´é¾„æ¯æ¬¡å‘ç”ŸMinorGCï¼Œå¯¹è±¡å¹´é¾„åŠ 1ï¼Œè¾¾åˆ°é˜ˆå€¼(æœ€å¤§å€¼æ˜¯15å¯é€šè¿‡â€XX:+MaxTenuringThresholdè°ƒ)ï¼Œè¿›å…¥è€å¹´ä»£ã€‚ åˆ†é…æ‹…ä¿æœºåˆ¶å› ä¸ºSurvivoråŒºåªå å¹´è½»ä»£10%çš„ç©ºé—´ï¼Œå‘ç”ŸMinorGCæ—¶æ— æ³•ä¿è¯æ¯æ¬¡Eden+å…¶ä¸­ä¸€ä¸ªSurvivorå­˜æ´»çš„å¯¹è±¡å¤§å°éƒ½å°äºå¦ä¸€ä¸ªSurvivoråŒºç©ºé—´ï¼Œé€šè¿‡åˆ†é…æ‹…ä¿æœºåˆ¶ï¼Œå¦ä¸€ä¸ªSurvivoråŒºæ”¾ä¸ä¸‹çš„å¯¹è±¡ç›´æ¥è¿›å…¥è€å¹´ä»£ã€‚JVMæ¯æ¬¡MinorGCå‰ä¼šæ£€æŸ¥è€å¹´ä»£æœ€å¤§å¯ç”¨è¿ç»­å†…å­˜ç©ºé—´æ˜¯å¦å¤§äºæ–°ç”Ÿä»£å¯¹è±¡çš„æ€»ç©ºé—´ï¼Œå¦‚æœæ˜¯çš„è¯ç¡®ä¿MinorGCæ˜¯å®‰å…¨çš„ã€‚ å¤§å¯¹è±¡ç›´æ¥è¿›å…¥è€å¹´ä»£è¶…è¿‡ä¸€å®šå¤§å°çš„å¯¹è±¡ç›´æ¥è¿›å…¥è€å¹´ä»£ã€‚(é€šè¿‡-XX:PretenureSizeThresholdè®¾ç½®ï¼Œé»˜è®¤0è¡¨ç¤ºéƒ½è¦å…ˆèµ°å¹´è½»ä»£) åŠ¨æ€å¹´é¾„åˆ¤å®šä¸ºäº†ä½¿å†…å­˜åˆ†é…æ›´çµæ´»ï¼ŒJVMä¸ä¸€å®šè¦æ±‚å¯¹è±¡å¹´é¾„è¾¾åˆ°MaxTenuringThreshold(15)æ‰æ™‹å‡ä¸ºè€å¹´ä»£ï¼Œè‹¥SurvivoråŒºç›¸åŒå¹´é¾„å¯¹è±¡æ€»å¤§å°å¤§äºSurvivoråŒºç©ºé—´çš„ä¸€åŠï¼Œåˆ™å¤§äºç­‰äºè¿™ä¸ªå¹´é¾„çš„å¯¹è±¡å°†ä¼šåœ¨MinorGCæ—¶ç§»åˆ°è€å¹´ä»£ã€‚ JVMå¸¸è§åƒåœ¾å›æ”¶å™¨ï¼šå¦‚æœåƒåœ¾æ”¶é›†ç®—æ³•æ˜¯JVMåƒåœ¾å›æ”¶çš„æ–¹æ³•è®ºï¼Œé‚£åƒåœ¾å›æ”¶å™¨å°±æ˜¯ä¸Šè¿°ç®—æ³•çš„å®ç°ã€‚ å¹´è½»ä»£åƒåœ¾å›æ”¶å™¨ Serialåƒåœ¾å›æ”¶å™¨å•çº¿ç¨‹çš„åƒåœ¾å›æ”¶å™¨ï¼Œåƒåœ¾å›æ”¶æ—¶æš‚åœä¸€åˆ‡ç”¨æˆ·çº¿ç¨‹ï¼Œä½¿ç”¨å¤åˆ¶ç®—æ³•ã€‚ä¼˜ç‚¹ï¼šç®€å•è½»é‡çº§ï¼Œä½¿ç”¨èµ„æºå°‘ã€‚åœºæ™¯ï¼šç”¨äºå®¢æˆ·ç«¯åº”ç”¨ï¼Œå› ä¸ºå®¢æˆ·ç«¯åº”ç”¨ä¸ä¼šé¢‘ç¹åˆ›å»ºå¯¹è±¡ã€‚ ParNewåƒåœ¾å›æ”¶å™¨Serialå›æ”¶å™¨çš„å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼Œå¤šæ¡GCçº¿ç¨‹å¹¶è¡Œå›æ”¶ï¼Œåƒåœ¾å›æ”¶æ—¶ä»æš‚åœä¸€åˆ‡ç”¨æˆ·çº¿ç¨‹ä¼˜ç‚¹ï¼šå¤šCPUç¯å¢ƒä¸‹æ”¶é›†æ•ˆç‡é«˜äº›ï¼ŒGCåœé¡¿æ—¶é—´ç¼©çŸ­ã€‚åœºæ™¯ï¼šå¤šCPUåœºæ™¯ä¸‹ä½¿ç”¨ï¼ŒParNewé€‚åˆäº¤äº’å¤šè®¡ç®—å°‘çš„åœºæ™¯ã€‚ Parallel Scavengeåƒåœ¾å›æ”¶å™¨å¤šçº¿ç¨‹å›æ”¶å™¨åœºæ™¯ï¼šå¤šCPUä¸‹ä½¿ç”¨ï¼Œè¿½æ±‚CPUååé‡ï¼Œé€‚ç”¨äºäº¤äº’å°‘è®¡ç®—å¤šçš„åœºæ™¯ã€‚ è€å¹´ä»£åƒåœ¾å›æ”¶å™¨ Serial Oldåƒåœ¾å›æ”¶å™¨ä¸å¹´è½»ä»£çš„Serialåƒåœ¾å›æ”¶å™¨å¯¹åº”ï¼Œä¹Ÿæ˜¯å•çº¿ç¨‹ï¼Œä½¿ç”¨æ ‡è®°-æ•´ç†ç®—æ³•ã€‚ä¼˜ç‚¹ï¼šç®€å•è½»é‡çº§ï¼Œä½¿ç”¨èµ„æºå°‘ã€‚åœºæ™¯ï¼šä¹Ÿé€‚ç”¨äºå®¢æˆ·ç«¯åº”ç”¨ Parallel Oldåƒåœ¾å›æ”¶å™¨Parallel Scavengeåƒåœ¾å›æ”¶å™¨çš„è€å¹´ä»£ç‰ˆæœ¬ã€‚åœºæ™¯ï¼šå¤šCPUä¸‹ä½¿ç”¨ï¼Œè¿½æ±‚CPUååé‡ï¼Œé€‚ç”¨äºäº¤äº’å°‘è®¡ç®—å¤šçš„åœºæ™¯ã€‚ CMSåƒåœ¾å›æ”¶å™¨ä»¥æœ€çŸ­GCæ—¶é—´ä¸ºç›®æ ‡ï¼Œç”¨æˆ·çº¿ç¨‹ä¸GCçº¿ç¨‹å¯å¹¶å‘æ‰§è¡Œï¼Œåƒåœ¾å›æ”¶è¿‡ç¨‹ç”¨æˆ·ä¸ä¼šæ„Ÿåˆ°æ˜æ˜¾å¡é¡¿ã€‚é•¿æœŸæ¥çœ‹G1ã€ZGCç­‰æ›´é«˜çº§çš„åƒåœ¾å›æ”¶å™¨æ˜¯è¶‹åŠ¿ã€‚ CMSåƒåœ¾å›æ”¶å™¨ å…¨ç§°ï¼šMostly Concurrent Mark and Sweep Garbage Collectorï¼ˆä¸»è¦å¹¶å‘Â­æ ‡è®°Â­æ¸…é™¤Â­åƒåœ¾æ”¶é›†å™¨ï¼‰ CMSåœ¨å¹´è½»ä»£ä½¿ç”¨å¤åˆ¶ç®—æ³•ï¼Œåœ¨è€å¹´ä»£ä½¿ç”¨æ ‡è®°-æ¸…é™¤ç®—æ³•ã€‚å®ƒæŠŠè€—æ—¶çš„GCæ“ä½œé€šè¿‡å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œçš„ã€‚ ä¼˜ç‚¹ï¼šé¿å…è€å¹´ä»£GCå‡ºç°é•¿æ—¶é—´å¡é¡¿ ç¼ºç‚¹ï¼šå¯¹è€å¹´ä»£çš„å›æ”¶æ²¡æ•´ç†é˜¶æ®µï¼Œäº§ç”Ÿå†…å­˜ç¢ç‰‡éšæ—¶é—´æ¨ç§»å¢å¤šæ—¶å¿…é¡»è¦FullGCæ‰èƒ½æ¸…ç†ã€‚å¯èƒ½ä¼šå¯¼è‡´å¤§å¯¹è±¡åˆ›å»ºå¤±è´¥ã€‚ åœºæ™¯ï¼šä¸å¸Œæœ›GCåœé¡¿æ—¶é—´é•¿ä¸”CPUèµ„æºè¾ƒå……è¶³ å›æ”¶è¿‡ç¨‹ï¼š1.åˆå§‹æ ‡è®°é˜¶æ®µï¼šåªæ ‡è®°GC Rootsç›´æ¥å…³è”çš„å¯¹è±¡å’Œå¹´è½»ä»£ä¸­çš„å¼•ç”¨ï¼Œä¸å‘ä¸‹è¿½æº¯ï¼Œç¼©çŸ­äº†æ ‡è®°æ—¶GCæš‚åœæ—¶é—´ã€‚2.å¹¶å‘æ ‡è®°é˜¶æ®µï¼Œå¹¶å‘åœ°è¿½æº¯å¯è¾¾å¯¹è±¡ï¼ŒæŒç»­æ—¶é—´è¾ƒé•¿ä½†è·Ÿç”¨æˆ·çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œã€‚3.å¹¶å‘é¢„æ¸…ç†ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæ¸…ç†dirtyçŠ¶æ€çš„è€å¹´ä»£å¯¹è±¡ã€‚4.å¯é€‰çš„é¢„æ¸…ç†ã€‚5.æœ€ç»ˆæ ‡è®°ï¼Œä¼šGCæš‚åœã€‚6.å¹¶å‘æ¸…ç†ï¼Œç”¨æˆ·çº¿ç¨‹é‡æ–°æ¿€æ´»ï¼Œåˆ é™¤ä¸å¯è¾¾å¯¹è±¡ã€‚ å…³äºCMSçš„ç¢ç‰‡æ•´ç†é—®é¢˜ï¼šä¸¤ä¸ªå‚æ•° UseCMSCompactAtFullCollectionï¼ˆé»˜è®¤å¼€å¯ï¼‰ï¼šFullGCæ—¶å‹ç¼©ï¼Œæ•´ç†å†…å­˜ç¢ç‰‡ï¼Œä¼šé€ æˆè¾ƒé•¿æ—¶é—´åœé¡¿ã€‚ CMSFullGCsBeforeCompactionï¼šæ¯éš”å‡ æ¬¡FullGCåæ‰§è¡Œä¸€æ¬¡å¸¦å‹ç¼©çš„FullGCã€‚ æ€»ç»“CMSä¸­æœ‰å“ªäº›ä¼šé€ æˆSTW(GCåœé¡¿)çš„æ“ä½œï¼š STW = stop the world. åˆå§‹æ ‡è®°é˜¶æ®µ-è¾ƒçŸ­åœé¡¿ æœ€ç»ˆæ ‡è®°é˜¶æ®µ-è¾ƒçŸ­åœé¡¿ è€å¹´ä»£çš„å›æ”¶-è¾ƒé•¿åœé¡¿ Full GCé˜¶æ®µ-è¾ƒé•¿åœé¡¿ G1åƒåœ¾å›æ”¶å™¨ å…¨ç§°ï¼šGarbage Firstï¼ˆç›®çš„æ˜¯å°½å¯èƒ½æ¥è¿‘é¢„æœŸæš‚åœæ—¶é—´ï¼‰ ç›®å‰æ¯”è¾ƒå¥½çš„æ”¶é›†å™¨ï¼Œå…³æ³¨ä½å»¶è¿Ÿï¼Œç”¨äºæ›¿ä»£CMSçš„åŠŸèƒ½æ›´å¼ºå¤§çš„æ–°å‹æ”¶é›†å™¨ã€‚ å¼•å…¥äº†åˆ†åŒºæ¦‚å¿µï¼Œå¼±åŒ–äº†åˆ†å¸¦æ¦‚å¿µ ä¼˜ç‚¹ï¼šé¿å…è€å¹´ä»£GCå‡ºç°é•¿æ—¶é—´å¡é¡¿ï¼ŒåŒæ—¶ä¸CMSç›¸æ¯”è§£å†³äº†CMSäº§ç”Ÿç¢ç‰‡çš„ç¼ºé™·ã€‚å°½å¯èƒ½è¾¾åˆ°é¢„æœŸçš„æš‚åœæ—¶é—´. ç¼ºç‚¹ï¼šG1æä¾›çš„ä¸¤ç§GCæ¨¡å¼,YoungGC,MixedGCéƒ½æ˜¯STWçš„. åœºæ™¯ï¼šä¸å¸Œæœ›GCåœé¡¿æ—¶é—´é•¿,å¸Œæœ›GCåœé¡¿æ—¶é—´è¾ƒçŸ­å¹¶ä¿æŒç¨³å®š,ä¸”CPUèµ„æºè¾ƒå……è¶³ å›æ”¶è¿‡ç¨‹ï¼šG1æä¾›çš„ä¸¤ç§GCæ¨¡å¼,YoungGC,MixedGC [Young GCä¸»è¦æ˜¯å¯¹EdenåŒºè¿›è¡ŒGCï¼Œå®ƒåœ¨Edenç©ºé—´è€—å°½æ—¶ä¼šè¢«è§¦å‘ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒEdenç©ºé—´çš„æ•°æ®ç§»åŠ¨åˆ°Survivorç©ºé—´ä¸­ï¼Œå¦‚æœSurvivorç©ºé—´ä¸å¤Ÿï¼ŒEdenç©ºé—´çš„éƒ¨åˆ†æ•°æ®ä¼šç›´æ¥æ™‹å‡åˆ°è€å¹´ä»£ç©ºé—´ã€‚SurvivoråŒºçš„æ•°æ®ç§»åŠ¨åˆ°æ–°çš„SurvivoråŒºä¸­ï¼Œä¹Ÿæœ‰éƒ¨åˆ†æ•°æ®æ™‹å‡åˆ°è€å¹´ä»£ç©ºé—´ä¸­ã€‚æœ€ç»ˆEdenç©ºé—´çš„æ•°æ®ä¸ºç©ºï¼ŒGCåœæ­¢å·¥ä½œï¼Œåº”ç”¨çº¿ç¨‹ç»§ç»­æ‰§è¡Œã€‚] [Mixed GCä¸ä»…è¿›è¡Œæ­£å¸¸çš„æ–°ç”Ÿä»£åƒåœ¾æ”¶é›†ï¼ŒåŒæ—¶ä¹Ÿå›æ”¶éƒ¨åˆ†åå°æ‰«æçº¿ç¨‹æ ‡è®°çš„è€å¹´ä»£åˆ†åŒºã€‚GCæ­¥éª¤åˆ†2æ­¥ï¼š å…¨å±€å¹¶å‘æ ‡è®°ï¼ˆglobal concurrent markingï¼‰å’Œ æ‹·è´å­˜æ´»å¯¹è±¡ï¼ˆevacuationï¼‰ã€‚åœ¨è¿›è¡ŒMixed GCä¹‹å‰ï¼Œä¼šå…ˆè¿›è¡ŒGlobal Concurrent Markingï¼ˆå…¨å±€å¹¶å‘æ ‡è®°ï¼‰,åœ¨G1 GCä¸­ï¼Œå®ƒä¸»è¦æ˜¯ä¸ºMixed GCæä¾›æ ‡è®°æœåŠ¡çš„ï¼Œå¹¶ä¸æ˜¯ä¸€æ¬¡GCè¿‡ç¨‹çš„ä¸€ä¸ªå¿…é¡»ç¯èŠ‚ã€‚] æ€»ç»“G1ä¸­æœ‰å“ªäº›ä¼šé€ æˆSTW(GCåœé¡¿)çš„æ“ä½œï¼š STW = stop the world. YoungGC MixedGC G1å‚æ•°ç¤ºä¾‹ -Xmx40G -XX:+UseG1GC -XX:MaxHeapFreeRatio=10 -XX:MinHeapFreeRatio=1 -XX:G1PeriodicGCInterval=1800000 -XX:G1PeriodicGCSystemLoadThreshold=10 -XX:InitiatingHeapOccupancyPercent=50 -XX:MaxGCPauseMillis=300 -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/xxx.hprof -XX:OnOutOfMemoryError=kill -9 %p -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=128M -Xloggc:/var/logs/gc.log G1å¸¸ç”¨å‚æ•°é…ç½® G1è°ƒå‚JDKæ–‡æ¡£ G1åƒåœ¾å›æ”¶å™¨åœ¨jdk12åæ”¯æŒè‡ªåŠ¨å°†æœªä½¿ç”¨çš„å †å†…å­˜è¿”è¿˜ç»™æ“ä½œç³»ç»Ÿ:æ ¹æ®JEP 346(Javaå¢å¼ºç‰¹æ€§:Promptly Return Unused Committed Memory from G1) G1ä»…ä»…å½“Full GCæˆ–å¹¶å‘å‘¨æœŸæ—¶æ‰ä¼šè¿”å›å†…å­˜ã€‚é€šå¸¸çš„ï¼Œé™¤éåœ¨å¤–éƒ¨å¼ºåˆ¶çš„æ‰§è¡Œï¼ŒG1åœ¨å¾ˆå¤šæƒ…å†µä¸‹ä¸ä¼šè¿”å›å †å†…å­˜ç»™æ“ä½œç³»ç»Ÿã€‚å‚è€ƒJDKæ–‡æ¡£garbage-first-garbage-collector-tuning,ç›¸å…³å‚æ•°: -XX:+UseG1GC -XX:MaxGCPauseMillis=300 -XX:G1HeapRegionSize=32M -XX:MaxHeapFreeRatio=5 [ç©ºé—²å †ç©ºé—´çš„æœ€å¤§ç™¾åˆ†æ¯”ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šHeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100ï¼Œå€¼çš„åŒºé—´ä¸º0åˆ°100ï¼Œé»˜è®¤å€¼ä¸º 70ã€‚å¦‚æœHeapFreeRatio &gt; MaxHeapFreeRatioï¼Œåˆ™éœ€è¦è¿›è¡Œå †ç¼©å®¹ï¼Œç¼©å®¹çš„æ—¶æœºåº”è¯¥åœ¨æ¯æ¬¡åƒåœ¾å›æ”¶ä¹‹åã€‚MaxHeapFreeRatioå¯ä»¥é€šè¿‡jinfo -flag [+|-]MaxHeapFreeRatio æˆ–è€… jinfo -flag MaxHeapFreeRatio= æ¥åŠ¨æ€å¼€å¯æˆ–è®¾ç½®å€¼] -XX:MinHeapFreeRatio=1 [ç©ºé—²å †ç©ºé—´çš„æœ€å°ç™¾åˆ†æ¯”ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šHeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100ï¼Œå€¼çš„åŒºé—´ä¸º0åˆ°100ï¼Œé»˜è®¤å€¼ä¸º 40ã€‚å¦‚æœHeapFreeRatio &lt; MinHeapFreeRatioï¼Œåˆ™éœ€è¦è¿›è¡Œå †æ‰©å®¹ï¼Œæ‰©å®¹çš„æ—¶æœºåº”è¯¥åœ¨æ¯æ¬¡åƒåœ¾å›æ”¶ä¹‹åã€‚] -XX:G1PeriodicGCInterval=1800000 # JEP346ç‰¹æ€§ç”¨äºåœ¨ç³»ç»Ÿç©ºé—²æ—¶æ”¶æ•›å †ç©ºé—´ï¼Œå°†ç©ºé—²å†…å­˜äº¤è¿˜ç»™æ“ä½œç³»ç»Ÿ,æ¯éš”å¤šé•¿æ—¶é—´è¿›è¡Œæ£€æŸ¥ï¼Œå•ä½ï¼šæ¯«ç§’ -XX:G1PeriodicGCSystemLoadThreshold=0 # ç³»ç»Ÿè´Ÿè½½é˜€å€¼,éœ€è¦æ ¹æ®å®é™…è´Ÿè½½æƒ…å†µè¿›è¡Œè°ƒæ•´ 0è¡¨ç¤ºå¿½ç•¥æ­¤æ¡ä»¶ è§£é‡Š:G1PeriodicGCInterval=1800000è‡ªä¸Šæ¬¡GCåå·²ç»è¶…è¿‡1800000æ¯«ç§’æœªè¿›è¡Œå›æ”¶,ä¸”G1PeriodicGCSystemLoadThreshold=20å½“å‰ä¸»æœºå¹³å‡ä¸€åˆ†é’Ÿå†…è´Ÿè½½å°äº20,åˆ™è§¦å‘å›æ”¶(å¦‚æœä¸æŒ‡å®š-XX:+G1PeriodicGCInvokesConcurrentåˆ™è§¦å‘FullGC),G1PeriodicGCSystemLoadThreshold=0ä¸ºå¿½ç•¥æ­¤æ¡ä»¶ ç³»ç»Ÿè°ƒç”¨äº†System.gc()è§¦å‘çš„FullGCå½±å“è¾ƒå¤§,ä¸”åœ¨ä¸èƒ½ä¿®æ”¹æºç çš„æƒ…å†µä¸‹,å¯ä»¥é€šè¿‡è®¾ç½®-XX:+ExplicitGCInvokesConcurrentå‚æ•°å‡å°FullGCå¯¹ç³»ç»Ÿåœé¡¿çš„å½±å“. -XX:MaxGCPauseMillis [æš‚åœæ—¶é—´ï¼Œé»˜è®¤å€¼200msã€‚è¿™æ˜¯ä¸€ä¸ªè½¯æ€§ç›®æ ‡ï¼ŒG1ä¼šå°½é‡è¾¾æˆï¼Œå¦‚æœè¾¾ä¸æˆï¼Œä¼šé€æ¸åšè‡ªæˆ‘è°ƒæ•´ã€‚å¯¹äºYoung GCæ¥è¯´ï¼Œä¼šé€æ¸å‡å°‘EdenåŒºä¸ªæ•°ï¼Œå‡å°‘Edenç©ºé—´é‚£ä¹ˆYoung GCçš„å¤„ç†æ—¶é—´å°±ä¼šç›¸åº”å‡å°‘ã€‚å¯¹äºMixed GCï¼ŒG1ä¼šè°ƒæ•´æ¯æ¬¡Choose Csetçš„æ¯”ä¾‹ï¼Œé»˜è®¤æœ€å¤§å€¼æ˜¯10%ï¼Œå½“ç„¶æ¯æ¬¡é€‰æ‹©çš„Csetå°‘äº†ï¼Œæ‰€è¦ç»å†çš„Mixed GCçš„æ¬¡æ•°ä¼šç›¸åº”å¢åŠ ã€‚å‡å°‘Edençš„æ€»ç©ºé—´æ—¶ï¼Œå°±ä¼šæ›´åŠ é¢‘ç¹çš„è§¦å‘Young GCï¼Œä¹Ÿå°±æ˜¯ä¼šåŠ å¿«Mixed GCçš„æ‰§è¡Œé¢‘ç‡ï¼Œå› ä¸ºMixed GCæ˜¯ç”±Young GCè§¦å‘çš„ï¼Œæˆ–è€…è¯´å€ŸæœºåŒæ—¶æ‰§è¡Œçš„ã€‚é¢‘ç¹GCä¼šå¯¹å¯¹åº”ç”¨çš„ååé‡é€ æˆå½±å“ï¼Œæ¯æ¬¡Mixed GCå›æ”¶æ—¶é—´å¤ªçŸ­ï¼Œå›æ”¶çš„åƒåœ¾é‡å¤ªå°‘ï¼Œå¯èƒ½æœ€åGCçš„åƒåœ¾æ¸…ç†é€Ÿåº¦èµ¶ä¸ä¸Šåº”ç”¨äº§ç”Ÿçš„é€Ÿåº¦ï¼Œé‚£ä¹ˆå¯èƒ½ä¼šé€ æˆä¸²è¡Œçš„Full GCï¼Œè¿™æ˜¯è¦æåŠ›é¿å…çš„ã€‚æ‰€ä»¥æš‚åœæ—¶é—´è‚¯å®šä¸æ˜¯è®¾ç½®çš„è¶Šå°è¶Šå¥½ï¼Œå½“ç„¶ä¹Ÿä¸èƒ½è®¾ç½®çš„åå¤§ï¼Œè½¬è€ŒæŒ‡æœ›G1è‡ªå·±ä¼šå°½å¿«çš„å¤„ç†ï¼Œè¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ä¸€æ¬¡å…¨éƒ¨å¹¶å‘æ ‡è®°åè§¦å‘çš„Mixed GCæ¬¡æ•°å˜å°‘ï¼Œä½†æ¯æ¬¡çš„æ—¶é—´å˜é•¿ï¼ŒSTWæ—¶é—´å˜é•¿ï¼Œå¯¹åº”ç”¨çš„å½±å“æ›´åŠ æ˜æ˜¾ã€‚] -XX:ConcGCThreadsæŒ‡å®šå¹¶å‘æ”¶é›†çº¿ç¨‹æ•°[é»˜è®¤æ˜¯-XX:ParallelGCThreads/4ï¼Œä¹Ÿå°±æ˜¯åœ¨éSTWæœŸé—´çš„GCå·¥ä½œçº¿ç¨‹æ•°,å½“å¹¶å‘å‘¨æœŸæ—¶é—´è¿‡é•¿æ—¶ï¼Œå¯ä»¥å°è¯•è°ƒå¤§GCå·¥ä½œçº¿ç¨‹æ•°ï¼Œä½†æ˜¯è¿™ä¹Ÿæ„å‘³ç€æ­¤æœŸé—´åº”ç”¨æ‰€å çš„çº¿ç¨‹æ•°å‡å°‘ï¼Œä¼šå¯¹ååé‡æœ‰ä¸€å®šå½±å“ã€‚] -XX:InitiatingHeapOccupancyPercentæŒ‡å®šè§¦å‘å…¨å±€å¹¶å‘æ ‡è®°(é€ æˆSTW)çš„è€å¹´ä»£ä½¿ç”¨å æ¯”ï¼Œé»˜è®¤å€¼45%.å¦‚æœMixed GCå‘¨æœŸç»“æŸåè€å¹´ä»£ä½¿ç”¨ç‡è¿˜æ˜¯è¶…è¿‡45%,é‚£ä¹ˆä¼šå†æ¬¡è§¦å‘å…¨å±€å¹¶å‘æ ‡è®°è¿‡ç¨‹ï¼Œè¿™æ ·å°±ä¼šå¯¼è‡´é¢‘ç¹çš„è€å¹´ä»£GCï¼Œå½±å“åº”ç”¨ååé‡ã€‚ GCå°æŠ€å·§ GCæ—¥å¿—æŸ¥çœ‹ åŠ -XX:+PrintGCDetailså‚æ•° æŸ¥çœ‹GCæ—¥å¿—ï¼Œæœ‰å…³GCæ—¥å¿—çš„è§£æåç»­æˆ‘ä¼šå•å†™ä¸€ä¸ªåšå®¢ã€‚ ä½¿ç”¨Sunå…¬å¸çš„gchistoï¼Œgcviewerç¦»çº¿åˆ†æå·¥å…· ä½¿ç”¨JDKè‡ªå¸¦çš„JConsole ä½¿ç”¨jstat -gcutil pidå‘½ä»¤ ä½¿ç”¨JvisualVMå·¥å…· æŸ¥çœ‹å½“å‰Javaç‰ˆæœ¬åƒåœ¾å›æ”¶ä¿¡æ¯ $ java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=266248768 -XX:MaxHeapSize=4259980288 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC java version &quot;1.8.0_191&quot; Java(TM) SE Runtime Environment (build 1.8.0_191-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) è®¾ç½®åº”ç”¨çš„åƒåœ¾å›æ”¶å™¨ï¼š -XX:+UseSerialGC å¹´è½»ä»£å’Œè€å¹´ä»£éƒ½ç”¨ä¸²è¡Œæ”¶é›†å™¨ -XX:+UseParNewGC å¹´è½»ä»£ä½¿ç”¨ ParNewï¼Œè€å¹´ä»£ä½¿ç”¨ Serial Old [JDK9è¢«æŠ›å¼ƒ] -XX:+UseParallelGC å¹´è½»ä»£ä½¿ç”¨ ParallerGCï¼Œè€å¹´ä»£ä½¿ç”¨ Serial Old -XX:+UseParallelOldGC æ–°ç”Ÿä»£å’Œè€å¹´ä»£éƒ½ä½¿ç”¨å¹¶è¡Œæ”¶é›†å™¨ -XX:+UseConcMarkSweepGCï¼Œè¡¨ç¤ºå¹´è½»ä»£ä½¿ç”¨ ParNewï¼Œè€å¹´ä»£çš„ç”¨ CMS -XX:+UseG1GC ä½¿ç”¨ G1åƒåœ¾å›æ”¶å™¨ -XX:+UseZGC ä½¿ç”¨ ZGC åƒåœ¾å›æ”¶å™¨ å¸¸é‡æ± åˆ†é™æ€å¸¸é‡æ± å’Œè¿è¡Œæ—¶å¸¸é‡æ± ï¼Œé™æ€å¸¸é‡æ± åœ¨ .class ä¸­ï¼Œè¿è¡Œæ—¶å¸¸é‡æ± åœ¨æ–¹æ³•åŒºä¸­ã€‚å­—ç¬¦ä¸²æ± åœ¨JDK 1.7 ä¹‹åè¢«åˆ†ç¦»åˆ°å †åŒºã€‚String str = new String(â€œHello worldâ€) åˆ›å»ºäº† 2 ä¸ªå¯¹è±¡ï¼Œä¸€ä¸ªé©»ç•™åœ¨å­—ç¬¦ä¸²æ± ï¼Œä¸€ä¸ªåˆ†é…åœ¨ Java å †ï¼Œstr æŒ‡å‘å †ä¸Šçš„å®ä¾‹ã€‚String.intern() èƒ½åœ¨è¿è¡Œæ—¶å‘å­—ç¬¦ä¸²æ± æ·»åŠ å¸¸é‡ã€‚ä¸ºä»€ä¹ˆStringä¸ºfinalï¼š1.ä¸ºäº†å®ç°å­—ç¬¦ä¸²æ± ï¼šåˆ›å»ºå­—ç¬¦ä¸²å¸¸é‡æ—¶ï¼ŒJVMä¼šæ£€æµ‹å­—ç¬¦ä¸²å¸¸é‡æ± ï¼Œå¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›å¸¸é‡æ± ä¸­çš„å®ä¾‹çš„å¼•ç”¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±å®ä¾‹åŒ–å¹¶æ”¾å…¥å­—ç¬¦ä¸²å¸¸é‡æ± ã€‚å› ä¸ºStringä¸ºFinalç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥ååˆ†è‚¯å®šå­—ç¬¦ä¸²å¸¸é‡æ± ä¸å­˜åœ¨ä¸¤ä¸ªç›¸åŒçš„å­—ç¬¦ä¸²ã€‚2.ä¸ºäº†çº¿ç¨‹å®‰å…¨ï¼šå› ä¸ºå®ƒä¸å¯å˜ï¼Œæœ¬èº«å°±æ˜¯çº¿ç¨‹å®‰å…¨çš„3.èŠ‚çº¦å†…å­˜4.HashMapçš„keyå¾€å¾€ç”¨Stringæ˜¯å› ä¸ºStringä¸å¯å˜ï¼Œåœ¨è¢«åˆ›å»ºæ—¶HashCodeå°±è¢«ç¼“å­˜äº†ä¸éœ€è¦é‡æ–°è®¡ç®—ã€‚ GCæ˜¯æ€ä¹ˆåˆ¤æ–­å¯¹è±¡æ˜¯è¢«æ ‡è®°çš„ï¼Ÿé€šè¿‡æšä¸¾æ ¹èŠ‚ç‚¹çš„æ–¹å¼ï¼Œé€šè¿‡jvmæä¾›çš„ä¸€ç§oopMapçš„æ•°æ®ç»“æ„ï¼Œç®€å•æ¥è¯´å°±æ˜¯ä¸è¦å†é€šè¿‡å»éå†å†…å­˜é‡Œçš„ä¸œè¥¿ï¼Œè€Œæ˜¯é€šè¿‡OOPMapçš„æ•°æ®ç»“æ„å»è®°å½•è¯¥è®°å½•çš„ä¿¡æ¯,æ¯”å¦‚è¯´å®ƒå¯ä»¥ä¸ç”¨å»éå†æ•´ä¸ªæ ˆï¼Œè€Œæ˜¯æ‰«ææ ˆä¸Šé¢å¼•ç”¨çš„ä¿¡æ¯å¹¶è®°å½•ä¸‹æ¥ã€‚æ€»ç»“:é€šè¿‡OOPMapæŠŠæ ˆä¸Šä»£è¡¨å¼•ç”¨çš„ä½ç½®å…¨éƒ¨è®°å½•ä¸‹æ¥ï¼Œé¿å…å…¨æ ˆæ‰«æï¼ŒåŠ å¿«æšä¸¾æ ¹èŠ‚ç‚¹çš„é€Ÿåº¦ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸€ä¸ªæä¸ºé‡è¦çš„ä½œç”¨ï¼Œå¯ä»¥å¸®HotSpotå®ç°å‡†ç¡®å¼GCã€è¿™è¾¹çš„å‡†ç¡®å…³é”®å°±æ˜¯ç±»å‹ï¼Œå¯ä»¥æ ¹æ®ç»™å®šä½ç½®çš„æŸå—æ•°æ®çŸ¥é“å®ƒçš„å‡†ç¡®ç±»å‹ï¼ŒHotSpotæ˜¯é€šè¿‡oopMapå¤–éƒ¨è®°å½•ä¸‹è¿™äº›ä¿¡æ¯ï¼Œå­˜æˆæ˜ å°„è¡¨ä¸€æ ·çš„ä¸œè¥¿ã€‘ã€‚ CMSæ”¶é›†å™¨æ˜¯å¦ä¼šæ‰«æå¹´è½»ä»£ï¼Ÿä¼šï¼Œåœ¨åˆå§‹æ ‡è®°çš„æ—¶å€™ä¼šæ‰«ææ–°ç”Ÿä»£ã€‚è™½ç„¶cmsæ˜¯è€å¹´ä»£æ”¶é›†å™¨ï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“å¹´è½»ä»£çš„å¯¹è±¡æ˜¯å¯ä»¥æ™‹å‡ä¸ºè€å¹´ä»£çš„ï¼Œä¸ºäº†ç©ºé—´åˆ†é…æ‹…ä¿ï¼Œè¿˜æ˜¯æœ‰å¿…è¦å»æ‰«æå¹´è½»ä»£ã€‚ commands:JDK9ä¹‹å‰ï¼šjmap -heap JDK9åŠä¹‹åï¼šjhsdb jmap â€“heap â€“pid G1 Heap(used) = G1 Young Generation(Eden Space+Survivor Space)(used) + G1 Old Generation(used) å°æ ‡é¢˜1å°æ ‡é¢˜2åŸç†ï¼ˆä¸­æ ‡é¢˜ï¼‰ å­—ä½“ æ–œä½“æ–‡æœ¬*æ–œä½“æ–‡æœ¬ *ç²—ä½“æ–‡æœ¬**ç²—ä½“æ–‡æœ¬ *ç²—æ–œä½“æ–‡æœ¬**ç²—æ–œä½“æ–‡æœ¬å¸¦ä¸‹åˆ’çº¿æ–‡æœ¬ å­—é¢œè‰²å¤§å°This is some text!This is some text!This is some text! ä¸€äº›å¸¸ç”¨Javaå‘½ä»¤å‚è€ƒèµ„æ–™JavaåŒäº²å§”æ´¾æœºåˆ¶åŠå…¶ä½œç”¨æ‹‰å‹¾ç½‘MetaSpaceæ•´ä½“ä»‹ç»æ·±å…¥ç†è§£JMMå’ŒGC","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://shmily-qjj.top/tags/Java/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"çº¿ç¨‹è¿›ç¨‹ä¸é”","slug":"çº¿ç¨‹è¿›ç¨‹ä¸é”","date":"2020-02-11T03:20:08.000Z","updated":"2022-12-11T05:35:07.924Z","comments":true,"path":"6f97dc89/","link":"","permalink":"https://shmily-qjj.top/6f97dc89/","excerpt":"","text":"çº¿ç¨‹è¿›ç¨‹ä¸é”çº¿ç¨‹ï¼Œè¿›ç¨‹ä¸é”æ˜¯ä¸€å®šè¦æŒæ¡çš„åŸºç¡€çŸ¥è¯†ç‚¹ï¼Œå¸Œæœ›èƒ½é€šè¿‡å†™åšå®¢çš„æ–¹å¼åŠ æ·±å°è±¡ï¼Œå¹¶ä¸”åœ¨ä»¥åèƒ½å¤Ÿéšæ—¶è¡¥å……å’Œå›çœ‹ã€‚ è¿›ç¨‹æ¦‚å¿µ1.ä»€ä¹ˆæ˜¯è¿›ç¨‹è¿›ç¨‹æ˜¯å¯å¹¶å‘æ‰§è¡Œçš„ç¨‹åºåœ¨æŸä¸ªæ•°æ®é›†åˆä¸Šçš„ä¸€æ¬¡è®¡ç®—æ´»åŠ¨ï¼Œä¹Ÿæ˜¯æ“ä½œç³»ç»Ÿè¿›è¡Œèµ„æºåˆ†é…å’Œè°ƒåº¦çš„åŸºæœ¬å•ä½ã€‚ 2.è¿›ç¨‹çš„ä¸‰ç§åŸºæœ¬çŠ¶æ€è¿è¡Œæ€ï¼šå½“è¿›ç¨‹å¾—åˆ°å¤„ç†æœºï¼Œå…¶æ‰§è¡Œç¨‹åºæ­£åœ¨å¤„ç†æœºä¸Šè¿è¡Œæ—¶çš„çŠ¶æ€ç§°ä¸ºè¿è¡ŒçŠ¶æ€ã€‚å°±ç»ªæ€ï¼šå½“ä¸€ä¸ªè¿›ç¨‹å·²ç»å‡†å¤‡å°±ç»ªï¼Œä¸€æ—¦å¾—åˆ°CPUï¼Œå°±å¯ç«‹å³è¿è¡Œï¼Œè¿™æ—¶è¿›ç¨‹æ‰€å¤„çš„çŠ¶æ€ç§°ä¸ºå°±ç»ªçŠ¶æ€ã€‚é˜»å¡æ€ï¼šè‹¥ä¸€ä¸ªè¿›ç¨‹æ­£ç­‰å¾…ç€æŸä¸€äº‹ä»¶å‘ç”Ÿ(å¦‚ç­‰å¾…è¾“å…¥è¾“å‡ºæ“ä½œçš„å®Œæˆ)è€Œæš‚æ—¶åœæ­¢æ‰§è¡Œçš„çŠ¶æ€ç§°ä¸ºç­‰å¾…çŠ¶æ€ã€‚å¤„äºç­‰å¾…çŠ¶æ€çš„è¿›ç¨‹ä¸å…·å¤‡è¿è¡Œçš„æ¡ä»¶ï¼Œå³ä½¿ç»™å®ƒCPUï¼Œä¹Ÿæ— æ³•æ‰§è¡Œã€‚ç³»ç»Ÿä¸­æœ‰å‡ ä¸ªç­‰å¾…è¿›ç¨‹é˜Ÿåˆ—ï¼ˆæŒ‰ç­‰å¾…çš„äº‹ä»¶ç»„æˆç›¸åº”çš„ç­‰å¾…é˜Ÿåˆ—ï¼‰ã€‚ è¿›ç¨‹çš„äº”ç§çŠ¶æ€ï¼šæ–°å»º-å°±ç»ª-è¿è¡Œ-é˜»å¡-æ­»äº¡å°±ç»ª-&gt;è¿è¡Œ:ç­‰å¾…cpuè°ƒåº¦è¿è¡Œ-&gt;é˜»å¡:æ”¾å¼ƒcpuæ—¶é—´ç‰‡/IOè¯·æ±‚è¿è¡Œ-&gt;æ­»äº¡:runè¿è¡Œå®Œæˆ–æŠ›å¼‚å¸¸ 3.è¿›ç¨‹çŠ¶æ€åˆ‡æ¢è¿‡ç¨‹è¿è¡Œåˆ°ç­‰å¾…ï¼šç­‰å¾…æŸäº‹ä»¶çš„å‘ç”Ÿï¼ˆå¦‚ç­‰å¾…I/Oå®Œæˆï¼‰ç­‰å¾…åˆ°å°±ç»ªï¼šäº‹ä»¶å·²ç»å‘ç”Ÿï¼ˆå¦‚I/Oå®Œæˆï¼‰è¿è¡Œåˆ°å°±ç»ªï¼šæ—¶é—´ç‰‡åˆ°ï¼ˆä¾‹å¦‚ï¼Œä¸¤èŠ‚è¯¾æ—¶é—´åˆ°ï¼Œä¸‹è¯¾ï¼‰æˆ–å‡ºç°æ›´é«˜ä¼˜å…ˆçº§è¿›ç¨‹ï¼Œå½“å‰è¿›ç¨‹è¢«è¿«è®©å‡ºå¤„ç†å™¨ã€‚å°±ç»ªåˆ°è¿è¡Œï¼šå½“å¤„ç†æœºç©ºé—­æ—¶ï¼Œç”±è°ƒåº¦ï¼ˆåˆ†æ´¾ï¼‰ç¨‹åºä»å°±ç»ªè¿›ç¨‹é˜Ÿåˆ—ä¸­é€‰æ‹©ä¸€ä¸ªè¿›ç¨‹å ç”¨CPUã€‚ 4.å¹¶å‘ä¸å¹¶è¡ŒåŒºåˆ«ï¼Ÿå¹¶å‘çš„å…³é”®æ˜¯ä½ æœ‰å¤„ç†å¤šä¸ªä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ä¸€å®šè¦åŒæ—¶ã€‚å¹¶è¡Œçš„å…³é”®æ˜¯ä½ æœ‰åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡çš„èƒ½åŠ›ã€‚ çº¿ç¨‹æ¦‚å¿µ1.ä»€ä¹ˆæ˜¯çº¿ç¨‹çº¿ç¨‹æ˜¯è¿›ç¨‹çš„ä¸€ä¸ªå®ä½“,æ˜¯CPUè°ƒåº¦å’Œåˆ†æ´¾çš„åŸºæœ¬å•ä½,å®ƒæ˜¯æ¯”è¿›ç¨‹æ›´å°çš„èƒ½ç‹¬ç«‹è¿è¡Œçš„åŸºæœ¬å•ä½.çº¿ç¨‹è‡ªå·±åŸºæœ¬ä¸Šä¸æ‹¥æœ‰ç³»ç»Ÿèµ„æº,åªæ‹¥æœ‰ä¸€ç‚¹åœ¨è¿è¡Œä¸­å¿…ä¸å¯å°‘çš„èµ„æº(å¦‚ç¨‹åºè®¡æ•°å™¨,ä¸€ç»„å¯„å­˜å™¨å’Œæ ˆ),ä½†æ˜¯å®ƒå¯ä¸åŒå±ä¸€ä¸ªè¿›ç¨‹çš„å…¶ä»–çš„çº¿ç¨‹å…±äº«è¿›ç¨‹æ‰€æ‹¥æœ‰çš„å…¨éƒ¨èµ„æº. 2.çº¿ç¨‹ä¸è¿›ç¨‹çš„å…³ç³» è¿›ç¨‹æ˜¯ç³»ç»Ÿèµ„æºåˆ†é…çš„åŸºæœ¬å•ä½-çº¿ç¨‹æ˜¯ä»»åŠ¡è°ƒåº¦æ‰§è¡Œçš„åŸºæœ¬å•ä½ è¿›ç¨‹åˆ‡æ¢å¼€é”€å¤§-çº¿ç¨‹é—´åˆ‡æ¢å¼€é”€å° è¿›ç¨‹é—´èµ„æºå’Œåœ°å€ç©ºé—´ç‹¬ç«‹-åŒä¸€è¿›ç¨‹ä¸‹çº¿ç¨‹å…±äº«èµ„æºå’Œåœ°å€ç©ºé—´ è¿›ç¨‹å´©æºƒä¸å½±å“å…¶ä»–è¿›ç¨‹-çº¿ç¨‹å´©æºƒæ•´ä¸ªè¿›ç¨‹éƒ½å´©æºƒï¼ˆå¤šè¿›ç¨‹å¥å£®æ€§å¼ºï¼‰ 3.çº¿ç¨‹é—´å…±äº«çš„èµ„æº çº¿ç¨‹é—´å…±äº«å †å’Œæ–¹æ³•åŒº è™šæ‹Ÿæœºæ ˆã€æœ¬åœ°æ–¹æ³•æ ˆã€ç¨‹åºè®¡æ•°å™¨ä¸å…±äº« ä¸ºä»€ä¹ˆç¨‹åºè®¡æ•°å™¨èµ„æºä¸å…±äº«ï¼šç¨‹åºè®¡æ•°å™¨ä¸å…±äº«æ˜¯ä¸ºäº†çº¿ç¨‹åˆ‡æ¢åèƒ½æ¢å¤åˆ°æ­£ç¡®çš„æ‰§è¡Œä½ç½® ä¸ºä»€ä¹ˆè™šæ‹Ÿæœºæ ˆå’Œæœ¬åœ°æ–¹æ³•æ ˆç§æœ‰ï¼šæ ˆå¸§ç”¨äºå­˜å‚¨å±€éƒ¨å˜é‡è¡¨ã€æ“ä½œæ•°æ ˆã€å¸¸é‡æ± å¼•ç”¨ç­‰ä¿¡æ¯ï¼Œè€Œæœ¬åœ°æ–¹æ³•æ ˆåˆ™ä¸ºè™šæ‹Ÿæœºä½¿ç”¨åˆ°çš„Nativeæ–¹æ³•æœåŠ¡ï¼Œä¸¤è€…ç›¸ä¼¼ã€‚ä¸ºäº†ä¿è¯å±€éƒ¨å˜é‡ä¸è¢«å…¶ä»–çº¿ç¨‹è®¿é—®ï¼Œæ‰€ä»¥ä¸å…±äº«ã€‚ ä¸ºä»€ä¹ˆå †å’Œæ–¹æ³•åŒºå…±äº«ï¼šæ–°åˆ›å»ºçš„å¯¹è±¡å­˜æ”¾åœ¨å †ä¸­ï¼Œç±»ä¿¡æ¯ã€å¸¸é‡ã€é™æ€å˜é‡ã€å³æ—¶ç¼–è¯‘å™¨ç¼–è¯‘åçš„ä»£ç å­˜æ”¾åœ¨æ–¹æ³•åŒºã€‚ å¤šçº¿ç¨‹ä¸ä¸€å®šæé«˜æ•ˆç‡ï¼Œåªæ˜¯è®©CPUåˆ©ç”¨ç‡æ›´é«˜ï¼Œ**å¦‚æœé¢‘ç¹åˆ‡æ¢å¯èƒ½æ•ˆç‡åè€Œæ›´ä½(è§ç¬¬7æ¡è§£é‡Š)**ã€‚ 4.çº¿ç¨‹é—´é€šä¿¡æ–¹å¼ å…¨å±€å˜é‡ çº¿ç¨‹ä¸Šä¸‹æ–‡ å…±äº«å†…å­˜ å¥—æ¥å­—Socket IPCé€šä¿¡å¦‚ä½•å®ç°çº¿ç¨‹é—´é€šè®¯ï¼š1.é€šè¿‡ç±»å˜é‡ç›´æ¥å°†æ•°æ®æ”¾åˆ°ä¸»å­˜ä¸­ 2.é€šè¿‡å¹¶å‘çš„æ•°æ®ç»“æ„æ¥å­˜å‚¨æ•°æ® 3.ä½¿ç”¨volatileå˜é‡æˆ–è€…é” 4.è°ƒç”¨atomicç±» 5.è¿›ç¨‹é—´é€šä¿¡æ–¹å¼ ç®¡é“ æœ‰åç®¡é“ ä¿¡å·é‡ å…±äº«å†…å­˜ æ¶ˆæ¯é˜Ÿåˆ— ä¿¡å· å¥—æ¥å­— 6.å®ˆæŠ¤çº¿ç¨‹ï¼Ÿä¸€ç§åå°ç‰¹æ®Šè¿›ç¨‹ï¼Œæ‰€æœ‰ç”¨æˆ·çš„çº¿ç¨‹éƒ½é€€å‡ºäº†ï¼Œæ²¡æœ‰è¢«å®ˆæŠ¤çš„å¯¹è±¡äº†ï¼Œä¹Ÿä¸éœ€è¦å®ˆæŠ¤çº¿ç¨‹äº†ï¼Œè¿™æ—¶å®ˆæŠ¤çº¿ç¨‹å…³é—­ã€‚ï¼ˆJVMé€€å‡ºäº†å®ƒä¼šå…³é—­ï¼‰æ¯”å¦‚GCçº¿ç¨‹å°±æ˜¯å®ˆæŠ¤çº¿ç¨‹ã€‚ç”¨æˆ·çº¿ç¨‹å¯ä»¥è½¬ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼šthread.setDaemon(true); 7.çº¿ç¨‹åˆ‡æ¢å¼€é”€ä¸­æ–­å¤„ç†ï¼Œå¤šä»»åŠ¡å¤„ç†ï¼Œç”¨æˆ·æ€åˆ‡æ¢ç­‰åŸå› å¯¼è‡´CPUä»ä¸€ä¸ªçº¿ç¨‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªçº¿ç¨‹ã€‚çº¿ç¨‹ä¸Šä¸‹æ–‡åˆ‡æ¢ä»£ä»·æ˜¯é«˜æ˜‚çš„ï¼Œä¸Šä¸‹æ–‡åˆ‡æ¢çš„å»¶è¿Ÿç”±å¾ˆå¤šå› ç´ å†³å®šï¼Œå¹³å‡è¦50-100nsï¼Œè€ŒCPUæ¯æ ¸å¿ƒæ¯nsæ‰§è¡Œåå‡ æ¡æŒ‡ä»¤ï¼Œåˆ‡æ¢çš„è¿‡ç¨‹å°±èŠ±è´¹å‡ ç™¾è‡³å‡ åƒæ¡æŒ‡ä»¤çš„æ‰§è¡Œæ—¶é—´ã€‚å¦‚æœè·¨æ ¸ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œä»£ä»·æ›´åŠ é«˜æ˜‚ã€‚ 7.çº¿ç¨‹çš„å‡ ç§çŠ¶æ€ï¼Ÿ æ–°å»º(NEW):çº¿ç¨‹è¢«åˆ›å»ºå‡ºæ¥ä½†è¿˜æœªå¯åŠ¨å°±ç»ª(RUNNABLE):è°ƒç”¨start()åï¼Œçº¿ç¨‹å‡†å¤‡å°±ç»ªï¼Œç­‰åˆ°CPUåˆ†é…èµ„æºå°±å¯ä»¥è¿è¡Œé˜»å¡(BLOCKED):çº¿ç¨‹å¤„äºæ´»è·ƒçŠ¶æ€ï¼Œç­‰å¾…Monitorç›‘è§†å™¨é”ï¼Œç­‰å¾…çº¿ç¨‹åŒæ­¥é”ç­‰å¾…(WAITING):ç­‰å¾…å¦ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œï¼Œå¦‚è°ƒç”¨äº†wait(),å°±è¦ç­‰å¦ä¸€ä¸ªçº¿ç¨‹notify()æˆ–notifyAll()æ‰å”¤é†’è®¡æ—¶ç­‰å¾…(TIMED_WAITING):è°ƒç”¨äº†wait(timeout)æˆ–join(timeout)æŒ‡å®šäº†è¶…æ—¶æ—¶é—´ç»ˆæ­¢(TERMINATED):çº¿ç¨‹æ‰§è¡Œå®Œæ¯• ç»ˆæ­¢å¹¶é‡Šæ”¾èµ„æº æ€»ç»“ï¼š çº¿ç¨‹æ±  ä¸ºä»€ä¹ˆè¦ç”¨çº¿ç¨‹æ± ï¼Ÿ é¢‘ç¹åˆ›å»ºå’Œé”€æ¯çº¿ç¨‹è´¹æ—¶ä½æ•ˆè€Œä¸”æµªè´¹å†…å­˜(çº¿ç¨‹æ­»äº¡ï¼Œç›¸å…³å¯¹è±¡å˜æˆåƒåœ¾)ã€‚çº¿ç¨‹æ± å°½å¯èƒ½å‡å°‘äº†å¯¹è±¡åˆ›å»ºå’Œé”€æ¯çš„æ¬¡æ•°ï¼Œè®©çº¿ç¨‹è¿è¡Œå®Œä¸ç«‹å³é”€æ¯ï¼Œè€Œæ˜¯é‡å¤ä½¿ç”¨ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚ ThreadPoolExecutoræ˜¯çº¿ç¨‹æ± çš„æ ¸å¿ƒç±»ï¼Œæ„é€ å‚æ•°çš„æ„ä¹‰ï¼š corePoolSizeï¼šå¸¸é©»æ ¸å¿ƒçº¿ç¨‹æ•°ï¼Œå¦‚æœä¸º0ä¸”æ± ä¸­æ— çº¿ç¨‹æ‰§è¡Œæ—¶è‡ªåŠ¨é”€æ¯çº¿ç¨‹æ± ã€‚å¦‚æœå¤§äº0ï¼Œçº¿ç¨‹æ‰§è¡Œå®Œæ¯•åä¸ä¼šé”€æ¯çº¿ç¨‹ï¼Œè€Œæ˜¯è¿›å…¥ç¼“å­˜é˜Ÿåˆ—ç­‰å¾…å†æ¬¡è¢«è¿è¡Œã€‚è¿™ä¸ªå‚æ•°è®¾ç½®è¿‡å°ä¼šé¢‘ç¹åˆ›å»ºé”€æ¯çº¿ç¨‹ï¼Œè¿‡å¤§ä¼šæµªè´¹ç³»ç»Ÿèµ„æºã€‚ maximunPoolSizeï¼šçº¿ç¨‹æ± èƒ½åˆ›å»ºæœ€å¤§çš„çº¿ç¨‹æ•°é‡ã€‚å¦‚æœå¸¸é©»æ ¸å¿ƒçº¿ç¨‹æ•°å’Œç¼“å­˜é˜Ÿåˆ—éƒ½å·²ç»æ»¡äº†ï¼Œæ–°çš„ä»»åŠ¡è¿›æ¥å°±ä¼šåˆ›å»ºæ–°çš„çº¿ç¨‹æ¥æ‰§è¡Œã€‚ä½†æ˜¯æ•°é‡ä¸èƒ½è¶…è¿‡maximunPoolSizeï¼Œå¦ä¾§ä¼šé‡‡å–æ‹’ç»æ¥å—ä»»åŠ¡ç­–ç•¥ã€‚ keepAliveTimeï¼šçº¿ç¨‹å­˜æ´»æ—¶é—´ï¼Œæœªæ‰§è¡Œçš„çº¿ç¨‹ç©ºé—²è¶…è¿‡è¯¥æ—¶é—´åˆ™ç»ˆæ­¢ã€‚å¤šä½™çº¿ç¨‹ä¼šé”€æ¯ç›´åˆ°çº¿ç¨‹æ•°é‡è¾¾åˆ°corePoolSizeã€‚å¦‚æœcorePoolSizeç­‰äºMaximumPoolSizeï¼Œè¶…è¿‡ç©ºé—²æ—¶é—´ä¹Ÿä¸ä¼šé”€æ¯ä»»ä½•çº¿ç¨‹ã€‚ unitï¼šå­˜æ´»æ—¶é—´å•ä½ï¼Œå’ŒkeepAliveTimeé…åˆä½¿ç”¨ã€‚ workQueueï¼šçº¿ç¨‹æ± æ‰§è¡Œçš„ä»»åŠ¡é˜Ÿåˆ—ï¼ˆç¼“å­˜é˜Ÿï¼‰åˆ—ï¼Œç”¨æ¥å­˜æ”¾ç­‰å¾…è¢«æ‰§è¡Œçš„ä»»åŠ¡ã€‚ threadFactoryï¼šçº¿ç¨‹å·¥å‚ï¼Œç”¨æ¥åˆ›å»ºçº¿ç¨‹ï¼Œä¸€èˆ¬æœ‰ä¸‰ç§é€‰æ‹©ç­–ç•¥ã€‚ï¼ˆä¸€èˆ¬ç”¨é»˜è®¤å³å¯ï¼‰ ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; handlerï¼šæ‹’ç»å¤„ç†ç­–ç•¥ï¼Œçº¿ç¨‹æ•°é‡å¤§äºæœ€å¤§çº¿ç¨‹æ•°å°±ä¼šé‡‡ç”¨æ‹’ç»å¤„ç†ç­–ç•¥ï¼Œå››ç§ç­–ç•¥ä¸º ThreadPoolExecutor.AbortPolicy:ä¸¢å¼ƒä»»åŠ¡å¹¶æŠ›å‡ºRejectedExecutionExceptionå¼‚å¸¸ã€‚ ThreadPoolExecutor.DiscardPolicyï¼šå¿½ç•¥æœ€æ–°ä»»åŠ¡ ThreadPoolExecutor.DiscardOldestPolicyï¼šå¿½ç•¥æœ€æ—©çš„ä»»åŠ¡ï¼ˆæœ€å…ˆåŠ å…¥é˜Ÿåˆ—çš„ä»»åŠ¡ï¼‰ ThreadPoolExecutor.CallerRunsPolicyï¼šæŠŠä»»åŠ¡äº¤ç»™å½“å‰çº¿ç¨‹æ‰§è¡Œã€‚ è‡ªå®šä¹‰æ‹’ç»ç­–ç•¥ï¼šæ–°å»ºRejectedExecutionHandlerå¯¹è±¡ç„¶åé‡å†™rejectedExecutionæ–¹æ³• å¸¸è§çº¿ç¨‹æ±  *newCachedThreadPool**ï¼šåˆ›å»ºä¸€ä¸ªå¯ç¼“å­˜çº¿ç¨‹æ± ï¼Œå¦‚æœçº¿ç¨‹æ± é•¿åº¦è¶…è¿‡å¤„ç†éœ€è¦ï¼Œå¯çµæ´»å›æ”¶ç©ºé—²çº¿ç¨‹ï¼Œè‹¥æ— å¯å›æ”¶ï¼Œåˆ™æ–°å»ºçº¿ç¨‹ã€‚ å·¥ä½œçº¿ç¨‹çš„åˆ›å»ºæ•°é‡å‡ ä¹æ²¡æœ‰é™åˆ¶(å…¶å®ä¹Ÿæœ‰é™åˆ¶çš„,æ•°ç›®ä¸ºInterger. MAX_VALUE), è¿™æ ·å¯çµæ´»çš„å¾€çº¿ç¨‹æ± ä¸­æ·»åŠ çº¿ç¨‹ã€‚ å¦‚æœé•¿æ—¶é—´æ²¡æœ‰å¾€çº¿ç¨‹æ± ä¸­æäº¤ä»»åŠ¡ï¼Œå³å¦‚æœå·¥ä½œçº¿ç¨‹ç©ºé—²äº†æŒ‡å®šçš„æ—¶é—´(é»˜è®¤ä¸º1åˆ†é’Ÿ)ï¼Œåˆ™è¯¥å·¥ä½œçº¿ç¨‹å°†è‡ªåŠ¨ç»ˆæ­¢ã€‚ç»ˆæ­¢åï¼Œå¦‚æœä½ åˆæäº¤äº†æ–°çš„ä»»åŠ¡ï¼Œåˆ™çº¿ç¨‹æ± é‡æ–°åˆ›å»ºä¸€ä¸ªå·¥ä½œçº¿ç¨‹ã€‚ é€‚ç”¨å¤§é‡è€—æ—¶è¾ƒå°‘çš„çº¿ç¨‹ä»»åŠ¡ *newFixedThreadPool**ï¼šåˆ›å»ºä¸€ä¸ªæŒ‡å®šå·¥ä½œçº¿ç¨‹æ•°é‡çš„çº¿ç¨‹æ± ã€‚æ¯å½“æäº¤ä¸€ä¸ªä»»åŠ¡å°±åˆ›å»ºä¸€ä¸ªå·¥ä½œçº¿ç¨‹ï¼Œå¦‚æœå·¥ä½œçº¿ç¨‹æ•°é‡è¾¾åˆ°çº¿ç¨‹æ± åˆå§‹çš„æœ€å¤§æ•°ï¼Œåˆ™å°†æäº¤çš„ä»»åŠ¡å­˜å…¥åˆ°æ± é˜Ÿåˆ—ä¸­ã€‚ çº¿ç¨‹æ± ç©ºé—²æ—¶ï¼Œå³çº¿ç¨‹æ± ä¸­æ²¡æœ‰å¯è¿è¡Œä»»åŠ¡æ—¶ï¼Œå®ƒä¸ä¼šé‡Šæ”¾å·¥ä½œçº¿ç¨‹ï¼Œè¿˜ä¼šå ç”¨ä¸€å®šçš„ç³»ç»Ÿèµ„æºã€‚ *newSingleThreadExecutor**ï¼šå•çº¿ç¨‹ä¸²è¡Œæ‰§è¡Œä»»åŠ¡ï¼Œä¿è¯æ‰€æœ‰ä»»åŠ¡æŒ‰ç…§æŒ‡å®šé¡ºåº(FIFO, LIFO, ä¼˜å…ˆçº§)æ‰§è¡Œã€‚å¦‚æœè¿™ä¸ªçº¿ç¨‹å¼‚å¸¸ç»“æŸï¼Œä¼šæœ‰å¦ä¸€ä¸ªå–ä»£å®ƒï¼Œä¿è¯é¡ºåºæ‰§è¡Œã€‚ å•å·¥ä½œçº¿ç¨‹æœ€å¤§çš„ç‰¹ç‚¹æ˜¯å¯ä¿è¯é¡ºåºåœ°æ‰§è¡Œå„ä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨ä»»æ„ç»™å®šçš„æ—¶é—´ä¸ä¼šæœ‰å¤šä¸ªçº¿ç¨‹æ˜¯æ´»åŠ¨çš„ã€‚ *newScheduleThreadPool**ï¼šåˆ›å»ºä¸€ä¸ªå®šé•¿çš„çº¿ç¨‹æ± ï¼Œè€Œä¸”æ”¯æŒå®šæ—¶çš„ä»¥åŠå‘¨æœŸæ€§çš„ä»»åŠ¡æ‰§è¡Œï¼Œæ”¯æŒå®šæ—¶åŠå‘¨æœŸæ€§ä»»åŠ¡æ‰§è¡Œã€‚ è¯¥çº¿ç¨‹æ± å¤šç”¨äºæ‰§è¡Œå»¶è¿Ÿä»»åŠ¡æˆ–è€…å›ºå®šå‘¨æœŸçš„ä»»åŠ¡ã€‚ å¸¸è§çº¿ç¨‹æ± çº¿ç¨‹æ± æ‰§è¡Œæµç¨‹ï¼š åç¨‹åç¨‹æ˜¯ä¸€ç§ç”¨æˆ·æ€çš„è½»é‡çº§çº¿ç¨‹ï¼Œåç¨‹çš„è°ƒåº¦å®Œå…¨ç”±ç”¨æˆ·æ§åˆ¶ï¼Œåç¨‹é—´åˆ‡æ¢åªéœ€è¦ä¿å­˜ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰å†…æ ¸å¼€é”€ã€‚ é”åˆ†ç±»1.å…¬å¹³é”å’Œéå…¬å¹³é”ï¼šå…¬å¹³é”æŒ‡å¤šä¸ªçº¿ç¨‹æŒ‰ç…§ç”³è¯·é”çš„é¡ºåºæ’é˜Ÿæ¥ä¾æ¬¡è·å–é”ã€‚éå…¬å¹³é”æ˜¯æ²¡æœ‰é¡ºåºè·å–é”ã€‚(å› ä¸ºå…¬å¹³é”æŒ‚èµ·å’Œæ¢å¤å­˜åœ¨ä¸€å®šå¼€é”€ï¼Œæ‰€ä»¥éå…¬å¹³é”æ€§èƒ½å¥½äº›)(éå…¬å¹³é”è·å–æ–¹å¼æ˜¯éšæœºæŠ¢å ã€‚å…¬å¹³é”å’Œéå…¬å¹³é”å°±å·®åœ¨ !hasQueuedPredecessors() ï¼Œä¹Ÿå°±æ˜¯å‰è¾¹æ²¡æœ‰æ’é˜Ÿè€…çš„è¯ï¼Œæˆ‘å°±å¯ä»¥è·å–é”äº†ã€‚tryAcquireæ–¹æ³•ã€‚)2.å¯é‡å…¥é”ï¼šåˆåé€’å½’é”ï¼Œæ˜¯æŒ‡åŒä¸€ä¸ªçº¿ç¨‹åœ¨å¤–å±‚çš„æ–¹æ³•è·å–åˆ°äº†é”ï¼Œåœ¨è¿›å…¥å†…å±‚æ–¹æ³•ä¼šè‡ªåŠ¨è·å–åˆ°é”ã€‚3.å…±äº«é”Så’Œæ’å®ƒé”Xï¼šå¤šä¸ªçº¿ç¨‹å¯ä»¥åŒæ—¶è·å–ä¸€ä¸ªå…±äº«é”ï¼Œä¸€ä¸ªå…±äº«é”å¯è¢«å¤šä¸ªçº¿ç¨‹æ‹¥æœ‰ã€‚æ’å®ƒé”ä¹Ÿå«ç‹¬å é”ï¼ŒåŒä¸€æ—¶åˆ»åªèƒ½è¢«ç»Ÿä¸€çº¿ç¨‹å ç”¨ï¼Œå…¶ä»–çº¿ç¨‹éœ€è¦ç­‰å¾…ã€‚4.äº’æ–¥é”å’Œè¯»å†™é”ï¼šä¸€æ¬¡åªèƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹æ‹¥æœ‰äº’æ–¥é”ï¼Œè¯»å†™é”å¤šä¸ªè¯»è€…å¯åŒæ—¶è¯»ï¼Œå†™å¿…é¡»äº’æ–¥ã€‚å†™ä¼˜å…ˆäºè¯»ï¼Œå¦‚æœæœ‰å†™ï¼Œè¯»å¿…é¡»ç­‰å¾…ã€‚5.ä¹è§‚é”å’Œæ‚²è§‚é”ï¼šä¹è§‚é”è®¤ä¸ºè¯»å–æ•°æ®æ—¶å…¶ä»–çº¿ç¨‹ä¸ä¼šå¯¹æ•°æ®åšä¿®æ”¹ï¼Œä¸åŠ é”ï¼Œæ›´æ–°æ•°æ®æ—¶é‡‡ç”¨å°è¯•æ›´æ–°ä¸æ–­é‡è¯•çš„æ–¹å¼ã€‚æ‚²è§‚é”è®¤ä¸ºè¯»å–æ•°æ®æ—¶å…¶ä»–çº¿ç¨‹ä¼šå¯¹æ•°æ®åšä¿®æ”¹ï¼Œä¼šå‡ºé—®é¢˜ï¼Œæ‰€ä»¥é»˜è®¤åŠ é”ã€‚6.åˆ†æ®µé”ï¼šæå‡å¹¶å‘ç¨‹åºæ€§èƒ½çš„æ‰‹æ®µä¹‹ä¸€ï¼Œç²’åº¦æ›´å°ã€‚å°†æ•°æ®åˆ†æˆä¸€æ®µä¸€æ®µçš„å­˜å‚¨ï¼ˆå¦‚ConcurrentHashMapçš„Segmentï¼‰ï¼Œç„¶åç»™æ¯ä¸€æ®µæ•°æ®é…ä¸€æŠŠé”ï¼Œå½“ä¸€ä¸ªçº¿ç¨‹å ç”¨é”è®¿é—®å…¶ä¸­ä¸€ä¸ªæ®µæ•°æ®çš„æ—¶å€™ï¼Œå…¶ä»–æ®µçš„æ•°æ®ä¹Ÿèƒ½è¢«å…¶ä»–çº¿ç¨‹è®¿é—®ï¼Œèƒ½å¤Ÿå®ç°çœŸæ­£çš„å¹¶å‘è®¿é—®ã€‚æ‰€ä»¥è¯´ï¼ŒConcurrentHashMapåœ¨å¹¶å‘æƒ…å†µä¸‹ï¼Œä¸ä»…ä¿è¯äº†çº¿ç¨‹å®‰å…¨ï¼Œè€Œä¸”æé«˜äº†æ€§èƒ½ã€‚7.é”çš„çŠ¶æ€ï¼šæ— é”ã€åå‘é”ã€è½»é‡çº§é”ã€é‡é‡çº§é”ï¼šåå‘é”æ˜¯å‡å°‘æ— ç«äº‰ä¸”åªæœ‰ä¸€ä¸ªçº¿ç¨‹ä½¿ç”¨é”çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è½»é‡çº§é”äº§ç”Ÿçš„æ€§èƒ½æ¶ˆè€—ã€‚è½»é‡çº§é”æ¯æ¬¡ç”³è¯·ã€é‡Šæ”¾é”éƒ½è‡³å°‘éœ€è¦ä¸€æ¬¡CASï¼Œä½†åå‘é”åªæœ‰åˆå§‹åŒ–æ—¶éœ€è¦ä¸€æ¬¡CASã€‚é‡é‡çº§é”ï¼Œå…¶ä»–çº¿ç¨‹è¯•å›¾è·å–é”æ—¶ï¼Œéƒ½ä¼šè¢«é˜»å¡ï¼Œåªæœ‰æŒæœ‰é”çš„çº¿ç¨‹é‡Šæ”¾é”ä¹‹åæ‰ä¼šå”¤é†’è¿™äº›çº¿ç¨‹ï¼Œè¿›è¡Œç«äº‰ã€‚ æ— é”çŠ¶æ€-&gt;åå‘é”çŠ¶æ€-&gt;è½»é‡çº§é”çŠ¶æ€-&gt;é‡é‡çº§é”çŠ¶æ€ éšç«äº‰æƒ…å†µé€æ¸å‡çº§ ä¸å¯é€† åç»­ä¼šåœ¨**æ¦‚å¿µ**éƒ¨åˆ†è¯¦ç»†è¯´ åå‘é”ï¼šä»…æœ‰ä¸€ä¸ªçº¿ç¨‹è¿›å…¥ä¸´ç•ŒåŒº è½»é‡çº§é”ï¼šå¤šä¸ªçº¿ç¨‹äº¤æ›¿è¿›å…¥ä¸´ç•ŒåŒº é‡é‡çº§é”ï¼šå¤šä¸ªçº¿ç¨‹åŒæ—¶è¿›å…¥ä¸´ç•ŒåŒº 8.è‡ªæ—‹é”ï¼šçº¿ç¨‹æ²¡è·å¾—é”æ—¶ä¸ä¼šè¢«æŒ‚èµ·è€Œæ˜¯ç©ºå¾ªç¯ï¼Œå¯ä»¥å‡å°‘çº¿ç¨‹é˜»å¡é€ æˆçš„çº¿ç¨‹åˆ‡æ¢çš„æ¦‚ç‡ã€‚é¦–å…ˆï¼Œé˜»å¡æˆ–å”¤é†’Javaçº¿ç¨‹éœ€è¦æ“ä½œç³»ç»Ÿåˆ‡æ¢CPUçŠ¶æ€æ¥å®Œæˆï¼ŒçŠ¶æ€åˆ‡æ¢è€—è´¹CPUï¼Œå¯èƒ½åˆ‡æ¢CPUçš„æ—¶é—´æ¯”åŒæ­¥ä»£ç å—ä¸­ä»£ç çš„æ‰§è¡Œæ—¶é—´éƒ½è¦é•¿ï¼Œä¸ºäº†å¾ˆçŸ­çš„åŒæ­¥é”å®šæ—¶é—´è€ŒèŠ±è´¹å¾ˆé•¿çš„çº¿ç¨‹åˆ‡æ¢æ—¶é—´æ˜¯ä¸å€¼å¾—çš„ï¼Œå¦‚æœç‰©ç†æœºå™¨æœ‰å¤šä¸ªå¤„ç†å™¨ï¼Œèƒ½å¤Ÿè®©ä¸¤ä¸ªæˆ–ä»¥ä¸Šçš„çº¿ç¨‹åŒæ—¶å¹¶è¡Œæ‰§è¡Œï¼Œæˆ‘ä»¬å°±å¯ä»¥è®©åé¢é‚£ä¸ªè¯·æ±‚é”çš„çº¿ç¨‹ä¸æ”¾å¼ƒCPUçš„æ‰§è¡Œæ—¶é—´ï¼Œçœ‹çœ‹æŒæœ‰é”çš„çº¿ç¨‹æ˜¯å¦å¾ˆå¿«å°±ä¼šé‡Šæ”¾é”ã€‚è€Œä¸ºäº†è®©å½“å‰çº¿ç¨‹â€œç¨ç­‰ä¸€ä¸‹â€ï¼Œæˆ‘ä»¬éœ€è®©å½“å‰çº¿ç¨‹è¿›è¡Œè‡ªæ—‹ï¼Œå¦‚æœåœ¨è‡ªæ—‹å®Œæˆåå‰é¢é”å®šåŒæ­¥èµ„æºçš„çº¿ç¨‹å·²ç»é‡Šæ”¾äº†é”ï¼Œé‚£ä¹ˆå½“å‰çº¿ç¨‹å°±å¯ä»¥ä¸å¿…é˜»å¡è€Œæ˜¯ç›´æ¥è·å–åŒæ­¥èµ„æºï¼Œä»è€Œé¿å…åˆ‡æ¢çº¿ç¨‹çš„å¼€é”€ã€‚è¿™å°±æ˜¯è‡ªæ—‹é”ã€‚8.è‡ªé€‚åº”è‡ªæ—‹é”ï¼šè‡ªé€‚åº”æ˜¯å€¼è™šæ‹Ÿæœºä¼šè®°å½•ä¸€ä¸ªé”å¯¹è±¡è‡ªæ—‹æ—¶é—´å’ŒçŠ¶æ€ï¼Œå¦‚æœä¹‹å‰è‡ªæ—‹ç­‰å¾…æˆåŠŸè·å¾—é”ï¼Œè¿™æ¬¡è‡ªæ—‹ä¹Ÿå¾ˆå¤§æ¦‚ç‡æˆåŠŸï¼Œä¼šå…è®¸æŒç»­æ›´é•¿æ—¶é—´çš„è‡ªé€‰ç­‰å¾…ï¼Œåé¢çš„è‡ªæ—‹ä¼šå¤§æ¦‚ç‡æ‹¿åˆ°é”ã€‚å¦‚æœä¸€ä¸ªå¯¹è±¡çš„é”è‡ªæ—‹ç­‰å¾…å¾ˆå°‘èƒ½æˆåŠŸè·å–åˆ°é”ï¼Œåç»­å‡å°‘è‡ªæ—‹æ¬¡æ•°ç”šè‡³å¿½ç•¥è‡ªæ—‹è¿‡ç¨‹ï¼Œç›´æ¥é˜»å¡ï¼Œé¿å…æµªè´¹CPUèµ„æºã€‚ï¼ˆç®€è€Œè¨€ä¹‹ï¼Œè‡ªæ—‹æ—¶é—´æ ¹æ®ä¹‹å‰é”å’Œçº¿ç¨‹çŠ¶æ€åŠ¨æ€å˜åŒ–ï¼Œæ¥å‡å°‘çº¿ç¨‹é˜»å¡æ—¶é—´ï¼‰11.è¯»å†™é”ï¼šå¯¹èµ„æºè¯»å–å’Œå†™å…¥çš„æ—¶å€™æ‹†åˆ†ä¸º2éƒ¨åˆ†å¤„ç†ï¼Œè¯»çš„æ—¶å€™å¯ä»¥å¤šçº¿ç¨‹ä¸€èµ·è¯»ï¼Œå†™çš„æ—¶å€™å¿…é¡»åŒæ­¥åœ°å†™ã€‚(å¦‚æœå·²å ç”¨è¯»é”ï¼Œå…¶ä»–çº¿ç¨‹æƒ³å†™éœ€è¦ç­‰è¯»é”é‡Šæ”¾ï¼Œå…¶ä»–çº¿ç¨‹æƒ³è¯»å¯ä»¥è¯»ï¼›å¦‚æœå·²å ç”¨å†™é”ï¼Œå…¶ä»–çº¿ç¨‹æ— è®ºæƒ³è¯»æƒ³å†™ï¼Œéƒ½è¦ç­‰å†™é”é‡Šæ”¾) æ¦‚å¿µ1.ä»åº•å±‚è§’åº¦çœ‹ï¼Œæœ¬è´¨ä¸Šåªæœ‰ä¸€ä¸ªå…³é”®å­—å’Œä¸¤ä¸ªæ¥å£ï¼šSynchronizedå’ŒLockæ¥å£ä»¥åŠReadWriteLockæ¥å£ï¼ˆè¯»å†™é”ï¼‰ 2.synchronizedä¸reentrantLockç®€è¿° synchronizedæ˜¯ä¸€ä¸ªéšå¼çš„é‡å…¥é”ï¼Œæ¯”è¾ƒç¬¨é‡ï¼Œå®ç°æ–¹å¼æ˜¯é”ä¸»å­˜å’Œç¼“å­˜ä¸€è‡´æ€§ã€‚ç°åœ¨çš„JDKç‰ˆæœ¬å·²ç»åšäº†å¾ˆå¤šä¼˜åŒ–synchronizedçš„æªæ–½ï¼šè‡ªé€‚åº”çš„è‡ªæ—‹é”ã€é”ç²—åŒ–ã€é”æ¶ˆé™¤ã€è½»é‡çº§é”ç­‰ reentrantLockæ˜¯ä¸€ä¸ªæ˜¾å¼çš„é‡å…¥é”ï¼Œæ¯”è¾ƒçµæ´»ï¼Œå¯ä»¥æ‰©å±•ä¸ºåˆ†æ®µé”ï¼Œå®ç°æ–¹å¼æ˜¯AQS+åŒå‘é“¾è¡¨ï¼ˆé»˜è®¤æ˜¯NonFairSyncéå…¬å¹³é”å®ç°ï¼Œè€ŒNonFairSyncç»§æ‰¿Syncï¼ŒSyncç»§æ‰¿AbstractQueuedSynchronizerï¼ŒAQSç»´æŠ¤volatileå˜é‡stateä½œä¸ºåŒæ­¥çŠ¶æ€ï¼‰ã€‚ä¸€ä¸ªçº¿ç¨‹å¯å¤šæ¬¡è·å–é”ï¼Œæ¯æ¬¡è·å–éƒ½ä¼šè®¡æ•°count++ï¼Œè§£é”æ—¶count--ç›´åˆ°å˜0. ReentrantLocké‡‡ç”¨çš„æ˜¯ç‹¬å é”ã€‚Semaphoreï¼ŒCountDownLatchç­‰é‡‡ç”¨çš„æ˜¯å…±äº«é”ï¼Œå³æœ‰å¤šä¸ªçº¿ç¨‹å¯ä»¥åŒæ—¶è·å–é”ã€‚ 3.synchronizedå’ŒLockåŒºåˆ«ï¼š 1ã€synchronizeæ˜¯javaå†…ç½®å…³é”®å­—ï¼Œè€ŒLockæ˜¯ä¸€ä¸ªç±»ã€‚ï¼ˆé€šè¿‡javapçœ‹å­—èŠ‚ç ï¼Œå‘ç°æœ‰monitorenterå’Œmonitorexitå‘½ä»¤ï¼Œåˆ†åˆ«å¯¹åº”è¿›å…¥monitoråŠ é”å’Œé‡Šæ”¾ï¼‰ 2ã€synchronizeå¯ä»¥ä½œç”¨äºå˜é‡ã€æ–¹æ³•ã€ä»£ç å—ï¼Œè€ŒLockæ˜¯æ˜¾å¼åœ°æŒ‡å®šå¼€å§‹å’Œç»“æŸä½ç½®ã€‚ 3ã€synchronizeä¸éœ€è¦æ‰‹åŠ¨è§£é”ï¼Œå½“çº¿ç¨‹æŠ›å‡ºå¼‚å¸¸çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨é‡Šæ”¾é”ï¼›è€ŒLockåˆ™éœ€è¦æ‰‹åŠ¨é‡Šæ”¾ï¼Œæ‰€ä»¥lock.unlock()éœ€è¦æ”¾åœ¨finallyä¸­å»æ‰§è¡Œã€‚ 4ã€æ€§èƒ½æ–¹é¢ï¼Œå¦‚æœç«äº‰ä¸æ¿€çƒˆçš„æ—¶å€™ï¼Œsynchronizeå’ŒLockçš„æ€§èƒ½å·®ä¸å¤šï¼Œå¦‚æœç«äº‰æ¿€çƒˆçš„æ—¶å€™ï¼ŒLockçš„æ•ˆç‡ä¼šæ¯”synchronizeé«˜ã€‚ 5ã€Lockå¯ä»¥çŸ¥é“æ˜¯å¦å·²ç»è·å¾—é”ï¼Œsynchronizeä¸èƒ½çŸ¥é“ã€‚Lockæ‰©å±•äº†ä¸€äº›å…¶ä»–åŠŸèƒ½å¦‚è®©ç­‰å¾…çš„é”ä¸­æ–­ã€çŸ¥é“æ˜¯å¦è·å¾—é”ç­‰åŠŸèƒ½ï¼ŒLock å¯ä»¥æé«˜æ•ˆç‡ã€‚ 6ã€synchronizeæ˜¯æ‚²è§‚é”çš„å®ç°ï¼Œè€ŒLockåˆ™æ˜¯ä¹è§‚é”çš„å®ç°ï¼Œé‡‡ç”¨çš„CASçš„å°è¯•æœºåˆ¶ã€‚ 4.synchronizedå’ŒReenTrantLockåŒºåˆ«ï¼š 1ã€ReenTrantLockå¯ä»¥ä¸­æ–­é”çš„ç­‰å¾…ï¼Œæä¾›äº†ä¸€äº›é«˜çº§åŠŸèƒ½ã€‚ 2ã€ReenTrantLocké»˜è®¤éå…¬å¹³é”ï¼Œå¯ä»¥è®¾ä¸ºå…¬å¹³é”ï¼Œsynchronizedä¸è¡Œ 3ã€ReenTrantLockå¯ä»¥ç»‘å®šå¤šä¸ªé”æ¡ä»¶ã€‚ 4ã€synchronizedæ˜¯JVMå…³é”®å­—ï¼ŒReentrantLockæ˜¯Java API 5ã€ReentrantLockåªèƒ½ä¿®é¥°ä»£ç å—ï¼Œsynchronizedæ—¢å¯ä»¥ä¿®é¥°æ–¹æ³•ä¹Ÿå¯ä»¥ä¿®é¥°ä»£ç å— 6ã€ReentrantLockéœ€è¦æ‰‹åŠ¨åŠ é”å’Œé‡Šæ”¾é”ï¼Œsynchronizedè‡ªåŠ¨é‡Šæ”¾ 7ã€ReentrantLockå¯ä»¥çŸ¥é“æ˜¯å¦æˆåŠŸè·å¾—äº†é”ï¼Œsynchronizedä¸èƒ½ 5.é”è†¨èƒ€ï¼šæ— é”ã€åå‘é”ã€è½»é‡çº§é”ã€é‡é‡çº§é”åŠå‡çº§è¿‡ç¨‹æ— é”ï¼šæ¯ä¸ªå¯¹è±¡éƒ½æœ‰ä¸€æŠŠçœ‹ä¸è§çš„é”ï¼ˆå†…éƒ¨é” ä¹Ÿå« Monitoré”ï¼‰ ç‰¹ç‚¹ï¼šä¸æ–­å°è¯•ä¿®æ”¹èµ„æºï¼Œå¤±è´¥çš„çº¿ç¨‹é‡è¯•ç›´åˆ°æˆåŠŸã€‚åå‘é”ï¼šç‰¹ç‚¹ï¼šä¸€æ®µåŒæ­¥ä»£ç ä¸€ç›´è¢«åŒä¸€ä¸ªçº¿ç¨‹è®¿é—®ï¼Œé”æ€»æ˜¯è¢«åŒä¸€çº¿ç¨‹å¤šæ¬¡è·å¾—ï¼Œé™ä½é”çš„è·å–ä»£ä»·ã€‚åªæœ‰ä¸€ä¸ªçº¿ç¨‹æ‰§è¡ŒåŒæ­¥ä»£ç å—æ—¶èƒ½æé«˜æ€§èƒ½ã€‚ å½“ä¸€ä¸ªçº¿ç¨‹è®¿é—®åŒæ­¥ä»£ç å—å¹¶è·å–é”æ—¶ï¼Œä¼šåœ¨Mark Wordé‡Œå­˜å‚¨é”åå‘çš„çº¿ç¨‹IDï¼ˆé”æ ‡å¿—ä½ï¼‰ã€‚ åœ¨çº¿ç¨‹è¿›å…¥å’Œé€€å‡ºåŒæ­¥å—æ—¶ä¸å†é€šè¿‡CASæ“ä½œæ¥åŠ é”å’Œè§£é”ï¼Œè€Œæ˜¯æ£€æµ‹Mark Wordé‡Œæ˜¯å¦å­˜å‚¨ç€æŒ‡å‘å½“å‰çº¿ç¨‹çš„åå‘é”ã€‚ å¼•å…¥åå‘é”æ˜¯ä¸ºäº†åœ¨æ— å¤šçº¿ç¨‹ç«äº‰çš„æƒ…å†µä¸‹å°½é‡å‡å°‘ä¸å¿…è¦çš„è½»é‡çº§é”æ‰§è¡Œè·¯å¾„ï¼Œå› ä¸ºè½»é‡çº§é”çš„è·å–åŠé‡Šæ”¾ä¾èµ–å¤šæ¬¡CASåŸå­æŒ‡ä»¤ï¼Œè€Œåå‘é”åªéœ€è¦åœ¨ç½®æ¢ThreadIDçš„æ—¶å€™ä¾èµ–ä¸€æ¬¡CASåŸå­æŒ‡ä»¤å³å¯ã€‚ åå‘é”åªæœ‰é‡åˆ°å…¶ä»–çº¿ç¨‹å°è¯•ç«äº‰åå‘é”æ—¶ï¼ŒæŒæœ‰åå‘é”çš„çº¿ç¨‹æ‰ä¼šé‡Šæ”¾é”ï¼Œçº¿ç¨‹ä¸ä¼šä¸»åŠ¨é‡Šæ”¾åå‘é”ã€‚ åå‘é”çš„æ’¤é”€ï¼Œéœ€è¦ç­‰å¾…å…¨å±€å®‰å…¨ç‚¹ï¼ˆåœ¨è¿™ä¸ªæ—¶é—´ç‚¹ä¸Šæ²¡æœ‰å­—èŠ‚ç æ­£åœ¨æ‰§è¡Œï¼‰ï¼Œå®ƒä¼šé¦–å…ˆæš‚åœæ‹¥æœ‰åå‘é”çš„çº¿ç¨‹ï¼Œåˆ¤æ–­é”å¯¹è±¡æ˜¯å¦å¤„äºè¢«é”å®šçŠ¶æ€ã€‚ æ’¤é”€åå‘é”åæ¢å¤åˆ°æ— é”ï¼ˆæ ‡å¿—ä½ä¸ºâ€œ01â€ï¼‰æˆ–è½»é‡çº§é”ï¼ˆæ ‡å¿—ä½ä¸ºâ€œ00â€ï¼‰çš„çŠ¶æ€ã€‚ è½»é‡çº§é”ï¼šå½“å‰å¯¹è±¡æŒæœ‰åå‘é”æ—¶è¢«å¦å¤–çš„çº¿ç¨‹è®¿é—®ï¼Œåå‘é”å°±ä¼šå‡çº§ä¸ºè½»é‡çº§é”ï¼Œå…¶ä»–çº¿ç¨‹ä¼šé€šè¿‡è‡ªæ—‹çš„å½¢å¼å°è¯•è·å–é”ï¼Œä¸ä¼šé˜»å¡ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚é‡é‡çº§é”ï¼šå½“è‡ªæ—‹è¶…è¿‡ä¸€å®šçš„æ¬¡æ•°ï¼Œæˆ–è€…ä¸€ä¸ªçº¿ç¨‹åœ¨æŒæœ‰é”ï¼Œä¸€ä¸ªåœ¨è‡ªæ—‹ï¼Œåˆæœ‰ç¬¬ä¸‰ä¸ªæ¥è®¿æ—¶ï¼Œè½»é‡çº§é”å‡çº§ä¸ºé‡é‡çº§é”ã€‚ ç‰¹ç‚¹ï¼šç­‰å¾…é”çš„çº¿ç¨‹éƒ½ä¼šè¿›å…¥é˜»å¡çŠ¶æ€ã€‚ 6.é”æ¶ˆé™¤ï¼š å…ˆè¯´â€œé€ƒé€¸åˆ†ææŠ€æœ¯â€ï¼Œè¯¥æŠ€æœ¯åœ¨ç¼–è¯‘æœŸä½¿ç”¨ï¼Œåˆ†æå¯¹è±¡çš„åŠ¨æ€ä½œç”¨åŸŸï¼Œå½“ä¸€ä¸ªå¯¹è±¡åœ¨æ–¹æ³•ä¸­è¢«å®šä¹‰åï¼Œå®ƒå¯èƒ½è¢«å¤–éƒ¨æ–¹æ³•æ‰€å¼•ç”¨ï¼Œä¾‹å¦‚ä½œä¸ºè°ƒç”¨å‚æ•°ä¼ é€’åˆ°å…¶ä»–åœ°æ–¹ä¸­ï¼Œç§°ä¸ºæ–¹æ³•é€ƒé€¸ã€‚ è¯¥æŠ€æœ¯å°†ç¡®å®šä¸ä¼šå‘ç”Ÿé€ƒé€¸çš„å¯¹è±¡æ”¾å…¥æ ˆå†…å­˜è€Œéå †(æ•…ä¸æ˜¯æ‰€æœ‰å¯¹è±¡éƒ½åœ¨å †ä¸­)ã€‚ ä¸ºäº†å‡å°‘é”çš„è¯·æ±‚å’Œé‡Šæ”¾æ“ä½œï¼Œâ€œé€ƒé€¸åˆ†ææŠ€æœ¯â€åœ¨ç¼–è¯‘æœŸåˆ†æå‡ºé‚£äº›æœ¬æ¥ä¸å­˜åœ¨ç«äº‰å´åŠ äº†é”çš„ä»£ç ï¼Œè®©ä»–ä»¬çš„é”å¤±æ•ˆï¼Œä»è€Œè¾¾åˆ°å‡å°‘é”çš„è¯·æ±‚å’Œé‡Šæ”¾çš„ç›®çš„ã€‚ è€Œé”æ¶ˆé™¤å°±æ˜¯ï¼Œå‘ç°ä¸å­˜åœ¨å¤šçº¿ç¨‹å¹¶å‘æŠ¢å é—®é¢˜çš„æ—¶å€™ï¼Œç¼–è¯‘åå»æ‰è¯¥é”ã€‚ 7.é”åå‘ï¼š å…ˆè¯´ä¸€ä¸‹Javaå¯¹è±¡å¤´ï¼Œå®ƒåŒ…æ‹¬Mark Wordsã€Klass Wordsä¸¤éƒ¨åˆ†ï¼Œå¯èƒ½è¿˜åŒ…æ‹¬æ•°ç»„çš„é•¿åº¦ï¼ˆå¦‚æœå¯¹è±¡æ˜¯ä¸ªæ•°ç»„ï¼‰ã€‚ Klass Wordé‡Œé¢å­˜çš„æ˜¯ä¸€ä¸ªåœ°å€ï¼Œå 32ä½æˆ–64ä½ï¼Œæ˜¯ä¸€ä¸ªæŒ‡å‘å½“å‰å¯¹è±¡æ‰€å±äºçš„ç±»çš„åœ°å€ï¼Œå¯ä»¥é€šè¿‡è¿™ä¸ªåœ°å€è·å–åˆ°å®ƒçš„å…ƒæ•°æ®ä¿¡æ¯ã€‚ Mark Wordï¼é‡ç‚¹ï¼Œè¿™é‡Œé¢ä¸»è¦åŒ…å«å¯¹è±¡çš„Hashcodeã€å¹´é¾„åˆ†ä»£ã€é”æ ‡å¿—ä½ç­‰ï¼Œå¤§å°ä¸º32ä½æˆ–64ä½ã€‚é”çŠ¶æ€ä¸åŒæ—¶MarkWordé‡Œå†…å®¹ä¼šä¸åŒã€‚ é”åå‘ï¼šå½“ç¬¬ä¸€ä¸ªçº¿ç¨‹è¯·æ±‚æ—¶ï¼Œä¼šåˆ¤æ–­é”çš„å¯¹è±¡å¤´é‡Œçš„ThreadIdå­—æ®µçš„å€¼ï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™è®©è¯¥çº¿ç¨‹æŒæœ‰åå‘é”ï¼Œå¹¶å°†ThreadIdçš„å€¼ç½®ä¸ºå½“å‰çº¿ç¨‹IDã€‚å½“å‰çº¿ç¨‹å†æ¬¡è¿›å…¥æ—¶ï¼Œå¦‚æœçº¿ç¨‹IDä¸ThreadIdçš„å€¼ç›¸ç­‰ï¼Œåˆ™è¯¥çº¿ç¨‹å°±ä¸ä¼šå†é‡å¤è·å–é”äº†ã€‚å› ä¸ºé”çš„è¯·æ±‚ä¸é‡Šæ”¾æ˜¯è¦æ¶ˆè€—ç³»ç»Ÿèµ„æºçš„ã€‚ å¦‚æœæœ‰å…¶ä»–çº¿ç¨‹ä¹Ÿæ¥è¯·æ±‚è¯¥é”ï¼Œåˆ™åå‘é”å°±ä¼šæ’¤é”€ï¼Œç„¶åå‡çº§ä¸ºè½»é‡çº§é”ã€‚å¦‚æœé”çš„ç«äº‰ååˆ†æ¿€çƒˆï¼Œåˆ™è½»é‡çº§é”åˆä¼šå‡çº§ä¸ºé‡é‡çº§é”ã€‚ å¯¹è±¡å¤´ï¼š 8.é”ç²—åŒ–ï¼šåœ¨ç¼–è¯‘æœŸé—´å°†ç›¸é‚»çš„åŒæ­¥ä»£ç å—åˆå¹¶æˆä¸€ä¸ªå¤§åŒæ­¥å—ã€‚è¿™æ ·åšå¯ä»¥å‡å°‘åå¤ç”³è¯·å’Œé‡Šæ”¾åŒä¸€ä¸ªé”å¯¹è±¡å¯¼è‡´çš„ç³»ç»Ÿå¼€é”€ã€‚å½“ä¸€ä¸ªå¾ªç¯ä¸­å­˜åœ¨åŠ é”æ“ä½œæ—¶ï¼Œå¯ä»¥å°†åŠ é”æ“ä½œæåˆ°å¾ªç¯å¤–é¢æ‰§è¡Œï¼Œä¸€æ¬¡åŠ é”ä»£æ›¿å¤šæ¬¡åŠ é”ï¼Œæå‡æ€§èƒ½ã€‚ 9.volatileï¼šæ˜¯Javaå…³é”®å­—ï¼ŒåŠŸèƒ½æ˜¯ä¿è¯è¢«ä¿®é¥°çš„å…ƒç´ ï¼š ä»»ä½•è¿›ç¨‹è¯»å–æ—¶éƒ½ä¼šæ¸…ç©ºæœ¬è¿›ç¨‹æŒæœ‰çš„å…±äº«å˜é‡å€¼ï¼Œè€Œå¼ºåˆ¶ä»ä¸»å­˜è·å–(æ¯æ¬¡è®¿é—®å˜é‡éƒ½åˆ·æ–°ï¼Œæ¯æ¬¡éƒ½èƒ½å¾—åˆ°æœ€æ–°å˜é‡) ä»»ä½•è¿›ç¨‹å†™å®Œæ¯•æ—¶éƒ½å¼ºåˆ¶å°†å…±äº«å˜é‡å†™å›ä¸»å­˜ é˜²æ­¢æŒ‡ä»¤é‡æ’(æŒ‡ä»¤é‡æ’ä½œç”¨æ˜¯JVMå•çº¿ç¨‹ç¨‹åºåœ¨ä¸å½±å“è¿è¡Œç»“æœçš„æ¡ä»¶ä¸‹çš„ä¼˜åŒ–ã€‚å¤šçº¿ç¨‹æŒ‡ä»¤é‡æ’ä¼šå‡ºé—®é¢˜ã€‚) ä¿è¯å†…å­˜å¯è§æ€§(ä¸€ä¸ªçº¿ç¨‹å¯¹å…±äº«å˜é‡æ”¹åŠ¨ï¼Œè¦è®©å…¶ä»–çº¿ç¨‹ç«‹åˆ»çŸ¥é“) å¹¶ä¸ä¿è¯æ“ä½œåŸå­æ€§å®ƒæ˜¯æ€ä¹ˆé¿å…æŒ‡ä»¤é‡æ’çš„å‘¢ï¼Ÿé€šè¿‡å†…å­˜å±éšœå†…å­˜å±éšœæ˜¯é€šè¿‡JVMç”Ÿæˆå†…å­˜å±éšœæŒ‡ä»¤ï¼Œåœ¨è¯»å†™æ“ä½œå‰åæ·»åŠ å†…å­˜å±éšœ(è§£å†³L1,L2,L3é«˜é€Ÿç¼“å­˜ä¸ä¸»å­˜é€Ÿåº¦ä¸ä¸€è‡´å¯¼è‡´çš„ç¼“å­˜ä¸€è‡´æ€§é—®é¢˜)volatileè¯»æ“ä½œçš„åé¢æ’å…¥ä¸€ä¸ªLoadLoadå±éšœã€‚volatileå†™æ“ä½œçš„åé¢æ’å…¥ä¸€ä¸ªStoreLoadå±éšœã€‚å®ƒçš„å†…å­˜è¯»å†™è¿‡ç¨‹ï¼šLoadStoreLoadLoadStoreStoreStoreLoadæ³¨ï¼šStoreLoadå°±æ˜¯è§¦å‘åç»­æŒ‡ä»¤ä¸­çš„çº¿ç¨‹ç¼“å­˜å›å†™åˆ°å†…å­˜; è€ŒLoadLoadä¼šè§¦å‘çº¿ç¨‹é‡æ–°ä»ä¸»å­˜é‡Œé¢è¯»æ•°æ®è¿›è¡Œå¤„ç†ã€‚ åº”ç”¨ï¼švolatileåº”ç”¨äºå•ä¾‹æ¨¡å¼ï¼Œé˜²æ­¢å› æŒ‡ä»¤é‡æ’å¯¼è‡´å•ä¾‹æ¨¡å¼è¿”å›ä¸€ä¸ªåˆå§‹åŒ–åˆ°ä¸€åŠçš„å¯¹è±¡ public class Singleton &#123; /** * åŒé‡æ ¡éªŒ ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡ï¼ŒèŠ‚çº¦å†…å­˜ç©ºé—´ */ private static volatile Singleton instance; //é˜²æ­¢JVMæŒ‡ä»¤é‡æ’ private Singleton()&#123;&#125; public static Singleton getInstance()&#123; /** * å¦‚æœinstanceä¸åŠ volatileï¼ŒJVMæŒ‡ä»¤é‡æ’ä¼šå…ˆåˆ†é…åœ°å€å†åˆå§‹åŒ–ï¼ˆæ­¤æ—¶è¿™ä¸ªåœ°å€å­˜åœ¨ä½†æ²¡å€¼ï¼‰ï¼Œ * æ‰€ä»¥è¿™é‡Œåˆ¤æ–­ä¸ä¸ºnullä¸ºtrueæ—¶æœ‰å¯èƒ½å¯¹è±¡è¿˜æœªå®Œæˆåˆå§‹åŒ–ï¼Œå•ä¾‹å¯èƒ½è¿”å›äº†ä¸€ä¸ªæœªåˆå§‹åŒ–å®Œçš„å¯¹è±¡ã€‚ */ if(instance == null)&#123; synchronized (Singleton.class)&#123; if(instance == null)&#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 10.CASï¼šå…¨ç§°Compare-And-Swapï¼Œå®ƒçš„åŠŸèƒ½æ˜¯åˆ¤æ–­å†…å­˜æŸä¸ªä½ç½®çš„å€¼æ˜¯å¦ä¸ºé¢„æœŸå€¼ï¼Œå¦‚æœæ˜¯åˆ™æ›´æ”¹ä¸ºæ–°çš„å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯åŸå­çš„ã€‚CASæ˜¯CPUçš„åŸå­æŒ‡ä»¤ï¼Œåº•å±‚æ˜¯æ±‡ç¼–è¯­è¨€ï¼ŒUnsafeç±»ä¸­çš„compareAndSwapIntï¼Œæ˜¯ä¸€ä¸ªæœ¬åœ°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„å®ç°ä½äºunsafe.cppã€‚CASç¼ºç‚¹ å¾ªç¯æ—¶é—´é•¿CPUå¼€é”€å¤§ åªèƒ½ä¿è¯ä¸€ä¸ªå…±äº«å˜é‡çš„åŸå­æ“ä½œ ä¼šå¼•å‘ABAé—®é¢˜(å½“å˜é‡ä»Aä¿®æ”¹ä¸ºBå†ä¿®æ”¹å›Aæ—¶ï¼Œå˜é‡å€¼ç­‰äºæœŸæœ›å€¼Aï¼Œä½†æ˜¯æ— æ³•åˆ¤æ–­æ˜¯å¦ä¿®æ”¹ï¼ŒCASæ“ä½œåœ¨ABAä¿®æ”¹åä¾ç„¶æˆåŠŸã€‚æ¯”å¦‚è¯´ä¸€ä¸ªçº¿ç¨‹oneä»å†…å­˜ä½ç½®Vä¸­å–å‡ºAï¼Œè¿™ä¸ªæ—¶å€™å¦ä¸€ä¸ªçº¿ç¨‹twoä¹Ÿä»å†…å­˜ä½ç½®Vä¸­å–å‡ºAï¼Œå¹¶ä¸”çº¿ç¨‹twoè¿›è¡Œäº†ä¸€äº›æ“ä½œå°†å€¼å˜æˆäº†Bï¼Œç„¶åçº¿ç¨‹twoåˆå°†Vä½ç½®çš„æ•°æ®å˜æˆAï¼Œè¿™æ—¶å€™çº¿ç¨‹oneè¿›è¡ŒCASæ“ä½œæ—¶å‘ç°å†…å­˜ä¸­ä»ç„¶æ˜¯Aï¼Œç„¶åçº¿ç¨‹oneæ“ä½œæˆåŠŸã€‚è¿™ä¸ªçº¿ç¨‹oneçš„æ“ä½œå¯èƒ½å‡ºé—®é¢˜ã€‚) 11.AQSï¼šAbstractQueuedSynchronizerï¼Œç»´æŠ¤ä¸€ä¸ªvolatile int stateï¼ˆä»£è¡¨å…±äº«èµ„æºçŠ¶æ€ï¼‰å’Œä¸€ä¸ªFIFOçº¿ç¨‹ç­‰å¾…é˜Ÿåˆ—ã€‚ æ­»é”1.ä»€ä¹ˆæ˜¯æ­»é”ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹äº’ç›¸æŒæœ‰å¯¹æ–¹éœ€è¦çš„èµ„æºï¼Œå¯¼è‡´ä¸€ç›´ç­‰å¾…çŠ¶æ€ï¼Œäº’ç›¸ç­‰å¾…å¯¹æ–¹é‡Šæ”¾èµ„æºï¼Œå¦‚æœæ²¡æœ‰ä¸»åŠ¨é‡Šæ”¾èµ„æºï¼Œå°±ä¼šæ­»é”ã€‚ 2.æ­»é”äº§ç”Ÿæ¡ä»¶ 1ã€å­˜åœ¨å¾ªç¯ç­‰å¾… 2ã€å­˜åœ¨èµ„æºç«äº‰ 3ã€å·²ç»è·å¾—çš„èµ„æºä¸ä¼šè¢«å‰¥å¤º 4ã€è¯·æ±‚ä¸ä¿æŒï¼Œä¸€ä¸ªçº¿ç¨‹å› è¯·æ±‚èµ„æºè¢«é˜»å¡æ—¶ï¼Œæ‹¥æœ‰èµ„æºçš„çº¿ç¨‹çš„çŠ¶æ€ä¸ä¼šæ”¹å˜ã€‚ 3.é¿å…æ­»é” äº§ç”Ÿæ­»é”æ¡ä»¶ä¸­ä»»æ„ä¸€æ¡ä¸æ»¡è¶³ï¼Œå°±ä¸ä¼šäº§ç”Ÿæ­»é” 4.æ­»é”å®šä½ä¿®å¤ æ‰§è¡Œç¨‹åºæ—¶ï¼Œç¨‹åºæ²¡åœæ­¢ï¼Œä¹Ÿä¸ç»§ç»­è¿è¡Œï¼Œåˆ™æ˜¯æ­»é” é€šè¿‡jpsè·å–ç«¯å£å·ï¼Œå†jstackå·¥å…·å¯ä»¥çœ‹åˆ° å…¶ä»–å…³äºAQSå’ŒCASè¯¦ç»†ï¼šhttps://www.jianshu.com/p/2a48778871a9é”çŠ¶æ€å‡çº§è¯¦è§£ï¼šhttp://www.jetchen.cn/synchronized-status/Javaå¹¶å‘-volatileå†…å­˜å¯è§æ€§å’ŒæŒ‡ä»¤é‡æ’ï¼šhttps://blog.csdn.net/jiyiqinlovexx/article/details/50989328","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"çº¿ç¨‹ã€è¿›ç¨‹","slug":"çº¿ç¨‹ã€è¿›ç¨‹","permalink":"https://shmily-qjj.top/tags/%E7%BA%BF%E7%A8%8B%E3%80%81%E8%BF%9B%E7%A8%8B/"},{"name":"æ¦‚å¿µ","slug":"æ¦‚å¿µ","permalink":"https://shmily-qjj.top/tags/%E6%A6%82%E5%BF%B5/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Apache Zeppelinåˆæ¢","slug":"Apache Zeppelinåˆæ¢","date":"2020-02-11T02:16:00.000Z","updated":"2022-12-11T05:35:07.901Z","comments":true,"path":"174820fd/","link":"","permalink":"https://shmily-qjj.top/174820fd/","excerpt":"","text":"Apache Zeppelinä»€ä¹ˆæ˜¯ZeppelinApache Zeppelinæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ï¼Œé«˜å¯ç”¨ï¼Œé«˜å¯é çš„åˆ†å¸ƒå¼Key-Valueå­˜å‚¨ä¸å¯è§†åŒ–å¹³å°ï¼Œå®ƒæ˜¯é›†æ•°æ®æ‘„å–ï¼Œæ•°æ®åˆ†æï¼Œæ•°æ®å¯è§†åŒ–ä¸åä½œäºä¸€èº«çš„notebookå½¢å¼çš„åŸºäºWebçš„å·¥å…·ï¼Œæ”¯æŒå¤šç§è§£é‡Šå™¨(Interpreter),èƒ½å¹¿æ³›æ”¯æŒå¤šç§å¤§æ•°æ®æŸ¥è¯¢å¼•æ“å’Œè®¡ç®—å¼•æ“(å¦‚Sparkï¼ŒFlinkï¼ŒPrestoï¼ŒKylinâ€¦)ï¼Œå¤šç§å­˜å‚¨ç³»ç»Ÿ(å¦‚JDBCæ•°æ®æºï¼ŒHBaseï¼ŒElasticsearchï¼ŒHiveï¼ŒNeo4jï¼ŒAlluxioï¼ŒIgniteâ€¦),ä»¥åŠå¤šç§è„šæœ¬è¯­è¨€(å¦‚python,scala,R,shellâ€¦)å’Œmarkdownã€‚ Apache Zeppelinæ”¯æŒçš„éƒ¨åˆ†ç»„ä»¶ï¼š Zeppelinä¼˜åŠ¿ ä¸ºæ•°æ®åˆ†æä¸å¯è§†åŒ–æä¾›ä¾¿åˆ©ï¼šåœ¨Zeppelinä¸­ä»¥ç¬”è®°æœ¬ï¼ˆnotebookï¼‰çš„å½¢å¼ç»„ç»‡å’Œç®¡ç†äº¤äº’å¼æ•°æ®æ¢ç´¢ä»»åŠ¡ï¼Œä¸€ä¸ªç¬”è®°æœ¬ï¼ˆnoteï¼‰å¯ä»¥åŒ…æ‹¬å¤šä¸ªæ®µï¼ˆparagraphï¼‰ã€‚æ®µæ˜¯è¿›è¡Œæ•°æ®åˆ†æçš„æœ€å°å•ä½ï¼Œå³åœ¨æ®µä¸­å¯ä»¥å®Œæˆæ•°æ®åˆ†æä»£ç çš„ç¼–å†™ä»¥åŠç»“æœçš„å¯è§†åŒ–æŸ¥çœ‹ ä¸ºå¤šäººåä½œæä¾›ä¾¿åˆ©ï¼šå¯ä»¥å…±äº«ä½ çš„notebookï¼Œä½¿ä»–äººä¹Ÿèƒ½çœ‹åˆ°ä½ çš„æ•°æ®åˆ†æç¬”è®°å’Œç»“æœ æä¾›æƒé™ç®¡ç†ï¼šå¯ä»¥ç®¡ç†notebookçš„æƒé™ä»¥åŠæ‰§è¡Œè€…æ˜¯å¦å¯¹å·²æœ‰æ•°æ®æœ‰ä¿®æ”¹æƒé™ æ”¯æŒå¤šç§æŸ¥è¯¢è®¡ç®—å¼•æ“ï¼šå…¼å®¹å¤šç§ä¸»æµå¤§æ•°æ®æŸ¥è¯¢ï¼Œè®¡ç®—å¼•æ“ï¼Œä½¿å¾—æ•°æ®åˆ†ææ›´åŠ æ–¹ä¾¿ï¼Œæ•°æ®åˆ†æäººå‘˜å¯ä»¥å¯¹åº•å±‚æ— æ„ŸçŸ¥ ä¸ºä¸´æ—¶è·å–æŸäº›æ•°æ®æä¾›ä¾¿åˆ©ï¼šæœ‰éœ€è¦ä¸´æ—¶è·å–ä¸€äº›æ•°æ®çš„éœ€æ±‚ï¼Œé€šè¿‡é…ç½®Interpreterå³å¯ é…ç½®ä¸éƒ¨ç½²ç®€å•ï¼šå·²å®Œå…¨æ”¯æŒçš„ç»„ä»¶åªéœ€ç®€å•å¡«å†™è§£é‡Šå™¨å‚æ•°å³å¯ä½¿ç”¨ï¼Œæ”¯æŒå®‰è£…ç¬¬ä¸‰æ–¹è§£é‡Šå™¨ æ”¯æŒç®€å•ä»»åŠ¡è°ƒåº¦ï¼šLinux Crontabè°ƒåº¦å™¨åŠŸèƒ½ Zeppeliné€‚ç”¨åœºæ™¯ å¤šä¸ªéƒ¨é—¨éœ€è¦åœ¨å¤§æ•°æ®å¹³å°å–æ•°æ®åšåˆ†æçš„åœºæ™¯ éœ€è¦å¤šç§æŸ¥è¯¢å¼•æ“åšæ•°æ®åˆ†æçš„åœºæ™¯ éœ€è¦å¯¹å¤šç§æ•°æ®æºè¿›è¡Œæ•°æ®å¯è§†åŒ–çš„åœºæ™¯ éœ€è¦å¤šäººåä½œçš„åœºæ™¯ æ•°æ®å¹³å°ä¸æ•°æ®åˆ†æåˆ†ç¦»ï¼Œå¯¹æ•°æ®åˆ†æäººå‘˜æ— æ„ŸçŸ¥çš„åœºæ™¯ Zeppelinè¯¦ç»†è§£é‡Šå™¨Interpretersï¼ˆé‡è¦ï¼‰Zeppelin Interpreteræ˜¯ä¸€ä¸ªæ’ä»¶ï¼Œå…è®¸å°†æ”¯æŒçš„è¯­è¨€/æ•°æ®å¤„ç†åç«¯æ’å…¥Zeppelinã€‚é€šè¿‡ç®€å•çš„é…ç½®å³å¯å°†è¯­è¨€/æ•°æ®å¤„ç†æŸ¥è¯¢åç«¯æ¥å…¥Zeppelinã€‚Zeppelinè§£é‡Šå™¨Zeppelin-Sparkè§£é‡Šå™¨ NotebookZeppelinçš„å·¥ä½œç°¿(Notebook)æ”¯æŒåˆ†ä¸ºå¤šä¸ªæ®µï¼Œæ¯æ®µæ”¯æŒç»‘å®šå¤šä¸ªä¸åŒçš„è§£é‡Šå™¨ï¼Œæ”¯æŒåšå•ç‹¬å¤„ç†å’Œæ‰§è¡Œä¸åŒçš„æ“ä½œï¼Œç»“æœä¼šä¸€ç›´è¢«ä¿ç•™ï¼Œå¯ä»¥é€‰æ‹©è®©å…¶ä»–äººæµè§ˆæˆ–è€…ä¿®æ”¹ã€‚Notebookæä¾›ç»™æ•°æ®åˆ†æäººå‘˜çš„å‰ç«¯å·¥ä½œç¯å¢ƒï¼Œæ–¹ä¾¿æ•°æ®åˆ†æå’Œæ•°æ®å¯è§†åŒ–ã€‚ Interpreter Groupè§£é‡Šå™¨ç»„ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè§£é‡Šå™¨å±äºä¸€ä¸ªè§£é‡Šå™¨ç»„ï¼Œä¸€ä¸ªè§£é‡Šå™¨ç»„å¯èƒ½åŒ…å«å¤šä¸ªè§£é‡Šå™¨åŒä¸€InterpreterGroupä¸­çš„Interpreterå¯ä»¥ç›¸äº’å¼•ç”¨ä¾‹å¦‚Sparkè§£é‡Šå™¨ç»„åŒ…æ‹¬Sparkæ”¯æŒï¼ŒPSparkï¼ŒSparkSqlå’Œå…¶ä»–ä¾èµ–é¡¹åŒä¸€è§£é‡Šå™¨ç»„ä¸­çš„Zeppelinç¨‹åºåœ¨åŒä¸€JVMè¿è¡Œè§£é‡Šå™¨ç»„æ˜¯å¼€å¯ã€åœæ­¢è§£é‡Šå™¨è¿è¡Œçš„åŸºæœ¬å•ä½ã€‚(åŒæ—¶å¼€å¯ï¼Œåœæ­¢) Interpreter binding modeè§£é‡Šå™¨ç»‘å®šæ¨¡å¼ï¼šå¯é€‰â€™sharedâ€™, â€˜scopedâ€™, â€˜isolatedâ€™ å…¶ä¸€sharedï¼šå…±äº«æ¨¡å¼ï¼Œç»‘å®šè§£é‡Šå™¨çš„æ¯ä¸ªNotebookå…±äº«å•ä¸ªè§£é‡Šå™¨å®ä¾‹(æ–¹ä¾¿ä¸åŒNotebooké—´å…±äº«å˜é‡ï¼Œä½†èµ„æºåˆ©ç”¨ç‡ä½)scopedï¼šä½œç”¨åŸŸæ¨¡å¼ï¼Œåœ¨ç›¸åŒè§£é‡Šå™¨ç¨‹åºä¸­åˆ›å»ºæ–°çš„è§£é‡Šå™¨å®ä¾‹(æ¯ä¸ªNotebookæ‹¥æœ‰è‡ªå·±çš„å›è¯ï¼Œèµ„æºåˆ©ç”¨ç‡ç•¥é«˜ï¼Œä¸èƒ½ç›´æ¥å…±äº«å˜é‡)isolatedï¼šéš”ç¦»æ¨¡å¼ï¼Œæ¯ä¸ªNotebookåˆ›å»ºæ–°çš„è§£é‡Šå™¨ç¨‹åº(ç¬”è®°æœ¬ä¹‹é—´äº’ä¸å½±å“ï¼Œä¸èƒ½ç›´æ¥å…±äº«å˜é‡) æ¯”å¦‚sharedæ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªNotebookéƒ½å¯ä»¥ä½¿ç”¨SparkInterpreterä½†æ˜¯åªæœ‰ä¸€ä¸ªSparkContext å¦‚æœisolatedæ¨¡å¼ï¼Œæ¯ä¸ªNotebookéƒ½å¯ä»¥ä½¿ç”¨SparkInterpreterä½†æ¯ä¸ªNotebookæœ‰å•ç‹¬çš„SparkContext è§£é‡Šå™¨ç»‘å®šæ¨¡å¼-å®˜æ–¹è¯¦ç»†ä»‹ç» Interpreterç”Ÿå‘½å‘¨æœŸZeppelin 0.8.0ä»¥åæ”¯æŒLifecycleManageræ¥æ§åˆ¶è§£é‡Šå™¨ç”Ÿå‘½å‘¨æœŸ(ä¹‹å‰æ˜¯å…³é—­UIç•Œé¢åç”Ÿå‘½å‘¨æœŸç»“æŸ)NullLifecycleManagerä¸åšæ“ä½œï¼Œè¦åƒä»¥å‰ä¸€æ ·è‡ªè¡Œæ§åˆ¶ç”Ÿå‘½å‘¨æœŸTimeoutLifecycleManager(é»˜è®¤ç”Ÿå‘½å‘¨æœŸç®¡ç†)é»˜è®¤è¶…è¿‡1å°æ—¶å…³é—­è§£é‡Šå™¨ï¼Œå¯ä»¥æ›´æ”¹ Generic ConfInterpreterZeppelinè§£é‡Šå™¨é…ç½®ç”±æ‰€æœ‰ç”¨æˆ·å’ŒNotebookå…±äº«ï¼Œå¦‚æœæƒ³ä½¿ç”¨å…¶ä»–çš„è®¾ç½®ï¼Œéœ€è¦åˆ›å»ºæ–°çš„è§£é‡Šå™¨ï¼Œèƒ½å®ç°ä½†ä¸æ–¹ä¾¿ï¼ŒConfInterpreterå¯ä»¥æä¾›å¯¹è§£é‡Šå™¨è®¾ç½®çš„æ›´ç»†ç²’åº¦çš„æ§åˆ¶å’Œæ›´å¤§çš„çµæ´»æ€§ã€‚ConfInterpreteræ˜¯å¯ä»¥è¢«ä»»ä½•è§£é‡Šå™¨ä½¿ç”¨çš„é€šç”¨è§£é‡Šå™¨ï¼Œè¾“å…¥æ ¼å¼åº”ä¸ºå±æ€§æ–‡ä»¶æ ¼å¼ã€‚å®ƒç”¨äºä¸ºä»»ä½•è§£é‡Šå™¨è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®ã€‚ç”¨æˆ·éœ€è¦å°†ConfInterpreteræ”¾åœ¨Notebookçš„ç¬¬ä¸€æ®µå¦‚ä¸Šå›¾%spark.confç‹¬ç«‹è®¾ç½®äº†è¯¥Notebookä¸­çš„Sparkè§£é‡Šå™¨ Interpreterè¿›ç¨‹æ¢å¤0.8.0ç‰ˆæœ¬å‰ï¼Œå…³é—­Zeppelinä¼šåŒæ—¶å…³é—­æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„è§£é‡Šå™¨ç¨‹åºï¼Œä½†æ˜¯æˆ‘ä»¬å¯èƒ½åªæ˜¯æƒ³ç»´æŠ¤ZeppelinæœåŠ¡å™¨è€Œä¸æƒ³å…³é—­è§£é‡Šå™¨ç¨‹åºï¼ŒInterpreterè¿›ç¨‹æ¢å¤å°±æ´¾ä¸Šç”¨åœºäº†ã€‚0.8.0ç‰ˆæœ¬åï¼Œè®¾ç½®zeppelin.recovery.storage.classå±æ€§çš„å€¼é»˜è®¤org.apache.zeppelin.interpreter.recovery.NullRecoveryStorageä¸å¼€å¯è¿›ç¨‹æ¢å¤è®¾ç½®ä¸ºorg.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorageå¼€å¯è¿›ç¨‹æ¢å¤ï¼Œå…³é—­Zeppelinä¸ä¼šå…³é—­è§£é‡Šå™¨ç¨‹åºå¦‚æœå¼€å¯äº†è¿›ç¨‹æ¢å¤ï¼Œå…³é—­äº†Zeppelinï¼Œåˆæƒ³å†å…³é—­è§£é‡Šå™¨ç¨‹åºï¼Œåˆ™æ‰§è¡Œbin/stop-interpreter.sh å®˜æ–¹æ–‡æ¡£å®˜æ–¹Docs å¸¸è§é—®é¢˜åŠé”™è¯¯æ’é™¤ *Interpreter ** is not found**ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»é…ç½®äº†è¯¥è§£é‡Šå™¨ï¼Œå¦‚æœé…ç½®äº†ï¼Œæ£€æŸ¥è¯¥è§£é‡Šå™¨æ˜¯å¦å·²è¢«ç‚¹äº®(å³ä¸Šè§’è®¾ç½®å›¾æ ‡ç‚¹ä¸ºè“è‰²å¹¶ä¿å­˜) è¯¦ç»†æ·±å…¥äº†è§£: Apache Zeppelinå®˜ç½‘","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zeppelin","slug":"Zeppelin","permalink":"https://shmily-qjj.top/tags/Zeppelin/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"åˆ†äº«æˆ‘çš„æŠ€æœ¯è°ƒç ”æµç¨‹","slug":"åˆ†äº«æˆ‘çš„æŠ€æœ¯è°ƒç ”æµç¨‹","date":"2020-01-03T13:10:00.000Z","updated":"2022-12-11T05:35:07.913Z","comments":true,"path":"4b21953d/","link":"","permalink":"https://shmily-qjj.top/4b21953d/","excerpt":"","text":"æŠ€æœ¯è°ƒç ”æµç¨‹åˆ†äº«ä¸ºä½•åˆ¶å®šæµç¨‹ï¼Ÿæ˜ç¡®è°ƒç ”éœ€æ±‚ï¼Œæé«˜è°ƒç ”æ–‡æ¡£è´¨é‡ï¼Œè§„èŒƒè°ƒç ”æµç¨‹ï¼Œä¿è¯è°ƒç ”äº§å‡ºã€‚ æˆ‘æ˜¯å¦‚ä½•åˆ¶å®šè°ƒç ”æµç¨‹çš„ï¼Ÿå…ˆæ€»ç»“ä¸€ç‰ˆè‡ªå·±çš„è°ƒç ”æµç¨‹ï¼Œå†æŸ¥é˜…èµ„æ–™ä»¥åŠæŸ¥çœ‹é˜¿é‡Œç­‰å¤§å‚çš„è°ƒç ”æŠ¥å‘Šï¼Œç»“åˆéƒ¨é—¨ç›®å‰çš„å®é™…æƒ…å†µæ¥æ˜ç¡®éƒ¨é—¨çš„è°ƒç ”æµç¨‹ã€‚ æŠ€æœ¯è°ƒç ”æµç¨‹æ•´ä¸ªè°ƒç ”æµç¨‹åˆ†å››ä¸ªé˜¶æ®µ ç¬¬ä¸€é˜¶æ®µï¼šéœ€æ±‚åˆ†æ åˆ†æç›®å‰/æœªæ¥å¯èƒ½å‡ºç°çš„ç“¶é¢ˆç‚¹ æ˜ç¡®è°ƒç ”ç›®æ ‡å’Œæ–¹å‘ï¼ˆä¸ºäº†å®ç°æ–°éœ€æ±‚ï¼Ÿä¸ºäº†ä¼˜åŒ–ç“¶é¢ˆç‚¹ï¼Ÿï¼‰ å¼•å…¥æ–°å·¥å…·åçš„ç»“æœè¡¡é‡ï¼ˆæ•ˆç‡æå‡ã€æˆæœ¬é™ä½ç­‰ï¼Œå¦‚ä½•è¡¡é‡ï¼‰ ç»“æ„åŒ–æ€è€ƒæ–°å·¥å…·å¼•å…¥çš„ç›®æ ‡å’Œè¡¡é‡æ ‡å‡†ï¼šåœºæ™¯ï¼ˆé€‚ç”¨åœºæ™¯ã€çŸ¥è¯†è¦æ±‚ï¼‰ã€æ•ˆç‡ï¼ˆæ€§èƒ½ã€æ•ˆæœé¢„æµ‹ï¼‰ã€æˆæœ¬ï¼ˆå®¹é‡ã€ç¡¬ä»¶èµ„æºã€ç»´æŠ¤æˆæœ¬ï¼‰ã€ç¨³å®šæ€§ï¼ˆæ•…éšœåˆ†æå·¥å…·ã€ç›‘æ§å®Œå–„åº¦ï¼‰ç­‰ ç¬¬äºŒé˜¶æ®µï¼šå‡†å¤‡é˜¶æ®µ ç†è§£éœ€æ±‚ ç»“åˆç°çŠ¶è¯„ä¼°å¯è¡Œæ€§å’Œæ”¶ç›Š ç¬¬ä¸‰é˜¶æ®µï¼šè°ƒç ”é˜¶æ®µ ç®€å•è°ƒç ” çŸ­æ—¶é—´å†…ç²—ç•¥äº†è§£æ‰€è°ƒç ”æŠ€æœ¯çš„åº”ç”¨åœºæ™¯å’Œéƒ¨ç½²ç¯å¢ƒï¼Œè¿›ä¸€æ­¥è¯„ä¼°å’Œæƒè¡¡å¯è¡Œæ€§å’Œæ”¶ç›Š ç»è¿‡æƒè¡¡åå‘ç°å€¼å¾—è°ƒç ”ï¼Œå‘é€é‚®ä»¶è‡³ç›´æ¥ä¸Šçº§å¹¶æŠ„é€éƒ¨é—¨Leader (æ ‡é¢˜ï¼šç”³è¯·è°ƒç ”xxx å†…å®¹ï¼šç®€è¿°xxxå€¼å¾—è¯¦ç»†è°ƒç ”çš„ç†ç”±) åå•†å†³å®šæ˜¯å¦æ‰¹å‡†ï¼Œè‹¥æ‰¹å‡†ï¼Œåˆ™å¼€å§‹è¿›å…¥è¯¦ç»†è°ƒç ”é˜¶æ®µ è¯¦ç»†è°ƒç ” åŒ…æ‹¬ä½†ä¸é™äºï¼š å…ˆè®¾è®¡è°ƒç ”æ–¹æ³•ä¸è°ƒç ”è¿‡ç¨‹ é¢„ä¼°è°ƒç ”æ—¶é—´ï¼Œå¹¶åœ¨åŒ—æ£®è®¾å®šDeadlineï¼Œæ ¹æ®è°ƒç ”æŠ¥å‘Šçš„è¦æ±‚æŒ‰æ—¶å®Œæˆè°ƒç ”æŠ¥å‘Š äº†è§£ç›¸å…³æŠ€æœ¯åœ¨å…¶ä»–å…¬å¸çš„åº”ç”¨åŠæ”¶ç›Š åŸç†åŠæ ¸å¿ƒæŠ€æœ¯è°ƒç ” æ€»ç»“é€‚åˆæˆ‘ä»¬çš„åœºæ™¯åŠè§£å†³æ–¹æ¡ˆ è°ƒç ”è¿‡ç¨‹é‡åˆ°çš„é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ æœªè§£å†³çš„é—®é¢˜/æ”¶é›†éœ€æ±‚å¯éšæ—¶è®¨è®ºï¼Œæœ‰å¿…è¦çš„è¯å¯ä»¥å¼€è®¨è®ºä¼š è®¾è®¡è½åœ°æ–¹æ¡ˆ ç¬¬å››é˜¶æ®µï¼šåé¦ˆä¸è½åœ° è°ƒç ”åé¦ˆ å¿…é¡»äº§å‡ºä¸€ä»½è°ƒç ”æŠ¥å‘Š é€‰æ‹©åé¦ˆå½¢å¼ï¼šåˆ†äº«ä¼šã€æ–‡æ¡£ã€é‚®ä»¶ã€ç¾¤é€šçŸ¥ï¼ˆå¦‚æœæ˜¯åˆ†äº«ä¼šï¼Œåˆ™è¦æœ‰å®Œå–„çš„PPTï¼Œä¼šå‰å…±äº«å‡ºæ¥ï¼‰ æŠ€æœ¯è½åœ° æ ¹æ®è‡ªå·±è®¾è®¡çš„è½åœ°æ–¹æ¡ˆå¾—å‡ºè¯¦ç»†çš„éƒ¨ç½²æ–‡æ¡£å’Œä½¿ç”¨æ–‡æ¡£ é…åˆè¿ç»´éƒ¨ç½² åç»­é˜¶æ®µï¼šè½åœ°åå¦‚ä½•è·Ÿè¿› å‡ºç°é—®é¢˜åŠæ—¶è·Ÿè¿›è§£å†³ï¼Œå¹¶æŠŠé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆæ›´æ–°åˆ°ä½¿ç”¨æ–‡æ¡£ä¸­ï¼Œå¦‚æœå½±å“è¾ƒå¤§ï¼Œè¦åœ¨æ›´æ–°å®Œä½¿ç”¨æ–‡æ¡£åå‘ç¾¤é€šçŸ¥ã€‚ ç›¸å…³çš„æ–°äººæ–‡æ¡£/Wikiæ›´æ–° æ–‡æ¡£è¦æ±‚æ‰€æœ‰è°ƒç ”æ–‡æ¡£ç»Ÿä¸€ä¿å­˜åœ¨gitlabéƒ¨é—¨æ–‡æ¡£ä¸­çš„æŠ€æœ¯è°ƒç ”æ–‡æ¡£ç›®å½• éƒ¨ç½²æ–‡æ¡£è¦æ±‚ è¿™éƒ¨åˆ†ä¸ºäº†æ–¹ä¾¿è®©è¿ç»´äººå‘˜å‚»ç“œå¼éƒ¨ç½²ï¼Œå¹¶å¯ä»¥æŠŠç®€å•çš„è¿ç»´å·¥ä½œäº¤ç»™è¿ç»´ã€‚ å°½é‡æ‰“åŒ…å¥½ä¸»ä»èŠ‚ç‚¹çš„åˆ†å‘åŒ…ï¼ˆæå‰ç¼–è¯‘å¥½ï¼‰ ä¾‹ï¼š xxx-master.zip xxx-worker.zip æˆ–æ•´ç†confé…ç½®æ–‡ä»¶åŒ… xxx-master-conf.zip xxx-worker-conf.zip å°½é‡é‡‡ç”¨å‚»ç“œå¼å‘½ä»¤ ä¾‹:sudo -uhdfs tar -zxvf /opt/alluxio-2.1.0-bin.tar.gz -C /opt/ sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh master sudo -uhdfs sh /opt/alluxio-2.1.0/bin/alluxio-start.sh worker chmod 777 /opt/alluxio-2.1.0/logs/user ... å°½é‡å†™å‡ºéƒ¨ç½²è¿‡ç¨‹å¯èƒ½çš„æŠ¥é”™åŠè§£å†³æ–¹æ¡ˆ å¸¸ç”¨ç»´æŠ¤æ–¹æ¡ˆæ€»ç»“ ä½¿ç”¨æ–‡æ¡£è¦æ±‚ è¿™éƒ¨åˆ†ç›®çš„æ˜¯æ–¹ä¾¿å¤§å®¶ä½¿ç”¨æ–°æŠ€æœ¯æ–°ç»„ä»¶ã€‚ æ ¼å¼åŒ…æ‹¬ä½†ä¸é™äºï¼šåœºæ™¯1ï¼šç¤ºä¾‹ä»£ç /æ“ä½œ åœºæ™¯2ï¼šç¤ºä¾‹ä»£ç /æ“ä½œ åœºæ™¯3ï¼šç¤ºä¾‹ä»£ç /æ“ä½œ ... å¸¸è§é”™è¯¯åŠè§£å†³ é‡åˆ°é—®é¢˜è¯·è”ç³»ï¼šè°ƒç ”äºº è°ƒç ”æŠ¥å‘Šè¦æ±‚ è¿™éƒ¨åˆ†çš„ç›®çš„æ˜¯è°ƒç ”æ—¶å¯èƒ½æœ‰é—æ¼çš„ç‚¹ï¼Œå¯ä»¥ä»è¿™ä¸ªåˆ—è¡¨é‡Œåšå‚è€ƒã€‚ å¼€å¤´å†™æ¸… æ ‡é¢˜ + è°ƒç ”äºº + è°ƒç ”æ—¶é—´ å¯ä»¥å‚è€ƒä½†ä¸é™äºè¿™äº›ç‚¹ï¼š xxæ˜¯ä»€ä¹ˆ xxçš„ä¼˜ç¼ºç‚¹ xxçš„åº”ç”¨åœºæ™¯ xxçš„åŠŸèƒ½/ç‰¹æ€§ xxçš„åŸç†ä¸æ¶æ„ç®€è¿° ç›¸ä¼¼æŠ€æœ¯æ¨ªå‘å¯¹æ¯” åˆæ­¥è¯„ä¼°å¸¦æ¥çš„æ”¶ç›Š é‡åˆ°çš„é—®é¢˜Q&amp;A xxçš„å…¼å®¹æ€§ï¼ˆæ”¯æŒä»€ä¹ˆä¸æ”¯æŒä»€ä¹ˆï¼‰ xxæŠ€æœ¯çš„æ ¸å¿ƒç‚¹ xxçš„æ€§èƒ½ä¸æ‰©å±•æ€§ï¼ˆæµ‹è¯•ç»“æœï¼‰ xxçš„éƒ¨ç½²éš¾åº¦ å¦‚ä½•éƒ¨ç½²ä¸ç®€å•å®è·µ åº”ç”¨è¯¥æŠ€æœ¯å¸¦æ¥çš„å·¥ä½œé‡å’Œå­¦ä¹ æˆæœ¬ æ€»ç»“ æ³¨æ„äº‹é¡¹ å…³æ³¨ç¼ºç‚¹çš„ä¼˜å…ˆçº§é«˜äºå…³æ³¨ä¼˜ç‚¹çš„ä¼˜å…ˆçº§ï¼ˆä¼˜ç‚¹å†å¤šï¼Œä¹Ÿå¯èƒ½å› ä¸ºä¸€ä¸ªç¼ºç‚¹è€Œä¸èƒ½è¢«åº”ç”¨ï¼‰ æ˜ç¡®åœºæ™¯ï¼ŒåŠæ—¶æ²Ÿé€šéœ€æ±‚ï¼Œæ˜ç¡®éœ€æ±‚ç»†èŠ‚ å¤šæœé›†ä¿¡æ¯ï¼Œä¸æ€¥äºå‡ºç»“æœï¼ˆæœé›†è¶³å¤Ÿçš„ä¿¡æ¯æ‰èƒ½åšå‡ºæ¯”è¾ƒå‡†ç¡®çš„åˆ¤æ–­ï¼‰ è¦ä»å¯è¡Œæ€§ï¼Œç¨³å®šæ€§ï¼Œå¯ç»´æŠ¤æ€§ï¼Œå·¥ä½œé‡å’Œå­¦ä¹ æˆæœ¬ç­‰å‡ ä¸ªé‡è¦æ–¹é¢è€ƒè™‘ åˆç†å®‰æ’æ—¶é—´ï¼Œè‡ªå·±è§„å®šäº†Deadlineï¼Œå°±è¦åŠæ—¶äº¤ä»˜åé¦ˆ","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"æŠ€æœ¯è°ƒç ”","slug":"æŠ€æœ¯è°ƒç ”","permalink":"https://shmily-qjj.top/tags/%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Alluxio-åŸºäºå†…å­˜çš„è™šæ‹Ÿåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ","slug":"Alluxio-åŸºäºå†…å­˜çš„è™šæ‹Ÿåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ","date":"2020-01-01T14:16:00.000Z","updated":"2022-12-11T05:35:07.899Z","comments":true,"path":"44511/","link":"","permalink":"https://shmily-qjj.top/44511/","excerpt":"","text":"ä»€ä¹ˆæ˜¯AlluxioAlluxio æ˜¯ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªè™šæ‹Ÿçš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œå®ƒä¸ºè®¡ç®—æ¡†æ¶å’Œå­˜å‚¨ç³»ç»Ÿæ„å»ºäº†æ¡¥æ¢ï¼Œä½¿è®¡ç®—æ¡†æ¶èƒ½å¤Ÿé€šè¿‡ä¸€ä¸ªå…¬å…±æ¥å£è¿æ¥åˆ°å¤šä¸ªç‹¬ç«‹çš„å­˜å‚¨ç³»ç»Ÿ,ä½¿è®¡ç®—ä¸å­˜å‚¨éš”ç¦»ã€‚ Alluxio æ˜¯å†…å­˜ä¸ºä¸­å¿ƒçš„æ¶æ„ï¼Œä»¥å†…å­˜é€Ÿåº¦ç»Ÿä¸€äº†æ•°æ®è®¿é—®é€Ÿåº¦ï¼Œä½¿å¾—æ•°æ®çš„è®¿é—®é€Ÿåº¦èƒ½æ¯”ç°æœ‰æ–¹æ¡ˆå¿«å‡ ä¸ªæ•°é‡çº§,ä¸ºå¤§æ•°æ®è½¯ä»¶æ ˆå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡åœ¨å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸­ï¼ŒAlluxio ä½äºæ•°æ®é©±åŠ¨æ¡†æ¶æˆ–åº”ç”¨ï¼ˆå¦‚ Apache Sparkã€Prestoã€Tensorflowã€Apache HBaseã€Apache Hive æˆ– Apache Flinkï¼‰å’Œå„ç§æŒä¹…åŒ–å­˜å‚¨ç³»ç»Ÿï¼ˆå¦‚ Amazon S3ã€Google Cloud Storageã€OpenStack Swiftã€GlusterFSã€HDFSã€IBM Cleversafeã€EMC ECSã€Cephã€NFS å’Œ Alibaba OSSï¼‰ä¹‹é—´,Alluxio ç»Ÿä¸€äº†å­˜å‚¨åœ¨è¿™äº›ä¸åŒå­˜å‚¨ç³»ç»Ÿä¸­çš„æ•°æ®,ä¸ºå…¶ä¸Šå±‚æ•°æ®æ¡†æ¶æä¾›ç»Ÿä¸€çš„å®¢æˆ·ç«¯APIå’Œå…¨å±€å‘½åç©ºé—´ Alluxioæœ€æ–°åŠ¨æ€:ä¸ºäº†æ–¹ä¾¿å¤§å®¶å¯ä»¥æŒç»­è·Ÿè¿›Alluxioå‘å±•åŠ¨æ€ï¼Œè¿™é‡Œç»™å‡ºä¸¤æ¡è·Ÿè¿›Alluxioæœ€æ–°å‘å±•å’ŒåŠ¨æ€çš„é€”å¾„:Alluxioå®˜æ–¹æ–‡æ¡£AlluxioçŸ¥ä¹ä¸“æ  Alluxioä¼˜åŠ¿ å†…å­˜é€Ÿåº¦ I/O:Alluxio èƒ½å¤Ÿç”¨äºåˆ†å¸ƒå¼å…±äº«ç¼“å­˜æœåŠ¡ï¼Œè¿™æ ·ä¸ Alluxio é€šä¿¡çš„è®¡ç®—åº”ç”¨ç¨‹åºå¯ä»¥é€æ˜åœ°ç¼“å­˜é¢‘ç¹è®¿é—®çš„æ•°æ®ï¼ˆå°¤å…¶æ˜¯ä»è¿œç¨‹ä½ç½®ï¼‰,ä»¥æä¾›è¿‘ä¼¼äºå†…å­˜çº§ I/O ååç‡ï¼ŒåŒæ—¶æå‡ç¨³å®šæ€§ã€‚ ç®€åŒ–äº‘å­˜å‚¨å’Œå¯¹è±¡å­˜å‚¨æ¥å…¥:ä¸ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿç›¸æ¯”,äº‘å­˜å‚¨ç³»ç»Ÿå’Œå¯¹è±¡å­˜å‚¨ç³»ç»Ÿä½¿ç”¨ä¸åŒçš„è¯­ä¹‰,è¿™äº›è¯­ä¹‰å¯¹æ€§èƒ½çš„å½±å“ä¹Ÿä¸åŒäºä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿã€‚å¸¸è§çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼ˆå¦‚åˆ—å‡ºç›®å½•å’Œé‡å‘½åï¼‰é€šå¸¸ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½å¼€é”€ã€‚å½“è®¿é—®äº‘å­˜å‚¨ä¸­çš„æ•°æ®æ—¶ï¼Œåº”ç”¨ç¨‹åºæ²¡æœ‰èŠ‚ç‚¹çº§æ•°æ®æœ¬åœ°æ€§æˆ–è·¨åº”ç”¨ç¨‹åºç¼“å­˜ã€‚å°† Alluxio ä¸äº‘å­˜å‚¨æˆ–å¯¹è±¡å­˜å‚¨ä¸€èµ·éƒ¨ç½²å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜,å› ä¸ºè¿™æ ·å°†ä»Alluxioä¸­æ£€ç´¢è¯»å–æ•°æ®,è€Œä¸æ˜¯ä»åº•å±‚äº‘å­˜å‚¨æˆ–å¯¹è±¡å­˜å‚¨ä¸­æ£€ç´¢è¯»å–ã€‚ ç®€åŒ–æ•°æ®ç®¡ç†:Alluxio æä¾›å¯¹å¤šæ•°æ®æºçš„å•ç‚¹è®¿é—®,ä¾¿æ·åœ°ç®¡ç†è¿œç¨‹çš„å­˜å‚¨ç³»ç»Ÿ,å¹¶å‘ä¸Šå±‚æä¾›ç»Ÿä¸€çš„å‘½åç©ºé—´ã€‚é™¤äº†è¿æ¥ä¸åŒç±»å‹çš„æ•°æ®æºä¹‹å¤–,Alluxio è¿˜å…è®¸ç”¨æˆ·åŒæ—¶è¿æ¥åˆ°ä¸åŒç‰ˆæœ¬çš„åŒä¸€å­˜å‚¨ç³»ç»Ÿ,å¦‚å¤šä¸ªç‰ˆæœ¬çš„HDFS,å¹¶ä¸”æ— éœ€å¤æ‚çš„ç³»ç»Ÿé…ç½®å’Œç®¡ç†ï¼Œæé«˜äº†æ•°æ®è®¿é—®çµæ´»æ€§ã€‚ åº”ç”¨ç¨‹åºéƒ¨ç½²ç®€æ˜“:Alluxio ç®¡ç†åº”ç”¨ç¨‹åºå’Œæ–‡ä»¶æˆ–å¯¹è±¡å­˜å‚¨ä¹‹é—´çš„é€šä¿¡ï¼Œå°†åº”ç”¨ç¨‹åºçš„æ•°æ®è®¿é—®è¯·æ±‚è½¬æ¢ä¸ºåº•å±‚å­˜å‚¨æ¥å£çš„è¯·æ±‚ã€‚Alluxio ä¸ Hadoop å…¼å®¹,ç°æœ‰çš„æ•°æ®åˆ†æåº”ç”¨ç¨‹åº,å¦‚Sparkå’ŒMapReduceç¨‹åº,æ— éœ€æ›´æ”¹ä»»ä½•ä»£ç å°±èƒ½åœ¨Alluxioä¸Šè¿è¡Œã€‚ åˆ†å±‚å­˜å‚¨ç‰¹æ€§:ç»¼åˆä½¿ç”¨äº†å†…å­˜ã€SSDå’Œç£ç›˜å¤šç§å­˜å‚¨èµ„æºã€‚é€šè¿‡Alluxioæä¾›çš„LRUã€LFUç­‰ç¼“å­˜ç­–ç•¥å¯ä»¥ä¿è¯çƒ­æ•°æ®ä¸€ç›´ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œå†·æ•°æ®åˆ™è¢«æŒä¹…åŒ–åˆ°level 2ç”šè‡³level 3çš„å­˜å‚¨è®¾å¤‡ä¸Š æ–¹ä¾¿è¿ç§»å¯æ’æ‹”:Alluxioæä¾›å¤šç§æ˜“ç”¨çš„APIæ–¹ä¾¿å°†æ•´ä¸ªç³»ç»Ÿè¿ç§»åˆ°Alluxio Alluxioçš„ç‰¹å¾:å¯¹Alluxioçš„ä¼˜åŠ¿å’Œç‰¹å¾è¿›è¡Œäº†æ¦‚æ‹¬ ç‚¹å‡»å¯è¿›å…¥å®˜ç½‘ä»‹ç»è¶…å¤§è§„æ¨¡å·¥ä½œè´Ÿè½½:æ”¯æŒè¶…å¤§è§„æ¨¡å·¥ä½œè´Ÿè½½å¹¶å…·æœ‰HAé«˜å¯ç”¨æ€§çµæ´»çš„API :è®¡ç®—æ¡†æ¶å¯ä½¿ç”¨HDFSã€S3ã€Javaã€RESTfulæˆ–POSIXä¸ºåŸºç¡€çš„APIæ¥è®¿é—®Alluxioæ™ºèƒ½æ•°æ®ç¼“å­˜å’Œåˆ†å±‚ : ä½¿ç”¨åŒ…æ‹¬å†…å­˜åœ¨å†…çš„æœ¬åœ°å­˜å‚¨ï¼Œæ¥å……å½“åˆ†å¸ƒå¼ç¼“å­˜,å¾ˆå¤§ç¨‹åº¦ä¸Šæ”¹å–„I/Oæ€§èƒ½ï¼Œä¸”ç¼“å­˜å¯¹ç”¨æˆ·é€æ˜å­˜å‚¨ç³»ç»Ÿæ¥å£ : é€šè¿‡ä¸€ç³»åˆ—æ¥å£é›†æˆHDFSï¼ŒS3ï¼ŒAzure Blob Storeï¼ŒGoogle Cloud Storeç­‰å­˜å‚¨ç³»ç»Ÿç»Ÿä¸€å…¨å±€å‘½åç©ºé—´ : å¤šä¸ªå­˜å‚¨ç³»ç»Ÿå®‰è£…åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åç§°ç©ºé—´ä¸­ï¼Œä¸éœ€è¦åˆ›å»ºæ°¸ä¹…æ•°æ®å‰¯æœ¬ï¼Œæ–¹ä¾¿ç®¡ç†å¤šæ•°æ®æºå®‰å…¨æ€§ : é€šè¿‡å†…ç½®å®¡æ ¸ã€åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ã€LDAPã€æ´»åŠ¨ç›®å½•å’ŒåŠ å¯†é€šä¿¡ï¼Œæä¾›æ•°æ®ä¿æŠ¤ç›‘æ§å’Œç®¡ç† : æä¾›äº†ç”¨æˆ·å‹å¥½çš„Webç•Œé¢å’Œå‘½ä»¤è¡Œå·¥å…·ï¼Œå…è®¸ç”¨æˆ·ç›‘æ§å’Œç®¡ç†é›†ç¾¤åˆ†å±‚æ¬¡çš„æœ¬åœ°æ€§ : å°†æ›´å¤šçš„è¯»å†™å®‰æ’åœ¨æœ¬åœ°,å®ç°æˆæœ¬å’Œæ€§èƒ½çš„ä¼˜åŒ– Alluxioçš„åº”ç”¨åœºæ™¯Alluxio çš„è½åœ°éå¸¸ä¾èµ–åœºæ™¯ï¼Œå¦åˆ™ä¼˜åŒ–æ•ˆæœå¹¶ä¸æ˜æ˜¾ï¼ˆæ— æ³•å‘æŒ¥å†…å­˜è¯»å–çš„ä¼˜åŠ¿ï¼‰1.è®¡ç®—åº”ç”¨éœ€è¦åå¤è®¿é—®è¿œç¨‹äº‘ç«¯æˆ–æœºæˆ¿çš„æ•°æ®ï¼ˆå­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼‰2.æ··åˆäº‘,è®¡ç®—ä¸å­˜å‚¨åˆ†ç¦»,å¼‚æ„çš„æ•°æ®å­˜å‚¨å¸¦æ¥çš„ç³»ç»Ÿè€¦åˆï¼ˆAlluxioæä¾›ç»Ÿä¸€å‘½åç©ºé—´ï¼Œç»Ÿä¸€è®¿é—®æ¥å£ï¼‰3.å¤šä¸ªç‹¬ç«‹çš„å¤§æ•°æ®åº”ç”¨ï¼ˆæ¯”å¦‚ä¸åŒçš„Spark Jobï¼‰éœ€è¦é«˜é€Ÿæœ‰æ•ˆçš„å…±äº«æ•°æ®ï¼ˆæ•°æ®å¹¶å‘è®¿é—®ï¼‰4.è®¡ç®—æ¡†æ¶æ‰€åœ¨æœºå™¨å†…å­˜å ç”¨è¾ƒé«˜,GCé¢‘ç¹,æˆ–è€…ä»»åŠ¡å¤±è´¥ç‡è¾ƒé«˜,Alluxioé€šè¿‡æ•°æ®çš„OffHeapæ¥å‡å°‘GCå¼€é”€5.æœ‰æ˜æ˜¾çƒ­è¡¨/çƒ­æ•°æ®ï¼Œç›¸åŒæ•°æ®è¢«å•åº”ç”¨å¤šæ¬¡è®¿é—®6.éœ€è¦åŠ é€Ÿäººå·¥æ™ºèƒ½äº‘ä¸Šåˆ†æï¼ˆå¦‚TensorFlowæœ¬åœ°è®­ç»ƒï¼Œå¯é€šè¿‡FUSEæŒ‚è½½Alluxio FSåˆ°æœ¬åœ°ï¼‰ æˆ‘ä¹Ÿåšäº†å¾ˆå¤šAllxuioçš„æ€§èƒ½æµ‹è¯•å·¥ä½œ,æ•ˆæœéƒ½ä¸æ˜¯å¾ˆç†æƒ³,æœ‰å¹¸ä¸Alluxio PMCèŒƒæ–Œå’Œææµ©æºäº¤æµäº†æµ‹è¯•ç»“æœä¸å¦‚äººæ„çš„åŸå› ,å¤§ä½¬æ˜¯è¿™ä¹ˆè¯´çš„:â€å¦‚æœHDFSæœ¬èº«å·²ç»å’ŒSparkå’ŒHiveå…±ç½®äº†ï¼Œé‚£ä¹ˆè¿™ä¸ªåœºæ™¯å¹¶ä¸ç®—Alluxioçš„ç›®æ ‡åœºæ™¯ã€‚è®¡ç®—å’Œå­˜å‚¨åˆ†ç¦»çš„æƒ…å†µä¸‹æ‰ä¼šæœ‰æ˜æ˜¾æ•ˆæœï¼Œå¦åˆ™é€šå¸¸æ˜¯HDFSå·²ç»æˆä¸ºç“¶é¢ˆæ—¶æ‰ä¼šæœ‰å¸®åŠ©ã€‚â€œè¿˜æœ‰,å¦‚æœHDFSéƒ¨ç½²åœ¨è®¡ç®—æ¡†æ¶æœ¬åœ°,ä½œä¸šçš„è¾“å…¥æ•°æ®å¯èƒ½ä¼šå­˜åœ¨äºç³»ç»Ÿçš„é«˜é€Ÿç¼“å­˜åŒº,åˆ™Alluxioå¯¹æ•°æ®åŠ é€Ÿä¹Ÿå¹¶ä¸æ˜æ˜¾ã€‚æ‰€ä»¥:åº”ç”¨åœºæ™¯å¾ˆå…³é”®,æ–°æŠ€æœ¯äº§ç”Ÿæ—¶,ä¸€å®šè¦äº†è§£å…¶åº”ç”¨åœºæ™¯å’ŒåŸç†å¹¶ç»è¿‡è€ƒè™‘ä¹‹åå†åšä¸€äº›æ€§èƒ½æµ‹è¯•ä¹‹ç±»çš„åç»­å·¥ä½œ!å®˜æ–¹ä»‹ç»çš„Alluxioåº”ç”¨åœºæ™¯ AlluxioåŸç†å¦‚å›¾ï¼Œä¸€ä¸ªå®Œæ•´çš„Alluxioé›†ç¾¤éƒ¨ç½²åœ¨é€»è¾‘ä¸ŠåŒ…æ‹¬masterã€workerã€clientåŠåº•å±‚å­˜å‚¨(UFS)ã€‚masterå’Œworkerè¿›ç¨‹é€šå¸¸ç”±é›†ç¾¤ç®¡ç†å‘˜ç»´æŠ¤å’Œç®¡ç†ï¼Œå®ƒä»¬é€šè¿‡RPCé€šä¿¡ç›¸äº’åä½œï¼Œä»è€Œæ„æˆäº†AlluxioæœåŠ¡ç«¯ã€‚è€Œåº”ç”¨ç¨‹åºåˆ™é€šè¿‡Alluxio Clientæ¥å’ŒAlluxioæœåŠ¡äº¤äº’ï¼Œè¯»å†™æ•°æ®æˆ–æ“ä½œæ–‡ä»¶ã€ç›®å½•ã€‚ Alluxioæ ¸å¿ƒç»„ä»¶Alluxioä½¿ç”¨äº†å•Masterå’Œå¤šWorkerçš„æ¶æ„,Masterå’ŒWorkerä¸€èµ·ç»„æˆäº†Alluxioçš„æœåŠ¡ç«¯ï¼Œå®ƒä»¬æ˜¯ç³»ç»Ÿç®¡ç†å‘˜ç»´æŠ¤å’Œç®¡ç†çš„ç»„ä»¶,Clienté€šå¸¸æ˜¯åº”ç”¨ç¨‹åºï¼Œå¦‚Sparkæˆ–MapReduceä½œä¸šï¼Œæˆ–è€…Alluxioçš„å‘½ä»¤è¡Œç”¨æˆ·ã€‚Alluxioç”¨æˆ·ä¸€èˆ¬åªä¸Alluxioçš„Clientç»„ä»¶è¿›è¡Œäº¤äº’ã€‚ Master: è´Ÿè´£ç®¡ç†æ•´ä¸ªé›†ç¾¤çš„å…¨å±€å…ƒæ•°æ®å¹¶å“åº”Clientå¯¹æ–‡ä»¶ç³»ç»Ÿçš„è¯·æ±‚ã€‚åœ¨Alluxioæ–‡ä»¶ç³»ç»Ÿå†…éƒ¨ï¼Œæ¯ä¸€ä¸ªæ–‡ä»¶è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®å—(block)ï¼Œå¹¶ä»¥æ•°æ®å—ä¸ºå•ä½å­˜å‚¨åœ¨Workerä¸­ã€‚MasterèŠ‚ç‚¹è´Ÿè´£ç®¡ç†æ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®(å¦‚æ–‡ä»¶ç³»ç»Ÿçš„inodeæ ‘ã€æ–‡ä»¶åˆ°æ•°æ®å—çš„æ˜ å°„)ã€æ•°æ®å—çš„å…ƒæ•°æ®(å¦‚blockåˆ°Workerçš„ä½ç½®æ˜ å°„)ï¼Œä»¥åŠWorkerå…ƒæ•°æ®(å¦‚é›†ç¾¤å½“ä¸­æ¯ä¸ªWorkerçš„çŠ¶æ€)ã€‚æ‰€æœ‰Workerå®šæœŸå‘Masterå‘é€å¿ƒè·³æ¶ˆæ¯æ±‡æŠ¥è‡ªå·±çŠ¶æ€ï¼Œä»¥ç»´æŒå‚ä¸æœåŠ¡çš„èµ„æ ¼ã€‚Masteré€šå¸¸ä¸ä¸»åŠ¨ä¸å…¶ä»–ç»„ä»¶é€šä¿¡ï¼Œåªé€šè¿‡RPCæœåŠ¡è¢«åŠ¨å“åº”è¯·æ±‚ï¼ŒåŒæ—¶Masterè¿˜è´Ÿè´£å®æ—¶è®°å½•æ–‡ä»¶ç³»ç»Ÿçš„æ—¥å¿—(Journal)ï¼Œä»¥ä¿è¯é›†ç¾¤é‡å¯ä¹‹åå¯ä»¥å‡†ç¡®æ¢å¤æ–‡ä»¶ç³»ç»Ÿçš„çŠ¶æ€ã€‚Masteråˆ†ä¸ºPrimary Masterå’ŒSecondary Masterï¼ŒSecondary Masteréœ€è¦å°†æ–‡ä»¶ç³»ç»Ÿæ—¥å¿—å†™å…¥æŒä¹…åŒ–å­˜å‚¨ï¼Œä»è€Œå®ç°åœ¨å¤šMasterï¼ˆHAæ¨¡å¼ä¸‹ï¼‰é—´å…±äº«æ—¥å¿—ï¼Œå®ç°Masterä¸»ä»åˆ‡æ¢æ—¶å¯ä»¥æ¢å¤Masterçš„çŠ¶æ€ä¿¡æ¯ã€‚Alluxioé›†ç¾¤ä¸­å¯ä»¥æœ‰å¤šä¸ªSecondary Masterï¼Œæ¯ä¸ªSecondary Masterå®šæœŸå‹ç¼©æ–‡ä»¶ç³»ç»Ÿæ—¥å¿—å¹¶ç”ŸæˆCheckpointä»¥ä¾¿å¿«é€Ÿæ¢å¤ï¼Œå¹¶åœ¨åˆ‡æ¢æˆPrimary Masteræ—¶è¯»å–ä¹‹å‰Primary Masterå†™å…¥çš„æ—¥å¿—ã€‚Secondary Masterä¸å¤„ç†ä»»ä½•Alluxioç»„ä»¶çš„ä»»ä½•è¯·æ±‚ã€‚ Worker: Alluxio Masteråªè´Ÿè´£å“åº”Clientå¯¹æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®çš„æ“ä½œï¼Œè€Œå…·ä½“æ–‡ä»¶æ•°æ®ä¼ è¾“çš„ä»»åŠ¡ç”±Workerè´Ÿè´£ï¼Œå¦‚å›¾ï¼Œæ¯ä¸ªWorkerè´Ÿè´£ç®¡ç†åˆ†é…ç»™Alluxioçš„æœ¬åœ°å­˜å‚¨èµ„æº(å¦‚RAM,SSD,HDD),è®°å½•æ‰€æœ‰è¢«ç®¡ç†çš„æ•°æ®å—çš„å…ƒæ•°æ®ï¼Œå¹¶æ ¹æ®Clientå¯¹æ•°æ®å—çš„è¯»å†™è¯·æ±‚åšå‡ºå“åº”ã€‚Workerä¼šæŠŠæ–°çš„æ•°æ®å­˜å‚¨åœ¨æœ¬åœ°å­˜å‚¨ï¼Œå¹¶å“åº”æœªæ¥çš„Clientè¯»è¯·æ±‚ï¼ŒClientæœªå‘½ä¸­æœ¬åœ°èµ„æºæ—¶ä¹Ÿå¯èƒ½ä»åº•å±‚æŒä¹…åŒ–å­˜å‚¨ç³»ç»Ÿä¸­è¯»æ•°æ®å¹¶ç¼“å­˜è‡³Workeræœ¬åœ°ã€‚Workerä»£æ›¿Clientåœ¨æŒä¹…åŒ–å­˜å‚¨ä¸Šæ“ä½œæ•°æ®æœ‰ä¸¤ä¸ªå¥½å¤„:1.åº•å±‚è¯»å–çš„æ•°æ®å¯ç›´æ¥å­˜å‚¨åœ¨Workerä¸­ï¼Œå¯ç«‹å³ä¾›å…¶ä»–Clientä½¿ç”¨ 2.Alluxio Workerçš„å­˜åœ¨è®©Clientä¸ä¾èµ–åº•å±‚å­˜å‚¨çš„è¿æ¥å™¨ï¼Œæ›´åŠ è½»é‡åŒ–ã€‚Alluxioé‡‡å–å¯é…ç½®çš„ç¼“å­˜ç­–ç•¥ï¼ŒWorkerç©ºé—´æ»¡äº†çš„æ—¶å€™æ·»åŠ æ–°æ•°æ®å—éœ€è¦æ›¿æ¢å·²æœ‰æ•°æ®å—ï¼Œç¼“å­˜ç­–ç•¥æ¥å†³å®šä¿ç•™å“ªäº›æ•°æ®å—ã€‚ Client: å…è®¸åˆ†æå’ŒAI/MLåº”ç”¨ç¨‹åºä¸Alluxioè¿æ¥å’Œäº¤äº’ï¼Œå®ƒå‘èµ·ä¸Masterçš„é€šä¿¡ï¼Œæ‰§è¡Œå…ƒæ•°æ®æ“ä½œï¼Œå¹¶ä»Workerè¯»å–å’Œå†™å…¥å­˜å‚¨åœ¨Alluxioä¸­çš„æ•°æ®ã€‚å®ƒæä¾›äº†Javaçš„æœ¬æœºæ–‡ä»¶ç³»ç»ŸAPIï¼Œæ”¯æŒå¤šç§å®¢æˆ·ç«¯è¯­è¨€åŒ…æ‹¬RESTï¼ŒGoï¼ŒPythonç­‰ï¼Œè€Œä¸”è¿˜å…¼å®¹HDFSå’ŒAmazon S3çš„APIã€‚å¯ä»¥æŠŠClientç†è§£ä¸ºä¸€ä¸ªåº“ï¼Œå®ƒå®ç°äº†æ–‡ä»¶ç³»ç»Ÿçš„æ¥å£ï¼Œæ ¹æ®ç”¨æˆ·è¯·æ±‚è°ƒç”¨AlluxioæœåŠ¡ï¼Œå®¢æˆ·ç«¯è¢«ç¼–è¯‘ä¸ºalluxio-2.0.1-client.jaræ–‡ä»¶ï¼Œå®ƒåº”å½“ä½äºJVMç±»è·¯å¾„ä¸Šï¼Œæ‰èƒ½æ­£å¸¸è¿è¡Œã€‚å½“Clientå’ŒWorkeråœ¨åŒä¸€èŠ‚ç‚¹æ—¶ï¼Œå®¢æˆ·ç«¯å¯¹æœ¬åœ°ç¼“å­˜æ•°æ®çš„è¯»å†™è¯·æ±‚å¯ä»¥ç»•è¿‡RPCæ¥å£ï¼Œä½¿æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå¯ä»¥ç›´æ¥è®¿é—®Workeræ‰€ç®¡ç†çš„æ•°æ®ï¼Œè¿™ç§æƒ…å†µè¢«ç§°ä¸ºçŸ­è·¯å†™ï¼Œé€Ÿåº¦æ¯”è¾ƒå¿«ï¼Œå¦‚æœè¯¥èŠ‚ç‚¹æ²¡æœ‰Workeråœ¨è¿è¡Œï¼Œåˆ™Clientçš„è¯»å†™éœ€è¦é€šè¿‡ç½‘ç»œè®¿é—®å…¶ä»–èŠ‚ç‚¹ä¸Šçš„Workerï¼Œé€Ÿåº¦å—ç½‘ç»œå®½å¸¦çš„é™åˆ¶ã€‚ Alluxioè¯»å†™åœºæ™¯ä¸å‚æ•° Alluxioè¯»åœºæ™¯ä¸æ€§èƒ½åˆ†æ: å‘½ä¸­æœ¬åœ°Worker Clientå‘Masteræ£€ç´¢å­˜å‚¨è¯¥æ•°æ®çš„Workerä½ç½® å¦‚æœæœ¬åœ°å­˜æœ‰è¯¥æ•°æ®ï¼Œåˆ™â€çŸ­è·¯è¯»â€,é¿å…ç½‘ç»œä¼ è¾“ çŸ­è·¯è¯»æä¾›å†…å­˜çº§åˆ«è®¿é—®é€Ÿåº¦ï¼Œæ˜¯Alluxioæœ€é«˜è¯»æ€§èƒ½çš„æ–¹å¼ å‘½ä¸­è¿œç¨‹Worker Clientè¯·æ±‚çš„æ•°æ®ä¸åœ¨æœ¬åœ°Workeråˆ™Clientå°†ä»è¿œç¨‹Workerè¯»å–æ•°æ® è¿œç¨‹Workerå°†æ•°æ®è¿”å›æœ¬åœ°Workerå¹¶å†™ä¸€ä¸ªæœ¬åœ°å‰¯æœ¬ï¼Œè¯·æ±‚é¢‘ç¹çš„æ•°æ®ä¼šæœ‰æ›´å¤šå‰¯æœ¬ï¼Œä»è€Œå®ç°çƒ­åº¦ä¼˜åŒ–è®¡ç®—çš„æœ¬åœ°æ€§ï¼Œä¹Ÿå¯é€‰NO_CACHEè¯»å–æ–¹å¼ç¦ç”¨æœ¬åœ°å‰¯æœ¬å†™å…¥ è¿œç¨‹ç¼“å­˜å‘½ä¸­ï¼Œè¯»å–é€Ÿåº¦å—ç½‘ç»œé€Ÿåº¦é™åˆ¶ æœªå‘½ä¸­Worker Alluxioä»»ä½•ä¸€ä¸ªWorkeræ²¡æœ‰ç¼“å­˜æ‰€éœ€æ•°æ®ï¼Œåˆ™ClientæŠŠè¯·æ±‚å§”æ‰˜ç»™æœ¬åœ°Workerä»åº•å±‚å­˜å‚¨ç³»ç»Ÿ(UFS)è¯»å–ï¼Œç¼“å­˜æœªå‘½ä¸­çš„æƒ…å†µä¸‹å»¶è¿Ÿè¾ƒé«˜ Alluxio 1.7å‰Workerä»åº•å±‚è¯»å–å®Œæ•´æ•°æ®å—ç¼“å­˜ä¸‹æ¥å¹¶è¿”å›ç»™Clientï¼Œ1.7ç‰ˆæœ¬åæ”¯æŒå¼‚æ­¥ç¼“å­˜ï¼ŒClientè¯»å–ï¼ŒWorkerç¼“å­˜ï¼Œä¸éœ€è¦ç­‰å¾…ç¼“å­˜å®Œæˆå³å¯è¿”å›ç»“æœ æŒ‡å®šNO_CACHEè¯»å–æ–¹å¼åˆ™ç¦ç”¨æœ¬åœ°ç¼“å­˜ Alluxioå†™åœºæ™¯ä¸æ€§èƒ½åˆ†æ: ä»…å†™ç¼“å­˜ å†™å…¥ç±»å‹é€šè¿‡alluxio.user.file.writetype.defaultæ¥è®¾ç½®ï¼ŒMUST_CACHEä»…å†™æœ¬åœ°ç¼“å­˜è€Œä¸å†™å…¥UFS å¦‚æœâ€çŸ­è·¯å†™â€å¯ç”¨ï¼Œåˆ™ç›´æ¥å†™æœ¬åœ°Workeré¿å…ç½‘ç»œä¼ è¾“ï¼Œæ€§èƒ½æœ€é«˜ å¦‚æœæ— æœ¬åœ°Workerï¼Œå³â€çŸ­è·¯å†™â€ä¸å¯ç”¨ï¼Œæ•°æ®å†™å…¥è¿œç«¯Workerï¼Œå†™é€Ÿåº¦å—é™äºç½‘ç»œIO æ•°æ®æ²¡æœ‰æŒä¹…åŒ–ï¼Œæœºå™¨å´©æºƒæˆ–éœ€è¦é‡Šæ”¾æ•°æ®ç”¨äºè¾ƒæ–°çš„å†™å…¥æ—¶ï¼Œæ•°æ®å¯èƒ½ä¸¢å¤± åŒæ­¥å†™ç¼“å­˜å’ŒæŒä¹…åŒ–å­˜å‚¨ alluxio.user.file.writetype.default=CACHE_THROUGHï¼ŒåŒæ­¥å†™å…¥Workerå’ŒUFS é€Ÿåº¦æ¯”ä»…å†™ç¼“å­˜çš„æ–¹å¼æ…¢å¾ˆå¤šï¼Œéœ€è¦æ•°æ®æŒä¹…åŒ–æ—¶ä½¿ç”¨ ä»…å†™æŒä¹…åŒ–å­˜å‚¨ alluxio.user.file.writetype.default=THROUGHï¼Œåªå°†æ•°æ®å†™å…¥UFSï¼Œä¸ä¼šåˆ›å»ºAlluxioç¼“å­˜ä¸­çš„å‰¯æœ¬ è¾“å…¥æ•°æ®é‡è¦ä½†ä¸ç«‹åˆ»ä½¿ç”¨çš„æƒ…å†µä¸‹ä½¿ç”¨è¯¥æ–¹å¼ å¼‚æ­¥å†™æŒä¹…åŒ–å­˜å‚¨(ç›®å‰2.0.1ä¸ºå®éªŒæ€§) alluxio.user.file.writetype.default=ASYNC_THROUGH å¯ä»¥ä»¥å†…å­˜çš„é€Ÿåº¦å†™å…¥Alluxio Workerï¼Œå¹¶å¼‚æ­¥å®ŒæˆæŒä¹…åŒ– å®éªŒæ€§åŠŸèƒ½-å¦‚æœå¼‚æ­¥æŒä¹…åŒ–åˆ°åº•å±‚å­˜å‚¨å‰æœºå™¨å´©æºƒï¼Œæ•°æ®ä¸¢å¤±ï¼Œå¼‚æ­¥å†™æœºåˆ¶è¦æ±‚æ–‡ä»¶æ‰€æœ‰å—éƒ½åœ¨åŒä¸€ä¸ªWorkerä¸­ Alluxioè¯»å†™å‚æ•°æ€»ç»“ å†™å‚æ•°: alluxio.user.file.writetype.default CACHE_THROUGH:æ•°æ®è¢«åŒæ­¥å†™å…¥AlluxioWorkerå’Œåº•å±‚å­˜å‚¨ MUST_CACHE:æ•°æ®è¢«åŒæ­¥å†™å…¥AlluxioWorker,ä¸å†™åº•å±‚å­˜å‚¨ THROUGH:æ•°æ®åªå†™åº•å±‚å­˜å‚¨,ä¸å†™å…¥AlluxioWorker ASYNC_THROUGH:æ•°æ®åŒæ­¥å†™å…¥AlluxioWorkerå¹¶å¼‚æ­¥å†™åº•å±‚å­˜å‚¨(é€Ÿåº¦å¿«) è¯»å‚æ•°: alluxio.user.file.readtype.default CACHE_PROMOTE:æ•°æ®åœ¨Workerä¸Š,åˆ™è¢«ç§»åŠ¨åˆ°Workerçš„æœ€é«˜å±‚,å¦åˆ™åˆ›å»ºå‰¯æœ¬åˆ°æœ¬åœ°Worker CACHE:æ•°æ®ä¸åœ¨æœ¬åœ°Workerä¸­æ—¶ç›´æ¥åˆ›å»ºå‰¯æœ¬åˆ°æœ¬åœ°Worker NO_CACHE:ä»…è¯»æ•°æ®,ä¸å†™å‰¯æœ¬åˆ°Worker æ˜¯å¦ç¼“å­˜å…¨éƒ¨æ•°æ®å—: alluxio.user.file.cache.partially.read.block (v1.7ä»¥å‰,V1.7ä»¥åé‡‡å–å¼‚æ­¥ç¼“å­˜ç­–ç•¥) falseè¯»å¤šå°‘ç¼“å­˜å¤šå°‘,ä¸€ä¸ªæ•°æ®å—åªæœ‰å®Œå…¨è¢«è¯»å–æ—¶ï¼Œæ‰èƒ½è¢«ç¼“å­˜ trueè¯»éƒ¨åˆ†ç¼“å­˜å…¨éƒ¨,æ²¡æœ‰å®Œå…¨è¯»å–çš„æ•°æ®å—ä¹Ÿä¼šè¢«å…¨éƒ¨å­˜åˆ°Alluxioå†… Workerå†™æ–‡ä»¶æ•°æ®å—çš„æ•°æ®åˆ†å¸ƒç­–ç•¥: alluxio.user.block.write.location.policy.class LocalFirstPolicy (alluxio.client.block.policy.LocalFirstPolicy) é»˜è®¤å€¼,é¦–å…ˆè¿”å›æœ¬åœ°ä¸»æœºï¼Œå¦‚æœæœ¬åœ°workeræ²¡æœ‰è¶³å¤Ÿçš„å—å®¹é‡ï¼Œå®ƒä»æ´»åŠ¨workeråˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€åworkerã€‚ MostAvailableFirstPolicy (alluxio.client.block.policy.MostAvailableFirstPolicy) è¿”å›å…·æœ‰æœ€å¤šå¯ç”¨å­—èŠ‚çš„workerã€‚ RoundRobinPolicy (alluxio.client.block.policy.RoundRobinPolicy) ä»¥å¾ªç¯æ–¹å¼é€‰æ‹©ä¸‹ä¸€ä¸ªworkerï¼Œè·³è¿‡æ²¡æœ‰è¶³å¤Ÿå®¹é‡çš„workerã€‚ SpecificHostPolicy (alluxio.client.block.policy.SpecificHostPolicy) è¿”å›å…·æœ‰æŒ‡å®šä¸»æœºåçš„workerã€‚æ­¤ç­–ç•¥ä¸èƒ½è®¾ç½®ä¸ºé»˜è®¤ç­–ç•¥ã€‚ ç›®å‰æœ‰å…­ç§ç­–ç•¥,è¯¦è§é…ç½®é¡¹åˆ—è¡¨Alluxioçš„åˆ†å±‚å­˜å‚¨æ¦‚å¿µ: Alluxio workersèŠ‚ç‚¹ä½¿ç”¨åŒ…æ‹¬å†…å­˜åœ¨å†…çš„æœ¬åœ°å­˜å‚¨æ¥å……å½“åˆ†å¸ƒå¼ç¼“å†²ç¼“å­˜åŒº,å¯ä»¥å¾ˆå¤§ç¨‹åº¦ä¸Šæ”¹å–„I/Oæ€§èƒ½ã€‚æ¯ä¸ªAlluxioèŠ‚ç‚¹ç®¡ç†çš„å­˜å‚¨æ•°é‡å’Œç±»å‹ç”±ç”¨æˆ·é…ç½®,Alluxioè¿˜æ”¯æŒå±‚æ¬¡åŒ–å­˜å‚¨,è®©æ•°æ®å­˜å‚¨è·å¾—ç±»ä¼¼äºL1/L2 cpuç¼“å­˜çš„ä¼˜åŒ–ã€‚å•å±‚å­˜å‚¨è®¾ç½®(æ¨è): é»˜è®¤ä½¿ç”¨ä¸¤ä¸ªå‚æ•°alluxio.worker.memory.size=16GB + alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdiskæ¥è®¾ç½®Alluxio Workerçš„ç¼“å­˜å¤§å° ä¹Ÿå¯ä»¥å•å±‚å¤šä¸ªå­˜å‚¨ä»‹è´¨å¹¶æŒ‡å®šæ¯ä¸ªä»‹è´¨å¯ç”¨ç©ºé—´å¤§å°alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd + alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB alluxio.worker.memory.sizeå’Œalluxio.worker.tieredstore.level0.dirs.quotaçš„åŒºåˆ«-&gt;ramdiskçš„å¤§å°é»˜è®¤ç”±å‰è€…å†³å®š,åè€…å¯ä»¥å†³å®šé™¤å†…å­˜å¤–çš„å…¶ä»–ä»‹è´¨å¦‚ssdå’Œhddçš„å¤§å° å¤šå±‚å­˜å‚¨è®¾ç½®: å¤šå±‚å­˜å‚¨çš„é…ç½®-ä½¿ç”¨ä¸¤å±‚å­˜å‚¨MEMå’ŒHDDalluxio.worker.tieredstore.levels=2 # æœ€å¤§å­˜å‚¨çº§æ•° åœ¨Alluxioä¸­é…ç½®äº†ä¸¤çº§å­˜å‚¨ alluxio.worker.tieredstore.level0.alias=MEM # alluxio.worker.tieredstore.level0.alias=MEM é…ç½®äº†é¦–å±‚(é¡¶å±‚)æ˜¯å†…å­˜å­˜å‚¨å±‚ alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # è®¾ç½®äº†ramdiskçš„é…é¢æ˜¯100GB alluxio.worker.tieredstore.level0.dirs.quota=100GB alluxio.worker.tieredstore.level0.watermark.high.ratio=0.9 # å›æ”¶ç­–ç•¥çš„é«˜æ°´ä½ alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 alluxio.worker.tieredstore.level1.alias=HDD # é…ç½®äº†ç¬¬äºŒå±‚æ˜¯ç¡¬ç›˜å±‚ alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3 # å®šä¹‰äº†ç¬¬äºŒå±‚3ä¸ªæ–‡ä»¶è·¯å¾„å„è‡ªçš„é…é¢ alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB alluxio.worker.tieredstore.level1.watermark.high.ratio=0.9 alluxio.worker.tieredstore.level1.watermark.low.ratio=0.7 å†™æ•°æ®é»˜è®¤å†™å…¥é¡¶å±‚å­˜å‚¨,ä¹Ÿå¯ä»¥æŒ‡å®šå†™æ•°æ®çš„é»˜è®¤å±‚çº§ alluxio.user.file.write.tier.default é»˜è®¤0æœ€é¡¶å±‚,1è¡¨ç¤ºç¬¬äºŒå±‚,-1å€’æ•°ç¬¬ä¸€å±‚ Alluxioæ”¶åˆ°å†™è¯·æ±‚,ç›´æ¥æŠŠæ•°æ®å†™å…¥æœ‰è¶³å¤Ÿç¼“å­˜çš„å±‚,å¦‚æœç¼“å­˜å…¨æ»¡,åˆ™ç½®æ¢æ‰åº•å±‚çš„ä¸€ä¸ªBlock. Alluxioç¼“å­˜å›æ”¶ç­–ç•¥ç¼“å­˜å›æ”¶: Alluxioä¸­çš„æ•°æ®æ˜¯åŠ¨æ€å˜åŒ–çš„,å­˜å‚¨ç©ºé—´ä¸è¶³æ—¶ä¼šä¸ºæ–°æ•°æ®è…¾å‡ºç©ºé—´ å¼‚æ­¥ç¼“å­˜å›æ”¶ä¸åŒæ­¥ç¼“å­˜å›æ”¶ alluxio.worker.tieredstore.reserver.enabled=true (é»˜è®¤å¼‚æ­¥å›æ”¶) åœ¨è¯»å†™ç¼“å­˜å·¥ä½œè´Ÿè½½è¾ƒé«˜çš„æƒ…å†µä¸‹å¼‚æ­¥å›æ”¶å¯ä»¥æå‡æ€§èƒ½ alluxio.worker.tieredstore.reserver.enabled=false (åŒæ­¥å›æ”¶) è¯·æ±‚æ‰€ç”¨ç©ºé—´æ¯”Workerä¸Šè¯·æ±‚ç©ºé—´æ›´å¤šæ—¶,åŒæ­¥å›æ”¶å¯ä»¥æœ€å¤§åŒ–Alluxioç©ºé—´åˆ©ç”¨ç‡,åŒæ­¥å›æ”¶å»ºè®®ä½¿ç”¨å°æ•°æ®å—é…ç½®(64-128MB)æ¥é™ä½å›æ”¶å»¶è¿Ÿ ç¼“å­˜å›æ”¶ä¸­ç©ºé—´é¢„ç•™å™¨çš„æ°´ä½(é˜ˆå€¼) Workerå­˜å‚¨åˆ©ç”¨ç‡è¾¾åˆ°é«˜æ°´ä½æ—¶,åŸºäºå›æ”¶ç­–ç•¥å›æ”¶Workerç¼“å­˜ç›´åˆ°è¾¾åˆ°é…ç½®çš„ä½æ°´ä½ é«˜æ°´ä½: alluxio.worker.tieredstore.level0.watermark.high.ratio=0.95 (é»˜è®¤95%) ä½æ°´ä½: alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7 (é»˜è®¤70%) æ¯”å¦‚é…ç½®äº†32GB(MEM)+100GB(SSD)=132GBçš„Workerå†…å­˜,å½“å†…å­˜è¾¾åˆ°é«˜æ°´ä½132x0.95=125.4GBæ—¶å¼€å§‹å›æ”¶ç¼“å­˜,ç›´åˆ°åˆ°è¾¾ä½æ°´ä½132x0.7=92.4GBæ—¶åœæ­¢å›æ”¶ç¼“å­˜ è‡ªå®šä¹‰å›æ”¶ç­–ç•¥ alluxio.worker.allocator.class=alluxio.worker.block.allocator.MaxFreeAllocator (Alluxioä¸­æ–°æ•°æ®å—åˆ†é…ç­–ç•¥çš„ç±»å) alluxio.worker.evictor.class=alluxio.worker.block.evictor.LRUEvictor (å½“å­˜å‚¨å±‚ç©ºé—´ç”¨å°½æ—¶å—å›æ”¶ç­–ç•¥çš„ç±»å) è´ªå¿ƒå›æ”¶ç­–ç•¥: å›æ”¶ä»»æ„æ•°æ®å—ç›´åˆ°é‡Šæ”¾å‡ºæ‰€éœ€ç©ºé—´ LRUå›æ”¶ç­–ç•¥: å›æ”¶æœ€è¿‘æœ€å°‘ä½¿ç”¨æ•°æ®å—ç›´åˆ°é‡Šæ”¾å‡ºæ‰€éœ€ç©ºé—´ éƒ¨åˆ†LRUå›æ”¶ç­–ç•¥: åœ¨æœ€å¤§å‰©ä½™ç©ºé—´çš„ç›®å½•å›æ”¶æœ€è¿‘æœ€å°‘ä½¿ç”¨æ•°æ®å— LRFUå›æ”¶ç­–ç•¥: åŸºäºæƒé‡åˆ†é…çš„æœ€è¿‘æœ€å°‘ä½¿ç”¨å’Œæœ€ä¸ç»å¸¸ä½¿ç”¨ç­–ç•¥å›æ”¶æ•°æ®å—,å¦‚æœæƒé‡å®Œå…¨åå‘æœ€è¿‘æœ€å°‘ä½¿ç”¨,åˆ™LRFUå˜ä¸ºLRU Alluxioå¼‚æ­¥ç¼“å­˜ç­–ç•¥ Alluxio v1.7ä»¥åæ”¯æŒå¼‚æ­¥ç¼“å­˜ å¼‚æ­¥ç¼“å­˜æ˜¯å°†Alluxioçš„ç¼“å­˜å¼€é”€ç”±å®¢æˆ·ç«¯è½¬ç§»åˆ°Workerä¸Š,ç¬¬ä¸€æ¬¡è¯»æ•°æ®æ—¶,åœ¨ä¸è®¾ç½®è¯»å±æ€§ä¸ºNO_CACHEçš„æƒ…å†µä¸‹Clientåªè´Ÿè´£ä»åº•å±‚å­˜å‚¨è¯»æ•°æ®,ç„¶åç¼“å­˜ä»»åŠ¡ç”±Workeræ¥æ‰§è¡Œ,å¯¹Clientè¯»æ€§èƒ½æ²¡æœ‰å½±å“,ä¹Ÿä¸éœ€è¦åƒV1.7ç‰ˆæœ¬å‰é‚£æ ·è®¾ç½®alluxio.user.file.cache.partially.read.blockæ¥å†³å®šç¼“å­˜éƒ¨åˆ†æˆ–å…¨éƒ¨æ•°æ®,è€Œä¸”Workerå†…éƒ¨ä¹Ÿåœ¨Clientè¯»å–åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®æ–¹é¢åšäº†ä¼˜åŒ–,è®¾ç½®è¯»å±æ€§ä¸ºCACHEçš„æƒ…å†µä¸‹: Clienté¡ºåºè¯»å®Œæ•´æ•°æ®å—æ—¶Workeré¡ºä¾¿ç¼“å­˜å®Œæ•´æ•°æ®å— Clientåªè¯»éƒ¨åˆ†æ•°æ®æˆ–éé¡ºåºè¯»æ•°æ®æ—¶Workerä¸ä¼šè¯»å–æ—¶é¡ºä¾¿ç¼“å­˜,ç­‰å®¢æˆ·ç«¯è¯»å–å®Œä»¥åå†å‘Workerç³»æ¬¸ç‚¹å‘é€å¼‚æ­¥ç¼“å­˜å‘½ä»¤,WorkerèŠ‚ç‚¹å†ä»åº•å±‚å­˜å‚¨ä¸­è·å–å®Œæ•´çš„å— å¼‚æ­¥ç¼“å­˜ä½¿å¾—ç¬¬ä¸€æ¬¡ä»Alluxioè¯»å–å’Œç›´æ¥ä»åº•å±‚å­˜å‚¨è¯»å–èŠ±è´¹ç›¸åŒæ—¶é—´,ä¸”æ•°æ®å¼‚æ­¥ç¼“å­˜åˆ°Alluxioä¸­,æé«˜é›†ç¾¤æ•´ä½“æ€§èƒ½ å¼‚æ­¥ç¼“å­˜å‚æ•°è°ƒæ•´ Workeråœ¨å¼‚æ­¥ç¼“å­˜çš„åŒæ—¶ä¹Ÿå“åº”Clientè¯»å–è¯·æ±‚,å¯é€šè¿‡è®¾ç½®Workerç«¯çš„çº¿ç¨‹æ± å¤§å°æ¥åŠ å¿«å¼‚æ­¥ç¼“å­˜çš„é€Ÿåº¦ alluxio.worker.network.netty.async.cache.manager.threads.max æŒ‡å®šWorkerçº¿ç¨‹æ± å¤§å°,è¯¥å±æ€§é»˜è®¤ä¸º8,è¡¨ç¤ºæœ€å¤šåŒæ—¶ç”¨å…«æ ¸ä»å…¶ä»–Workeræˆ–åº•å±‚å­˜å‚¨è¯»æ•°æ®å¹¶ç¼“å­˜,æé«˜æ­¤å€¼å¯ä»¥åŠ å¿«åå°å¼‚æ­¥ç¼“å­˜çš„é€Ÿåº¦,ä½†ä¼šå¢åŠ CPUä½¿ç”¨ç‡ Alluxioå…ƒæ•°æ® Alluxioå…ƒæ•°æ®çš„å­˜å‚¨åœ¨Alluxioæ–°çš„2.xç‰ˆæœ¬ä¸­ï¼Œå¯¹å…ƒæ•°æ®å­˜å‚¨åšäº†ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½åº”å¯¹æ•°ä»¥äº¿çº§çš„å…ƒæ•°æ®å­˜å‚¨ã€‚é¦–å…ˆï¼Œæ–‡ä»¶ç³»ç»Ÿæ˜¯INode-Treeç»„æˆçš„ï¼Œå³æ–‡ä»¶ç›®å½•æ ‘ï¼ŒAlluxio Masterç®¡ç†å¤šä¸ªåº•å±‚å­˜å‚¨ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼Œæ¯ä¸ªæ–‡ä»¶ç›®å½•éƒ½æ˜¯INode-Treeçš„èŠ‚ç‚¹ï¼Œåœ¨Javaå¯¹è±¡ä¸­ï¼Œå¯èƒ½ä¸€ä¸ªç›®å½•ä¿¡æ¯æœ¬èº«å ç”¨ç©ºé—´ä¸å¤§ï¼Œä½†æ˜ å°„åœ¨JavaHeapå†…å­˜ä¸­ï¼Œç®—ä¸Šé™„åŠ ä¿¡æ¯ï¼Œæ¯ä¸ªæ–‡ä»¶å¤§æ¦‚è¦æœ‰1KBå·¦å³çš„å…ƒæ•°æ®ï¼Œå¦‚æœæœ‰åäº¿ä¸ªæ–‡ä»¶å’Œè·¯å¾„ï¼Œåˆ™è¦æœ‰çº¦1TBçš„å †å†…å­˜æ¥å­˜å‚¨å…ƒæ•°æ®ï¼Œå®Œå…¨æ˜¯ä¸ç°å®çš„ã€‚æ‰€ä»¥ï¼Œä¸ºäº†æ–¹ä¾¿ç®¡ç†å…ƒæ•°æ®ï¼Œå‡å°å› ä¸ºå…ƒæ•°æ®è¿‡å¤šå¯¹Masteræ€§èƒ½é€ æˆçš„å½±å“ï¼ŒAlluxioçš„å…ƒæ•°æ®é€šè¿‡RocksDBé”®å€¼æ•°æ®åº“æ¥ç®¡ç†å…ƒæ•°æ®ï¼ŒMasterä¼šCacheå¸¸ç”¨æ•°æ®çš„å…ƒæ•°æ®ï¼Œè€Œå¤§éƒ¨åˆ†å…ƒæ•°æ®åˆ™å­˜åœ¨RocksDBä¸­ï¼Œè¿™æ ·å¤§å¤§å‡å°äº†Master Heapçš„å‹åŠ›ï¼Œé™ä½OOMå¯èƒ½æ€§ï¼Œä½¿Alluxioå¯ä»¥åŒæ—¶ç®¡ç†å¤šä¸ªå­˜å‚¨ç³»ç»Ÿçš„å…ƒæ•°æ®ã€‚é€šè¿‡RocksDBçš„è¡Œé”ï¼Œä¹Ÿå¯ä»¥æ–¹ä¾¿é«˜å¹¶å‘çš„æ“ä½œAlluxioå…ƒæ•°æ®ã€‚é«˜å¯ç”¨è¿‡ç¨‹ä¸­ï¼ŒINode-Treeæ˜¯è¿›ç¨‹ä¸­çš„èµ„æºï¼Œä¸å…±äº«ï¼Œå¦‚æœActiveMasteræŒ‚æ‰ï¼ŒStandByMasterèŠ‚ç‚¹å¯ä»¥ä»JournalæŒä¹…æ—¥å¿—ï¼ˆä½äºæŒä¹…åŒ–å­˜å‚¨ä¸­å¦‚HDFSï¼‰æ¢å¤çŠ¶æ€ã€‚è¿™æ ·ä¼šä¾èµ–æŒä¹…å­˜å‚¨ï¼ˆå¦‚HDFSï¼‰çš„å¥åº·çŠ¶å†µï¼Œå¦‚æœæŒä¹…å­˜å‚¨æœåŠ¡å®•æœºï¼ŒJournalæ—¥å¿—ä¹Ÿä¸èƒ½å†™ï¼ŒAlluxioé«˜å¯ç”¨æœåŠ¡å°±ä¼šå—åˆ°å½±å“ã€‚æ‰€ä»¥ï¼ŒAlluxioé€šè¿‡Raftç®—æ³•ä¿è¯å…ƒæ•°æ®çš„å®Œæ•´æ€§ï¼Œå³ä½¿å®•æœºï¼Œä¹Ÿä¸ä¼šä¸¢å¤±å·²ç»æäº¤çš„å…ƒæ•°æ®ã€‚ Alluxioå…ƒæ•°æ®ä¸€è‡´æ€§ Alluxioè¯»å–ç£å±‚å­˜å‚¨ç³»ç»Ÿçš„å…ƒæ•°æ®,åŒ…æ‹¬æ–‡ä»¶å,æ–‡ä»¶å¤§å°,åˆ›å»ºè€…,ç»„åˆ«,ç›®å½•ç»“æ„ç­‰ å¦‚æœç»•è¿‡Alluxioä¿®æ”¹åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„ç›®å½•ç»“æ„,Alluxioä¼šåŒæ­¥æ›´æ–°alluxio.user.file.metadata.sync.interval=-1 Alluxioä¸ä¸»åŠ¨åŒæ­¥åº•å±‚å­˜å‚¨å…ƒæ•°æ®alluxio.user.file.metadata.sync.interval=æ­£æ•´æ•° æ­£æ•´æ•°æŒ‡å®šäº†æ—¶é—´çª—å£,è¯¥æ—¶é—´çª—å£å†…ä¸è§¦å‘å…ƒæ•°æ®åŒæ­¥alluxio.user.file.metadata.sync.interval=0 æ—¶é—´çª—å£ä¸º0,æ¯æ¬¡è¯»å–éƒ½è§¦å‘å…ƒæ•°æ®åŒæ­¥æ—¶é—´çª—å£è¶Šå¤§,åŒæ­¥å…ƒæ•°æ®é¢‘ç‡è¶Šä½,Alluxio Masteræ€§èƒ½å—å½±å“è¶Šå° Alluxioä¸åŠ è½½å…·ä½“æ•°æ®,åªåŠ è½½å…ƒæ•°æ®,è‹¥è¦åŠ è½½æ–‡ä»¶æ•°æ®,å¯ä»¥é€šè¿‡loadå‘½ä»¤æˆ–FileStream API åœ¨Alluxioä¸­åˆ›å»ºæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹æ—¶å¯ä»¥æŒ‡å®šæ˜¯å¦æŒä¹…åŒ–alluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH mkdir /xxxalluxio fs -Dalluxio.user.file.writetype.default=CACHE_THROUGH touch /xxx/xx Alluxio RPCAlluxio 1.xä¸­ Master RPC using Thriftï¼ˆå…ƒæ•°æ®æ“ä½œï¼‰ Workers RPC using Nettyï¼ˆæ•°æ®æ“ä½œï¼‰è€Œæ–°çš„Alluxio 2.xä¸­ ä½¿ç”¨gRPCä¿è¯é«˜ååï¼Œæ–¹ä¾¿ä»£ç ç»´æŠ¤ Alluxioçš„Metricsåº¦é‡æŒ‡æ ‡ä¿¡æ¯å¯ä»¥è®©ç”¨æˆ·æ·±å…¥äº†è§£é›†ç¾¤ä¸Šè¿è¡Œçš„ä»»åŠ¡,æ˜¯ç›‘æ§å’Œè°ƒè¯•çš„å®è´µèµ„æºã€‚Alluxioçš„åº¦é‡æŒ‡æ ‡ä¿¡æ¯è¢«åˆ†é…åˆ°å„ç§ç›¸å…³Alluxioç»„ä»¶çš„å®ä¾‹ä¸­ã€‚æ¯ä¸ªå®ä¾‹ä¸­ï¼Œç”¨æˆ·å¯ä»¥é…ç½®ä¸€ç»„åº¦é‡æŒ‡æ ‡æ§½ï¼Œæ¥å†³å®šæŠ¥å‘Šå“ªäº›åº¦é‡æŒ‡æ ‡ä¿¡æ¯ã€‚ç°æ”¯æŒMasterè¿›ç¨‹,Workerè¿›ç¨‹å’ŒClientè¿›ç¨‹çš„åº¦é‡æŒ‡æ ‡ ã€‚ åº¦é‡æŒ‡æ ‡çš„sinkå‚æ•°ä¸ºalluxio.metrics.sink.xxxConsoleSink: è¾“å‡ºæ§åˆ¶å°çš„åº¦é‡å€¼ã€‚CsvSink: æ¯éš”ä¸€æ®µæ—¶é—´å°†åº¦é‡æŒ‡æ ‡ä¿¡æ¯å¯¼å‡ºåˆ°CSVæ–‡ä»¶ä¸­ã€‚JmxSink: æŸ¥çœ‹JMXæ§åˆ¶å°ä¸­æ³¨å†Œçš„åº¦é‡ä¿¡æ¯ã€‚GraphiteSink: ç»™GraphiteæœåŠ¡å™¨å‘é€åº¦é‡ä¿¡æ¯ã€‚MetricsServlet: æ·»åŠ Web UIä¸­çš„servletï¼Œä½œä¸ºJSONæ•°æ®æ¥ä¸ºåº¦é‡æŒ‡æ ‡æ•°æ®æœåŠ¡ã€‚ å¯é€‰åº¦é‡çš„é…ç½® Masterçš„Metrics é…ç½®æ–¹æ³• master.* ä¾‹å¦‚:master.CapacityTotalå¸¸è§„ä¿¡æ¯CapacityTotal: æ–‡ä»¶ç³»ç»Ÿæ€»å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚CapacityUsed: æ–‡ä»¶ç³»ç»Ÿä¸­å·²ä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚CapacityFree: æ–‡ä»¶ç³»ç»Ÿä¸­æœªä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚PathsTotal: æ–‡ä»¶ç³»ç»Ÿä¸­æ–‡ä»¶å’Œç›®å½•çš„æ•°ç›®ã€‚UnderFsCapacityTotal: åº•å±‚æ–‡ä»¶ç³»ç»Ÿæ€»å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚UnderFsCapacityUsed: åº•å±‚æ–‡ä»¶ç³»ç»Ÿä¸­å·²ä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚UnderFsCapacityFree: åº•å±‚æ–‡ä»¶ç³»ç»Ÿä¸­æœªä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚Workers: Workerçš„æ•°ç›®ã€‚é€»è¾‘æ“ä½œDirectoriesCreated: åˆ›å»ºçš„ç›®å½•æ•°ç›®ã€‚FileBlockInfosGot: è¢«æ£€ç´¢çš„æ–‡ä»¶å—æ•°ç›®ã€‚FileInfosGot: è¢«æ£€ç´¢çš„æ–‡ä»¶æ•°ç›®ã€‚FilesCompleted: å®Œæˆçš„æ–‡ä»¶æ•°ç›®ã€‚FilesCreated: åˆ›å»ºçš„æ–‡ä»¶æ•°ç›®ã€‚FilesFreed: é‡Šæ”¾æ‰çš„æ–‡ä»¶æ•°ç›®ã€‚FilesPersisted: æŒä¹…åŒ–çš„æ–‡ä»¶æ•°ç›®ã€‚FilesPinned: è¢«å›ºå®šçš„æ–‡ä»¶æ•°ç›®ã€‚NewBlocksGot: è·å¾—çš„æ–°æ•°æ®å—æ•°ç›®ã€‚PathsDeleted: åˆ é™¤çš„æ–‡ä»¶å’Œç›®å½•æ•°ç›®ã€‚PathsMounted: æŒ‚è½½çš„è·¯å¾„æ•°ç›®ã€‚PathsRenamed: é‡å‘½åçš„æ–‡ä»¶å’Œç›®å½•æ•°ç›®ã€‚PathsUnmounted: æœªè¢«æŒ‚è½½çš„è·¯å¾„æ•°ç›®ã€‚RPCè°ƒç”¨CompleteFileOps: CompleteFileæ“ä½œçš„æ•°ç›®ã€‚CreateDirectoryOps: CreateDirectoryæ“ä½œçš„æ•°ç›®ã€‚CreateFileOps: CreateFileæ“ä½œçš„æ•°ç›®ã€‚DeletePathOps: DeletePathæ“ä½œçš„æ•°ç›®ã€‚FreeFileOps: FreeFileæ“ä½œçš„æ•°ç›®ã€‚GetFileBlockInfoOps: GetFileBlockInfoæ“ä½œçš„æ•°ç›®ã€‚GetFileInfoOps: GetFileInfoæ“ä½œçš„æ•°ç›®ã€‚GetNewBlockOps: GetNewBlockæ“ä½œçš„æ•°ç›®ã€‚MountOps: Mountæ“ä½œçš„æ•°ç›®ã€‚RenamePathOps: RenamePathæ“ä½œçš„æ•°ç›®ã€‚SetStateOps: SetStateæ“ä½œçš„æ•°ç›®ã€‚UnmountOps: Unmountæ“ä½œçš„æ•°ç›®ã€‚ Workerçš„Metrics é…ç½®æ–¹æ³• 192_168_1_1.* ä¾‹å¦‚:192_168_1_1.CapacityTotalå¸¸è§„ä¿¡æ¯CapacityTotal: è¯¥Workerçš„æ€»å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚CapacityUsed: è¯¥Workerå·²ä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚CapacityFree: è¯¥Workeræœªä½¿ç”¨çš„å®¹é‡ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚é€»è¾‘æ“ä½œBlocksAccessed: è®¿é—®çš„æ•°æ®å—æ•°ç›®ã€‚BlocksCached: è¢«ç¼“å­˜çš„æ•°æ®å—æ•°ç›®ã€‚BlocksCanceled: è¢«å–æ¶ˆçš„æ•°æ®å—æ•°ç›®ã€‚BlocksDeleted: è¢«åˆ é™¤çš„æ•°æ®å—æ•°ç›®ã€‚BlocksEvicted: è¢«æ›¿æ¢çš„æ•°æ®å—æ•°ç›®ã€‚BlocksPromoted: è¢«æå‡åˆ°å†…å­˜çš„æ•°æ®å—æ•°ç›®ã€‚BytesReadAlluxio: é€šè¿‡è¯¥workerä»Alluxioå­˜å‚¨è¯»å–çš„æ•°æ®é‡ï¼Œå•ä½ä¸ºbyteã€‚å…¶ä¸­ä¸åŒ…æ‹¬UFSè¯»ã€‚BytesWrittenAlluxio: é€šè¿‡è¯¥workerå†™åˆ°Alluxioå­˜å‚¨çš„æ•°æ®é‡ï¼Œå•ä½ä¸ºbyteã€‚å…¶ä¸­ä¸åŒ…æ‹¬UTFå†™ã€‚BytesReadUfs-UFS:${UFS}: é€šè¿‡è¯¥workerä»æŒ‡å®šUFSè¯»å–çš„æ•°æ®é‡ï¼Œå•ä½ä¸ºbyteã€‚BytesWrittenUfs-UFS:${UFS}: é€šè¿‡è¯¥workerå†™åˆ°æŒ‡å®šUFSçš„æ•°æ®é‡ï¼Œå•ä½ä¸ºbyteã€‚ Clientçš„Metrics é…ç½®æ–¹æ³• client.* ä¾‹å¦‚:clien.BytesReadRemoteå¸¸è§„ä¿¡æ¯NettyConnectionOpen: å½“å‰Nettyç½‘ç»œè¿æ¥çš„æ•°ç›®ã€‚é€»è¾‘æ“ä½œBytesReadRemote: è¿œç¨‹è¯»å–çš„å­—èŠ‚æ•°ç›®ã€‚BytesWrittenRemote: è¿œç¨‹å†™å…¥çš„å­—èŠ‚æ•°ç›®ã€‚BytesReadUfs: ä»ufsä¸­è¯»å–çš„å­—èŠ‚æ•°ç›®ã€‚BytesWrittenUfs: å†™å…¥ufsçš„å­—èŠ‚æ•°ç›®ã€‚ é…ç½®ç¤ºä¾‹ vim metrics.properties # List of available sinks and their properties. alluxio.metrics.sink.ConsoleSink alluxio.metrics.sink.CsvSink alluxio.metrics.sink.JmxSink alluxio.metrics.sink.MetricsServlet alluxio.metrics.sink.PrometheusMetricsServlet alluxio.metrics.sink.GraphiteSink master.GetFileBlockInfoOps master.GetNewBlockOps master.FreeFileOps 192_168_1_101.BytesReadAlluxio 192_168_1_101.BytesWrittenAlluxio 192_168_1_101.BlocksAccessed 192_168_1_101.BlocksCached 192_168_1_101.BlocksCanceled 192_168_1_101.BlocksDeleted 192_168_1_101.BlocksEvicted 192_168_1_101.BlocksPromoted 192_168_1_102.BytesReadAlluxio 192_168_1_102.BytesWrittenAlluxio 192_168_1_102.BlocksAccessed 192_168_1_102.BlocksCached 192_168_1_102.BlocksCanceled 192_168_1_102.BlocksDeleted 192_168_1_102.BlocksEvicted 192_168_1_102.BlocksPromoted 192_168_1_103.BytesReadAlluxio 192_168_1_103.BytesWrittenAlluxio 192_168_1_103.BlocksAccessed 192_168_1_103.BlocksCached 192_168_1_103.BlocksCanceled 192_168_1_103.BlocksDeleted 192_168_1_103.BlocksEvicted 192_168_1_103.BlocksPromoted ç„¶åè®¿é—® http://192.168.1.101:19999/metrics/json/ å¯å¾—åˆ°ç›‘æ§ä¿¡æ¯å–œæ¬¢çœ‹æºç çš„å°ä¼™ä¼´å¯ä»¥æˆ³è¿™é‡Œå“Ÿ-&gt;Alluxioæºç å…¥å£ Alluixoå®¡è®¡æ—¥å¿—Alluxioæä¾›å®¡è®¡æ—¥å¿—æ¥æ–¹ä¾¿ç®¡ç†å‘˜å¯ä»¥è¿½è¸ªç”¨æˆ·å¯¹å…ƒæ•°æ®çš„è®¿é—®æ“ä½œã€‚å¼€å¯å®¡è®¡æ—¥å¿—ï¼š è®²JVMå‚æ•°alluxio.master.audit.logging.enabledè®¾ä¸ºtrueå®¡è®¡æ—¥å¿—åŒ…å«å¦‚ä¸‹æ¡ç›®ï¼š key desc succeeded å¦‚æœå‘½ä»¤æˆåŠŸè¿è¡Œï¼Œå€¼ä¸ºtrueã€‚åœ¨å‘½ä»¤æˆåŠŸè¿è¡Œå‰ï¼Œè¯¥å‘½ä»¤å¿…é¡»æ˜¯è¢«å…è®¸çš„ã€‚ allowed å¦‚æœå‘½ä»¤æ˜¯è¢«å…è®¸çš„ï¼Œå€¼ä¸ºtrueã€‚å³ä½¿ä¸€æ¡å‘½ä»¤æ˜¯è¢«å…è®¸çš„å®ƒä¹Ÿå¯èƒ½è¿è¡Œå¤±è´¥ã€‚ ugi ç”¨æˆ·ç»„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨æˆ·åï¼Œä¸»è¦ç»„ï¼Œè®¤è¯ç±»å‹ã€‚ ip å®¢æˆ·ç«¯IPåœ°å€ã€‚ cmd ç”¨æˆ·è¿è¡Œçš„å‘½ä»¤ã€‚ src æºæ–‡ä»¶æˆ–ç›®å½•åœ°å€ã€‚ dst ç›®æ ‡æ–‡ä»¶æˆ–ç›®å½•çš„åœ°å€ã€‚å¦‚æœä¸é€‚ç”¨ï¼Œå€¼ä¸ºç©ºã€‚ perm user:group:maskï¼Œå¦‚æœä¸é€‚ç”¨å€¼ä¸ºç©ºã€‚ Alluxioå®‰è£…å’Œéƒ¨ç½²å‡†å¤‡å·¥ä½œ1.ä¸‹è½½Alluxioå‹ç¼©åŒ…å¹¶ä¸Šä¼ åˆ°NNæ‰€åœ¨é›†ç¾¤2.è§£å‹å¹¶è¿›å…¥å®‰è£…ç›®å½• tar -zxvf alluxio-2.0.1-bin.tar.gz -C /opt/module/ mv /opt/module/alluxio-2.0.1 /opt/module/alluxio cd /opt/module/alluxio cp conf/alluxio-site.properties.template conf/alluxio-site.properties cp conf/alluxio-env.sh.template conf/alluxio-env.sh å¸¸è§„é›†ç¾¤å‚æ•°é…ç½®å¸¸è§„éé«˜å¯ç”¨é›†ç¾¤é…ç½®ï¼Œé’ˆå¯¹1.xå’Œ2.xç‰ˆæœ¬é€šç”¨conf/alluxio-env.sh vim conf/alluxio-env.sh ALLUXIO_HOME=/opt/module/alluxio-2.0.1 ALLUXIO_LOGS_DIR=/opt/module/alluxio-2.1.0/logs ALLUXIO_MASTER_HOSTNAME=hadoop101 ALLUXIO_RAM_FOLDER=/mnt/ramdisk ALLUXIO_UNDERFS_ADDRESS=hdfs://hadoop101:9000/alluxio ALLUXIO_WORKER_MEMORY_SIZE=512MB JAVA_HOME=/opt/module/jdk1.8.0_161 # è®¾ç½®ALLUXIO_MASTER_JAVA_OPTSä½œç”¨äºmaster JVM # è®¾ç½®ALLUXIO_WORKER_JAVA_OPTSä½œç”¨äºworker JVM # ä»¥åŠALLUXIO_JAVA_OPTSåŒæ—¶ä½œç”¨äºmasterä»¥åŠworker JVM # å¢åŠ worker JVM GCäº‹ä»¶çš„logging, è¾“å‡ºå†™è‡³workerèŠ‚ç‚¹çš„logs/worker.outæ–‡ä»¶ä¸­ ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # è®¾ç½®master JVMçš„çš„heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; conf/alluxio-site.properties vim conf/alluxio-site.properties # Common properties alluxio.master.hostname=hadoop101 alluxio.master.mount.table.root.ufs=hdfs://192.168.1.101:9000/alluxio alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk vim conf/masters hadoop101 vim conf/workers hadoop102 hadoop103 scp -r /opt/module/alluxio/ root@hadoop102:/opt/module/ scp -r /opt/module/alluxio/ root@hadoop103:/opt/module/ # æ‰“å¼€AlluxioæœåŠ¡ alluxio format alluxio-start.sh master alluxio-start.sh workers NoMount æˆ–ç›´æ¥ alluxio-start.sh all è®¿é—®MasterèŠ‚ç‚¹çš„WEB UI: hadoop101:19999 è®¿é—®WorkerèŠ‚ç‚¹çš„WEB UI: hadoop102:30000 #æµ‹è¯•éƒ¨ç½²æ˜¯å¦æˆåŠŸ bin/alluxio runTests # å¦‚æœå‡ºç°Passed the teståˆ™è¯´æ˜éƒ¨ç½²æˆåŠŸ bin/alluxio-stop.sh all # å…³é—­é›†ç¾¤ å‡ºç°ç±»ä¼¼ä»¥ä¸‹ç•Œé¢å³ä¸ºéƒ¨ç½²æˆåŠŸæ­¤æ—¶å¯ä»¥é€šè¿‡å‘½ä»¤alluxio fsdamin reportæ¥æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ é«˜å¯ç”¨é›†ç¾¤å‚æ•°é…ç½®é«˜å¯ç”¨(HA)é€šè¿‡æ”¯æŒåŒæ—¶è¿è¡Œå¤šä¸ªmasteræ¥ä¿è¯æœåŠ¡çš„é«˜å¯ç”¨æ€§ï¼Œå¤šä¸ªmasterä¸­æœ‰ä¸€ä¸ªmasterè¢«é€‰ä¸ºprimary masterä½œä¸ºæ‰€æœ‰workerå’Œclientçš„é€šä¿¡é¦–é€‰ï¼Œå…¶ä½™masterä¸ºå¤‡é€‰çŠ¶æ€(StandBy)ï¼Œå®ƒä»¬é€šè¿‡å’Œprimary masterå…±äº«æ—¥å¿—æ¥ç»´æŠ¤åŒæ ·çš„æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®ï¼Œå¹¶åœ¨primary masterå¤±æ•ˆæ—¶è¿…é€Ÿæ¥æ›¿å…¶å·¥ä½œ(masterä¸»ä»åˆ‡æ¢è¿‡ç¨‹ä¸­ï¼Œå®¢æˆ·ç«¯å¯èƒ½ä¼šå‡ºç°çŸ­æš‚çš„å»¶è¿Ÿæˆ–ç¬æ€é”™è¯¯)æ­å»ºé«˜å¯ç”¨é›†ç¾¤å‰çš„å‡†å¤‡:â‘ ç¡®ä¿ZookeeperæœåŠ¡å·²ç»è¿è¡Œâ‘¡ä¸€ä¸ªå•ç‹¬å®‰è£…çš„å¯é çš„å…±äº«æ—¥å¿—å­˜å‚¨ç³»ç»Ÿ(å¯ç”¨HDFSæˆ–S3ç­‰ç³»ç»Ÿ)â‘¢è¿™ä¸ªé…ç½®é’ˆå¯¹Alluxio 2.xç‰ˆæœ¬ï¼Œä¸é€‚ç”¨äº1.xç‰ˆæœ¬â‘£éœ€è¦äº‹å…ˆåˆ›å»ºå¥½ramdiskæŒ‚è½½ç›®å½• æ³¨æ„å»æ‰ä¸­æ–‡æ³¨é‡Š å¦åˆ™ä¼šæŠ¥é”™ åœ¨æ‰€æœ‰æœºå™¨ä¸Šé…ç½®env.sh vim alluxio-env.sh ALLUXIO_HOME=/opt/alluxio ALLUXIO_LOGS_DIR=/opt/alluxio/logs ALLUXIO_RAM_FOLDER=/mnt/ramdisk JAVA_HOME=/opt/module/jdk1.8.0_161 # è®¾ç½®ALLUXIO_MASTER_JAVA_OPTSä½œç”¨äºmaster JVM # è®¾ç½®ALLUXIO_WORKER_JAVA_OPTSä½œç”¨äºworker JVM # ä»¥åŠALLUXIO_JAVA_OPTSåŒæ—¶ä½œç”¨äºmasterä»¥åŠworker JVM # å¢åŠ worker JVM GCäº‹ä»¶çš„logging, è¾“å‡ºå†™è‡³workerèŠ‚ç‚¹çš„logs/worker.outæ–‡ä»¶ä¸­ ALLUXIO_WORKER_JAVA_OPTS=&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimestamps&quot; # è®¾ç½®master JVMçš„çš„heap size ALLUXIO_MASTER_JAVA_OPTS=&quot; -Xms2048M -Xmx4096M&quot; åœ¨101æœºå™¨ä¸Šé…ç½®Masterå’ŒWorker vim alluxio-site.properties # 192.168.1.101 Master Worker # Common properties alluxio.master.hostname=192.168.1.101 # è¦å†™å…¶ä»–æœºå™¨èƒ½è¯†åˆ«çš„åœ°å€è€Œélocalhostç­‰ alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml # å¦‚æœåº•å±‚HDFSå­˜å‚¨ä¸ºé«˜å¯ç”¨ï¼Œåˆ™è¦å†™hdfsé…ç½®æ–‡ä»¶åœ°å€ alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # æŒ‡å‘é«˜å¯ç”¨æˆ–éé«˜å¯ç”¨çš„HDFSåœ°å€ï¼ˆå¯ä»¥æ˜¯æ ¹ç›®å½•ï¼Œä¹Ÿå¯ä»¥æ˜¯æŸä¸ªæ–‡ä»¶å¤¹ï¼‰ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 # Zookeeperåœ°å€ä¸­é—´é€—å·éš”å¼€ alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal # å›æ»šæ—¥å¿—çš„åœ°å€ï¼Œå†™å…¥å¯é çš„åˆ†å¸ƒå¼HDFS alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* # å¯ä»¥æ¨¡æ‹Ÿå¾ˆå¤šç”¨æˆ·æ¥å®ç°æƒé™æ§åˆ¶ alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* åœ¨102æœºå™¨ä¸Šé…ç½®Masterå’ŒWorker vim alluxio-site.properties # 192.168.1.102 Master Worker # Common properties alluxio.master.hostname=192.168.1.102 # è¦å†™å…¶ä»–æœºå™¨èƒ½è¯†åˆ«çš„åœ°å€è€Œélocalhostç­‰ alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.master.journal.type=UFS alluxio.master.journal.folder=hdfs://192.168.1.101:9000/alluxio/journal alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* åœ¨103æœºå™¨ä¸Šé…ç½®Worker vim alluxio-site.properties # 192.168.1.103 Worker # Common properties # Workerä¸éœ€è¦å†™alluxio.master.hostnameå‚æ•°å’Œalluxio.master.journal.folderå‚æ•° alluxio.underfs.hdfs.configuration=/opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml:/opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml alluxio.master.mount.table.root.ufs=hdfs://hadoop101:9000/ # Worker properties alluxio.worker.memory.size=512MB alluxio.worker.tieredstore.levels=1 alluxio.worker.tieredstore.level0.alias=MEM alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # HA properties alluxio.zookeeper.enabled=true alluxio.zookeeper.address=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 alluxio.worker.block.heartbeat.timeout.ms=300000 alluxio.zookeeper.session.timeout=120s # User properties alluxio.user.file.readtype.default=CACHE_PROMOTE alluxio.user.file.writetype.default=ASYNC_THROUGH alluxio.user.metrics.collection.enable=true alluxio.master.metrics.time.series.interval=1000 # Security properties alluxio.security.authorization.permission.enabled=true alluxio.security.authentication.type=SIMPLE alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* alluxio.master.security.impersonation.hdfs.users=* alluxio.master.security.impersonation.hdfs.groups=* åœ¨æ‰€æœ‰æœºå™¨ä¸ŠæŒ‡å®šMasterå’ŒWorkerèŠ‚ç‚¹ vim masters 192.168.1.101 192.168.1.102 vim workers 192.168.1.101 192.168.1.102 192.168.1.103 # æµ‹è¯•éƒ¨ç½²æ˜¯å¦æˆåŠŸ alluxio format alluxio-start.sh all SudoMount alluxio fsadmin report alluxio runTests # å¦‚æœå‡ºç°Passed the teståˆ™è¯´æ˜éƒ¨ç½²æˆåŠŸ # æµ‹è¯•é«˜å¯ç”¨æ¨¡å¼çš„è‡ªåŠ¨æ•…éšœå¤„ç†: (å‡è®¾æ­¤æ—¶hadoop101ä½primary master) ssh hadoop101 jps | grep AlluxioMaster kill -9 &lt;AlluxioMaster PID&gt; alluxio fs leader # æ˜¾ç¤ºæ–°çš„primary Master(å¯èƒ½éœ€è¦ç­‰å¾…ä¸€å°æ®µæ—¶é—´é€‰ä¸¾) éƒ¨ç½²è¯´æ˜ Alluxioå¯ä»¥åƒCMä¸€æ ·ï¼Œéƒ¨ç½²åœ¨åŒä¸€ç½‘ç»œä¸­çš„èŠ‚ç‚¹ä¸Šä¸”ä¸éœ€è¦æœºå™¨é—´å…å¯†ç™»é™†ã€‚å…å¯†ç™»é™†åªæ˜¯ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨start-all.shè„šæœ¬ä¸€é”®å¯åŠ¨ã€‚éå…å¯†ç™»é™†çš„é›†ç¾¤å¯ä»¥ä½¿ç”¨Ansibleè‡ªåŠ¨åŒ–è¿ç»´å·¥å…·å¯¹æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œå¯åŠ¨å’ŒæŒ‚è½½ç­‰æ“ä½œ(åœ¨æ¯ä¸ªMasterä¸Šä½¿ç”¨éƒ¨ç½²Alluxioçš„ç”¨æˆ·åˆ†åˆ«æ‰§è¡Œalluxio-start.sh master,ç„¶åå¦‚æœä½¿ç”¨érootç”¨æˆ·å¯åŠ¨AlluxioæœåŠ¡ï¼Œåˆ™è¦åœ¨æ¯ä¸ªworkerçš„rootç”¨æˆ·æ‰§è¡Œalluxio-mount.sh Mount local ,ç„¶åç”¨éƒ¨ç½²Alluxioçš„ç”¨æˆ·æ‰§è¡Œalluxio-start.sh worker,å¹¶åœ¨æ‰€æœ‰èŠ‚ç‚¹alluxio-start.sh job_master,alluxio-start.sh job_workerå³å¯)ï¼Œä½œç”¨ç­‰åŒäºstart-all.shè„šæœ¬ï¼Œä¸ä¼šå¯¹AlluxioæœåŠ¡çš„è¿è¡Œé€ æˆå½±å“ã€‚ Mountå’ŒSudoMountéœ€è¦åœ¨rootæƒé™ä¸‹æ‰§è¡Œï¼Œå› ä¸ºåªæœ‰rootç”¨æˆ·æœ‰æƒé™åˆ›å»ºå’Œè®¿é—®RamFSï¼Œå¯åŠ¨Alluxioçš„ç”¨æˆ·è¦æœ‰è¿™ä¸ªRamFSçš„è¯»å†™æ‰§è¡Œæƒé™ï¼ŒAlluxioçš„RAM FLODERï¼ˆramdiskï¼‰å¯ä»¥ç†è§£ä¸ºæ˜¯åœ¨æ™®é€šHDDç£ç›˜ç›®å½•ä¸ŠæŒ‚è½½çš„ä¸€ä¸ªRamFSæ–‡ä»¶ç³»ç»Ÿï¼ŒRamFSæ˜¯æŠŠç³»ç»Ÿçš„RAMä½œä¸ºå­˜å‚¨ï¼Œä¸”RamFSä¸ä¼šä½¿ç”¨swapäº¤æ¢å†…å­˜åˆ†åŒºï¼ŒLinuxä¼šæŠŠRamFSè§†ä¸ºä¸€ä¸ªç£ç›˜æ–‡ä»¶ç›®å½•ã€‚ æŸ¥çœ‹RamFSçš„æ–¹æ³•ï¼š mount | grep -E â€œ(tmpfs|ramfs)â€ è¿™é‡Œçš„tmpFSä¹Ÿæ˜¯åŸºäºå†…å­˜çš„å­˜å‚¨ç³»ç»Ÿï¼Œä½†å®ƒä¼šä½¿ç”¨åˆ°Swapåˆ†åŒºï¼Œä½¿è¯»å†™æ•ˆç‡é™ä½ï¼ŒAlluxioä¹Ÿå¯ä»¥ä½¿ç”¨tmpFSä½œä¸ºç¼“å­˜ã€‚ äº†è§£æ›´å¤š:ramfså’Œtmpfsçš„åŒºåˆ« Alluxioçš„â€/â€œç›®å½•æƒé™ç”±å¯åŠ¨Materå’ŒWorkerçš„ç”¨æˆ·å†³å®šï¼Œå¹¶ä¸UFSä¸­å¯¹åº”çš„æ–‡ä»¶å¤¹æƒé™ä¸€è‡´ï¼Œå¯ä»¥ä¿®æ”¹Alluxioæ ¹ç›®å½•æƒé™ï¼ŒAlluxioåˆ›å»ºæ–‡ä»¶å’Œæ–‡ä»¶å¤¹çš„ç”¨æˆ·å’Œç»„ä¸Linuxç”¨æˆ·åˆç»„ä¸€è‡´ï¼Œå¹¶ä¸”ä¸æŒä¹…åŒ–åˆ°HDFSçš„æ–‡ä»¶çš„ç”¨æˆ·å’Œç»„ä¸€è‡´ã€‚ Mount|SudoMount|Umount|SudoUmountè¯´ä¸€ä¸‹è¿™å››ä¸ªå‚æ•°ï¼ŒMountå’ŒSudoMountæ˜¯æŒ‚è½½RamFSï¼Œåè€…å¸¦sudoæƒé™ï¼ŒUmountå’ŒSudoUmountæ˜¯å¸è½½RamFSï¼Œåè€…å¸¦sudoæƒé™ã€‚Mountå’ŒSudoMountä¼šæ ¼å¼åŒ–å·²å­˜åœ¨çš„RamFSã€‚ å…³äºç”¨æˆ·æ¨¡æ‹Ÿçš„ä¸€äº›ç†è§£å’Œä½¿ç”¨å¾ˆé‡è¦å‚è€ƒè¿™ç¯‡æ–‡ç« ï¼šUser Impersonationç›¸å…³é…ç½®é—®é¢˜åˆ†æä¸è§£å†³ Alluxioéƒ¨ç½²å‰ï¼Œè¦å†³å®šç”¨å“ªä¸ªç”¨æˆ·å¯åŠ¨Alluxioï¼Œå¦‚æœåº•å±‚å­˜å‚¨æ˜¯HDFSï¼Œå»ºè®®ä½¿ç”¨å¯åŠ¨NameNodeè¿›ç¨‹çš„ç”¨æˆ·æ¥å¯åŠ¨Alluxio Masterå’ŒWorkers,ä¿è¯HDFSæƒé™æ˜ å°„ï¼šAlluxio On HDFS Mountå‚æ•°ä¸€èˆ¬åªåœ¨WorkerèŠ‚ç‚¹ä½¿ç”¨ å¯ä»¥åœ¨HDFSå»ºç«‹ä¸€ä¸ª777æƒé™çš„æ–‡ä»¶è·¯å¾„ä½œä¸ºAlluxioçš„åº•å±‚å­˜å‚¨ job_masterå’Œjob_workerå®˜ç½‘æ²¡åšä»‹ç»ï¼Œä½†åœ¨å½“å‰ç‰ˆæœ¬è¿™ä¸¤ä¸ªç»„ä»¶å¿…é¡»å¯åŠ¨ï¼Œå¦åˆ™ä¼šå½±å“persiståŠŸèƒ½ä»¥åŠä¸€äº›å…¶ä»–åŠŸèƒ½(æˆ‘ç›®å‰åªçŸ¥é“persistä¼šTime Out) é…ç½®è¿™å—è¸©äº†å¥½å¤šå‘ï¼Œç»ˆäºï¼ŒAlluxioåŸºæœ¬æœåŠ¡éƒ¨ç½²å®Œæ¯•,ä¸€äº›å…³äºä¼˜åŒ–å’Œç»†èŠ‚çš„å‚æ•°åœ¨AlluxioåŸç†éƒ¨åˆ†ä¸­æ¶‰åŠåˆ°,ä¹Ÿå¯æŸ¥é˜…Alluxioé…ç½®å‚æ•°å¤§å…¨ Alluxio2.1.0ç‰ˆæœ¬å®˜æ–¹ä»‹ç»è¯´ä½¿ç”¨ASYNC_THROUGHè¿›è¡Œå†™å…¥æ—¶é˜²æ­¢æ•°æ®ä¸¢å¤±ï¼Œæ‰€ä»¥æˆ‘è¿™é‡Œè®¾ç½®äº†ASYNC_THROUGHå¼‚æ­¥å†™ç£ç›˜ï¼Œæ—¢èƒ½ä¿è¯å†™å…¥é€Ÿåº¦ï¼Œåˆèƒ½å°†æ–‡ä»¶æŒä¹…åŒ–ä¹‹å‰é…ç½®Alluxioé«˜å¯ç”¨ï¼Œä¸€ç›´ä¸ç¨³å®šï¼Œå¿ƒè·³ä¸­æ–­ï¼ŒMasterå’ŒWorkeræ‰çº¿é—®é¢˜é¢‘å‘ï¼ŒAlluxio2.1ç‰ˆæœ¬å®˜æ–¹è¯´ä¿®å¤äº†å„ç§å¿ƒè·³ä¸­æ–­é—®é¢˜,å½“ç„¶Alluxioçš„é«˜å¯ç”¨è¦æ±‚åº•å±‚çš„Journalæ—¥å¿—å­˜å‚¨ç³»ç»Ÿçš„ç¨³å®šæ€§å¾ˆé«˜ï¼Œå¦‚æœåº•å±‚Journalå­˜å‚¨ç³»ç»Ÿä¸ç¨³å®šï¼ˆæ¯”å¦‚HDFS No More Good DataNodeçš„æƒ…å†µï¼‰ï¼Œå°±ä¼šå¯¼è‡´Masterå´©æºƒã€‚ Alluxioå¸¸ç”¨å‘½ä»¤Alluxioå‘½ä»¤é€ŸæŸ¥è¡¨åŒ…æ‹¬ç¼“å­˜è½½å…¥,é©»ç•™,é‡Šæ”¾,æ•°æ®ç”Ÿå­˜æ—¶é—´ç­‰é‡è¦å‘½ä»¤Alluxioå¸¸ç”¨Shellå‘½ä»¤é€ŸæŸ¥è¡¨: #æ–‡ä»¶åŸºæœ¬æ“ä½œ å¯ä»¥åœ¨æ‰§è¡Œå‘½ä»¤æ—¶æŒ‡å®šå‚æ•° æ–¹æ³•: alluxio fs -D...æŒ‡å®šå‚æ•° copyFromLocal .... alluxio fs cat &lt;path/file&gt; # æ‰“å¼€æ–‡ä»¶ alluxio fs ls [-d|-f|-p|-R|-h|--sort=option|-r] &lt;path&gt; #æŸ¥çœ‹ç›®å½• alluxio fs copyFromLocal [--thread &lt;num&gt;] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;remoteDst&gt; # ä»æœ¬åœ°ä¸Šä¼ æ–‡ä»¶åˆ°Alluxio alluxio fs copyToLocal [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;localDst&gt; #ä»Alluxioè½½æ–‡ä»¶åˆ°æœ¬åœ° alluxio fs count &lt;path&gt; # ç»Ÿè®¡Alluxioç›®å½•çš„æ–‡ä»¶æ•°,æ–‡ä»¶å¤¹æ•°å’Œæ€»å¤§å° alluxio fs du [-h|-s|--memory] &lt;path&gt; # æ–‡ä»¶å¤§å° alluxio fs cp [-R] [--buffersize &lt;bytes&gt;] &lt;src&gt; &lt;dst&gt; #å¤åˆ¶æ–‡ä»¶ alluxio fs mv &lt;src&gt; &lt;dst&gt; # ç§»åŠ¨æ–‡ä»¶ alluxio fs rm [-R] [-U] [--alluxioOnly] &lt;path&gt; # åˆ é™¤æ–‡ä»¶ alluxio fs mkdir &lt;path1&gt; [path2] ... [pathn] # åˆ›å»ºæ–‡ä»¶å¤¹ alluxio fs touch &lt;path&gt; #åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ alluxio fs setTtl [--action delete|free] &lt;path&gt; &lt;time to live&gt; # è®¾ç½®ä¸€ä¸ªæ–‡ä»¶çš„TTLæ—¶é—´ alluxio fs unsetTtl &lt;path&gt; # åˆ é™¤æ–‡ä»¶çš„TTLå€¼ alluxio fs checksum &lt;Alluxio path&gt; # å¾—åˆ°æ–‡ä»¶çš„MD5å€¼ alluxio fs stat &lt;path&gt; # æ˜¾ç¤ºæ–‡ä»¶è·¯å¾„ä¿¡æ¯ alluxio fs tail &lt;path&gt; # æ˜¾ç¤ºæ–‡ä»¶æœ€å1KBçš„å†…å®¹ alluxio fs location &lt;path&gt; # è¾“å‡ºåŒ…å«æŸä¸ªæ–‡ä»¶æ•°æ®çš„ä¸»æœº,ä½¿ç”¨locationå‘½ä»¤å¯ä»¥è°ƒè¯•æ•°æ®å±€éƒ¨æ€§ alluxio fs help &lt;command&gt; # æŸ¥çœ‹å‘½ä»¤ä»‹ç»å’Œç”¨æ³• alluxio fs distributedMv &lt;src&gt; &lt;dst&gt; # å¹¶è¡Œç§»åŠ¨æ–‡ä»¶æˆ–ç›®å½• alluxio fs distributedCp &lt;src&gt; &lt;dst&gt; # å¹¶è¡Œå¤åˆ¶æ–‡ä»¶æˆ–ç›®å½• alluxio fs distributedLoad [--replication &lt;num&gt;] &lt;path&gt; # åœ¨alluxioç©ºé—´ä¸­åŠ è½½æ–‡ä»¶æˆ–ç›®å½•ï¼Œä½¿å…¶é©»ç•™åœ¨å†…å­˜ä¸­ #ä¸åº•å±‚å­˜å‚¨çš„äº¤äº’æ“ä½œ alluxio fs load [--local] &lt;path&gt; # loadå‘½ä»¤å¯ä¸ºæ•°æ®åˆ†æç¼–æ’æ•°æ®,åŠ å¿«æ•°æ®åˆ†æçš„æ•ˆç‡,loadå‘½ä»¤å°†åº•å±‚æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ•°æ®è½½å…¥åˆ°Alluxioä¸­,å¦‚æœè¿è¡Œè¯¥å‘½ä»¤çš„æœºå™¨ä¸Šæ­£åœ¨è¿è¡Œä¸€ä¸ªAlluxio worker,é‚£ä¹ˆæ•°æ®å°†ç§»åŠ¨åˆ°è¯¥workerä¸Š,å¦åˆ™æ•°æ®ä¼šè¢«éšæœºç§»åŠ¨åˆ°ä¸€ä¸ªworkerä¸Šã€‚ å¦‚æœè¯¥æ–‡ä»¶å·²ç»å­˜åœ¨åœ¨Alluxioä¸­,è®¾ç½®äº†--localé€‰é¡¹,å¹¶ä¸”æœ‰æœ¬åœ°worker,åˆ™æ•°æ®å°†ç§»åŠ¨åˆ°è¯¥workerä¸Šã€‚å¦åˆ™è¯¥å‘½ä»¤ä¸è¿›è¡Œä»»ä½•æ“ä½œã€‚å¦‚æœè¯¥å‘½ä»¤çš„ç›®æ ‡æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹,é‚£ä¹ˆå…¶å­æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹ä¼šè¢«é€’å½’è½½å…¥ã€‚ alluxio fs persist [-p|--parallelism &lt;#&gt;] [-t|--timeout &lt;milliseconds&gt;] [-w|--wait &lt;milliseconds&gt;] &lt;path&gt; [&lt;path&gt; ...] # æŒä¹…åŒ–Alluxioä¸­çš„æ•°æ®åˆ°åº•å±‚å­˜å‚¨ alluxio fs checkConsistency [-r] [-t|--threads &lt;threads&gt;] &lt;Alluxio path&gt; # æ£€æŸ¥Alluxioä¸åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„å…ƒæ•°æ®ä¸€è‡´æ€§(ç¡®å®šæ–‡ä»¶åœ¨åº•å±‚å­˜å‚¨è¿˜æ˜¯åœ¨under storage system.) alluxio fs free -f &lt;&gt; # å·²ç»æŒä¹…åŒ–åˆ°åº•å±‚å­˜å‚¨,ä½†å†…å­˜ä¸­è¿˜ä¿ç•™ç€çš„æ–‡ä»¶å¯ä»¥é€šè¿‡freeä»å†…å­˜ä¸­é‡Šæ”¾,æœªè¢«æŒä¹…åŒ–çš„æ–‡ä»¶ä¸èƒ½è¢«free alluxio fs mount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; &lt;ufsURI&gt;] # å°†åº•å±‚æ–‡ä»¶ç³»ç»Ÿçš„&quot;ufsURI&quot;è·¯å¾„æŒ‚è½½åˆ°Alluxioå‘½åç©ºé—´ä¸­çš„&quot;alluxioPath&quot;è·¯å¾„ä¸‹ï¼Œ&quot;path&quot;è·¯å¾„äº‹å…ˆä¸èƒ½å­˜åœ¨å¹¶ç”±è¯¥å‘½ä»¤ç”Ÿæˆã€‚ æ²¡æœ‰ä»»ä½•æ•°æ®æˆ–è€…å…ƒæ•°æ®ä»åº•å±‚æ–‡ä»¶ç³»ç»ŸåŠ è½½ã€‚å½“æŒ‚è½½å®Œæˆåï¼Œå¯¹è¯¥æŒ‚è½½è·¯å¾„ä¸‹çš„æ“ä½œä¼šåŒæ—¶ä½œç”¨äºåº•å±‚æ–‡ä»¶ç³»ç»Ÿçš„æŒ‚è½½ç‚¹ã€‚monutå‘½ä»¤å¯ä»¥æŒ‚è½½linuxæœåŠ¡å™¨ä¸Šçš„æŸä¸ªæ–‡ä»¶å¤¹alluxio fs mount /demo file:///tmp/alluxio-demo alluxio fs unmount &lt;alluxioPath&gt; # å–æ¶ˆæŒ‚è½½ alluxio fs updateMount [--readonly] [--shared] [--option &lt;key=val&gt;] &lt;alluxioPath&gt; # ä¿ç•™å…ƒæ•°æ®çš„åŒæ—¶æ›´æ”¹æŒ‚è½½ç‚¹è®¾ç½® alluxio fs pin &lt;path&gt; media1 media2 media3 ... å¦‚æœç®¡ç†å‘˜å¯¹ä½œä¸šè¿è¡Œæµç¨‹ååˆ†æ¸…æ¥šï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨pinå‘½ä»¤æ‰‹åŠ¨æé«˜æ€§èƒ½ã€‚pinå‘½ä»¤å¯¹Alluxioä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è¿›è¡Œæ ‡è®°ã€‚è¯¥å‘½ä»¤åªé’ˆå¯¹å…ƒæ•°æ®è¿›è¡Œæ“ä½œï¼Œä¸ä¼šå¯¼è‡´ä»»ä½•æ•°æ®è¢«åŠ è½½åˆ°Alluxioä¸­ã€‚å¦‚æœä¸€ä¸ªæ–‡ä»¶åœ¨Alluxioä¸­è¢«æ ‡è®°äº†ï¼Œè¯¥æ–‡ä»¶çš„ä»»ä½•æ•°æ®å—éƒ½ä¸ä¼šä»Alluxio workerä¸­è¢«å‰”é™¤ã€‚å¦‚æœå­˜åœ¨è¿‡å¤šçš„è¢«é”å®šçš„æ–‡ä»¶ï¼ŒAlluxio workerå°†ä¼šå‰©ä½™å°‘é‡å­˜å‚¨ç©ºé—´ï¼Œä»è€Œå¯¼è‡´æ— æ³•å¯¹å…¶ä»–æ–‡ä»¶è¿›è¡Œç¼“å­˜ã€‚ alluxio fs unpin &lt;path&gt; # å°†Alluxioä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è§£é™¤æ ‡è®°ã€‚è¯¥å‘½ä»¤ä»…ä½œç”¨äºå…ƒæ•°æ®ï¼Œä¸ä¼šå‰”é™¤æˆ–è€…åˆ é™¤ä»»ä½•æ•°æ®å—ã€‚ä¸€æ—¦æ–‡ä»¶è¢«è§£é™¤é”å®šï¼ŒAlluxio workerå¯ä»¥å‰”é™¤è¯¥æ–‡ä»¶çš„æ•°æ®å—ã€‚ alluxio fs startSync &lt;path&gt; # å¯åŠ¨æŒ‡å®šè·¯å¾„çš„è‡ªåŠ¨åŒæ­¥è¿›ç¨‹ alluxio fs stopSync &lt;path&gt; # å…³é—­æŒ‡å®šè·¯å¾„çš„è‡ªåŠ¨åŒæ­¥è¿›ç¨‹ alluxio fs setReplication [-R] [--max &lt;num&gt; | --min &lt;num&gt;] &lt;path&gt; # è®¾ç½®ç»™å®šè·¯å¾„æˆ–æ–‡ä»¶çš„æœ€å¤§/æœ€å°å‰¯æœ¬æ•° (-1è¡¨ç¤ºä¸é™åˆ¶æœ€å¤§å‰¯æœ¬æ•°) -Ré€’å½’ #æƒé™ç›¸å…³æ“ä½œåŠç®¡ç†å‘˜å‘½ä»¤ alluxio fs chgrp [-R] &lt;group&gt; &lt;path&gt; # æ¢ç»„ alluxio fs chmod [-R] &lt;mode&gt; &lt;path&gt; # æ›´æ”¹è¯»å†™æ‰§è¡Œç­‰æƒé™ alluxio fs chown [-R] &lt;owner&gt;[:&lt;group&gt;] &lt;path&gt; # æ‰€æœ‰è€… alluxio fsadmin backup [directory] [--local] # å¤‡ä»½Alluxioçš„å…ƒæ•°æ®åˆ°å¤‡ä»½ç›®å½•(é»˜è®¤ç›®å½•ç”±alluxio.master.backup.directoryå†³å®š) alluxio fsadmin doctor [category] # æ˜¾ç¤ºé”™è¯¯å’Œè­¦å‘Š alluxio fsadmin report [category] [category args] # æŠ¥å‘Šè¿è¡Œé›†ç¾¤çš„ä¿¡æ¯ alluxio fsadmin ufs --mode &lt;noAccess/readOnly/readWrite&gt; &quot;ufsPath&quot; # æ›´æ–°æŒ‚è½½çš„åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„å±æ€§ alluxio formatMaster åˆå§‹åŒ–Masterå…ƒæ•°æ® alluxio formatWorker åˆå§‹åŒ–Workeræ•°æ®ï¼ŒWorkeræ•°æ®ä¼šè¢«æ¸…ç©º alluxio getConf [key] æŸ¥çœ‹å„ä¸ªç»„ä»¶çš„å‚æ•°å’Œé…ç½® key:[--master / --source] alluxio runJournalCrashTest æµ‹è¯•Alluxio é«˜å¯ç”¨æ—¥å¿—ç³»ç»Ÿï¼ˆä¼šåœæ­¢æœåŠ¡ä¸€æ®µæ—¶é—´ï¼‰ alluxio runUfsTests --path &lt;ufs path&gt; alluxio validateConf ä½¿ä¿®æ”¹çš„é…ç½®ç”Ÿæ•ˆ alluxio validateEnv &lt;args&gt; ä½¿è¿è¡Œç¯å¢ƒç”Ÿæ•ˆ alluxio copyDir &lt;PATH&gt; ç±»ä¼¼äºxsyncè„šæœ¬ï¼Œå¯ä»¥å‘å„ä¸ªèŠ‚ç‚¹åˆ†å‘æ–‡ä»¶ #é›†ç¾¤ç›¸å…³ä¿¡æ¯ alluxio fs masterInfo # è·å¾—masterèŠ‚ç‚¹çš„ä¿¡æ¯ alluxio fs leader # æ‰“å°å½“å‰Alluxioçš„leader masterèŠ‚ç‚¹ä¸»æœºåã€‚ alluxio fs getCapacityBytes # è·å–Alluxioæ€»å®¹é‡ alluxio fs getSyncPathList # è·å–åŒæ­¥è·¯å¾„åˆ—è¡¨ alluxio fs getUsedBytes # è·å–å·²ç”¨ç©ºé—´å¤§å° alluxio fs getfacl &lt;path&gt; # æ˜¾ç¤ºè®¿é—®æ§åˆ¶åˆ—è¡¨(ACLs) alluxio fs setfacl [-d] [-R] [--set | -m | -x &lt;acl_entries&gt; &lt;path&gt;] | [-b | -k &lt;path&gt;] # è®¾ç½®è®¿é—®æ§åˆ¶åˆ—è¡¨(ACLs) ä¸Šé¢çš„å‘½ä»¤ä¸èƒ½å¸®åˆ°ä½ ? é‚£å°±æˆ³è¿™é‡Œ:Alluxioå‘½ä»¤ä½¿ç”¨ç¤ºä¾‹ç®¡ç†å‘˜å‘½ä»¤ä½¿ç”¨ç¤ºä¾‹ Alluxio WEB UIä»‹ç»åŠä½¿ç”¨Alluxio masteræä¾›äº†Webç•Œé¢ä»¥ä¾¿ç”¨æˆ·ç®¡ç†Alluxio master Webç•Œé¢çš„é»˜è®¤ç«¯å£æ˜¯19999:è®¿é—® http://MASTER IP:19999 å³å¯æŸ¥çœ‹Alluxio worker Webç•Œé¢çš„é»˜è®¤ç«¯å£æ˜¯30000:è®¿é—® http://WORKER IP:30000 å³å¯æŸ¥çœ‹WEB UIå®˜æ–¹ä»‹ç»å¾ˆæ˜ç¡®-&gt;æˆ³è¿™é‡Œ:Alluxio Web UI Alluxioä¸è®¡ç®—æ¡†æ¶æ•´åˆ Alluxio+Hiveé¢‘ç¹ä½¿ç”¨çš„è¡¨å­˜åœ¨Alluxioä¸Šï¼Œå¯é€šè¿‡å†…å­˜è¯»æ–‡ä»¶è·å¾—æ›´é«˜çš„ååé‡å’Œæ›´ä½çš„å»¶è¿Ÿ å‡†å¤‡å·¥ä½œ: cd /opt/module/hive vim conf/hive-env.sh export HADOOP_HOME=/opt/module/hadoop-2.7.2 # æ·»åŠ  export HIVE_AUX_JARS_PATH=$ALLUXIO_HOME/client:$HIVE_AUX_JARS_PATH å››ç§æƒ…å†µ: åˆ›å»ºä¸€ä¸ªHiveè¡¨å¹¶æŒ‡å®šå…¶å­˜å‚¨åœ¨Alluxio bin/hive create table alluxio_test( id int, name string, color string ) row format delimited fields terminated by &#39;\\t&#39; LOCATION &quot;alluxio://hadoop101:19998/user/hive/warehouse/alluxio_test&quot;; # æŸ¥çœ‹è¡¨ä½ç½® describe extended alluxio_test; å·²å­˜åœ¨HDFSçš„å†…éƒ¨è¡¨ bin/hive describe extended table_name; # æŸ¥çœ‹Hiveè¡¨å­˜å‚¨ä½ç½® alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; msck repair table table_name; # ç¡®å®šalluxioå¯¹åº”ä½ç½®å­˜åœ¨è¡¨æ•°æ®åä¿®å¤Hiveè¡¨å…ƒæ•°æ® ç¬¬ä¸€æ¬¡è®¿é—®alluxioä¸­çš„æ–‡ä»¶é»˜è®¤ä¼šè¢«è®¤ä¸ºè®¿é—®hdfsçš„æ–‡ä»¶ï¼Œä¸€æ—¦æ•°æ®è¢«ç¼“å­˜åœ¨Alluxioä¸­ï¼Œä¹‹åçš„æŸ¥è¯¢æ•°æ®éƒ½ä¼šä»Alluxioè¯»å–ã€‚ å·²å­˜åœ¨HDFSçš„å¤–éƒ¨è¡¨ bin/hive describe extended table_name; # å°†è¡¨æ•°æ®æ”¹ä¸ºAlluxioå­˜å‚¨ alter table table_name set location &quot;alluxio://hadoop101:19998/user/hive/warehouse/table_name&quot; describe extended table_name; # è¿˜åŸè¡¨æ•°æ®åˆ°HDFS alter table table_name set location &quot;hdfs://hadoop101:9000/user/hive/warehouse/table_name&quot; describe extended table_name; Hiveä½¿ç”¨Alluxioä½œä¸ºé»˜è®¤å­˜å‚¨ç³»ç»Ÿ vim conf/hive-site.xml # æ·»åŠ ä»¥ä¸‹å±æ€§ &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;alluxio://hadoop101:19998&lt;/value&gt; &lt;description&gt;Hive Use Alluxio As Default FileSystem&lt;/description&gt; &lt;/property&gt; # å¯¹HiveæŒ‡å®šçš„Alluxioé…ç½®å±æ€§ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°æ¯ä¸ªç»“ç‚¹çš„Hadoopé…ç½®ç›®å½•ä¸‹core-site.xmlä¸­ã€‚ä¾‹å¦‚ï¼Œå°†alluxio.user.file.writetype.default å±æ€§ç”±é»˜è®¤çš„MUST_CACHEä¿®æ”¹æˆCACHE_THROUGH: &lt;property&gt; &lt;name&gt;alluxio.user.file.writetype.default&lt;/name&gt; &lt;value&gt;CACHE_THROUGH&lt;/value&gt; &lt;/property&gt; # Alluxioä¸­ä¸ºHiveåˆ›å»ºç›®å½• alluxio fs mkdir /tmp alluxio fs mkdir /user/hive/warehouse alluxio fs chmod 775 /tmp alluxio fs chmod 775 /user/hive/warehouse # æ£€æŸ¥Hiveä¸Alluxioçš„é›†æˆæƒ…å†µ integration/checker/bin/alluxio-checker.sh -h # æŸ¥çœ‹è¯¥å‘½ä»¤å¸®åŠ© integration/checker/bin/alluxio-checker.sh hive -hiveurl [HIVE_URL] æ³¨:CMé›†ç¾¤è®¾ç½®Hiveè¿æ¥Alluxio Clientçš„æ–¹å¼: æ’å‘: å®‰å…¨è®¤è¯é—®é¢˜: alluxio-site.propertiesä¸­æ·»åŠ è¦æ¨¡æ‹Ÿçš„ç”¨æˆ·: alluxio.master.security.impersonation.hive.users=* alluxio.master.security.impersonation.hive.groups=* alluxio.master.security.impersonation.yarn.users=* alluxio.master.security.impersonation.yarn.groups=* Alluxio+SparkSparkå¯ä»¥åœ¨è¿›è¡Œç®€å•é…ç½®åç›´æ¥ä½¿ç”¨Alluxioä½œä¸ºæ•°æ®è®¿é—®å±‚ï¼ŒSparkåº”ç”¨ç¨‹åºå¯ä»¥é€šè¿‡Alluxioé€æ˜åœ°è®¿é—®è®¸å¤šä¸åŒç±»å‹çš„æŒä¹…åŒ–å­˜å‚¨æœåŠ¡ï¼ˆä¾‹å¦‚ï¼ŒAWS S3 bucketã€Azure Object Store bucketsã€è¿œç¨‹éƒ¨ç½²çš„ HDFS ç­‰ï¼‰çš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥é€æ˜åœ°è®¿é—®åŒä¸€ç±»å‹æŒä¹…åŒ–å­˜å‚¨æœåŠ¡ä¸åŒå®ä¾‹ä¸­çš„æ•°æ®ã€‚ä¸ºäº†åŠ å¿«I/Oæ€§èƒ½ï¼Œç”¨æˆ·å¯ä»¥ä¸»åŠ¨è·å–æ•°æ®åˆ°Alluxioä¸­æˆ–å°†æ•°æ®é€æ˜åœ°ç¼“å­˜åˆ°Alluxioä¸­ï¼Œå°¤å…¶æ˜¯åœ¨Sparkéƒ¨ç½²ä½ç½®ä¸æ•°æ®ç›¸è·è¾ƒè¿œæ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è®¡ç®—å’Œç‰©ç†å­˜å‚¨è§£è€¦ï¼ŒAlluxio èƒ½å¤Ÿæœ‰åŠ©äºç®€åŒ–ç³»ç»Ÿæ¶æ„ã€‚å½“åº•å±‚æŒä¹…åŒ–å­˜å‚¨ä¸­çœŸå®æ•°æ®çš„è·¯å¾„å¯¹ Spark éšè—æ—¶ï¼Œå¯¹åº•å±‚å­˜å‚¨çš„æ›´æ”¹å¯ä»¥ç‹¬ç«‹äºåº”ç”¨ç¨‹åºé€»è¾‘ï¼›åŒæ—¶Alluxioä½œä¸ºé‚»è¿‘è®¡ç®—çš„ç¼“å­˜ï¼Œä»ç„¶å¯ä»¥ç»™è®¡ç®—æ¡†æ¶æä¾›ç±»ä¼¼ Spark æ•°æ®æœ¬åœ°æ€§çš„ç‰¹æ€§ã€‚ é…ç½®å‚æ•°é…ç½®ï¼ˆspark-defaults.confä¸­æ·»åŠ ï¼‰spark.driver.extraClassPath //client/alluxio-2.0.1-client.jarspark.executor.extraClassPath //client/alluxio-2.0.1-client.jaræˆ–è€…JaråŒ…æ‹·è´cp client/alluxio-2.0.1-client.jar $SPARK_HOME/jars/å¦‚æœé«˜å¯ç”¨çš„Alluxio,è¿˜éœ€åœ¨spark-defaultä¸­æŒ‡å®š: spark.driver.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true spark.executor.extraJavaOptions -Dalluxio.zookeeper.address=zkHost1:2181,zkHost2:2181,zkHost3:2181 -Dalluxio.zookeeper.enabled=true æˆ–è€…é…ç½®Hadoopæ–‡ä»¶core-site.xmlå¦‚ä¸‹ &lt;configuration&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;alluxio.zookeeper.address&lt;/name&gt; &lt;value&gt;zkHost1:2181,zkHost2:2181,zkHost3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; è‡ªå®šä¹‰Sparkä½œä¸šä¸­Alluxioçš„å±æ€§ï¼šspark-submitâ€¦. â€“driver-java-options â€œ-Dalluxio.user.file.writetype.default=CACHE_THROUGHâ€ è€Œä¸æ˜¯â€“conf val s = sc.textFile(&quot;alluxio://192.168.1.101:19998/LICENSE&quot;) val double = s.map(line =&gt; line + line) double.saveAsTextFile(&quot;alluxio://192.168.1.101:19998/out&quot;) df = spark.table(&quot;select ...&quot;) df.format.parquet(&quot;alluxio://xxxxx&quot;) å®˜æ–¹Alluxio+Sparké…ç½®è®¾ç½® æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åœ¨$ALLUXIO_HOMEè¿è¡Œ integration/checker/bin/alluxio-checker.sh spark spark://sparkMaster:7077 ä½¿ç”¨ å­˜å‚¨ RDD åˆ° Alluxio å†…å­˜ä¸­å°±æ˜¯å°† RDD ä½œä¸ºæ–‡ä»¶ä¿å­˜åˆ° Alluxio ä¸­: saveAsTextFileï¼šå°† RDD ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å†™å…¥ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯æ–‡ä»¶ä¸­çš„ä¸€è¡Œ saveAsObjectFileï¼šé€šè¿‡å¯¹æ¯ä¸ªå…ƒç´ ä½¿ç”¨ Java åºåˆ—åŒ–ï¼Œå°† RDD å†™åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ // as text file rdd.saveAsTextFile(&quot;alluxio://localhost:19998/rdd1&quot;) rdd = sc.textFile(&quot;alluxio://localhost:19998/rdd1&quot;) // as object file rdd.saveAsObjectFile(&quot;alluxio://localhost:19998/rdd2&quot;) rdd = sc.objectFile(&quot;alluxio://localhost:19998/rdd2&quot;) ç¼“å­˜ Dataframe åˆ° Alluxio ä¸­(å°† DataFrame ä½œä¸ºæ–‡ä»¶ä¿å­˜åˆ° Alluxio ä¸­): df.write.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) df = sqlContext.read.parquet(&quot;alluxio://localhost:19998/data.parquet&quot;) Alluxioå¯¹Shuffleçš„æå‡ç›®å‰ä¸‰ç§æ–¹æ¡ˆ:ä¸€æ˜¯åŸºäºAlluxio-Fuseå®¢æˆ·ç«¯,æ— éœ€ä¿®æ”¹æºç ,ç›´æ¥æŒ‚è½½Shuffleç›®å½•,ä½†Alluxio-Fuseç›®å‰çš„æ€§èƒ½ä¸æ˜¯å¾ˆå¥½äºŒæ˜¯é‡å†™Spark Shuffle Serviceåº•å±‚æºç å®ç°åŸºäºAlluxio Clientçš„Shuffleä¸‰æ˜¯å¯ä»¥Splash Shuffle Manageræ’ä»¶,æˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« æœ‰è®²åˆ° -&gt; QConæ€»ç»“-Splash Shuffle Manager å½“ç„¶ä¹Ÿå¯ä»¥é€‰æ‹©ç­‰Spark3.0çš„Remote Shuffle Service Alluxio+HadoopMRè¿è¡ŒHadoopMRç¨‹åº: bin/hadoop jar ../libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount -Dalluxio.user.file.writetype.default=CACHE_THROUGH -libjars /opt/module/alluxio/client/alluxio-2.0.1-client.jar \\&lt;INPUT FILES&gt; &lt;OUTPUT DIRECTORY&gt; Alluxio+Prestoåç»­æ›´æ–°â€¦ æ€§èƒ½æµ‹è¯•ä½¿ç”¨å®˜æ–¹æä¾›çš„æ²™ç®±ç”³è¯·å®˜æ–¹æµ‹è¯•æ²™ç®±Sandboxï¼š**ALLUXIO SANDBOX**ç”³è¯·æˆåŠŸåï¼ŒæŒ‰ç…§é‚®ä»¶çš„æŒ‡å¼•æ“ä½œï¼Œæ³¨æ„ï¼Œbin/sandbox setup &amp;çš„è¿‡ç¨‹ä¸­åƒä¸‡ä¸è¦Ctrl+Cä¸­æ­¢,éƒ¨ç½²å®ŒæˆçŠ¶æ€å¦‚ä¸‹å›¾ï¼š è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆTPC-DSï¼‰ï¼Œè€å¿ƒç­‰å¾…åçš„æµ‹è¯•ç»“æœï¼šå·²å®‰è£…TPC-DSåŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¿è¡Œæ€§èƒ½æµ‹è¯•ã€‚Sparkå·²å®‰è£…ä¸ºTPC-DSç”¨æ¥å°†å…¶ä½œä¸šå‘é€åˆ°çš„è®¡ç®—æ¡†æ¶ã€‚TPC-DSçš„æ¯”ä¾‹å› å­ä¸º100ï¼Œè¿™ä¸26GBçš„æ•°æ®é›†å¤§å°ç›¸å…³ã€‚ç”±ç´¢å¼•å•ç‹¬æ ‡è¯†çš„åŸºå‡†æŒ‰ä¸åŒçš„ä½¿ç”¨æ–¹æ¡ˆåˆ†ç»„ï¼Œå¹¶ä¸”å°†ç»“æœæŠ¥å‘Šä¸ºæ¯ä¸ªæ–¹æ¡ˆçš„æ±‡æ€»ã€‚å…¶ä¸­ w/oæ˜¯withoutï¼Œå³åªæ˜¯ç”¨S3ä¸ºç›´æ¥åº•å±‚å­˜å‚¨çš„æƒ…å†µï¼›w/æ˜¯withï¼Œå³ä½¿ç”¨äº†Alluxioä½œä¸ºä¸­é—´ä»¶ä¸‹çš„æ€§èƒ½ä»å›¾ä¸­æµ‹è¯•ç»“æœå¯ä»¥çœ‹å‡º,å½“è®¡ç®—æ•°æ®å­˜å‚¨åœ¨å…¬æœ‰äº‘è™šæ‹Ÿæœºå®ä¾‹ä¸­æ—¶ï¼ŒAlluxioä½œä¸ºå­˜å‚¨ä¸è®¡ç®—æ¡†æ¶çš„ä¸­é—´ä»¶ï¼Œèƒ½å¤Ÿæœ‰1.5-3å€å·¦å³çš„æ€§èƒ½æå‡å—åˆ°å„æ–¹é¢é™åˆ¶ï¼Œä»¥ä¸Šæµ‹è¯•ç»“æœå¹¶éAlluxioçš„æœ€ä½³é¢„æœŸã€‚å…¶ä»–äººçš„è¯•è¿‡ç¨‹ è‡ªæµ‹Spark Sqlåšæµ‹è¯•æ—¶å€™å¤šæ¬¡é‡å¤ä½œä¸šè¾“å…¥æ•°æ®ä½äºOSçš„é«˜é€Ÿç¼“å†²åŒº,Alluxioæ²¡æœ‰åŠ é€Ÿæ•ˆæœç”šè‡³å˜æ…¢æˆ‘çš„æµ‹è¯•ç¯å¢ƒæ˜¯ä¸‰å°æœºå™¨,æ¯å°101GBå†…å­˜,16æ ¸,åŒå°æœºå™¨éƒ¨ç½²CM Hadoop,Spark,Hive,AlluxioWorker,AlluxioClientAlluxioè¯»å‚æ•°CACHE_PROMOTE,å†™å‚æ•°CACHE_THROUGH æµ‹è¯•æ–¹æ³• æµ‹è¯•æ“ä½œ è¿è¡Œæ—¶é—´(HDFS) è¿è¡Œæ—¶é—´(Alluxio) è¡¨ç»“æ„ SparkSQL select count(1) from table; 4s 6s 13.5GB 17å­—æ®µ SparkSQL select count(1) from table; 5s 6s 13.5GB 17å­—æ®µ SparkSQL select count(1) from table; 6s 8s 13.5GB 17å­—æ®µ SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 80s 80s 13.5GB 17å­—æ®µ SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 77s 52s 13.5GB 17å­—æ®µ SparkSQL select first(ip),first(language),first(operation_channel),first(imei) from table group by product_name; 60s 73s 13.5GB 17å­—æ®µ SparkSQL select count(1) from test.table group by language; 11.5s 11.5s 13.5GB 17å­—æ®µ Spark Persist df.write.parquet(Path) 3.0min 4.0min 13.5GB 17å­—æ®µ Spark Persist spark.read.parquet(Path).count() 4s 5s 13.5GB 17å­—æ®µ Spark Persist spark.read.parquet(Path).count() 6s 6s 13.5GB 17å­—æ®µ åæ¥åˆåšäº†Spark Dataframeçš„Persiståˆ°MEMORY_ONLYå’ŒPersiståˆ°Alluxio,æ•ˆæœä¹Ÿä¸æ˜¯å¾ˆå¥½,ç©¶å…¶åŸå› ,æˆ‘è®¤ä¸ºæ˜¯æˆ‘çš„HDFS DataNodeå·²ç»å’Œè®¡ç®—æ¡†æ¶Sparkéƒ¨ç½²åœ¨ä¸€èµ·äº†,è€Œä¸”ç£ç›˜IOæ²¡æœ‰ç“¶é¢ˆ,æ‰€ä»¥è¿™ä¸ç¬¦åˆAlluxioçš„åº”ç”¨åœºæ™¯,ä»è€Œæ²¡æœ‰ä»¤äººæ»¡æ„çš„æ•ˆæœ.è‡³äºHDFSæ›´å¿«çš„åŸå› ,æˆ‘æƒ³æ˜¯Sparkè¦è¯»å–çš„æ•°æ®å¾ˆå¯èƒ½å·²ç»å­˜åœ¨OSçš„é«˜é€Ÿç¼“å†²åŒºAlluxioè¿˜æ˜¯è¦ç”¨å¯¹åœºæ™¯æ‰è¡Œ. Alluxio FUSEä»€ä¹ˆæ˜¯Alluxio FUSEAlluxio-FUSEå¯ä»¥åœ¨ä¸€å°Unixæœºå™¨ä¸Šçš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­æŒ‚è½½ä¸€ä¸ªAlluxioåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿã€‚é€šè¿‡ä½¿ç”¨è¯¥ç‰¹æ€§ï¼Œä¸€äº›æ ‡å‡†çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆä¾‹å¦‚lsã€ catä»¥åŠechoï¼‰å¯ä»¥ç›´æ¥è®¿é—®Alluxioåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ•°æ®ã€‚æ­¤å¤–æ›´é‡è¦çš„æ˜¯ç”¨ä¸åŒè¯­è¨€å®ç°çš„åº”ç”¨ç¨‹åºå¦‚C, C++, Python, Ruby, Perl, Javaéƒ½å¯ä»¥é€šè¿‡æ ‡å‡†çš„POSIXæ¥å£(ä¾‹å¦‚open, write, read)æ¥è¯»å†™Alluxioï¼Œè€Œä¸éœ€è¦ä»»ä½•Alluxioçš„å®¢æˆ·ç«¯æ•´åˆä¸è®¾ç½®ã€‚ Alluxio FUSEå±€é™æ€§ æ–‡ä»¶åªèƒ½é¡ºåºåœ°ä¸€æ¬¡å†™å…¥,ä¸èƒ½ä¿®æ”¹å’Œè¦†ç›–,å¦‚æœè¦ä¿®æ”¹å°±è¦åˆ é™¤åŸæ–‡ä»¶å†åˆ›å»º ä¸æ”¯æŒsoft-linkå’Œhard-link(å³ln) alluxio.security.group.mapping.classé€‰é¡¹è®¾ç½®ä¸ºShellBasedUnixGroupsMappingçš„å€¼æ—¶,ç”¨æˆ·ä¸åˆ†ç»„ä¿¡æ¯æ‰ä¸Unixç³»ç»Ÿçš„ç”¨æˆ·åˆ†ç»„å¯¹åº” ä¸ç›´æ¥ä½¿ç”¨Alluxioå®¢æˆ·ç«¯ç›¸æ¯”ï¼Œä½¿ç”¨æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿçš„æ€§èƒ½ä¼šç›¸å¯¹è¾ƒå·® Alluxio FUSEä½¿ç”¨ æŒ‚è½½ æŒ‚è½½alluxio_pathåˆ°æœ¬åœ°mount_point,mount_pointå¿…é¡»æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­çš„ä¸€ä¸ªç©ºæ–‡ä»¶å¤¹ï¼Œå¹¶ä¸”å¯åŠ¨Alluxio-FUSEè¿›ç¨‹çš„ç”¨æˆ·æ‹¥æœ‰è¯¥æŒ‚è½½ç‚¹åŠå¯¹å…¶çš„è¯»å†™æƒé™ã€‚å¯ä»¥å¤šæ¬¡è°ƒç”¨è¯¥å‘½ä»¤æ¥å°†AlluxioæŒ‚è½½åˆ°ä¸åŒçš„æœ¬åœ°ç›®å½•ä¸‹ã€‚æ‰€æœ‰çš„Alluxio-FUSEä¼šå…±äº«$ALLUXIO_HOME\\logs\\fuse.logè¿™ä¸ªæ—¥å¿—æ–‡ä»¶ã€‚ integration/fuse/bin/alluxio-fuse mount mount_point [alluxio_path] å¸è½½ integration/fuse/bin/alluxio-fuse umount mount_point æ£€æŸ¥æŒ‚è½½ç‚¹è¿è¡Œä¿¡æ¯ integration/fuse/bin/alluxio-fuse stat æ³¨æ„äº‹é¡¹è¦ä½¿ç”¨å¯åŠ¨masterå’Œworkerçš„ç”¨æˆ·æ¥æŒ‚è½½fuseï¼Œæ¯”å¦‚ä½¿ç”¨hdfsç”¨æˆ·å¯åŠ¨çš„Alluxioï¼Œåˆ™è¦ç”¨hdfsæ¥æŒ‚è½½ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¦‚æœä½¿ç”¨rootç”¨æˆ·æŒ‚è½½ï¼Œç›®å½•ä¿¡æ¯ä¼šä¹±ç ä¸”æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚hdfsç”¨æˆ·ä¸‹æˆåŠŸmountåï¼Œåˆ‡æ¢åˆ°rootç”¨æˆ·ä¹Ÿä¼šçœ‹åˆ°æŒ‚è½½ç‚¹ä¿¡æ¯ä¹±ç ã€‚Alluxioç›¸å…³æœåŠ¡æœªå¯åŠ¨ï¼ŒæŒ‚è½½ç‚¹ä¿¡æ¯ä¹Ÿä¼šä¹±ç ã€‚Alluxioé»˜è®¤åªèƒ½å†™æœ¬åœ°workerï¼Œå¦‚æœæ˜ç¡®çŸ¥é“è¦å†™å…¥çš„æ–‡ä»¶å¤§å°çš„èŒƒå›´ï¼Œå¯ä»¥ä½¿ç”¨ASYNC_THROUGHå¹¶åŠ å¤§workerçš„ç¼“å­˜å¤§å°ï¼Œæˆ–è€…é…ç½®å¤šçº§ç¼“å­˜ä½¿workerçš„ç¼“å­˜ç©ºé—´å¤§äºå†™å…¥æ–‡ä»¶çš„å¤§å°ï¼Œæ‰èƒ½é˜²æ­¢è¢«ç½®æ¢ï¼Œä»è€Œæé«˜æ•ˆç‡å¦‚æœä¸ç¡®å®šå†™çš„æ–‡ä»¶å¤§å°çš„èŒƒå›´ï¼Œå°±ä¸è¦ä½¿ç”¨ASYNC_THROUGHè¿™ä¸ªå‚æ•°ï¼Œå› ä¸ºå¦‚æœæœ¬åœ°Workerç¼“å­˜ç©ºé—´ä¸å¤Ÿå°±ä¼šå†™å…¥å¤±è´¥ï¼Œè¿™æ—¶ï¼Œä¸ºäº†ä¿é™©èµ·è§å¯ä»¥ä½¿ç”¨å†™å‚æ•°CACHE_THROUGHè¾¹ç¼“å­˜è¾¹å†™æˆ–å†™å‚æ•°THROUGHåªå†™åº•å±‚å­˜å‚¨ï¼Œæ¥é˜²æ­¢å†™å…¥æ–‡ä»¶å¤±è´¥ã€‚å½“ç„¶ï¼Œè¿˜æœ‰ä¸€ç§æ¯”è¾ƒå¥½çš„æ–¹æ¡ˆï¼Œå†™å‚æ•°è®¾ä¸ºASYNC_THROUGHé…åˆæ›´å¤§çš„Workerç¼“å­˜æ¥æé«˜æ•ˆç‡ï¼ŒåŒæ—¶è®¾ç½®alluxio.user.file.write.location.policy.class=alluxio.client.file.policy.RoundRobinPolicyå‚æ•°æ¥ä¿è¯å†™å…¥ä¸ä¼šå¤±è´¥ã€‚å¦‚æœåªå†™ä¸€æ¬¡ï¼Œå¯ä»¥åŠæ—¶freeæ‰æ— ç”¨çš„ç¼“å­˜ï¼Œå‡å°‘åé¢å†™æ•°æ®æ—¶å‘ç”Ÿçš„ç¼“å­˜ç½®æ¢ã€‚ Alluxio å®¢æˆ·ç«¯APIJava APIAlluxioæä¾›äº†ä¸¤ç§ä¸åŒçš„æ–‡ä»¶ç³»ç»ŸAPIï¼šAlluxio APIå’Œä¸Hadoopå…¼å®¹çš„API,Alluxio APIæä¾›äº†æ›´å¤šåŠŸèƒ½ï¼Œè€ŒHadoopå…¼å®¹APIä¸ºç”¨æˆ·æä¾›äº†ä½¿ç”¨Alluxioçš„çµæ´»æ€§ï¼Œæ— éœ€ä¿®æ”¹ä½¿ç”¨Hadoop APIç¼–å†™çš„ç°æœ‰ä»£ç .Mavené¡¹ç›®ä¾èµ–è®¾ç½® pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.my.alluxio&lt;/groupId&gt; &lt;artifactId&gt;AlluxioTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- alluxio-fs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.alluxio&lt;/groupId&gt; &lt;artifactId&gt;alluxio-core-client-fs&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hdfs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;7&lt;/source&gt; &lt;target&gt;7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- æ‰“jaræ’ä»¶ --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--JaråŒ…è¿è¡Œæ—¶çš„ä¸»ç±»--&gt; &lt;mainClass&gt;IOTestUtil&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Javaè¯»å†™æ–‡ä»¶API import alluxio.AlluxioURI; import alluxio.client.file.FileInStream; import alluxio.client.file.FileOutStream; import alluxio.exception.AlluxioException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import java.io.IOException; /** * HDFS &amp; Allxuio IOè¯»å–æ–‡ä»¶æµ‹è¯•å·¥å…· IOæ¥å£ æ–‡ä»¶API */ public class IOTestUtil &#123; public static void main(String[] args) throws IOException, AlluxioException &#123; String filePath = args[0]; HDFSUtil h = new HDFSUtil(&quot;hdfs://192.168.1.101:8020&quot;); h.readFile(filePath); AlluxioUtil a = new AlluxioUtil(); a.readFile(filePath); System.out.println(&quot;è¯»æ–‡ä»¶æµ‹è¯• Finished&quot;); System.out.println(&quot;------------------------&quot;); if (args.length != 1)&#123; String fileToWritePath = args[1]; a.writeFile(fileToWritePath); System.out.println(&quot;å†™æ–‡ä»¶æµ‹è¯• Finished&quot;); &#125; &#125; &#125; class HDFSUtil&#123; private Configuration conf = new Configuration(); public HDFSUtil(String HDFSURL)&#123; conf.set(&quot;fs.defaultFS&quot;,HDFSURL); System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;hdfs&quot;); &#125; public void readFile(String path) throws IOException &#123; FileSystem fs = FileSystem.get(conf); fs.getFileStatus(new Path(path)); FSDataInputStream in = fs.open(new Path(path)); try&#123; long hdfsStartTime=System.currentTimeMillis(); in = fs.open(new Path(path)); byte[] buffer = new byte[1024]; int byteRead = 0; while ((byteRead = in.read(buffer)) != -1) &#123; System.out.write(buffer, 0, byteRead); //è¾“å‡ºå­—ç¬¦æµ &#125; long hdfsEndTime=System.currentTimeMillis(); System.out.println(&quot;HDFSè¯»å–è¿è¡Œæ—¶é—´:&quot;+(hdfsEndTime-hdfsStartTime)+&quot; ms&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; finally &#123; in.close(); &#125; &#125; &#125; class AlluxioUtil&#123; private static final alluxio.client.file.FileSystem fs = alluxio.client.file.FileSystem.Factory.get(); public AlluxioUtil()&#123;&#125; public FileInStream readFile(String AlluxioPath) throws IOException, AlluxioException &#123; AlluxioURI path = new AlluxioURI(AlluxioPath); //å°è£…Alluxio æ–‡ä»¶è·¯å¾„çš„path FileInStream in = fs.openFile(path); try&#123; long startTime=System.currentTimeMillis(); in = fs.openFile(path); // è°ƒç”¨æ–‡ä»¶è¾“å…¥æµFileInStreamå®ä¾‹çš„read()æ–¹æ³•è¯»æ•°æ® byte[] buffer = new byte[1024]; int byteRead = 0; // è¯»å…¥å¤šä¸ªå­—èŠ‚åˆ°å­—èŠ‚æ•°ç»„ä¸­ï¼ŒbyteReadä¸ºä¸€æ¬¡è¯»å…¥çš„å­—èŠ‚æ•° while ((byteRead = in.read(buffer)) != -1) &#123; System.out.write(buffer, 0, byteRead); //è¾“å‡ºå­—ç¬¦æµ &#125; long endTime=System.currentTimeMillis(); System.out.println(&quot;Alluxioè¯»å–è¿è¡Œæ—¶é—´:&quot;+(endTime-startTime)+&quot; ms&quot;); &#125;catch (IOException | AlluxioException e)&#123; e.printStackTrace(); &#125;finally &#123; in.close(); &#125; in.close(); //å…³é—­æ–‡ä»¶å¹¶é‡Šæ”¾é” return in; &#125; public void writeFile(String AlluxioPath) throws IOException, AlluxioException &#123; AlluxioURI path = new AlluxioURI(AlluxioPath); // æ–‡ä»¶å¤¹è·¯å¾„ FileOutStream out = null; try &#123; out = fs.createFile(path); //åˆ›å»ºæ–‡ä»¶å¹¶å¾—åˆ°æ–‡ä»¶è¾“å…¥æµ out.write(&quot;qjj1234567&quot;.getBytes()); // è°ƒç”¨æ–‡ä»¶è¾“å‡ºæµFileOutStreamå®ä¾‹çš„write()æ–¹æ³•å†™å…¥æ•°æ® &#125;catch (IOException | AlluxioException e)&#123; e.printStackTrace(); &#125;finally &#123; out.close(); // å…³é—­å’Œé‡Šæ”¾æ–‡ä»¶ &#125; &#125; &#125; Python APIAlluxioçš„Pythonåº“åŸºäºREST APIå®ç°çš„CentOS6å’ŒWindowsçš„ç¯å¢ƒä¸‹å®‰è£…alluxioçš„pythonåº“å¤±è´¥ï¼Œæœ€ç»ˆåœ¨CentOS7 Python2.7.5çš„ç¯å¢ƒä¸‹æˆåŠŸæ‰§è¡Œäº†**pip install alluxio** if __name__ == &#39;__main__&#39;: print(&quot;åç»­ç”¨åˆ°APIå†æ›´æ–°&quot;) pass Q&amp;A åŠ é€Ÿä¸æ˜æ˜¾? Alluxioé€šè¿‡ä½¿ç”¨åˆ†å¸ƒå¼çš„å†…å­˜å­˜å‚¨ä»¥åŠåˆ†å±‚å­˜å‚¨,å’Œæ—¶é—´æˆ–ç©ºé—´çš„æœ¬åœ°åŒ–æ¥å®ç°æ€§èƒ½åŠ é€Ÿã€‚å¦‚æœæ•°æ®é›†æ²¡æœ‰ä»»ä½•æœ¬åœ°åŒ–, æ€§èƒ½åŠ é€Ÿæ•ˆæœå¹¶ä¸æ˜æ˜¾ã€‚ é€Ÿåº¦åè€Œæ›´æ…¢äº†? æµ‹è¯•æ—¶å°½é‡å¤šè§‚å¯Ÿé›†ç¾¤çš„CPUå ç”¨ç‡,Yarnå†…å­˜åˆ†é…å’Œç½‘ç»œIOç­‰å¤šç§å› ç´ ,å¯èƒ½ç“¶é¢ˆä¸åœ¨è¯»å–æ•°æ®çš„IOä¸Šã€‚ ç¡®ä¿è¦è¯»å–çš„æ•°æ®ç¼“å­˜åœ¨Alluxioä¸­,æ‰èƒ½åŠ é€ŸåŠ é€Ÿæ•°æ®çš„è¯»å–ã€‚ ä¸€å®šè¦æ˜ç¡®åº”ç”¨åœºæ™¯,Alluxioçš„è®¾è®¡ä¸»è¦æ˜¯é’ˆå¯¹è®¡ç®—ä¸å­˜å‚¨åˆ†ç¦»çš„åœºæ™¯ã€‚åœ¨æ•°æ®è¿œç«¯è¯»å–ä¸”ç½‘ç»œå»¶è¿Ÿå’Œååé‡å­˜åœ¨ç“¶é¢ˆçš„æƒ…å†µä¸‹,Alluxioçš„åŠ é€Ÿæ•ˆæœä¼šå¾ˆæ˜æ˜¾,ä½†å¦‚æœHDFSå’ŒSparkç­‰è®¡ç®—æ¡†æ¶å·²ç»å…±å­˜åœ¨ä¸€å°æœºå™¨(è®¡ç®—å’Œå­˜å‚¨æœªåˆ†ç¦»),Alluxioçš„åŠ é€Ÿæ•ˆæœå¹¶ä¸æ˜æ˜¾,ç”šè‡³å¯èƒ½å‡ºç°æ›´æ…¢çš„æƒ…å†µã€‚ å¤šæ¬¡é‡å¤ä½œä¸šè¾“å…¥æ•°æ®ä½äºOSçš„é«˜é€Ÿç¼“å†²åŒº,Alluxioæ²¡æœ‰åŠ é€Ÿæ•ˆæœç”šè‡³å˜æ…¢ã€‚ å†…å­˜çˆ†ç‚¸ï¼Œå‰¯æœ¬è¿‡å¤šå†…å­˜å ç”¨è¿‡å¤§ï¼Ÿ ä¸¤ç§æ–¹æ¡ˆï¼šå…³é—­è¢«åŠ¨ç¼“å­˜alluxio.user.file.passive.cache.enabled=falseå…³é—­è¢«åŠ¨ç¼“å­˜å¯¹äºä¸éœ€è¦æ•°æ®æœ¬åœ°æ€§ä½†å¸Œæœ›æ›´å¤§çš„Alluxioå­˜å‚¨å®¹é‡çš„å·¥ä½œè´Ÿè½½æ˜¯æœ‰ç›Šçš„ï¼Œæˆ–è€…é€šè¿‡å‘½ä»¤alluxio fs setReplication -R â€“max 5 é™åˆ¶æŸä¸ªç›®å½•çš„æ–‡ä»¶æœ€å¤§å‰¯æœ¬æ•° ä¸€äº›å®˜æ–¹çš„Q&amp;A Alluxioå®˜æ–¹é—®é¢˜ä¸ç­”æ¡ˆ æ€»ç»“ å¯¹æ–°æŠ€æœ¯çš„è°ƒç ”ï¼Œæœ€é‡è¦çš„æ˜¯äº†è§£å®ƒçš„åº”ç”¨åœºæ™¯ï¼Œåªæœ‰åœºæ™¯å¯¹äº†ï¼Œæ•ˆæœæ‰ä¼šå¾ˆæ˜æ˜¾ ä¸€å®šè¦å¤šçœ‹å®˜æ–¹æ–‡æ¡£ï¼Œè™½ç„¶Alluxioæ–‡æ¡£ä¸æ˜¯å¾ˆè¯¦ç»†ï¼Œä½†ä¹Ÿæœ‰å¸®åŠ©ï¼Œè¦è‡ªå·±æ‰¾ç»†èŠ‚ å¯¹è‡ªå·±é‡åˆ°çš„éš¾ä»¥è§£å†³çš„é—®é¢˜è¦ç§¯æä¸ç¤¾åŒºæ²Ÿé€šå’Œè®¨è®º è‡ªå·±é‡åˆ°çš„é—®é¢˜å¯èƒ½åˆ«äººä¹Ÿé‡åˆ°äº†ï¼Œæœ‰å¯èƒ½æ˜¯ç‰ˆæœ¬çš„BUGï¼Œæˆ–è®¸å·²ç»æœ‰äººæäº¤Issueäº†ï¼Œä¸€å®šå¤šç•™æ„ æ–°çš„ç¨³å®šç‰ˆå‘è¡Œï¼Œä¸€å®šè¦äº†è§£å®ƒçš„æ–°ç‰¹æ€§ä»¥åŠä¿®å¤äº†å“ªäº›æ¼æ´","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Alluxio","slug":"Alluxio","permalink":"https://shmily-qjj.top/tags/Alluxio/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"æˆ‘çš„2019å¹´åº¦æ€»ç»“åšå®¢","slug":"æˆ‘çš„2019å¹´åº¦æ€»ç»“åšå®¢","date":"2020-01-01T00:08:08.000Z","updated":"2022-12-11T05:35:07.919Z","comments":true,"path":"2019Summary/","link":"","permalink":"https://shmily-qjj.top/2019Summary/","excerpt":"","text":"æˆ‘çš„2019å¹´åº¦æ€»ç»“2019å¯¹æˆ‘æ¥è¯´æ˜¯å¾ˆæœ‰æ„ä¹‰çš„ä¸€å¹´ï¼Œç¦»å¼€å­¦æ ¡æ­¥å…¥ç¤¾ä¼šï¼Œæ‰¾åˆ°äº†äººç”Ÿçš„ç¬¬ä¸€ä»½å·¥ä½œï¼Œç¬¬ä¸€æ¬¡å»ºç«‹äº†ä¸ªäººåšå®¢ï¼ŒæŠ€æœ¯ä¹Ÿæ˜¯çªé£çŒ›è¿›ï¼Œè¿˜ç»“äº¤äº†è®¸å¤šæ–°ä¼™(da)ä¼´(lao),å¤´å‘ä¹Ÿå°‘äº†å‡ æ ¹å“ˆå“ˆå“ˆå“ˆâ€¦.æ„Ÿè°¢2019ï¼Œæ„Ÿè°¢å¥¹å¸¦ç»™æˆ‘è¿™ä¹ˆå¤šï¼Œä¸€å‘é«˜å†·çš„å¥¹åˆšåˆšç«Ÿç„¶ç”¨ä¸èˆçš„è¯­æ°”è·Ÿæˆ‘è¯´ï¼šâ€œå»å§ï¼Œ2020åœ¨ç­‰ç€ä½ å‘¢ï¼Œå¥¹æ›´éœ€è¦ä½ ï¼æˆ‘ä¼šæœ›ç€ä½ çš„èƒŒå½±æ¸æ¸è¿œå»ï¼ŒæœŸå¾…ç€2020ä½¿ä½ æ›´ä¼˜ç§€ï¼Œæˆ‘ä¹Ÿä¼šé»˜é»˜æ›¿ä½ é«˜å…´çš„ï¼æˆ‘ä¹Ÿèˆä¸å¾—ä½ ï¼Œæ—¶å…‰ä¸ç­‰äººï¼Œæˆ‘ä»¬åªå¥½å°±æ­¤åˆ«è¿‡äº†ã€‚â€è¿˜æœ‰å‡ ä¸ªå°æ—¶å°±æ˜¯2020å¹´äº†ï¼Œæˆ‘è¿˜çœŸæœ‰ç‚¹èˆä¸å¾—å¥¹(2019),å¯å¥¹ç»ˆç©¶è¿˜æ˜¯è¦ç¦»å¼€æˆ‘ï¼Œæ‰€ä»¥æˆ‘æŠŠå¯¹å¥¹(2019)çš„æ€€å¿µï¼Œä»¥åŠå¯¹å¥¹(2020)çš„æ†§æ†¬éƒ½è®°å½•åœ¨è¿™ç¯‡åšå®¢é‡Œå§ï¼(åƒç“œç¾¤ä¼—:tuiï¼Œä½ ä¸ªæ¸£ç”·!!!) å›é¡¾2019ä¸»æŒäººï¼šæ¬¢è¿ä»Šæ™šçš„å˜‰å®¾ï¼Œä½³å¢ƒåŒå­¦ï¼2019å³å°†ç»“æŸï¼Œ2020å°†å¦‚æœŸè€Œè‡³ï¼Œè¯´è¯´ä½ ç°åœ¨çš„æ„Ÿå—ï¼Ÿæˆ‘ï¼š2019å¹´ï¼Œæˆ‘æ”¶è·äº†å¥½å¤šï¼Œç¦»å¼€å­¦æ ¡ï¼Œæ­¥å…¥ç¤¾ä¼šï¼Œè¿™æ‰æ„Ÿè§‰è‡ªå·±çœŸæ­£é•¿å¤§äº†ï¼Œè‚©ä¸Šçš„è´£ä»»å¼€å§‹é‡äº†ï¼Œç”Ÿæ´»èŠ‚å¥ä¹Ÿå®Œå…¨ä¸ä¸€æ ·äº†ï¼Œæˆ‘è§‰å¾—æ­¥å…¥ç¤¾ä¼šçš„æ„Ÿè§‰æŒºå¥½ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¯æ€•ï¼Œå·¥ä½œè™½ä¸è½»æ¾ï¼Œä½†èƒ½å……å®è‡ªå·±ï¼Œèº«è¾¹ä¹Ÿéƒ½æ˜¯ä¸€äº›æœ‰è¶£çš„åŒäº‹ï¼Œå¾ˆèŠå¾—æ¥ï¼Œæ¯å¤©éƒ½æŒºå—¨çš®çš„ï¼æ„Ÿè°¢2019ç»™äºˆæˆ‘çš„è¿™äº›ï¼ä¸»æŒäººï¼šè¯´è¯´2019ä½ éƒ½ç»å†äº†ä»€ä¹ˆå§ï¼æˆ‘ï¼šä¸‰æœˆå››æœˆï¼Œå¤©å¤©æ³¡åœ¨æ›²åºœ(å­¦æ ¡è€æ ¡åŒºä¸œåŒºçš„607è‡ªä¹ å®¤ï¼Œæˆ‘çš„åºœä¸Š)ï¼Œæ•ˆç‡ä¸é«˜ï¼Œä½†ä¹Ÿèƒ½å­¦å­¦ä¹ ï¼ŒåŒå­¦ç»å¸¸æ¥å…‰é¡¾ï¼Œä¸€èµ·å­¦ä¹ ï¼ŒæŒºæ¬¢ä¹çš„ã€‚è¿˜æœ‰å¯å˜‰å’Œå¥å“¥æˆ‘ä»¬ä¸€èµ·å»åƒé¥­ï¼Œå»å­¦æ ¡å—åŒºçœ‹å°å§‘å¨˜ï¼Œå“ˆå“ˆã€‚ äº”æœˆï¼ŒåŠ³åŠ¨èŠ‚å‡æœŸå»äº†äº”å¸¸ç©ï¼Œåƒåˆ°äº†çº¯æ­£çš„äº”å¸¸å¤§ç±³ï¼Œçˆ¬äº†å±±ï¼Œåç€å°ç”µé©´æ”¾ç€ä¹¡æ‘çˆ±æƒ…ä¸»é¢˜æ›²ï¼Œç©çš„å¾ˆå°½å…´ï¼åƒè¿‡äº†ä¸œåŒºæœ€åä¸€é¡¿æ—©é¤ï¼Œä¸Šè¿‡äº†ä¸œåŒºæœ€åä¸€èŠ‚è¯¾ï¼Œæˆ‘ä»¬ç¦»å¼€äº†ç”Ÿæ´»ä¸‰å¹´çš„è€æ ¡åŒºï¼Œè™½ç„¶è€æ ¡åŒºå¾ˆç ´ï¼Œä½†ç”Ÿæ´»äº†è¿™ä¹ˆä¹…ï¼Œä¹Ÿæœ‰æ„Ÿæƒ…äº†ï¼Œç¦»å¼€è¿˜æŒºæ„Ÿæ…¨çš„ï¼Œå°±åƒç°åœ¨æˆ‘è¦ç¦»å¼€2019ä¸€æ ·ï¼ä¸è¿‡æ¥äº†æ–°æ ¡åŒºï¼Œä¹Ÿç®—æ˜¯ä½“éªŒåˆ°äº†å¤§å­¦ç”Ÿæ´»è¯¥æœ‰çš„æ ·å­ï¼Œè·Ÿéªå„¿ä»–ä»¬ååœ¨æ“åœºçœ‹æ™šä¼šï¼Œè¿˜è·Ÿä¿„ç½—æ–¯å°å§å§åˆç…§ï¼Œhinå¼€å¿ƒçš„ä¸€æ®µæ—¶å…‰ï¼ å…­æœˆï¼Œç«¯åˆå›å®¶ï¼Œç¬¬ä¸€æ¬¡å’Œçˆ¸å¦ˆå»æ­Œå…ï¼Œå”±åˆ°ã€Šæ—¶é—´éƒ½å»å“ªäº†ã€‹ï¼Œç¬é—´æ³ªå´©ï¼Œæ„Ÿå¹å²æœˆä¸é¥¶äººã€‚å…­æœˆçš„è‹±è¯­å…­çº§è€ƒè¯•è™½ç„¶æ²¡è¿‡æ²¡ï¼Œä½†å´æ”¶è·äº†ä¸€æšå¥³æœ‹å‹ï¼Œè¿˜è¶ç€å¹´è½»ç©äº†ä¸€åœºå¼‚åœ°æ‹å¥”ç°ã€‚æ²¡èƒ½æœ€ç»ˆèµ°åˆ°ä¸€èµ·ï¼Œä½†ä¹Ÿæ„Ÿè°¢å¥¹çš„å‡ºç°ä½¿æˆ‘æˆé•¿ï¼Œå¥¹æ˜¯ä¸ªå–„è‰¯çš„å¥½å§‘å¨˜ï¼Œæˆ‘åœ¨è¿™é‡Œä¹Ÿç¥å¥¹å¹¸ç¦ï¼Œæ¯å¤©å¼€å¿ƒå§ï¼ ä¸ƒæœˆï¼Œé‚£æ®µæ—¶é—´åšæŒæ—©èµ·è·‘æ­¥ï¼Œç˜¦äº†ï¼Œç˜¦çš„æŒºå¿«çš„ï¼Œä»ä¸€ä¸ªå°èƒ–å¢©å˜å¾—ä¸é‚£ä¹ˆèƒ–äº†ï¼é‚£æ®µæ—¶é—´æˆ‘åŠªåŠ›åœ°å¤ä¹ ç€å­¦è¿‡çš„çŸ¥è¯†ï¼ŒåŒæ—¶åœ¨å“ˆè¡Œå¼€å‘éƒ¨é—¨å®ä¹ ï¼Œä»€ä¹ˆä¹Ÿä¸è®©æˆ‘ä»¬å®ä¹ ç”Ÿç¢°ï¼Œæ‰€ä»¥æˆ‘æƒ³æ—©ç‚¹ç¦»å¼€ï¼Œäºæ˜¯ç´§å¼ åœ°å‡†å¤‡ç§‹æ‹›ï¼Œæ€»è§‰å¾—å¿ƒé‡Œæœ‰å—çŸ³å¤´ä¸€ç›´æ‚¬ç€ä¼¼çš„ï¼Œå› ä¸ºè¿˜æ²¡é¢è¯•ã€‚ å…«æœˆè¾æ‰äº†é‚£è¾¹ï¼Œæ¥ä¸Šæµ·è¿™è¾¹å·¥ä½œï¼Œç”±äºæˆ‘çš„ä¸“ä¸šé™åˆ¶ï¼Œåªæœ‰åœ¨åŒ—ä¸Šå¹¿æ·±æ‰æœ‰å‘å±•ï¼Œç¦»å®¶é‚£ä¹ˆè¿œï¼Œè¿˜æ˜¯æ¯…ç„¶å†³ç„¶åœ°æ¥äº†ï¼Œè¶ç€å¹´è½»ï¼Œå°±æ˜¯è¦é—¯è¡ä¸€ç•ªï¼ä¸€å¼€å§‹å¯¹é­”éƒ½å¾ˆé™Œç”Ÿï¼Œä»¥ä¸ºæ˜¯ä¸€åº§å†·å†°å†°çš„åŸå¸‚ï¼Œç°åœ¨å¥½å¤šäº†ã€‚ ä¹æœˆï¼Œç¬¬ä¸€æ¬¡å‚åŠ éƒ¨é—¨å›¢å»ºï¼Œåƒé˜³æ¾„æ¹–å¤§é—¸èŸ¹+æ‰“å¡è‹å·å›­æ—ï¼Œç©çš„å¥½å¼€å¿ƒï¼Œè·ŸåŒäº‹ä¹Ÿç†Ÿæ‚‰äº†ï¼ åæœˆï¼Œç½‘æ‹å¥”ç°ï¼Œå»å¤©æ´¥ç©äº†å‡ å¤©ï¼Œè·Ÿå¥¹å„ç§æ¸¸è¡ï¼Œå»äº†ä¸–çºªé’Ÿå¹¿åœºï¼Œå–äº†ç«ç‘°é…’ï¼Œçœ‹äº†ã€Šä¸­å›½æœºé•¿ã€‹ï¼Œå»äº†çŒ«å’–æ’¸çŒ«ï¼Œè¿˜å»äº†â€œåˆ†æ‰‹è½®â€å¤©æ´¥ä¹‹çœ¼ï¼Œæœ€ç»ˆè¿˜æ˜¯æ²¡èƒ½é€ƒè¿‡å®ƒçš„è¯…å’’å“ˆå“ˆï¼åŠå¹¿å¤§æƒ…ä¾£ä»¬å»å¤©æ´¥ä¹‹çœ¼è¦æ…é‡å•¦ï¼ä¸­æ—¬ï¼Œæ²¾Leaderçš„å…‰ï¼Œæœ‰æœºä¼šå‚åŠ QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼šï¼Œæ”¶è·äº†å¾ˆå¤šï¼ åä¸€æœˆåˆ°ç°åœ¨ï¼Œå†™ä»£ç ï¼Œæ”¹ä»£ç ï¼Œä¿®BUGï¼Œæµ‹è¯•ï¼Œè°ƒç ”ï¼Œè°ƒä¼˜â€¦ ä¸»æŒäººï¼šé‚£2019å¹´å¯¹ä½ æ¥è¯´è¿˜çœŸæ˜¯æœ‰æ„ä¹‰çš„ä¸€å¹´å•Šï¼é‚£ä½ è§‰å¾—2019å¹´ä½ æœ€å¤§çš„å˜åŒ–æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ï¼šå‡æœŸå˜å°‘äº†ç®—å—ã€‚ã€‚(ä¸»æŒäººä¸€è„¸æ— å¥ˆï¼šä¸ç®—ï¼ï¼ï¼)å¥½å­â€¦èº«ä½“ä¸Šï¼Œæˆ‘æ¯”ä»¥å‰ç˜¦äº†ï¼Œå“ˆå“ˆï¼Œå‡è‚¥æˆåŠŸä¸æ˜“ï¼å¿ƒç†ä¸Šå‘¢ã€‚ã€‚ã€‚æˆ‘è§‰å¾—æˆ‘æ›´åŠ ç‹¬ç«‹äº†ï¼Œæ›´åŠ æˆç†Ÿäº†ï¼Œä¸Šè¿›å¿ƒå’Œæ±‚çŸ¥æ¬²éƒ½UP UP!ä¸»æŒäººï¼šå—¯ï¼Œä½ ä¸€å®šå¯ä»¥åœ¨ä¿æŒå¥½èº«æçš„åŒæ—¶ï¼Œä¿æŒä¸Šè¿›è¡Œå’Œæ±‚çŸ¥æ¬²çš„ï¼2019å¹´ï¼Œæœ‰æ²¡æœ‰å‘ç°è‡ªå·±çš„ä¸è¶³ï¼Ÿæˆ‘ï¼šæœ‰çš„ï¼ä¸Šç­æ‰å‘ç°åŸæ¥å·¥ä½œä¸­æœ‰å¥½å¤šæé«˜æ•ˆç‡çš„æ–¹æ³•æˆ‘ä¸ä¼šï¼Œæœ‰ä¸ªç¼–ç å‰æ€è€ƒè¿‡å¤šçš„æ¯›ç—…ï¼Œè¿˜æœ‰æœ‰æ—¶å€™æƒ³é—®é¢˜ä¼šæƒ³å½“ç„¶ï¼Œè€Œä¸”ä¸€äº›åŸç†ç›¸å…³çš„ä¸œè¥¿æŒæ¡ä¸ç†Ÿè¿˜æœ‰ä¸€äº›å…³é”®é—®é¢˜ä¼šæ‹¿æä¸å‡†ï¼Œç®—æ³•åŸºç¡€ä¹Ÿæ¯”è¾ƒè–„å¼±ï¼Œè¦å­¦è¦è¡¥çš„ä¸œè¥¿å¾ˆå¤šï¼å¹²å°±å®Œäº†ï¼æœ‰ä¼˜ç§€çš„è€å“¥ä»¬å¸¦ç€ï¼Œä¸æ€•äº†ï¼ä¸»æŒäººï¼šé‚£ç”Ÿæ´»æ–¹é¢å’Œåšäº‹æ–¹é¢å‘¢ï¼Ÿæˆ‘ï¼šæˆ‘è§‰å¾—è¿˜å¥½ï¼Œä»¥å‰æˆ‘æ˜¯å†…å‘å‹çš„ï¼Œä½†æ˜¯è·Ÿèº«è¾¹äººç†Ÿæ‚‰äº†å°±ä¼šå¼€æœ—è®¸å¤šäº†ï¼å¥¥ï¼Œå¯¹äº†ï¼Œæœ‰ä¸ªåä¹ æƒ¯ï¼Œä¸€å®šè¦æ”¹ï¼Œé‚£å°±æ˜¯ç†¬å¤œï¼Œå¸Œæœ›åœ¨2020æˆ‘èƒ½é€æ¸æ”¹æ‰ç†¬å¤œçš„åæ¯›ç—…ï¼Œæœ‰è§„å¾‹çš„ç”Ÿæ´»ã€‚è¿˜æœ‰è¦å¤šé”»ç‚¼èº«ä½“ï¼Œæ¯•ç«Ÿèº«ä½“æ˜¯é©å‘½çš„æœ¬é’±å˜›ï¼ä¸»æŒäººï¼šå¨±ä¹æ–¹é¢å‘¢ï¼Ÿè®°å¾—ä½ å–œæ¬¢å¼¹å‰ä»–ï¼Ÿæˆ‘ï¼šè¯¶ï¼ŒåŠå¹´æ²¡å¼¹äº†ï¼Œç”Ÿç–äº†ï¼Œä¸è¿‡ä¼šæ¡èµ·æ¥çš„ã€‚æœ€è¿‘åªé æ‰“æ¸¸æˆæ¥å¨±ä¹äº†ï¼Œåœ¨ç‹è€…å³¡è°·é‡Œä»–ä»¬éƒ½å¤¸æˆ‘æ˜¯æˆ‘å›½æœå…ƒæ­Œã€‚ã€‚ã€‚ä¸»æŒäººï¼š2019å¹´çš„éŸ³ä¹æ–¹é¢ä½œå“å°‘äº†å¾ˆå¤šï¼ŒåŸæ¥æ—¶é—´èŠ±åœ¨äº†ç©æ¸¸æˆä¸Šï¼æˆ‘ï¼šä¸ä¸ï¼Œ2019æ˜¯å¾ˆå¿™çš„ä¸€å¹´ï¼Œæ²¡æœ‰æ—¶é—´åšåæœŸâ€¦ä¸è¿‡ï¼Œæ¸¸æˆç¡®å®ä¹Ÿç©äº†ï¼Œç©äº†ä¸€ç›´å¾ˆå–œæ¬¢çš„ä¸€æ¬¾ã€Šåœ°é“ç¦»å»ã€‹ï¼Œé‡Œé¢ä¸»è§’é˜¿å°”ä¹”å§†å¼¹çš„å‰ä»–å¾ˆåŠ¨å¬ï¼Œå–ï¼Œæˆ‘è¿˜è‡ªå·±æ‰’äº†è°±å­ã€ŠMetro Exodusã€‹ã€‚ç°åœ¨ä¸€æƒ³ï¼Œç¡®å®æ¯”å»å¹´å°‘äº†å¾ˆå¤šä½œå“ï¼Œåœ¨äº‘æ‘åªå‘äº†æœ‰ä¸€é¦–ç¿»å¼¹ã€Šå°‘å¹´çš„æ¢¦ã€‹ï¼Œå’Œä¸¤é¦–ç¿»å”±ã€Šæ‡‚äº†å°±æ‡‚äº†ã€‹ã€ã€Šæµ®ç”Ÿæœªæ­‡ã€‹ã€‚å…¶å®è¿˜æœ‰ä¸¤é¦–å·²ç»å½•å¥½äº†ï¼Œè¿˜æ²¡æ¥å¾—åŠæ··éŸ³ï¼Œå“ˆå“ˆï¼ä¸»æŒäººï¼šæ­¤æ—¶æ­¤åˆ»ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³å¯¹æœ‹å‹å’Œäº²äººè¯´çš„ï¼Ÿæˆ‘ï¼šå¯¹æˆ‘çš„æœ‹å‹ä»¬è¯´ï¼šæœ‹å‹ä»¬ç­‰æˆ‘å›å»å—¨ï¼å•¤é…’è¸©ç€ç®±å­å–ï¼Œä¸€å¤©ä¸‰é¡¿å°çƒ§çƒ¤ç»™æˆ‘å®‰æ’ä¸Šï¼ç„¶åå¯¹çˆ¸å¦ˆè¯´ï¼Œè¿‡å¹´ç­‰æˆ‘å›å»è¿‡ä¸ªå›¢åœ†å¹´ï¼æˆ‘ä¼šæ³¨æ„èº«ä½“å°‘ç†¬å¤œçš„ï¼ç­‰æˆ‘å‘è¾¾äº†å¸¦ä½ ä»¬æ—…æ¸¸å»ï¼æˆ‘ä¸åœ¨èº«è¾¹ï¼Œä½ ä»¬å¯è¦æ³¨æ„èº«ä½“å‘€ï¼ä¸»æŒäººï¼šä½ çš„æœ‹å‹ä»¬å¬äº†å¾ˆå¼€å¿ƒå§ï¼Œä½ çš„çˆ¶æ¯ï¼Œä¹Ÿä¼šä¸ºä½ æ„ŸåŠ¨ï¼Œä¸ºä½ éª„å‚²ï¼Œå¸Œæœ›ä½ åœ¨2020å¹´ï¼Œæ— è®ºäº‹ä¸šè¿˜æ˜¯ç”Ÿæ´»ï¼Œéƒ½èƒ½æ›´ä¸Šä¸€å±‚æ¥¼ï¼æˆ‘ï¼šè°¢è°¢æ‚¨ï¼ä¸»æŒäººï¼šé‚£åœ¨èŠ‚ç›®çš„æœ€åï¼Œä½ è¿˜æœ‰ä»€ä¹ˆè¦ç»™å¤§å®¶åˆ†äº«çš„å—ï¼Ÿæˆ‘ï¼šå—¯ï¼Œä»Šå¤©å‡†å¤‡äº†å‡ å¼ ç…§ç‰‡ï¼ŒæŠŠ2019æœ€ç¾å¥½çš„å›å¿†åˆ†äº«ç»™å¤§å®¶ï¼ å±•æœ›2020æŒ¥æ‰‹å‘Šåˆ«2019ï¼Œ2020æˆ‘æ¥å•¦ï¼2020æˆ‘ä»¬ä¸€èµ·åŠ æ²¹ï¼ å°ç›®æ ‡ ä»£ç èƒ½åŠ›æé«˜ï¼Œæ·±å…¥ç†è§£è®¾è®¡æ¨¡å¼ æ¡†æ¶åº•å±‚åŸç†æŒæ¡ï¼Œå¼€å§‹é€æ­¥å‘æºç å±‚é¢æ·±å…¥ æé«˜å¯¹å¤§æ•°æ®æŠ€æœ¯æ ˆçš„æ•´ä½“è®¤è¯†ï¼Œå…ˆæœ‰æ·±åº¦ï¼Œåæœ‰å¹¿åº¦ å·¥ä½œæ•ˆç‡æé«˜ï¼Œç†Ÿæ‚‰å¿«æ·é”® å…»æˆå¥½çš„å†™æ–‡æ¡£ä¹ æƒ¯ï¼Œåªè¦å€¼å¾—æ·±å…¥å­¦ä¹ çš„ä¸œè¥¿éƒ½å°½é‡å†™åšå®¢ å‰ä»–èƒ½å¼¹å¾—æ›´å¥½å§ï¼Œç½‘æ˜“äº‘ä½œå“çš„è´¨é‡æé«˜ æ”¹æ‰ç†¬å¤œåä¹ æƒ¯ äº¤å‡ ä¸ªå¥½æœ‹å‹å•¥çš„ å‡‘ä¹æ¡ï¼Œå¸Œæœ›èƒ½ä¹…ä¹…é“­è®°æˆ‘çš„2020å°ç›®æ ‡ å°æœŸå¾… æœŸå¾…æ—©ç¡æ—©èµ·å¥åº·çš„è‡ªå·± æœŸå¾…æ±‚çŸ¥è¿›å–çˆ±æ‹¼çš„è‡ªå·± æœŸå¾…å¤©å¤©å‘ä¸Šè¿›æ­¥çš„è‡ªå·± æœŸå¾…äº‰åˆ†å¤ºç§’é«˜æ•ˆçš„è‡ªå·± æœŸå¾…æ‚ ç„¶è‡ªå¾—å¿«ä¹çš„è‡ªå·± æœŸå¾…â€¦â€¦..ä¸€å¤œæš´å¯Œå“ˆå“ˆå“ˆ å°æ„¿æœ›ç¬¬ä¸€ä¸ªæ„¿æœ›å°±æ˜¯å®¶äººèƒ½å¥å¥åº·åº·ï¼Œè‡ªå·±ä¹Ÿå…»æˆå¥åº·çš„ç”Ÿæ´»ä¹ æƒ¯ç¬¬äºŒä¸ªæ„¿æœ›æ˜¯åœ¨å¤§æ•°æ®æ–¹é¢æœ‰æ›´æ·±å…¥çš„å­¦ä¹ å’Œå®è·µç¬¬ä¸‰ä¸ªæ„¿æœ›æ˜¯ç»“äº¤å¥½æœ‹å‹ï¼Œè®¤è¯†æ–°æœ‹å‹æœ€åä¸€ä¸ªæ„¿æœ›å°±æ˜¯è¦æ­£åœ¨çœ‹æˆ‘åšå®¢çš„ä½ æ¯å¤©éƒ½å¼€å¿ƒå‘€ å°æƒ³æ³•çœŸè¯¥æ—©ç¡äº†ï¼Œæˆ‘æƒ³æ—©ç¡ï¼Œæˆ‘å¥½æƒ³æ—©ç¡å‘€ï¼å¯æ˜¯ä¸€åˆ°æ™šä¸Šå°±è´¼ç²¾ç¥ã€‚ã€‚ã€‚æœ‰æ²¡æœ‰å°ä¼™ä¼´å¯ä»¥äº’ç›¸ç›‘ç£çš„å‘€ï¼å¯ä»¥åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»™æˆ‘ç•™è¨€å¼ï¼æœ€åï¼Œç¥å¤§å®¶åœ¨2020å¹´ï¼Œèº«ä½“å€å„¿æ£’ï¼Œåƒå˜›å˜›é¦™ï¼Œå­¦ä¸šæœ‰æˆï¼Œç”Ÿæ„å…´æ—ºâ€¦è¿˜æœ‰ä¸€å¤œæš´å¯Œæ˜‚ï¼Œæœ€é‡è¦çš„æ˜¯æš´å¯Œäº†å¯åˆ«å¿˜äº†æˆ‘æ˜‚ï¼Œå“ˆå“ˆå“ˆå“ˆã€‚","categories":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"ä¸ªäººæ€»ç»“","slug":"ä¸ªäººæ€»ç»“","permalink":"https://shmily-qjj.top/tags/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"},{"name":"2019","slug":"2019","permalink":"https://shmily-qjj.top/tags/2019/"}],"keywords":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"æ•°æ®åº“äº‹åŠ¡ACIDç†è§£","slug":"æ•°æ®åº“äº‹åŠ¡ACIDç†è§£","date":"2019-12-28T12:22:00.000Z","updated":"2022-12-11T05:35:07.921Z","comments":true,"path":"1f7eb1b3/","link":"","permalink":"https://shmily-qjj.top/1f7eb1b3/","excerpt":"","text":"Introå¯¹äºäº‹åŠ¡ACIDï¼ŒçŸ¥é“å¤§æ¦‚çš„æ„æ€ï¼Œä½†æ€»è§‰å¾—å¯¹è¿™ä¸ªæ¦‚å¿µè¿˜æœ‰ç‚¹æ¨¡ç³Šï¼Œæ‰€ä»¥å†™ä¸€ç¯‡åšå®¢åŠ æ·±ä¸€ä¸‹å°è±¡ã€‚ æ•°æ®åº“çš„äº‹åŠ¡ä¸€ä¸ªäº‹åŠ¡ä¸­å¯èƒ½æœ‰å¤šä¸ªæ“ä½œï¼Œå½“æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸäº†çš„æƒ…å†µä¸‹è¿™ä¸ªäº‹åŠ¡æ‰ä¼šè¢«æäº¤ï¼Œå¦‚æœå…¶ä¸­ä¸€ä¸ªæ“ä½œå¤±è´¥ï¼Œæ•´ä¸ªäº‹åŠ¡éƒ½å°†å›æ»š(Rollback)åˆ°äº‹åŠ¡å¼€å§‹å‰çš„çŠ¶æ€ï¼Œå¥½åƒè¿™ä¸ªäº‹åŠ¡ä»æœªæ‰§è¡Œè¿‡ã€‚ç®€å•æ¥è¯´å°±æ˜¯:è¦ä¹ˆä»€ä¹ˆéƒ½ä¸åšï¼Œè¦ä¹ˆåšå…¨å¥—ï¼ˆAll or Nothingï¼‰ ACIDACIDæ˜¯æŒ‡æ•°æ®åº“äº‹åŠ¡æ­£ç¡®æ‰§è¡Œçš„å››ä¸ªåŸºæœ¬ç‰¹å¾çš„ç¼©å†™é€šè¿‡ä¸Šå›¾å¯ä»¥å¤§æ¦‚äº†è§£ACIDçš„åŸºæœ¬ç‰¹å¾ï¼Œä¸‹é¢åšè¯¦ç»†ä»‹ç» åŸå­æ€§ï¼ˆAtomicityï¼‰äº‹åŠ¡ä¸­åŒ…å«çš„æ“ä½œé›†åˆï¼Œè¦ä¹ˆå…¨éƒ¨æ“ä½œæ‰§è¡Œå®Œæˆï¼Œè¦ä¹ˆå…¨éƒ¨éƒ½ä¸æ‰§è¡Œã€‚å³å½“äº‹åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå‘ç”Ÿäº†æŸäº›å¼‚å¸¸æƒ…å†µï¼Œå¦‚ç³»ç»Ÿå´©æºƒã€æ‰§è¡Œå‡ºé”™ï¼Œåˆ™éœ€è¦å¯¹å·²æ‰§è¡Œçš„æ“ä½œè¿›è¡Œå›æ»šï¼Œæ¸…é™¤æ‰€æœ‰æ‰§è¡Œç—•è¿¹ã€‚ä¾‹å­ï¼šAå‘Bè½¬è´¦100ï¼Œè¿™ä¸ªäº‹åŠ¡åŒ…æ‹¬ä¸¤æ­¥(Aå¤±å»100ï¼ŒBå¾—åˆ°100)ï¼ŒåŸå­æ€§ä¿è¯è¿™ä¸¤æ­¥éƒ½æˆåŠŸæˆ–è€…éƒ½å¤±è´¥ã€‚ ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰äº‹åŠ¡æ‰§è¡Œå‰å’Œäº‹åŠ¡æ‰§è¡Œåï¼Œæ•°æ®åº“çš„å®Œæ•´æ€§çº¦æŸä¸è¢«ç ´åã€‚å³äº‹åŠ¡çš„æ‰§è¡Œæ˜¯ä»ä¸€ä¸ªæœ‰æ•ˆçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªæœ‰æ•ˆçŠ¶æ€ã€‚ä¾‹å­ï¼šAå‘Bè½¬è´¦100ï¼Œè¿™ä¸ªäº‹åŠ¡åŒ…æ‹¬ä¸¤æ­¥(Aå¤±å»100ï¼ŒBå¾—åˆ°100)ï¼ŒAå’ŒBæ‰€åœ¨çš„è¡¨æ”¶å…¥å’Œæ”¯å‡ºå­˜åœ¨å¤–é”®çº¦æŸï¼Œè‹¥Aæ”¯å‡ºå¢åŠ è€ŒBæ”¶å…¥æœªå¢åŠ ï¼Œåˆ™è¿åäº†ä¸€è‡´æ€§çº¦æŸã€‚ éš”ç¦»æ€§ï¼ˆIsolationï¼‰æ•°æ®åº“å…è®¸å¤šä¸ªå¹¶å‘äº‹åŠ¡åŒæ—¶å¯¹æ•°æ®è¿›è¡Œè¯»å†™å’Œä¿®æ”¹çš„èƒ½åŠ›ï¼Œå¦‚æœä¸€ä¸ªäº‹åŠ¡è¦è®¿é—®çš„æ•°æ®æ­£åœ¨è¢«å¦å¤–ä¸€ä¸ªäº‹åŠ¡ä¿®æ”¹ï¼Œåªè¦å¦å¤–ä¸€ä¸ªäº‹åŠ¡æœªæäº¤ï¼Œå®ƒæ‰€è®¿é—®çš„æ•°æ®å°±ä¸å—æœªæäº¤äº‹åŠ¡çš„å½±å“ã€‚éš”ç¦»æ€§å¯ä»¥é˜²æ­¢å¤šä¸ªäº‹åŠ¡å¹¶å‘æ‰§è¡Œæ—¶ç”±äºäº¤å‰æ‰§è¡Œè€Œå¯¼è‡´æ•°æ®çš„ä¸ä¸€è‡´ã€‚å¤šä¸ªäº‹åŠ¡å¹¶å‘æ‰§è¡Œæ—¶ï¼Œå½¼æ­¤ä¹‹é—´ä¸åº”è¯¥å­˜åœ¨ç›¸äº’å½±å“ã€‚éš”ç¦»ç¨‹åº¦ä¸æ˜¯ç»å¯¹çš„ï¼Œæ¯ä¸ªæ•°æ®åº“éƒ½æä¾›æœ‰è‡ªå·±çš„éš”ç¦»çº§åˆ«ï¼Œæ¯ä¸ªæ•°æ®åº“çš„é»˜è®¤éš”ç¦»çº§åˆ«ä¹Ÿä¸å°½ç›¸åŒã€‚ä¾‹å­ï¼šAå‘Bè½¬è´¦100ï¼Œäº¤æ˜“è¿˜æœªå®Œæˆæ—¶ï¼ŒBæŸ¥è¯¢ä¸åˆ°100å…ƒå…¥è´¦ã€‚ æŒä¹…æ€§ï¼ˆDurabilityï¼‰äº‹åŠ¡æ­£å¸¸æ‰§è¡Œå®Œæ¯•åï¼Œå¯¹æ•°æ®åº“çš„ä¿®æ”¹æ˜¯æ°¸ä¹…æ€§çš„ï¼Œå³ä¾¿ç³»ç»Ÿæ•…éšœä¹Ÿä¸ä¼šä¸¢å¤±ã€‚å³äº‹åŠ¡çš„ä¿®æ”¹æ“ä½œå·²ç»è®°å½•åˆ°äº†å­˜å‚¨ä»‹è´¨ä¸­ã€‚ä¾‹å­ï¼šAå‘Bè½¬è´¦100ï¼ŒAæ°¸ä¹…å¤±å»äº†100å…ƒè€ŒBæ°¸ä¹…å¾—åˆ°100å…ƒï¼Œä¸èƒ½èµ–è´¦ã€‚ æ€»ç»“ACID åŸå­æ€§ï¼šäº‹åŠ¡æ“ä½œçš„æ•´ä½“æ€§ã€‚ ä¸€è‡´æ€§ï¼šäº‹åŠ¡æ“ä½œä¸‹æ•°æ®çš„æ­£ç¡®æ€§ã€‚ éš”ç¦»æ€§ï¼šäº‹åŠ¡å¹¶å‘æ“ä½œä¸‹æ•°æ®çš„æ­£ç¡®æ€§ã€‚ æŒä¹…æ€§ï¼šäº‹åŠ¡å¯¹æ•°æ®ä¿®æ”¹çš„å¯é æ€§ã€‚ äº‹åŠ¡éš”ç¦»çº§åˆ«ä¸Šé¢è¯´è¿‡â€œæ¯ä¸ªæ•°æ®åº“éƒ½æä¾›æœ‰è‡ªå·±çš„éš”ç¦»çº§åˆ«ï¼Œæ¯ä¸ªæ•°æ®åº“çš„é»˜è®¤éš”ç¦»çº§åˆ«ä¹Ÿä¸å°½ç›¸åŒâ€ï¼Œäº‹ç‰©éš”ç¦»çº§åˆ«åˆ†ä¸ºå››ç§ï¼Œä¸‹é¢ä¸€ä¸€ä»‹ç»ã€‚é¦–å…ˆç®€è¿°å…±äº«é”ï¼ˆSï¼‰å’Œæ’å®ƒé”ï¼ˆXï¼‰ï¼Œæ–¹ä¾¿åç»­ç†è§£ï¼šå¤šä¸ªå…±äº«é”(S)å¯ä»¥åŒæ—¶è·å–ï¼Œä½†æ˜¯æ’å®ƒé”(X)ä¼šé˜»å¡å…¶å®ƒæ‰€æœ‰é” æœªæäº¤è¯»(Read Uncommitted)æŒ‡ä¸€ä¸ªäº‹åŠ¡è¯»å–åˆ°äº†å¦å¤–ä¸€ä¸ªäº‹åŠ¡æœªæäº¤çš„æ•°æ®ã€‚å³äº‹åŠ¡çš„ä¿®æ”¹é˜¶æ®µæœªåŠ æ’ä»–é”ï¼Œå¯¹å…¶ä»–äº‹åŠ¡å¯è§ã€‚ä¾‹å¦‚äº‹åŠ¡T1å¯èƒ½è¯»å–åˆ°åªæ˜¯äº‹åŠ¡T2ä¸­æŸä¸€æ­¥çš„ä¿®æ”¹çŠ¶æ€ï¼Œå³å­˜åœ¨è„è¯»çš„ç°è±¡ã€‚è„è¯»ï¼šäº‹åŠ¡è¯»å–åˆ°çš„æ•°æ®å¯èƒ½æ˜¯ä¸æ­£ç¡®ã€ä¸åˆç†æˆ–è€…å¤„äºéæ³•çŠ¶æ€çš„æ•°æ®ï¼Œä¾‹å¦‚åœ¨äº‹åŠ¡T1è¯»å–åï¼Œäº‹åŠ¡T2å¯èƒ½åˆå¯¹æ•°æ®åšäº†ä¿®æ”¹ï¼Œæˆ–è€…äº‹åŠ¡T2ä¸­æŸäº›æ“ä½œè¿åäº†ä¸€è‡´æ€§çº¦æŸï¼Œåšäº†å›æ»šæ“ä½œï¼Œè¯¥æƒ…å†µä¸‹äº‹åŠ¡T1è¯»å–åˆ°çš„æ•°æ®ç§°ä¹‹ä¸ºè„æ•°æ®ï¼Œè¯¥è¡Œä¸ºç§°ä¹‹ä¸ºè„è¯»ã€‚ æäº¤è¯»(Read Committed)ä¸€ä¸ªäº‹åŠ¡è¿‡ç¨‹ä¸­åªèƒ½è¯»å–åˆ°å…¶ä»–äº‹åŠ¡å¯¹æ•°æ®çš„æäº¤åä¿®æ”¹ï¼Œå³äº‹åŠ¡çš„ä¿®æ”¹é˜¶æ®µåŠ äº†æ’å®ƒé”ï¼Œç›´åˆ°äº‹åŠ¡ç»“æŸæ‰é‡Šæ”¾ï¼Œæ‰§è¡Œè¯»å‘½ä»¤é‚£ä¸€åˆ»åŠ äº†å…±äº«é”ï¼Œè¯»å®Œå³é‡Šæ”¾ï¼Œä»¥æ­¤ç»´æŒäº‹åŠ¡ä¿®æ”¹é˜¶æ®µå¯¹å…¶ä»–äº‹åŠ¡çš„ä¸å¯è§ã€‚ä¾‹å¦‚äº‹åŠ¡T2è¯»å–åˆ°çš„åªèƒ½æ˜¯äº‹åŠ¡T2æäº¤å®Œæˆåçš„çŠ¶æ€ã€‚è¯¥éš”ç¦»çº§åˆ«é¿å…äº†è„è¯»ç°è±¡ï¼Œä½†æ­£æ˜¯ç”±äºäº‹åŠ¡T1å¯èƒ½è¯»å–åˆ°çš„æ˜¯äº‹åŠ¡T2ä¿®æ”¹å®Œæˆåçš„æ•°æ®ï¼Œä»¥è‡´å‡ºç°äº†ä¸å¯é‡å¤è¯»ç°è±¡ã€‚ä¸å¯é‡å¤è¯»ï¼šå¯¹äºåŒä¸€ä¸ªäº‹åŠ¡çš„å‰åä¸¤æ¬¡è¯»å–æ“ä½œï¼Œè¯»å–åˆ°çš„å†…å®¹ä¸åŒã€‚ä¾‹å¦‚åœ¨äº‹åŠ¡T1è¯»å–æ“ä½œåï¼Œäº‹åŠ¡T2å¯èƒ½å¯¹æ•°æ®åšäº†ä¿®æ”¹ï¼Œäº‹åŠ¡T2ä¿®æ”¹å®Œæˆæäº¤åï¼Œäº‹åŠ¡T1åˆåšäº†è¯»å–æ“ä½œï¼Œå› ä¸ºå†…å®¹å·²è¢«ä¿®æ”¹ï¼Œå¯¼è‡´è¯»å–åˆ°çš„å†…å®¹ä¸ä¸Šä¸€æ¬¡ä¸åŒï¼Œå³å­˜åœ¨ä¸å¯é‡å¤è¯»ç°è±¡ã€‚ å¯é‡å¤è¯»(Repeatable Reads)ä¸€ä¸ªäº‹åŠ¡è¿‡ç¨‹ä¸­ä¸å…è®¸å…¶ä»–äº‹åŠ¡å¯¹æ•°æ®è¿›è¡Œä¿®æ”¹ã€‚å³äº‹åŠ¡çš„è¯»å–è¿‡ç¨‹åŠ äº†å…±äº«é”ï¼Œäº‹åŠ¡çš„ä¿®æ”¹è¿‡ç¨‹åŠ äº†æ’å®ƒé”ï¼Œå¹¶ä¸€ç›´ç»´æŒé”å®šçŠ¶æ€ç›´åˆ°äº‹åŠ¡ç»“æŸã€‚å› ä¸ºäº‹åŠ¡çš„è¯»å–æˆ–ä¿®æ”¹éƒ½éœ€è¦ç»´æŒæ•´ä¸ªé˜¶æ®µçš„é”å®šçŠ¶æ€ï¼Œæ‰€ä»¥é¿å…äº†è„è¯»å’Œä¸å¯é‡å¤è¯»ç°è±¡ã€‚ä½†æ˜¯å› ä¸ºåªå¯¹ç°æœ‰çš„è®°å½•ä¸Šè¿›è¡Œäº†é”å®šï¼Œå¹¶æœªç»´æŒé—´éš™é”/èŒƒå›´é”ï¼Œå¯¼è‡´æŸäº›æ•°æ®è®°å½•çš„æ’å…¥æœªå—é˜»æ‹¦ï¼ˆç»“æœå¤šäº†ä¸€è¡Œï¼‰ï¼Œå³å­˜åœ¨å¹»è¯»ç°è±¡ã€‚å¹»è¯»ï¼šäº‹åŠ¡ä¸­å‰åç›¸åŒçš„æŸ¥è¯¢è¯­å¥ï¼Œè¿”å›çš„ç»“æœé›†ä¸åŒã€‚ä¾‹å¦‚åœ¨äº‹åŠ¡T1æŸ¥è¯¢è¡¨è®°å½•åï¼Œäº‹åŠ¡T2å‘è¡¨ä¸­å¢åŠ äº†ä¸€æ¡è®°å½•ï¼Œå½“äº‹åŠ¡T1å†æ¬¡æ‰§è¡Œç›¸åŒçš„æŸ¥è¯¢æ—¶ï¼Œè¿”å›çš„ç»“æœé›†å¯èƒ½ä¸åŒï¼Œå³å­˜åœ¨å¹»è¯»ç°è±¡ã€‚ å¯ä¸²è¡ŒåŒ–(Serializable)ä¸€ä¸ªäº‹åŠ¡è¿‡ç¨‹ä¸­ä¸å…è®¸å…¶ä»–äº‹åŠ¡å¯¹æŒ‡å®šèŒƒå›´æ•°æ®è¿›è¡Œä¿®æ”¹ã€‚å³äº‹åŠ¡è¿‡ç¨‹ä¸­è‹¥æŒ‡å®šäº†æ“ä½œé›†åˆçš„èŒƒå›´ï¼Œåœ¨å¯é‡å¤è¯»çš„é”åŸºç¡€ä¸Šå¢åŠ äº†å¯¹æ“ä½œé›†åˆçš„èŒƒå›´é”ï¼Œé€šè¿‡å¢åŠ èŒƒå›´é”é¿å…äº†å¹»è¯»ç°è±¡ã€‚ å››ç§éš”ç¦»çº§åˆ«è®¾ç½®: çº§åˆ« è¯´æ˜ Serializable å¯é¿å…è„è¯»ã€ä¸å¯é‡å¤è¯»ã€è™šè¯»æƒ…å†µçš„å‘ç”Ÿ Repeatable read å¯é¿å…è„è¯»ã€ä¸å¯é‡å¤è¯»æƒ…å†µçš„å‘ç”Ÿ Read committed å¯é¿å…è„è¯»æƒ…å†µå‘ç”Ÿ Read uncommitted æœ€ä½çº§åˆ«ï¼Œä»¥ä¸Šæƒ…å†µå‡æ— æ³•ä¿è¯ é”çš„ä½¿ç”¨æ˜¯ä¸ºäº†åœ¨å¹¶å‘ç¯å¢ƒä¸­ä¿æŒæ¯ä¸ªä¸šåŠ¡æµå¤„ç†ç»“æœçš„æ­£ç¡®æ€§ï¼Œè¿™æ ·çš„æ¦‚å¿µåœ¨è®¡ç®—æœºé¢†åŸŸä¸­å¾ˆæ™®éï¼Œä½†æ˜¯éƒ½å¿…é¡»è¦åŸºäºä¸€ä¸ªå‰æï¼Œæˆ–è€…ç§°ä¹‹ä¸ºçº¦å®šï¼šåœ¨æ‰§è¡Œæ“ä½œå‰ï¼Œé¦–å…ˆå°è¯•å»è·å–é”ï¼Œè·å–æˆåŠŸåˆ™å¯ä»¥æ‰§è¡Œï¼Œè‹¥è·å–å¤±è´¥ï¼Œåˆ™ä¸æ‰§è¡Œæˆ–ç­‰å¾…é‡å¤è·å–ã€‚å› ä¸ºæ— è®ºä»»ä½•ç±»å‹çš„æ“ä½œï¼Œæœ‰æ²¡æœ‰é”éƒ½ä¸å½±å“ç¨‹åºæœ¬èº«çš„æ‰§è¡Œæµç¨‹ï¼Œä½†åªæœ‰éµä»è¿™ä¸ªçº¦å®šæ‰èƒ½ä½“ç°å‡ºå…¶ä»·å€¼ã€‚å°±åƒçº¢ç»¿ç¯å¹¶ä¸å½±å“è½¦è¾†æœ¬èº«çš„è¡Œé©¶èƒ½åŠ›ï¼Œåªæœ‰å£°æ˜æ‰€æœ‰ä¸ªä½“çš†éµå®ˆç›¸åŒçš„è§„åˆ™ï¼Œæ‰€ä»¥ä¸€åˆ‡æ‰å˜å¾—æœ‰åºã€‚åœ¨æ•°æ®åº“çš„å¹¶å‘ç¯å¢ƒä¸‹ï¼Œéš”ç¦»ç¨‹åº¦è¶Šé«˜ï¼Œä¹Ÿå°±æ„å‘³ç€å¹¶å‘ç¨‹åº¦è¶Šä½ï¼Œæ‰€ä»¥å„ä¸ªæ•°æ®åº“ä¸­ä¸€èˆ¬è®¾ç½®çš„éƒ½æ˜¯ä¸€ä¸ªæŠ˜ä¸­çš„éš”ç¦»çº§åˆ«ã€‚ åŸºäºMysqlæµ‹è¯•éš”ç¦»çº§åˆ« SELECT @@global.tx_isolation; # æŸ¥çœ‹å…¨å±€äº‹ç‰©éš”ç¦»çº§åˆ« SELECT @@session.tx_isolation; # æŸ¥çœ‹ä¼šè¯äº‹ç‰©éš”ç¦»çº§åˆ« SELECT @@tx_isolation; # æŸ¥çœ‹å½“å‰äº‹åŠ¡éš”ç¦»çº§åˆ« SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; # å¯é¿å…è„è¯»ã€ä¸å¯é‡å¤è¯»ã€è™šè¯»æƒ…å†µçš„å‘ç”Ÿ SET SESSION TRANSACTION ISOLATION LEVEL read committed; # å¯é¿å…è„è¯»æƒ…å†µå‘ç”Ÿ SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; # å¯é¿å…è„è¯»ã€ä¸å¯é‡å¤è¯»æƒ…å†µçš„å‘ç”Ÿ SET SESSION TRANSACTION ISOLATION LEVEL serializable; # å¯é¿å…è„è¯»ã€ä¸å¯é‡å¤è¯»ã€å¹»è¯»æƒ…å†µçš„å‘ç”Ÿ start transaction; --å»ºè¡¨ drop table AMOUNT; CREATE TABLE `AMOUNT` ( `id` varchar(10) NULL, `money` numeric NULL ) ; --æ’å…¥æ•°æ® insert into amount(id,money) values(&#39;A&#39;, 800); insert into amount(id,money) values(&#39;B&#39;, 200); insert into amount(id,money) values(&#39;C&#39;, 1000); --æµ‹è¯•å¯é‡å¤è¯»ï¼Œæ’å…¥æ•°æ® insert into amount(id,money) values(&#39;D&#39;, 1000); --è®¾ç½®äº‹åŠ¡ SET SESSION TRANSACTION ISOLATION LEVEL read uncommitted; SELECT @@tx_isolation; --å¼€å¯äº‹åŠ¡ start transaction; --è„è¯»æ¼”ç¤ºï¼Œè¯»åˆ°å…¶ä»–äº‹åŠ¡æœªæäº¤çš„æ•°æ® --æ¡ˆåˆ—1ï¼Œäº‹åŠ¡ä¸€ï¼šAå‘Bè½¬200ï¼Œäº‹åŠ¡äºŒï¼šæŸ¥çœ‹Bé‡‘é¢å˜åŒ–ï¼Œäº‹åŠ¡ä¸€å›æ»šäº‹åŠ¡ update amount set money = money - 200 where id = &#39;A&#39;; update amount set money = money + 200 where id = &#39;B&#39;; --ä¸å¯é‡å¤è¯»æ¼”ç¤ºï¼Œè¯»åˆ°äº†å…¶ä»–äº‹åŠ¡æäº¤çš„æ•°æ® --æ¡ˆåˆ—2ï¼Œäº‹åŠ¡ä¸€ï¼šBå‘Aè½¬200ï¼Œäº‹åŠ¡äºŒï¼šBå‘Cè½¬200è½¬100 SET SESSION TRANSACTION ISOLATION LEVEL read committed; --å¼€å¯äº‹åŠ¡ start transaction; --ä¸¤ä¸ªäº‹åŠ¡éƒ½æŸ¥ä¸€ä¸‹æ•°æ®(è½¬è´¦ä¹‹å‰éœ€è¦ï¼ŒæŸ¥ä¸€ä¸‹é‡‘é¢æ˜¯å¦å¤Ÿæ»¡è¶³è½¬è´¦) select * from amount; --äº‹åŠ¡ä¸€ï¼šBå‘Aè½¬200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --äº‹åŠ¡äºŒï¼šBå‘Cè½¬200è½¬100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --ä»äº‹åŠ¡äºŒçš„è§’åº¦æ¥çœ‹ï¼Œè¯»åˆ°äº†äº‹åŠ¡ä¸€æäº¤äº‹åŠ¡çš„æ•°æ®ï¼Œå¯¼è‡´é‡‘é¢å‡ºç°è´Ÿæ•° --å¹»è¯»æ¼”ç¤º --æ¡ˆåˆ—3ï¼Œäº‹åŠ¡ä¸€ï¼šBå‘Aè½¬200ï¼Œäº‹åŠ¡äºŒï¼šBå‘Cè½¬200è½¬100 SET SESSION TRANSACTION ISOLATION LEVEL repeatable read; --å¼€å¯äº‹åŠ¡ start transaction; --ä¸¤ä¸ªäº‹åŠ¡éƒ½æŸ¥ä¸€ä¸‹æ•°æ®(è½¬è´¦ä¹‹å‰éœ€è¦ï¼ŒæŸ¥ä¸€ä¸‹é‡‘é¢æ˜¯å¦å¤Ÿæ»¡è¶³è½¬è´¦) select * from amount; --äº‹åŠ¡ä¸€ï¼šBå‘Aè½¬200 update amount set money = money - 200 where id = &#39;B&#39;; update amount set money = money + 200 where id = &#39;A&#39;; commit; --äº‹åŠ¡äºŒï¼šBå‘Cè½¬200è½¬100 update amount set money = money - 100 where id = &#39;B&#39;; update amount set money = money + 100 where id = &#39;C&#39;; commit; --ä»äº‹åŠ¡äºŒçš„è§’åº¦æ¥çœ‹ï¼Œè¯»åˆ°äº†äº‹åŠ¡ä¸€æäº¤äº‹åŠ¡çš„æ•°æ®ï¼Œå¯¼è‡´é‡‘é¢å‡ºç°è´Ÿæ•° å‚è€ƒèµ„æ–™äº‹åŠ¡çš„ACIDäº‹åŠ¡ACIDç†è§£äº‹åŠ¡ACIDå±æ€§ä¸éš”ç¦»çº§åˆ«","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"æ•°æ®åº“","slug":"æ•°æ®åº“","permalink":"https://shmily-qjj.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"Hive3.xåˆæ¢","slug":"Hive3.xæ–°ç‰¹æ€§","date":"2019-12-27T07:18:25.000Z","updated":"2022-12-11T05:35:07.904Z","comments":true,"path":"7fbbfd34/","link":"","permalink":"https://shmily-qjj.top/7fbbfd34/","excerpt":"","text":"Hive3.xæ–°ç‰¹æ€§æ–°ç‰¹æ€§ç®€è¿° æ‰§è¡Œå¼•æ“å˜æ›´ä¸º**TEZ**,ä¸ä½¿ç”¨MR æˆç†Ÿçš„ACIDå¤§æ•°æ®äº‹åŠ¡æ”¯æŒ LLAPç”¨äºå¦™æï¼Œæ¯«ç§’çº§æŸ¥è¯¢è®¿é—® åŸºäºApache Rangerçš„ç»Ÿä¸€æƒé™ç®¡ç† é»˜è®¤å¼€å¯HDFS ACLs Beelineä»£æ›¿Hive Cliï¼Œé™ä½å¯åŠ¨å¼€é”€ ä¸å†æ”¯æŒå†…åµŒMetastore Spark Catalogä¸ä¸Hive Catalogé›†æˆï¼Œä½†å¯ä»¥äº’ç›¸è®¿é—® æ‰¹å¤„ç†ä½¿ç”¨TEZï¼Œå®æ—¶æŸ¥è¯¢ä½¿ç”¨LLAP Hive3æ”¯æŒè”é‚¦æŸ¥è¯¢ æ¶æ„åŸç† TEZæ‰§è¡Œå¼•æ“ Apache TEZ**æ˜¯ä¸€ä¸ªé’ˆå¯¹Hadoopæ•°æ®å¤„ç†åº”ç”¨ç¨‹åºçš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼ŒåŸºäºYarnä¸”æ”¯æŒDAGä½œä¸šçš„å¼€æºè®¡ç®—æ¡†æ¶ã€‚Tezäº§ç”Ÿçš„ä¸»è¦åŸå› æ˜¯ç»•å¼€MapReduceæ‰€æ–½åŠ çš„é™åˆ¶ï¼Œé€æ­¥å–ä»£MRï¼Œæä¾›æ›´é«˜çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚Apache TEZçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†Mapå’ŒReduceæ‹†åˆ†æˆè‹¥å¹²å­è¿‡ç¨‹ï¼Œå³Mapè¢«æ‹†åˆ†æˆInputã€Processorã€Sortã€Mergeå’ŒOutputï¼Œ Reduceè¢«æ‹†åˆ†æˆInputã€Shuffleã€Sortã€Mergeã€Processorå’ŒOutputç­‰ï¼Œåˆ†è§£åå¯ä»¥çµæ´»ç»„åˆæˆä¸€ä¸ªå¤§çš„DAGä½œä¸šã€‚Apache TEZå…¼å®¹MRä»»åŠ¡ï¼Œä¸éœ€è¦ä»£ç å±‚é¢çš„æ”¹åŠ¨ã€‚Apache TEZæä¾›äº†è¾ƒä½çº§åˆ«çš„æŠ½è±¡ï¼Œä¸ºäº†å¢å¼ºHive/Pigçš„åº•å±‚å®ç°ï¼Œè€Œä¸æ˜¯æœ€ç»ˆé¢å‘ç”¨æˆ·çš„ã€‚Hive3çš„TEZ+å†…å­˜æŸ¥è¯¢ç»“åˆ**çš„æ€§èƒ½æ®è¯´æ˜¯Hive2çš„50å€(ä¹Ÿæœ‰æ–‡ç« è¯´æ˜¯100å€ï¼Œè¿™ä¸ªæ•°å­—æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œå®ƒåˆ°åº•èƒ½ä¸èƒ½ä¸Sparkå†…å­˜è®¡ç®—é€Ÿåº¦åª²ç¾å‘¢)ã€‚ ä¸Šå›¾æ˜¯Hive On MRå’ŒHive On Tezæ‰§è¡Œä»»åŠ¡æµç¨‹å¯¹æ¯”å›¾ï¼Œè§£é‡Šï¼š Hive On MR Hive On Tez è®¡ç®—éœ€è¦å¤šä¸ªMRä»»åŠ¡è€Œä¸”ä¸­é—´ç»“æœéƒ½è¦è½ç›˜ åªæœ‰ä¸€ä¸ªä½œä¸šï¼Œåªå†™ä¸€æ¬¡HDFS æ²¡æœ‰èµ„æºé‡ç”¨ èµ„æºå¤ç”¨ å¤„ç†å®Œé‡Šæ”¾èµ„æº Applications Managerèµ„æºæ± å¯åŠ¨è‹¥å¹²Containerï¼Œå¤„ç†å®Œä¸é‡Šæ”¾ç›´æ¥åˆ†é…ç»™æœªè¿è¡Œä»»åŠ¡ Map:Reduce = 1:1 ä¸å†æ˜¯ä¸€ä¸ªMapåªå¯¹åº”ä¸€ä¸ªReduce åœ¨ç£ç›˜å¤„ç†æ•°æ®é›† å°çš„æ•°æ®é›†å®Œå…¨åœ¨å†…å­˜ä¸­å¤„ç†ä»¥åŠå†…å­˜Shuffle æ–°çš„HiveQLæ‰§è¡Œæµç¨‹Hiveç¼–è¯‘æŸ¥è¯¢-&gt;Tezæ‰§è¡ŒæŸ¥è¯¢-&gt;Yarnåˆ†é…èµ„æº-&gt;Hiveæ ¹æ®è¡¨ç±»å‹æ›´æ–°HDFSæˆ–Hiveä»“åº“ä¸­çš„æ•°æ®-&gt;Hiveé€šè¿‡JDBCè¿æ¥è¿”å›æŸ¥è¯¢ç»“æœ LLAPLLAP(Live Long and Process)å®æ—¶é•¿æœŸå¤„ç†ï¼Œæ˜¯Hive3çš„ä¸€ç§æŸ¥è¯¢æ¨¡å¼ï¼Œç”±ä¸€ä¸ªå®ˆæŠ¤è¿›ç¨‹å’Œä¸€ä¸ªåŸºäºDAGçš„æ¡†æ¶ç»„æˆï¼ŒLLAPä¸æ˜¯æ‰§è¡Œå¼•æ“(MR/Tez),å®ƒç”¨æ¥ä¿è¯Hiveçš„å¯ä¼¸ç¼©æ€§å’Œå¤šåŠŸèƒ½æ€§ï¼Œå¢å¼ºç°æœ‰çš„æ‰§è¡Œå¼•æ“ã€‚LLAPçš„å®ˆæŠ¤è¿›ç¨‹é•¿æœŸå­˜åœ¨ä¸”ä¸DataNodeç›´æ¥äº¤äº’ï¼Œç¼“å­˜ï¼Œé¢„è¯»å–ï¼ŒæŸäº›æŸ¥è¯¢å¤„ç†å’Œè®¿é—®æ§åˆ¶åŠŸèƒ½åŒ…å«åœ¨è¿™ä¸ªå®ˆæŠ¤ç¨‹åºä¸­ç”¨äºç›´æ¥å¤„ç†å°çš„æŸ¥è¯¢ï¼Œè€Œè®¡ç®—ä¸IOè¾ƒå¤§çš„ç¹é‡ä»»åŠ¡ä¼šæäº¤Yarnæ‰§è¡Œã€‚å®ˆæŠ¤ç¨‹åºä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒHiveä»èƒ½æ­£å¸¸å·¥ä½œã€‚å¯¹LLAPèŠ‚ç‚¹çš„è¯·æ±‚éƒ½åŒ…å«å…ƒæ•°æ®ä¿¡æ¯å’Œæ•°æ®ä½ç½®ï¼Œæ‰€ä»¥LLAPèŠ‚ç‚¹æ— çŠ¶æ€ã€‚å¯ä»¥ä½¿ç”¨Hive on Tez use LLAPæ¥åŠ é€ŸOLAPåœºæ™¯(OnLine Analytical Processingè”æœºåˆ†æå¤„ç†)LLAPä¸ºäº†é¿å…JVMå†…å­˜è®¾ç½®çš„é™åˆ¶ï¼Œä½¿ç”¨å †å¤–å†…å­˜ç¼“å­˜æ•°æ®ä»¥åŠå¤„ç†GROUP BY/JOINç­‰æ“ä½œï¼Œè€Œå®ˆæŠ¤ç¨‹åºä»…ä½¿ç”¨å°‘é‡å†…å­˜ã€‚Hive3æ”¯æŒä¸¤ç§æŸ¥è¯¢æ¨¡å¼Containerå’ŒLLAP å¦‚å›¾LLAPæ‰§è¡Œç¤ºä¾‹ï¼ŒTEZä½œä¸ºæ‰§è¡Œå¼•æ“ï¼Œåˆå§‹é˜¶æ®µæ•°æ®è¢«æ¨åˆ°LLAPï¼ŒLLAPç›´æ¥ä¸DataNodeäº¤äº’ã€‚è€Œåœ¨Reduceé˜¶æ®µï¼Œå¤§çš„Shuffleæ•°æ®åœ¨ä¸åŒçš„Containerå®¹å™¨ä¸­è¿›è¡Œï¼Œå¤šä¸ªæŸ¥è¯¢å’Œåº”ç”¨èƒ½åŒæ—¶è®¿é—®LLAPã€‚ æ›´æˆç†Ÿçš„ACIDæ”¯æŒHiveçš„UPDATEä¸€ç›´æ˜¯å¤§æ•°æ®ä»“åº“çš„ä¸€ä¸ªé—®é¢˜ï¼Œè™½ç„¶åœ¨Hive3.xä¹‹å‰ä¹Ÿæ”¯æŒUPDATEæ“ä½œï¼Œä½†æ˜¯æ€§èƒ½å¾ˆå·®ï¼Œè¿˜éœ€è¦è¿›è¡Œåˆ†æ¡¶ã€‚Hive3.xæ”¯æŒå…¨æ–°çš„æ›´æˆç†Ÿçš„ACIDã€‚Hive3é»˜è®¤å¯¹å†…éƒ¨è¡¨æ”¯æŒäº‹åŠ¡å’ŒACIDç‰¹æ€§ã€‚é»˜è®¤æƒ…å†µä¸‹å¯ç”¨ACIDä¸ä¼šå¯¼è‡´æ€§èƒ½æˆ–æ“ä½œè¿‡è½½ã€‚ ç‰©åŒ–è§†å›¾é‡å†™å’Œè‡ªåŠ¨æŸ¥è¯¢ç¼“å­˜å¤šä¸ªæŸ¥è¯¢å¯èƒ½éœ€è¦ç”¨åˆ°ç›¸åŒçš„ä¸­é—´è¡¨ï¼Œå¯ä»¥é€šè¿‡é¢„å…ˆè®¡ç®—å’Œå°†ä¸­é—´è¡¨ç¼“å­˜åˆ°è§†å›¾ä¸­æ¥é¿å…é‡å¤è®¡ç®—ã€‚æŸ¥è¯¢ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨åˆ©ç”¨é¢„å…ˆè®¡ç®—çš„ç¼“å­˜æ¥æé«˜æ€§èƒ½ã€‚ä¾‹å¦‚åŠ é€Ÿä»ªè¡¨ç›˜ä¸­çš„joinæ•°æ®æŸ¥è¯¢é€Ÿåº¦ã€‚ å…ƒæ•°æ®æ˜ å°„è¡¨Hiveä¼šä»JDBCæ•°æ®æºåˆ›å»ºä¸¤ä¸ªæ•°æ®åº“ï¼šinformation_schemaå’Œsysã€‚æ‰€æœ‰Metastoreè¡¨éƒ½æ˜ å°„åˆ°è¡¨ç©ºé—´ï¼Œå¹¶åœ¨sysä¸­å¯ç”¨ã€‚information_schemaæ•°æ®æ˜¾ç¤ºç³»ç»Ÿçš„çŠ¶æ€ã€‚ æ”¯æŒåŸºäºæˆæœ¬ä¼˜åŒ–çš„æ™ºèƒ½ä¸‹æ¨æŸ¥è¯¢ä¸€ä¸ªæ•°æ®æºæ—¶å¦‚æœè¯»å–å…¨éƒ¨æ•°æ®åå†è¿›è¡Œåˆ†ææ˜¯å¾ˆé«˜æˆæœ¬çš„ï¼Œé€šè¿‡JDBCè·å–è¿‡å¤šæ•°æ®å¯¼è‡´èµ„æºæµªè´¹ä»¥åŠæ€§èƒ½ä¸ä½³ï¼ŒHiveä¾é å…¶storage handleræ¥å£å’ŒApache Calciteæ”¯æŒçš„åŸºäºæˆæœ¬çš„ä¼˜åŒ–å™¨ï¼ˆCBOï¼‰å®ç°äº†å¯¹å…¶ä»–ç³»ç»Ÿçš„æ™ºèƒ½ä¸‹æ¨ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCalciteæä¾›ä¸æŸ¥è¯¢çš„é€»è¾‘è¡¨ç¤ºä¸­çš„è¿ç®—ç¬¦å­é›†åŒ¹é…çš„è§„åˆ™ï¼Œç„¶åç”Ÿæˆåœ¨å¤–éƒ¨ç³»ç»Ÿä¸­ç­‰æ•ˆçš„è¡¨ç¤ºä»¥æ‰§è¡Œæ›´å¤šæ“ä½œã€‚Hiveåœ¨å…¶æŸ¥è¯¢è®¡åˆ’å™¨ä¸­å°†è®¡ç®—æ¨é€åˆ°å¤–éƒ¨ç³»ç»Ÿï¼Œå¹¶ä¸”ä¾é Calciteç”Ÿæˆå¤–éƒ¨ç³»ç»Ÿæ”¯æŒçš„æŸ¥è¯¢è¯­è¨€ã€‚storage handlerçš„å®ç°è´Ÿè´£å°†ç”Ÿæˆçš„æŸ¥è¯¢å‘é€åˆ°å¤–éƒ¨ç³»ç»Ÿï¼Œæ£€ç´¢å…¶ç»“æœï¼Œå¹¶å°†ä¼ å…¥çš„æ•°æ®è½¬æ¢ä¸ºHiveå†…éƒ¨è¡¨ç¤ºï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶è¿›ä¸€æ­¥å¤„ç†ã€‚è¿™ä¸ä»…é™äºSQLç³»ç»Ÿï¼šä¾‹å¦‚ï¼ŒApache Hiveä¹Ÿå¯ä»¥è”é‚¦Apache Druidæˆ–Apache Kafkaè¿›è¡ŒæŸ¥è¯¢ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æœ€è¿‘çš„åšæ–‡ä¸­æ‰€æè¿°çš„ï¼ŒDruidå¯ä»¥éå¸¸é«˜æ•ˆçš„å¤„ç†æ—¶åºæ•°æ®çš„æ±‡æ€»å’Œè¿‡æ»¤ã€‚å› æ­¤ï¼Œå½“å¯¹å­˜å‚¨åœ¨Druidä¸­çš„æ•°æ®æºæ‰§è¡ŒæŸ¥è¯¢æ—¶ï¼ŒHiveå¯ä»¥å°†è¿‡æ»¤å’Œèšåˆæ¨é€ç»™Druidï¼Œç”Ÿæˆå¹¶å‘é€JSONæŸ¥è¯¢åˆ°å¼•æ“æš´éœ²çš„REST APIã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæ˜¯æŸ¥è¯¢Kafkaä¸Šçš„æ•°æ®ï¼ŒHiveå¯ä»¥åœ¨åˆ†åŒºæˆ–offsetä¸Šæ¨é€è¿‡æ»¤å™¨ï¼Œä»è€Œæ ¹æ®æ¡ä»¶è¯»å–topicä¸­çš„æ•°æ®ã€‚ Hive 3.0å…¶ä»–ç‰¹æ€§1ã€è¿æ¥Kafka Topicï¼Œç®€åŒ–äº†å¯¹Kafkaæ•°æ®çš„æŸ¥è¯¢2ã€æ‰§è¡ŒæŸ¥è¯¢æ‰€éœ€çš„å°‘é‡å®ˆæŠ¤è¿›ç¨‹ç®€åŒ–äº†ç›‘è§†å’Œè°ƒè¯•3ã€å·¥ä½œè´Ÿè½½ç®¡ç†(ä¼šè¯èµ„æºé™åˆ¶)ï¼šç”¨æˆ·ä¼šè¯æ•°ï¼ŒæœåŠ¡å™¨ä¼šè¯æ•°ï¼Œæ¯ä¸ªæœåŠ¡å™¨æ¯ä¸ªç”¨æˆ·ä¼šè¯æ•°ç­‰é™åˆ¶ï¼Œé˜²æ­¢èµ„æºäº‰ç”¨å¯¼è‡´èµ„æºä¸è¶³4ã€ä¼šè¯çŠ¶æ€ï¼Œå†…éƒ¨æ•°æ®ç»“æ„ï¼Œå¯†ç ç­‰é©»ç•™åœ¨å®¢æˆ·ç«¯è€Œä¸æ˜¯æœåŠ¡å™¨ä¸Š5ã€é»‘åå•å¯ä»¥é™åˆ¶å†…å­˜é…ç½®ä»¥é˜²æ­¢HiveServerä¸ç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç™½åå•å’Œé»‘åå•é…ç½®å¤šä¸ªHiveServerå®ä¾‹ï¼Œä»¥å»ºç«‹ä¸åŒçº§åˆ«çš„ç¨³å®šæ€§ è”é‚¦æŸ¥è¯¢æ”¯æŒOracleã€MySQLã€Kafkaã€Druidã€HDFSã€PostgreSQLç­‰å¤šä¸ªæ•°æ®æºçš„è”é‚¦æŸ¥è¯¢ï¼Œå¯ä»¥å¯¹å¤šä¸ªæ•°æ®æºç»Ÿä¸€è®¿é—®ã€‚è”é‚¦æŸ¥è¯¢çš„ä¼˜åŠ¿ï¼š1.å•ä¸ªSQLæ–¹è¨€å’ŒAPI2.é›†ä¸­ç»Ÿä¸€çš„æƒé™æ§åˆ¶å’Œå®¡è®¡è·Ÿè¸ªï¼ˆHiveæ”¯æŒè¡¨ã€è¡Œã€åˆ—çš„è®¿é—®æ§åˆ¶ï¼‰3.ç»Ÿä¸€æ²»ç†4.èƒ½å¤Ÿåˆå¹¶æ¥è‡ªå¤šä¸ªæ•°æ®æºçš„æ•°æ® ä¼˜ç¼ºç‚¹ ä¼˜ç‚¹ï¼šæ€§èƒ½ï¼Œå®‰å…¨æ€§ï¼Œå¯¹ACIDäº‹ç‰©çš„æ”¯æŒï¼Œå¯¹ä»»åŠ¡èµ„æºè°ƒåº¦çš„ä¼˜åŒ–ã€‚ ç¼ºç‚¹ï¼šç›®å‰æœ€æ–°çš„CDH6.3è¿˜ä¸å…¼å®¹Hive3ï¼Œè‡ªå·±å®‰è£…å‘ç‚¹å¤šï¼›ç›®å‰ç›¸å…³æ–‡çŒ®è¾ƒå°‘ï¼Œæ’é”™éš¾ã€‚ å®è·µhttps://link.zhihu.com/?target=https%3A//hortonworks.com/tutorial/interactive-sql-on-hadoop-with-hive-llap/https://link.zhihu.com/?target=https%3A//dzone.com/articles/3x-faster-interactive-query-with-apache-hive-llaphttps://link.zhihu.com/?target=https%3A//community.hortonworks.com/articles/149486/llap-sizing-and-setup.html å‚è€ƒèµ„æ–™Hive3æ–°ç‰¹æ€§Apache Tez äº†è§£Hive 3.x åŠŸèƒ½ä»‹ç»ä½¿ç”¨Apache Hive3å®ç°è·¨æ•°æ®åº“çš„è”é‚¦æŸ¥è¯¢","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hive","slug":"Hive","permalink":"https://shmily-qjj.top/tags/Hive/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"äººå£°æ··éŸ³æ€»ç»“","slug":"äººå£°æ··éŸ³æ€»ç»“","date":"2019-12-21T07:06:06.000Z","updated":"2022-12-11T05:35:07.912Z","comments":true,"path":"d2c4813b/","link":"","permalink":"https://shmily-qjj.top/d2c4813b/","excerpt":"","text":"äººå£°æ··éŸ³æ€»ç»“å‰è¨€2019å¹´å› ä¸ºæ¯”è¾ƒå¿™ï¼Œå‡ºçš„éŸ³ä¹ä½œå“å¹¶ä¸å¤šï¼Œæˆ‘è®°å¾—åªæ˜¯ç¿»å¼¹äº†ä¸€é¦–å²¸éƒ¨çœŸæ˜çš„æŒ‡å¼¹æ›²ã€Šå°‘å¹´çš„æ¢¦ã€‹å’Œç¿»å”±äº†ä¸€é¦–è‡ªå·±å¾ˆå–œæ¬¢çš„å¤é£æ­Œæ›²ã€Šæµ®ç”Ÿæœªæ­‡ã€‹â€¦2018å¹´æ¯”è¾ƒæ´»è·ƒï¼Œè®°å¾—2018å¹´çš„æš‘å‡ä¹°äº†å½•éŸ³ç¬”ï¼Œå¼€å§‹å½•å‰ä»–ï¼Œå½•ç¿»å”±ï¼Œå¯¹å½•éŸ³ç¬”çˆ±ä¸é‡Šæ‰‹ï¼Œæˆ‘çš„è®¾å¤‡å¾ˆç®€é™‹ï¼Œåªæœ‰ä¸€ä¸ªZOOM H1Nå°å‹å½•éŸ³ç¬”å’Œä¸€å°éŸ³è´¨ä¸å¥½çš„ç¬”è®°æœ¬ç”µè„‘ï¼ˆè¿™ä¸ªç”µè„‘å¯¹æˆ‘çš„æ··éŸ³é€ æˆäº†å¾ˆå¤§å½±å“ï¼Œç”¨å®ƒå¬èµ·æ¥ä¸é”™çš„éŸ³è‰²ï¼Œæ”¾åœ¨æ‰‹æœºä¸ŠæŒºå°±æ˜¯å¦ä¸€ä¸ªæ ·å­â€¦ï¼‰ï¼Œä½†æ˜¯é€šè¿‡è‡ªå·±æ‘¸ç´¢ï¼Œå‘ç°æ•ˆæœè¿˜æ˜¯å¯ä»¥çš„ã€‚ä½†ç°åœ¨å½“æˆ‘åˆé¢å¯¹ä¸€æ¡ä¸€æ¡çš„éŸ³è½¨ï¼Œçœ¼èŠ±ç¼­ä¹±çš„æ•ˆæœå™¨æ—¶ï¼Œæˆ‘æœ‰ç‚¹ä¸çŸ¥æ‰€æªçš„æ„Ÿè§‰ï¼Œçš„å´ï¼Œå°†è¿‘å¤§åŠå¹´æ—¶é—´æ²¡æ··éŸ³ï¼Œå¬æ„Ÿå’Œå¯¹æ•ˆæœå™¨çš„æŒæ¡å·²ç»å¿˜æ‰å¤ªå¤šï¼Œæ‰€ä»¥å†³å®šå†™è¿™ç¯‡åšå®¢ï¼Œç£ä¿ƒè‡ªå·±å­¦ä¹ æ··éŸ³çŸ¥è¯†ï¼Œåšåˆ°ç²¾ç›Šæ±‚ç²¾ã€‚é€šè¿‡æ·±å…¥æ··éŸ³æŠ€æœ¯ï¼Œæˆ‘ç›¸ä¿¡æˆ‘çš„æ··éŸ³ä½œå“èƒ½å¤Ÿå˜å¾—æ›´å¥½ï¼ç½‘æ˜“äº‘æ­Œæ‰‹é¡µï¼š**ä½³å¢ƒShmily** åŸºæœ¬æ­¥éª¤ä¹Ÿæ˜¯æˆ‘äººå£°éƒ¨åˆ†çš„æ’ä»¶æ•ˆæœå™¨é¡ºåºï¼Œå½“ç„¶ä¹Ÿä¸ç»å¯¹ å½•éŸ³ï¼ˆä¸€åˆ‡å§‹äºæºå¤´ï¼Œå¦‚æœå½•çš„å¹²å£°æœ‰é—®é¢˜ï¼ŒåæœŸå†å¼ºä¹Ÿæ— æ³•ä¿®å¤ï¼Œæ‰€ä»¥æœ‰ç‘•ç–µçš„åœ°æ–¹è¦åå¤å½•åˆ¶ï¼‰ é™å™ªå¹²å£°çš„ç©ºç™½éƒ¨åˆ†çš„å™ªéŸ³ç¼“è§£ï¼Œå¤šå¤šå°‘å°‘å¯¹å¹²å£°éŸ³è´¨æœ‰æŸå¯ä»¥ä½¿ç”¨æ¯”è¾ƒå¼ºå¤§çš„iZotope RX7ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨Waveså®¶çš„X-Noiseå’ŒZ-Noise ä¿®éŸ³ï¼ˆä½¿ç”¨WavesTuneä¿®å¤éŸ³å‡†ï¼Œè½»å¾®è·‘è°ƒæ˜¯åŸºæœ¬å¯ä»¥æ— æŸä¿®å¤çš„ï¼‰ äººå£°åŠ¨æ€è°ƒæ•´äººå£°çš„åŠ¨æ€å¾ˆå¤§ï¼Œè¿™é‡Œå¯ä»¥ä¸ºæ¯æ®µéŸ³é¢‘è°ƒæ•´è¾“å‡ºçš„åŠ¨æ€ï¼Œæœ‰Wavesçš„ä¹Ÿå¯ä»¥ä½¿ç”¨Vocal Rideræ¥è§£å†³äººå£°éŸ³é‡ä¸å‡åŒ€çš„é—®é¢˜Vocal Riderå¯ä»¥ä¾¦æµ‹éŸ³é‡ä»€ä¹ˆæ—¶å€™è¯¥é«˜ï¼Œä»€ä¹ˆæ—¶å€™è¯¥ä½ï¼Œå¹¶è‡ªåŠ¨è¿›è¡Œè°ƒæ•´ï¼ŒVocal Riderä¸æ˜¯å‹ç¼©å™¨ï¼Œä¸ä¼šå¯¹å£°éŸ³éŸ³è‰²äº§ç”Ÿå½±å“ç”¨æ³•ï¼šâ‘ è°ƒèŠ‚â€œç›®æ ‡â€é€‰ä¸€ä¸ªç†æƒ³çš„é»˜è®¤éŸ³é‡ â‘¡è°ƒèŠ‚rangeè‡ªåŠ¨éŸ³é‡è°ƒèŠ‚èŒƒå›´ï¼Œä¹Ÿå°±æ˜¯åŠ¨æ€å¤§å° å‡è¡¡å™¨è¿™æ­¥åº”è¯¥æ˜¯å¯¹äººå£°éŸ³è‰²å½±å“æœ€å¤§çš„ï¼Œå¯ä»¥ä½¿ç”¨å¤šä¸ªå‡è¡¡å™¨ä¸²è” é½¿éŸ³ç¼“è§£é½¿éŸ³ä¸èƒ½è¢«å®Œå…¨æ¶ˆé™¤ï¼Œåº”è¯¥åœ¨ä¿è¯äººå£°éŸ³è‰²æ— æŸçš„å‰æä¸‹å°½å¯èƒ½å‡å°‘é½¿éŸ³ä½¿ç”¨RDeEsseræˆ–DeEsser äººå£°é½¿éŸ³6khzå·¦å³ å‹é™å‹ç¼©å’Œé™åˆ¶ï¼Œé˜²æ­¢ä¿¡å·è¿‡è½½ï¼Œé˜²æ­¢äººå£°å¿½å¤§å¿½å°ï¼Œé˜²æ­¢äººå£°åŠ¨æ€è¿‡å¤§ä¸åŒå‹ç¼©å™¨æœ‰ä¸åŒçš„éŸ³æŸ“ç‰¹è‰²ï¼Œèƒ½ä½¿éŸ³è‰²æ›´ä¼˜ç¾å‹ç¼©å™¨ç§ç±»å¾ˆå¤šï¼Œæˆ‘å¸¸ç”¨CLA-76 C1ç­‰Waveså®¶çš„å‹ç¼©å™¨ é™åˆ¶å™¨ä¸€èˆ¬ä¹Ÿæ˜¯ä½¿ç”¨Waveså®¶çš„L3LL Ultra Stereoã€L1ã€L2ç­‰ é¥±å’Œåº¦ä¸å¤±çœŸå¾ˆå°çš„é¥±å’Œåº¦ä¸å¤±çœŸèƒ½è®©äººå£°æ›´åšå®ï¼Œæ·»åŠ æ³›éŸ³è®©äººå£°æ›´çªå‡ºå¯ç”¨NLS Channelçš„å¤±çœŸéƒ¨åˆ†ï¼Œä¹Ÿæœ‰æ¨èScheps 73ï¼Œä¸è¿‡æˆ‘çš„Wavesç‰ˆæœ¬æ²¡æœ‰è¿™ä¸ªæ’ä»¶ æ··å“å’Œå»¶è¿Ÿå¢åŠ ç©ºé—´æ„Ÿï¼Œæ··å“å’Œå»¶è¿Ÿå¯¹å•å£°é“å¯ä»¥å¢åŠ æ·±åº¦æ„Ÿï¼Œå¯¹åŒå£°é“å¯ä»¥å¢åŠ å®½åº¦æ„Ÿæ··å“çš„æ—¶é—´çŸ­ï¼Œäº§ç”Ÿçš„ç©ºé—´å°ï¼Œæ—¶é—´é•¿åˆ™äº§ç”Ÿçš„ç©ºé—´ä¹Ÿè¾ƒé•¿æ··å“ä½¿ç”¨Waves RVerbå»¶è¿Ÿä½¿ç”¨H-Delayï¼Œç”¨æ¥å¼ºè°ƒæŸäº›è¯ æ¿€åŠ±å™¨å¯¹ä¸€äº›éŸ³è½¨è¿›è¡ŒéŸ³é‡å¢ç›Šï¼Œå¹²æ¹¿åº¦å¢ç›Šç­‰æ“ä½œï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œä½¿ç”¨Wavesä¸­çš„OneKnobç³»åˆ—æ’ä»¶ï¼Œä¸€ä¸ªæ—‹é’®è§£å†³é—®é¢˜ï¼ æ¯å¸¦æ¯å¸¦å¤„ç†åŒ…æ‹¬æ€»ä½“æ¿€åŠ±ã€å‡è¡¡ã€æ··å“ã€éŸ³é‡æå‡ç­‰ï¼Œæ¨èä½¿ç”¨Ozone 8ã€‚æˆ‘ç›®å‰å¯¹æ¯å¸¦å¤„ç†è¿˜ä¸æ˜¯å¾ˆäº†è§£ã€‚ çŸ¥è¯†ç‚¹ å‡è¡¡å™¨EQ å‡è¡¡å™¨EQæ³¨æ„äº‹é¡¹ç½‘ä¸Šæœ‰æ¨èFabFilter Pro-Qè¿™ä¸ªæ’ä»¶ï¼Œä¸è¿‡æˆ‘ç”¨çš„Wavesçš„ä¸€å¥— EQé»„é‡‘å®šå¾‹ï¼š 0ã€å‡çª„å¢å®½ï¼Œåœ¨ç”¨å‡æ³•eqæ—¶è¦ç”¨è¾ƒé«˜çš„Qå€¼ï¼Œç”¨åŠ æ³•EQæ—¶é‡‡ç”¨è¾ƒä½çš„Qå€¼ã€‚ 1ã€å¦‚æœå£°éŸ³æµ‘æµŠï¼Œè¯·è¡°å‡250hzé™„è¿‘çš„é¢‘æ®µ 2ã€å¦‚æœå£°éŸ³å¬èµ·æ¥æœ‰å–‡å­éŸ³ï¼Œè¯·è¡°å‡800hzé™„è¿‘çš„é¢‘æ®µ 3ã€å½“ä½ è¯•å›¾è®©å£°éŸ³å¬èµ·æ¥æ›´å¥½ï¼Œè¯·è€ƒè™‘ç”¨è¡°å‡ 4ã€å½“ä½ è¯•å›¾è®©å£°éŸ³å¬èµ·æ¥ä¸ä¼—ä¸åŒï¼Œè¯·è€ƒè™‘ç”¨æå‡ 5ã€ä¸è¦æ”¾å¤§åŸå…ˆæ²¡æœ‰çš„å£°éŸ³ å‡æ³•å‡è¡¡å™¨F6 Dynamic EQ Waves Q10100hzä»¥ä¸‹åˆ‡æ‰ åŠ æ³•å‡è¡¡å™¨ å¸¸ç”¨æ•ˆæœå™¨çš„ä½¿ç”¨","categories":[{"name":"éŸ³ä¹","slug":"éŸ³ä¹","permalink":"https://shmily-qjj.top/categories/%E9%9F%B3%E4%B9%90/"}],"tags":[{"name":"äººå£°æ··éŸ³","slug":"äººå£°æ··éŸ³","permalink":"https://shmily-qjj.top/tags/%E4%BA%BA%E5%A3%B0%E6%B7%B7%E9%9F%B3/"}],"keywords":[{"name":"éŸ³ä¹","slug":"éŸ³ä¹","permalink":"https://shmily-qjj.top/categories/%E9%9F%B3%E4%B9%90/"}]},{"title":"ç¨‹åºçŒ¿æ˜¯æ€ä¹ˆè¡¨ç™½çš„","slug":"ç¨‹åºçŒ¿æ˜¯æ€ä¹ˆè¡¨ç™½çš„","date":"2019-12-11T13:10:21.000Z","updated":"2022-12-11T05:35:07.923Z","comments":true,"path":"d1c9241f/","link":"","permalink":"https://shmily-qjj.top/d1c9241f/","excerpt":"","text":"ç¨‹åºçŒ¿æ˜¯æ€ä¹ˆè¡¨ç™½çš„Hi Dear,è¦çœ‹çœ‹æˆ‘çš„ä»£ç å—ï¼Ÿä¸çœ‹æ€ä¹ˆçŸ¥é“æˆ‘çˆ±ä½ ï¼ç¨‹åºçŒ¿æ˜¯æ€ä¹ˆè¡¨ç™½çš„ï¼Ÿå½“ç„¶ä¹Ÿç¦»ä¸å¼€æ•²ä»£ç å•¦ï¼å°±ç¼ºä¸ªæ‡‚ä»£ç çš„å¥³æœ‹å‹äº†ã€‚ã€‚ã€‚ é›¶.ä¸€è¡Œä»£ç è¾“å‡ºä¸€ä¸ªå¿ƒprint(&#39;\\n&#39;.join([&#39;&#39;.join([(&#39;Love&#39;[(x-y) % len(&#39;Love&#39;)] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 &lt;= 0 else &#39; &#39;) for x in range(-30, 30)]) for y in range(30, -30, -1)])) python -c &quot;print(&#39;\\033[35m&#39;+&#39;\\n&#39;.join([&#39;&#39;.join([(&#39;Love&#39;[(x-y) % len(&#39;Love&#39;)] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 &lt;= 0 else &#39; &#39;) for x in range(-30, 30)]) for y in range(30, -30, -1)])+&#39;\\033[0m&#39;)&quot; ä¸€.æˆ‘çš„ä¸–ç•Œåªæœ‰å¤ªé˜³ã€æœˆäº®å’Œä½ /** * I love three things in this world.Sun, moon and you. Sun for morning, moon for night, and you forever. */ class LoveThreeThings extends Me &#123; const loveFirstThings = &#39;Sun&#39;; const loveSecondThings = &#39;Moon&#39;; const loveThirdThings = &#39;You&#39;; public function MyLove() &#123; return &#39;I Love&#39; . self::loveThirdThings . &#39;forever. Never change!&#39;; &#125; &#125; äºŒ.ç™¾å¹´å¥½åˆwhile(&#39;ILoveyou&#39;): for IBeWithYou in range(0,50*365): time.sleep(60*60*24) //ç¨‹åºèƒ½ä¸€ç›´æ‰§è¡Œï¼Œæ‰§è¡Œå®Œ50å¹´ï¼Œè‹¥æˆ‘ä»¬è¿˜æœ‰50å¹´ï¼Œä½™ç”Ÿç»§ç»­ã€‚ //ä»å‰çš„æ—¥è‰²å˜å¾—å¾ˆæ…¢ //è½¦é©¬é‚®ä»¶éƒ½æ…¢ //ä¸€ç”Ÿåªå¤Ÿçˆ±ä¸€äºº ä¸‰.è°éƒ½ä¸èƒ½æŒæ§å…¨ä¸–ç•Œï¼Œä½†ä½ è‡³å°‘å¯ä»¥æŒæ§æˆ‘ï¼Œè¿™æ˜¯æˆ‘çš„æ¸©æŸ” world.controlledBy(NoOne) withMyGentle() &#123; you.control(me).equals(true) &#125; //è°éƒ½ä¸èƒ½æŒæ§å…¨ä¸–ç•Œï¼Œä½†ä½ è‡³å°‘å¯ä»¥æŒæ§æˆ‘ï¼Œè¿™æ˜¯æˆ‘çš„æ¸©æŸ” å››.è‹¥çˆ±ï¼Œè¯·æ·±çˆ± if(love ==1) &#123; while(1) &#123; love_depth ++; &#125; &#125; äº”.å°†æˆ‘æ‰‹ä¸Šçš„æ¸©åº¦å…¨éƒ¨ç»™äºˆä½ ï¼Œæ¢å–ä½ å¹¸ç¦çš„è„¸åºif(you.hand==cold&amp;&amp;weather==winter): //å¦‚æœå†¬å¤©é‡Œä½ çš„æ‰‹æ˜¯å†°å†·çš„ giveyoulove(myhand.temp,yourhand.temp); //å°†æˆ‘æ‰‹ä¸Šçš„æ¸©åº¦å…¨éƒ¨ç»™äºˆä½  return you.happyface; //æ¢å–ä½ å¹¸ç¦çš„è„¸åº å…­.æˆ‘ä¸€ç›´åœ¨æ‰¾ä½ ï¼Œå½“æˆ‘æ‰¾åˆ°ä½ ï¼Œä¹Ÿå°±æ‰¾åˆ°äº†æ•´ä¸ªä¸–ç•Œwhile (i.findYou()) &#123; if (i.get() == you) &#123; System.out.print(&quot;Hello,Word!&quot;); &#125; &#125; ä¸ƒ.å¦‚æœå½¼æ­¤ç›¸çˆ±ï¼Œé‚£å°±ç™½å¤´å•è€if(you.love(me) &amp;&amp; I.love(you))&#123; // å¦‚æœå½¼æ­¤ç›¸çˆ± this.liveToOld();// é‚£å°±ç™½å¤´å•è€ &#125;else&#123;// å¦åˆ™ this.bestWishesToYou(); // ç¥ä½ ä¸€åˆ‡å®‰å¥½ &#125; å…«.é™ªä¼´æ˜¯æœ€é•¿æƒ…çš„å‘Šç™½StringBuilder love = newStringBuilder(&quot;&quot;); for(;;)ï½› love.append(Math.random()&gt;0.5?1:0); ï½ ä¹.è‡ªä»é‡è§ä½ ï¼Œå°±ä¸åœåœ°æƒ³ä½ public void missing_you(meet_you) int time = 0 forï¼ˆtime=meet_you;;time++ï¼‰ï½› missing_you(); ï½ å.ä½ è‹¥ä¸æ¥ï¼Œæˆ‘ä¾¿ä¸å¼ƒpublic Me waitYou()&#123; if(!appear)&#123; this.waitYou(); &#125;else&#123; this.waitYou(); &#125; &#125; åä¸€.å¬è¯´ä½ è¦èµ°ï¼Œç«™åœ¨é›¨é‡Œï¼Œä»»å‡­èº«ä½“è¢«æ°´ç æ’•è£‚æˆä¸€ä¸ªä¸ªæ²¡æœ‰æ„ä¹‰çš„å­—æ¯public void hearYouLeave()&#123; String body = myself.toString(); body.split(&quot;.&quot;); &#125; åäºŒ.æ¯ä¸€ä¸–ï¼Œæˆ‘éƒ½ä¼šåœ¨è¿™ç­‰ä½ ï¼å°±ç®—å®¹é¢œå˜è¿ï¼Œå°±ç®—æ—¶å…‰æµè½¬public Me findYou()&#123; for(int age = 0;age &lt;= 120; age++)&#123; try&#123; Thread.sleep(60); if(this.waitMyLove() != null)&#123; return this.waitMyLove(); &#125;cache(InterruptedException e)&#123; System.out.println(&quot;Time is nothing...&quot;); &#125; &#125; return null; &#125; åä¸‰.å¾…æ›´æ–°å¾…æ›´æ–°... å‚è€ƒèµ„æ–™ï¼šå¦‚ä½•ç”¨ä½ çš„ä¸“ä¸šæ¥è¡¨ç™½ï¼Ÿ","categories":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"è¡¨ç™½","slug":"è¡¨ç™½","permalink":"https://shmily-qjj.top/tags/%E8%A1%A8%E7%99%BD/"},{"name":"ç¨‹åºçŒ¿","slug":"ç¨‹åºçŒ¿","permalink":"https://shmily-qjj.top/tags/%E7%A8%8B%E5%BA%8F%E7%8C%BF/"}],"keywords":[{"name":"ç”Ÿæ´»","slug":"ç”Ÿæ´»","permalink":"https://shmily-qjj.top/categories/%E7%94%9F%E6%B4%BB/"}]},{"title":"Mysql Event Scheduler","slug":"Mysql Event Scheduler","date":"2019-11-15T13:25:04.000Z","updated":"2022-12-11T05:35:07.909Z","comments":true,"path":"3c26421b/","link":"","permalink":"https://shmily-qjj.top/3c26421b/","excerpt":"","text":"Mysqläº‹ä»¶è°ƒåº¦å™¨å·¥ä½œçš„æ—¶å€™é‡åˆ°ä¸€å¼ è¡¨éœ€è¦æ¯å¤©Truncateï¼Œå°±æƒ³åˆ°äº†Mysqlçš„Event Schedulerï¼Œä½†æ˜¯åˆå¿˜äº†å®ƒçš„è¯­æ³•äº†ï¼Œæ‰€ä»¥è¿™é‡Œæ¥å¤ä¹ ä¸€ä¸‹ã€‚ ä»€ä¹ˆæ˜¯Event Scheduleräº‹ä»¶è°ƒåº¦å™¨ï¼Œå¯ä»¥ä½œä¸ºå®šæ—¶è°ƒåº¦å™¨ï¼Œç±»ä¼¼äºCrontabï¼Œå¯ä»¥å–ä»£éƒ¨åˆ†æ“ä½œç³»ç»Ÿä»»åŠ¡è°ƒåº¦å™¨çš„å®šæ—¶ä»»åŠ¡å·¥ä½œã€‚Mysqlåœ¨5.1ç‰ˆæœ¬åæ–°å¢äº†äº‹ä»¶è°ƒåº¦å™¨ï¼Œå®ƒå¯ä»¥æ”¯æŒç§’çº§è°ƒåº¦ï¼Œå¾ˆå®ç”¨æ–¹ä¾¿ã€‚æ—¶é—´è°ƒåº¦å™¨ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªè§¦å‘å™¨ï¼Œæ˜¯é’ˆå¯¹æŸä¸ªè¡¨è¿›è¡Œæ“ä½œçš„ï¼Œæ—¶é—´è°ƒåº¦å™¨æ‰§è¡Œé‡‡ç”¨äº†å•ç‹¬ä¸€ä¸ªçº¿ç¨‹ï¼Œå¯é€šè¿‡**SHOW PROCESSLIST**å‘½ä»¤æŸ¥çœ‹ Event Schedulerè¯­æ³•CREATE [DEFINER = &#123; user | CURRENT_USER &#125;] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT &#39;string&#39;] DO event_body; schedule: AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...] interval: quantity &#123;YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND&#125; è¯­æ³•è¯´æ˜ï¼š DEFINERï¼šæŒ‡å®šå¯æ‰§è¡Œè¯¥å®šæ—¶å™¨çš„MySQLè´¦å·ï¼Œuserçš„æ ¼å¼æ˜¯â€™user_nameâ€™@â€™host_nameâ€™ï¼ŒCURRENT_USERæˆ–CURRENT_USER()ï¼Œå•å¼•å·æ˜¯éœ€è¦åœ¨è¯­å¥ä¸­è¾“å…¥çš„ã€‚å¦‚æœä¸æŒ‡å®šï¼Œé»˜è®¤æ˜¯DEFINER = CURRENT_USERã€‚ event_nameï¼šäº‹ä»¶åç§°ï¼Œæœ€å¤§64ä¸ªå­—ç¬¦ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼ŒMyEventå’Œmyeventæ˜¯ä¸€æ ·çš„ï¼Œå‘½åè§„åˆ™å’Œå…¶ä»–MySQLå¯¹è±¡æ˜¯ä¸€æ ·çš„ã€‚ ON SCHEDULE scheduleï¼šON SCHEDULEæŒ‡å®šäº‹ä»¶ä½•æ—¶æ‰§è¡Œï¼Œæ‰§è¡Œçš„é¢‘ç‡å’Œæ‰§è¡Œçš„æ—¶é—´æ®µï¼Œæœ‰ATå’ŒEVERYä¸¤ç§å½¢å¼ã€‚ [ON COMPLETION [NOT] PRESERVE]ï¼šå¯é€‰ï¼Œpreserveæ˜¯ä¿æŒçš„æ„æ€ï¼Œè¿™é‡Œæ˜¯è¯´è¿™ä¸ªå®šæ—¶å™¨ç¬¬ä¸€æ¬¡æ‰§è¡Œå®Œæˆä»¥åæ˜¯å¦è¿˜éœ€è¦ä¿æŒï¼Œå¦‚æœæ˜¯NOT PRESERVEï¼Œè¯¥å®šæ—¶å™¨åªæ‰§è¡Œä¸€æ¬¡ï¼Œå®Œæˆåè‡ªåŠ¨åˆ é™¤äº‹ä»¶ï¼›æ²¡æœ‰NOTï¼Œè¯¥å®šæ—¶å™¨ä¼šå¤šæ¬¡æ‰§è¡Œï¼Œå¯ä»¥ç†è§£ä¸ºè¿™ä¸ªå®šæ—¶å™¨æ˜¯æŒä¹…æ€§çš„ã€‚é»˜è®¤æ˜¯NOT PRESERVEã€‚ [ENABLE | DISABLE | DISABLE ON SLAVE]ï¼šå¯é€‰ï¼Œæ˜¯å¦å¯ç”¨è¯¥äº‹ä»¶ï¼ŒENABLE-å¯ç”¨ï¼ŒDISABLE-ç¦ç”¨ï¼Œå¯ä½¿ç”¨alter eventè¯­å¥ä¿®æ”¹è¯¥çŠ¶æ€ã€‚DISABLE ON SLAVEæ˜¯æŒ‡åœ¨ä¸»å¤‡å¤åˆ¶çš„æ•°æ®åº“æœåŠ¡å™¨ä¸­ï¼Œåœ¨å¤‡æœºä¸Šä¹Ÿåˆ›å»ºè¯¥å®šæ—¶å™¨ï¼Œä½†æ˜¯ä¸æ‰§è¡Œã€‚ COMMENT: æ³¨é‡Šï¼Œå¿…é¡»ç”¨å•å¼•å·æ‹¬ä½ã€‚ DO event_bodyï¼šäº‹ä»¶è¦æ‰§è¡Œçš„SQLè¯­å¥ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªSQLï¼Œä¹Ÿå¯ä»¥æ˜¯ä½¿ç”¨BEGINå’ŒENDçš„å¤åˆè¯­å¥ï¼Œå’Œå­˜å‚¨è¿‡ç¨‹ç›¸åŒã€‚ ON SCHEDULEæ—¶é—´ç±»å‹ä¸¤ç§æ—¶é—´ç±»å‹*AT timestamp**å’ŒEvery interval*** AT timestampç”¨äºåªæ‰§è¡Œä¸€æ¬¡çš„äº‹ä»¶ã€‚æ‰§è¡Œçš„æ—¶é—´ç”±timestampæŒ‡å®šï¼Œtimestampå¿…é¡»åŒ…å«å®Œæ•´çš„æ—¥æœŸå’Œæ—¶é—´ï¼Œå³å¹´æœˆæ—¥æ—¶åˆ†ç§’éƒ½è¦æœ‰ã€‚å¯ä»¥ä½¿ç”¨DATETIMEæˆ–TIMESTAMPç±»å‹ï¼Œæˆ–è€…å¯ä»¥è½¬æ¢æˆæ—¶é—´çš„å€¼ï¼Œä¾‹å¦‚â€œ2018-01-21 00:00:00â€ã€‚å¦‚æœæŒ‡å®šæ˜¯æ—¶é—´æ˜¯è¿‡å»çš„æ—¶é—´ï¼Œè¯¥äº‹ä»¶ä¸ä¼šæ‰§è¡Œï¼Œå¹¶ç”Ÿæˆè­¦å‘Šã€‚ mysql&gt; create table test(id int,name varchar(255)); Query OK, 0 rows affected (0.01 sec) mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:30:59 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:30:59&#39; DO show tables; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; show warnings\\G; *************************** 1. row *************************** Level: Note Code: 1588 Message: Event execution time is in the past and ON COMPLETION NOT PRESERVE is set. The event was dropped immediately after creation. 1 row in set (0.00 sec) ERROR: No query specified mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:34:59&#39; DO show tables; Query OK, 0 rows affected (0.01 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:37:59&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) æ—¶é—´è¿‡åæˆ‘å‘ç°æˆ‘çš„testè¡¨é‡Œä»ç„¶æ²¡æ•°æ® mysql&gt; show variables like &quot;event_scheduler&quot;; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | event_scheduler | OFF | +-----------------+-------+ 1 row in set (0.00 sec) åŸå› æ˜¯æˆ‘æ²¡å¼€å¯event_schedulervim /etc/my.cnf åœ¨[mysqld]è¿™ä¸€æ ä¸‹æ·»åŠ *event_scheduler = ON**æ¥æ°¸ä¹…å¯ç”¨event_scheduleré‡å¯mysqlæœåŠ¡systemctl restart mysqld.service*** mysql&gt; select NOW(); +---------------------+ | NOW() | +---------------------+ | 2019-11-16 11:40:37 | +---------------------+ 1 row in set (0.00 sec) mysql&gt; create event insert_test ON SCHEDULE AT &#39;2019-11-16 11:41:37&#39; DO insert into test(id,name) values (1,&#39;qjj&#39;); Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test; +------+------+ | id | name | +------+------+ | 1 | qjj | +------+------+ 1 row in set (0.00 sec) mysql&gt; show events; Empty set (0.00 sec) # ä¸€å°æ—¶åæ‰§è¡Œ å‘½ä»¤ç¤ºä¾‹ mysql&gt; CREATE EVENT update_test ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 HOUR DO UPDATE test SET id = 2; Query OK, 0 rows affected (0.00 sec) ä¸Šè¿°ç»“æœè¯´æ˜:å¿…é¡»å…ˆå¼€å¯event_schedulerä¹‹åeventæ‰ä¼šç”Ÿæ•ˆï¼ŒAT timestampçš„æ–¹å¼åªä¼šåœ¨æŒ‡å®šæ—¶é—´ç‚¹æ‰§è¡Œä¸€æ¬¡ï¼Œç„¶åè¿™ä¸ªeventå°±ä¼šè¢«é”€æ¯ï¼Œå¦‚æœæŒ‡å®šçš„æ—¶é—´æ˜¯è¿‡å»çš„æ˜¯æ—¶é—´ç‚¹ï¼Œåˆ™è¿™ä¸ªeventä¼šæœ‰è­¦å‘Šï¼Œä¸”ä¸æ‰§è¡Œä¹Ÿä¸ä¿ç•™eventã€‚ Every intervalè®©äº‹ä»¶å®šæœŸæ‰§è¡Œï¼Œæ¯å¤šä¹…æ‰§è¡Œä¸€æ¬¡ON SCHEDULEåé¢æ—¶é—´å†™æ³•çš„å‡ ä¸ªæ —å­ï¼šEVERY 6 WEEK æ¯å…­å‘¨EVERY 20 second æ¯20ç§’EVERY 3 MONTH STARTS CURRENT_TIMESTAMP + INTERVAL 1 WEEK ä¸€å‘¨ä»¥åå¼€å§‹ï¼Œæ¯éš”ä¸‰ä¸ªæœˆEVERY 2 WEEK STARTS CURRENT_TIMESTAMP + INTERVAL â€˜6:15â€™ HOUR_MINUTE 6å°æ—¶15åˆ†é’Ÿä»¥åå¼€å§‹ï¼Œæ¯éš”ä¸¤å‘¨æ‰§è¡ŒEVERY 1 DAY STARTS CURRENT_TIMESTAMP + INTERVAL 5 MINUTE ENDS CURRENT_TIMESTAMP + INTERVAL 2 WEEK 5åˆ†é’Ÿä»¥åå¼€å§‹ï¼Œæ¯éš”ä¸€å¤©æ‰§è¡Œï¼Œä¸¤å‘¨åç»“æŸ ä¸¾ä¸ªæ —å­ mysql&gt; create event daily_truncate_test -&gt; ON SCHEDULE -&gt; EVERY 1 DAY -&gt; COMMENT &#39;æ¯å¤©æ‰§è¡Œä¸€æ¬¡æ¸…ç©ºtestè¡¨æ•°æ®&#39; -&gt; DO -&gt; truncate test; Query OK, 0 rows affected (0.00 sec) mysql&gt; mysql&gt; show events; +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | Db | Name | Definer | Time zone | Type | Execute at | Interval value | Interval field | Starts | Ends | Status | Originator | character_set_client | collation_connection | Database Collation | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ | test | daily_truncate_test | root@CDH066 | SYSTEM | RECURRING | NULL | 1 | DAY | 2019-11-16 12:10:36 | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | | test | update_test | root@CDH066 | SYSTEM | ONE TIME | 2019-11-16 12:58:52 | NULL | NULL | NULL | NULL | ENABLED | 1 | utf8 | utf8_unicode_ci | utf8_unicode_ci | +------+---------------------+-------------+-----------+-----------+---------------------+----------------+----------------+---------------------+------+---------+------------+----------------------+----------------------+--------------------+ 2 rows in set (0.00 sec) mysql&gt; SHOW PROCESSLIST; +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ | 1 | event_scheduler | localhost | NULL | Daemon | 721 | Waiting for next activation | NULL | | 7 | root | CDH066:34902 | test | Query | 0 | starting | SHOW PROCESSLIST | +----+-----------------+--------------+------+---------+------+-----------------------------+------------------+ 2 rows in set (0.00 sec) # ç¤ºä¾‹2 æŒ‡å®šæ¯å¤©å…·ä½“æ—¶é—´ç‚¹çš„eventäº‹ä»¶ CREATE EVENT truncate_with_time ON SCHEDULE EVERY 1 day STARTS date_add(concat(current_date(), &#39; 00:00:00&#39;), interval 0 second) ON COMPLETION PRESERVE ENABLE COMMENT DO TRUNCATE test; æ“ä½œå’ŒæŸ¥çœ‹äº‹ä»¶show events; # æŸ¥çœ‹äº‹ä»¶åŠå…¶çŠ¶æ€ ALTER EVENT daily_truncate_test DISABLE; # ç¦ç”¨æŒ‡å®šäº‹ä»¶ ALTER EVENT daily_truncate_test ENABLE; # å¯ç”¨æŒ‡å®šäº‹ä»¶ ALTER EVENT daily_truncate_test RENAME TO daily_truncate; # é‡å‘½åäº‹ä»¶ ALTER EVENT test.daily_truncate_test RENAME TO qjj_test.daily_truncate_test; # äº‹ä»¶æ˜¯æ•°æ®åº“å±‚é¢çš„ï¼Œå¯ä»¥æŠŠäº‹ä»¶ä»ä¸€ä¸ªæ•°æ®åº“ç§»åŠ¨åˆ°å¦ä¸€ä¸ªæ•°æ®åº“(å¦ä¸€ä¸ªæ•°æ®åº“è¦æœ‰å¯¹åº”çš„è¡¨) DROP EVENT daily_truncate; # åˆ é™¤äº‹ä»¶ æ€»ç»“Mysqlä½œä¸ºæœ€çƒ­é—¨çš„å…³ç³»å‹æ•°æ®åº“ä¹‹ä¸€ï¼Œæœ‰å¾ˆå¤šä¸œè¥¿å€¼å¾—æˆ‘ä»¬å»æ¢ç´¢ï¼Œå¥½è®°æ€§ä¸å¦‚çƒ‚ç¬”å¤´ï¼Œå†™äº†åšå®¢ï¼Œå¯¹äº‹ä»¶è°ƒåº¦å™¨çš„ç†è§£æ›´åŠ æ·±åˆ»äº†ã€‚","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://shmily-qjj.top/tags/Mysql/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"æµ…è°ˆgroup byä¸distinctå»é‡","slug":"æµ…è°ˆgroup byä¸distinctå»é‡","date":"2019-11-13T12:45:37.000Z","updated":"2022-12-11T05:35:07.921Z","comments":true,"path":"96009187/","link":"","permalink":"https://shmily-qjj.top/96009187/","excerpt":"","text":"å‰è¨€ä»Šå¤©å¸¦æˆ‘çš„è€å“¥è®©æˆ‘æ”¹ä¸€ä¸‹æŠ¥è­¦æ¨¡å—ï¼ŒæŠŠä¸€äº›è”ç³»æ–¹å¼ç­‰ä¿¡æ¯å­˜åœ¨Mysqlé‡Œï¼Œæ–¹ä¾¿ä»¥åç®¡ç†å’Œç»´æŠ¤ï¼Œå¾ˆç®€å•çš„ä¸œè¥¿ï¼Œä¸ºäº†å‡å°‘Mysqlå¹¶å‘å‹åŠ›ï¼Œæˆ‘æƒ³æ¯æ¬¡æŠ¥è­¦åªæŸ¥è¯¢ä¸€æ¬¡æ•°æ®åº“ï¼Œä½†å­æŸ¥è¯¢è¿”å›çš„ç»“æœæœ‰é‡å¤è®°å½•ï¼Œäºæ˜¯æˆ‘å†™äº†ä¸ªç±»ä¼¼**SELECT col1,col2,col3,col4 FROM (selectâ€¦) GROUP BY col1;**çš„è¯­å¥æ¥å»é‡ï¼Œå¯ä»¥æ­£å¸¸æ‰§è¡Œä¸æŠ¥é”™ä¸”è¾¾åˆ°äº†ç›®çš„å°±ç›´æ¥ä½¿ç”¨äº†ï¼Œä¹Ÿæ²¡æ·±ç©¶ã€‚ç›´åˆ°è€å“¥çœ‹äº†ä»£ç è¯´æ‰¾æˆ‘è¯´è€å¼Ÿå‘€ä½ è¿™ä¸ªé€»è¾‘ï¼Œæœ‰ç‚¹é—®é¢˜å‘€ã€‚ã€‚ã€‚ æŒ‰ç†è¯´ï¼ŒGROUP BYéƒ½æ˜¯è¦ä¸èšåˆå‡½æ•°æ­é…ä½¿ç”¨çš„ï¼Œæ‰€ä»¥ç¡®å®æ˜¯é€»è¾‘æœ‰é—®é¢˜ï¼Œå†™ä»£ç è§„èŒƒå¾ˆé‡è¦ï¼Œè§„èŒƒçš„åŒæ—¶è¿˜è¦å¼„æ¸…æ¥šåŸç†ï¼Œäºæ˜¯å°±æœ‰äº†è¿™ç¯‡åšå®¢ï¼Œè¯¦ç»†è¯´ä¸€ä¸‹ä½¿ç”¨GROUP BYå’ŒDISTINCTå»é‡â€¦ DISTINCTå»é‡DISTINCTå¯å¤šå­—æ®µå»é‡ï¼Œæ¯ä¸ªå­—æ®µçš„å€¼éƒ½å®Œå…¨ç›¸åŒçš„æƒ…å†µä¸‹ä½¿ç”¨DISTINCTå»é‡DISTINCTä¹Ÿå¯ä»¥å•ä¸ªå­—æ®µå€¼å»é‡ select(name),id from table; mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | +--------+-------+-------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel from person_info where wxid in (&#39;SCALA&#39;); +-------+ | tel | +-------+ | 10010 | +-------+ 1 row in set (0.00 sec) mysql&gt; select distinct name from person_info where wxid in (&#39;SCALA&#39;); +--------+ | name | +--------+ | scala | | scala1 | +--------+ 2 rows in set (0.00 sec) mysql&gt; select distinct tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +-------+-------+ | tel | wxid | +-------+-------+ | 10010 | SCALA | +-------+-------+ 1 row in set (0.00 sec) é€šè¿‡ä¸Šé¢çš„å®éªŒå¯ä»¥çœ‹å‡ºï¼ŒDISTINCTçš„å»é‡æ•ˆæœæ˜¯ å¦‚æœå–å‡ºçš„æ‰€æœ‰å­—æ®µçš„å€¼éƒ½å®Œå…¨ç›¸åŒåˆ™å¯ä»¥å»é‡ï¼Œå¦‚æœå–å‡ºçš„å­—æ®µä¸å®Œå…¨ç›¸åŒï¼Œå°±æ— æ³•å»é‡ã€‚ GROUP BYGROUP BYä¸èšåˆå‡½æ•°è¿ç”¨ï¼Œä¸»è¦ç”¨äºåˆ†ç»„èšåˆï¼Œä½†å®ƒä¹Ÿå¯ä»¥ç”¨æ¥å»é‡ï¼Œä¸DISTINCTç›¸åï¼Œå®ƒæ”¯æŒå¤šä¸ªå­—æ®µå€¼ä¸å®Œå…¨ç›¸åŒçš„æƒ…å†µä¸‹å»é‡ï¼Œä½†ä¼šèˆå¼ƒä¸€äº›å€¼ã€‚å‡å¦‚A,B,Cä¸‰ä¸ªå­—æ®µï¼ŒAå’ŒBä¸¤ä¸ªå­—æ®µåœ¨å¤šæ¡è®°å½•ä¸­å€¼éƒ½ç›¸åŒï¼Œä½†Cä¸åŒï¼Œä½¿ç”¨GROUP BYå»é‡ååªä¼šå¾—åˆ°ä¸€æ¡è®°å½•ï¼ŒCçš„å€¼åªä¿ç•™ä¸€ä¸ªï¼Œå…¶ä½™è®°å½•Cå­—æ®µä¸åŒçš„å€¼èˆå¼ƒã€‚ mysql&gt; select distinct name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;); +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) # GROUP BYçš„æ­£ç¡®ç”¨æ³• åˆ†ç»„èšåˆ mysql&gt; select count(name),tel,wxid from person_info group by tel; +-------------+-------+--------+ | count(name) | tel | wxid | +-------------+-------+--------+ | 1 | 10000 | SBT | | 3 | 10010 | SCALA | | 1 | 10086 | PYTHON | | 1 | 110 | QJJ | | 1 | 114 | JAVA | | 1 | 119 | MAVEN | | 1 | 120 | JJQ | +-------------+-------+--------+ 7 rows in set (0.00 sec) # æŠ¥è­¦æ¨¡å—ä¸å¸Œæœ›é‡å¤æŠ¥è­¦ï¼Œæ‰€ä»¥åªæƒ³è·å–ä¸€ä¸ªç”µè¯å·ç  ä¸‹é¢çš„è¯­å¥é€»è¾‘æœ‰é—®é¢˜ï¼Œä¸ç¬¦åˆGROUP BYçš„ä½¿ç”¨è§„èŒƒï¼Œä½†æ˜¯èƒ½æ‰§è¡Œ mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) # æ”¹æˆç¬¦åˆä½¿ç”¨è§„èŒƒçš„ mysql&gt; select max(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | max(name) | tel | wxid | +-----------+-------+-------+ | scala2 | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) mysql&gt; select min(name),tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel; +-----------+-------+-------+ | min(name) | tel | wxid | +-----------+-------+-------+ | scala | 10010 | SCALA | +-----------+-------+-------+ 1 row in set (0.00 sec) # mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,name; +--------+-------+-------+ | name | tel | wxid | +--------+-------+-------+ | scala | 10010 | SCALA | | scala1 | 10010 | SCALA | | scala2 | 10010 | SCALA | +--------+-------+-------+ 3 rows in set (0.00 sec) mysql&gt; select name,tel,wxid from person_info where wxid in (&#39;SCALA&#39;) group by tel,tel; +-------+-------+-------+ | name | tel | wxid | +-------+-------+-------+ | scala | 10010 | SCALA | +-------+-------+-------+ 1 row in set (0.00 sec) ä»ä¸Šé¢å®éªŒå¯ä»¥å¾—å‡ºçš„ç»“è®º:å¦‚æœåªéœ€è¦telå’Œwxidä¸¤ä¸ªå­—æ®µï¼Œæ— æ‰€è°“nameçš„å­—æ®µå€¼ï¼Œå°±å¯ä»¥ç”¨GROUP BYçš„æ–¹å¼å»é‡ï¼Œä½†æ˜¯ä¹Ÿè¦å°½é‡å†™å¾—è§„èŒƒã€‚å¦‚æœéœ€è¦nameå­—æ®µçš„å€¼ï¼Œå°±ä¸èƒ½ç”¨GROUP BYæ¥å»é‡äº†ã€‚ é—®é¢˜æƒ…æ™¯é‡ç°æŠ¥è­¦æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯ä¼ äººåï¼Œè¿˜æœ‰ä¸€ç§æ˜¯ä¼ ç»„åï¼Œäººä¸ç»„æ˜¯å¤šå¯¹å¤šå…³ç³»è”ç³»æ–¹å¼ä¿¡æ¯å­˜ä¸ºä¸¤å¼ è¡¨ï¼Œperson_infoå’Œgroup_info,å¤§è‡´å¦‚ä¸‹name äººåï¼Œtelæ˜¯ç”µè¯ï¼Œwxidæ˜¯å¾®ä¿¡ï¼Œgroupnameæ˜¯ç»„å mysql&gt; show tables; +----------------+ | Tables_in_test | +----------------+ | group_info | | persion_info | +----------------+ 2 rows in set (0.00 sec) mysql&gt; desc person_info; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | tel | varchar(255) | NO | | NULL | | | wxid | varchar(255) | NO | | NULL | | +-------+--------------+------+-----+---------+-------+ 3 rows in set (0.00 sec) mysql&gt; desc group_info; +-----------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+-------+ | name | varchar(255) | NO | PRI | NULL | | | groupname | varchar(255) | NO | | NULL | | +-----------+--------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) mysql&gt; select * from person_info; +--------+-------+--------+ | name | tel | wxid | +--------+-------+--------+ | java | 114 | JAVA | | jjq | 120 | JJQ | | maven | 119 | MAVEN | | python | 10086 | PYTHON | | qjj | 110 | QJJ | | sbt | 10000 | SBT | | scala | 10010 | SCALA | +--------+-------+--------+ 7 rows in set (0.00 sec) mysql&gt; select * from group_info; +--------+-------------+ | name | groupname | +--------+-------------+ | java | languages | | jjq | person | | maven | build-tools | | python | languages | | qjj | person | | sbt | build-tools | | scala | languages | +--------+-------------+ 7 rows in set (0.00 sec) æŠ¥è­¦æ¥å£ä¼ è¿›æ¥çš„å‚æ•°å¯èƒ½æ˜¯å¤šä¸ªäººåæˆ–è€…ç»„åçš„ç»„åˆåˆ—è¡¨ï¼Œæˆ‘æƒ³é€šè¿‡ä¸€æ¬¡æŸ¥è¯¢è·å–åˆ°æ‰€æœ‰æŠ¥è­¦äººä¿¡æ¯ï¼Œäºæ˜¯æˆ‘å…ˆå†™äº†å†…éƒ¨å­æŸ¥è¯¢: SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;); # ç»“æœ: +------+------+------+-----------+ | name | tel | wxid | groupname | +------+------+------+-----------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | | jjq | 120 | JJQ | groupname | | qjj | 110 | QJJ | groupname | +------+------+------+-----------+ 4 rows in set (0.00 sec) å› ä¸ºæœ‰é‡å¤çš„äººåå’Œé‡å¤çš„è”ç³»æ–¹å¼ä¼šé‡å¤æŠ¥è­¦ï¼Œæ‰€ä»¥ä¸ºäº†é¿å…é‡å¤æŠ¥è­¦ï¼Œæˆ‘åˆåŠ äº†å¤–é¢çš„ä¸€å±‚: SELECT name,tel,wxid,max(groupname) FROM (SELECT t2.name,t2.tel,t2.wxid,t1.groupname FROM person_info t2 RIGHT JOIN group_info t1 ON t2.name = t1.name WHERE groupname IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;) UNION ALL SELECT IFNULL(name,0),tel,wxid,&#39;groupname&#39; FROM person_info WHERE name IN (&#39;qjj&#39;,&#39;jjq&#39;,&#39;person&#39;)) a GROUP BY name; # ç»“æœ: +------+------+------+----------------+ | name | tel | wxid | max(groupname) | +------+------+------+----------------+ | jjq | 120 | JJQ | person | | qjj | 110 | QJJ | person | +------+------+------+----------------+ 2 rows in set (0.01 sec) å‡†ç¡®åœ°æ‹¿åˆ°äº†name,tel,wxidï¼Œä½†æ˜¯groupnameå­—æ®µå‘¢ï¼Œåˆ°åº•æ˜¯å“ªä¸ªè¢«èˆå¼ƒäº†ï¼Ÿä¸åŒæƒ…å†µä¸‹ä¸ä¸€å®šã€‚å¦‚æœæˆ‘ä»¬åªè¦name,tel,wxidè¿™ä¸‰ä¸ªå­—æ®µï¼Œåœ¨groupnameå­—æ®µä¸ŠåŠ ä¸ªmax()å¥½äº†ï¼Œè¿™æ ·é€»è¾‘ä¹Ÿè¯´å¾—é€šï¼Œä¹Ÿæ¯”è¾ƒè§„èŒƒï¼Œå¦‚æœè¦æ±‚ç²¾ç¡®æ‹¿åˆ°groupnameå­—æ®µçš„å€¼ï¼Œå°±ä¸èƒ½ä½¿ç”¨group byå»é‡ã€‚ å…³äºæ•ˆç‡DISTINCTå’ŒGROUP BYåŒæ—¶é€‚ç”¨çš„åœºæ™¯ä¸‹ï¼Œä¸èƒ½è¯´ä¸€å®šæ˜¯è°çš„æ•ˆç‡æ›´é«˜DISTINCTå°±æ˜¯å­—æ®µå€¼å¯¹æ¯”çš„æ–¹å¼ï¼Œè¦éå†æ•´ä¸ªè¡¨ã€‚GROUP BYç±»ä¼¼äºå…ˆå»ºç´¢å¼•å†æŸ¥ç´¢å¼•ã€‚å°è¡¨DISTINCTå»é‡æ•ˆç‡é«˜ï¼Œå¤§è¡¨GROUP BYå»é‡æ•ˆç‡é«˜ æ€»ç»“ è®¤æ¸…DISTINCTå’ŒGROUP BYçš„å»é‡åœºæ™¯ æŸäº›åœºæ™¯ä¸‹DISTINCTä¸GROUP BYå»é‡åŒæ—¶é€‚ç”¨ï¼Œä½†DISTINCTæ•ˆç‡æ›´é«˜ ä»£ç è¦è§„èŒƒä¸”ç¬¦åˆé€»è¾‘ è¦å¯¹SQLæ¯ä¸ªè¯­æ³•çš„ä½¿ç”¨åœºæ™¯æœ‰æ˜ç¡®çš„è®¤è¯† å¤šæ€»ç»“é—®é¢˜","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://shmily-qjj.top/tags/SQL/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"ç‹è€…è£è€€-å…ƒæ­Œæ•™å­¦","slug":"ç‹è€…è£è€€-å…ƒæ­Œæ•™å­¦","date":"2019-11-03T15:05:06.000Z","updated":"2022-12-11T05:35:07.922Z","comments":true,"path":"26e6fc90/","link":"","permalink":"https://shmily-qjj.top/26e6fc90/","excerpt":"","text":"ä¸€æ³¢å…ƒæ­Œæ”»ç•¥åˆ†äº«æœ‰äº›è‹±é›„ï¼Œçœ‹ä¼¼å¾ˆç§€ï¼Œçœ‹ä¼¼å¾ˆéš¾ï¼Œå…¶å®åªè¦ç†è§£äº†å®ƒçš„æ¯ä¸ªæ­¥éª¤ï¼Œå°±ä¼šå‘ç°å®ƒå¹¶ä¸éš¾ã€‚ä»Šå¤©æˆ‘ä¸ºå¤§å®¶å¸¦æ¥ä¸€æ³¢å¹²è´§æ»¡æ»¡çš„å…ƒæ­Œæ”»ç•¥åˆ†äº«ï¼å…ƒæ­Œï¼Œæ˜¯ä¸€ä¸ªä½é£é™©ï¼Œé«˜å›æŠ¥çš„å¼ºåŠ¿è‹±é›„ï¼Œè™½ç„¶ç»å†è¿‡å¤©ç¾æ— æƒ…çš„å‰¥å‰Šï¼Œä½†ä¾ç„¶èƒ½åœ¨å³¡è°·é‡Œç»½æ”¾å…‰èŠ’ï¼å¾ˆå¤šç©å®¶ä¸€çœ‹åˆ°å…ƒæ­Œï¼Œå°±æƒ³èµ·é‚£ä¸ªå£è¯€â€œ1433223â€ï¼Œä½†å¦‚æœåªä¼šè®°å£è¯€ï¼Œé‚£å°±Outå•¦ï¼å…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªè¦æ˜ç¡®å®ƒæ¯ä¸€æ­¥éƒ½æ˜¯å¹²ä»€ä¹ˆçš„ï¼Œå°±æ›´é€å½»åœ°ç†è§£å…ƒæ­Œï¼Œå°±èƒ½è¿ç”¨è‡ªå¦‚ï¼å¦‚æœä¸€ç›´é è®°å£è¯€ç©ï¼Œé‚£å°†æ¯«æ— æ„ä¹‰ï¼ä¸‹é¢æˆ‘ä»¬å¼€å§‹å§ï¼ä»€ä¹ˆï¼Ÿæ²¡æœ‰ç§¯ææ€§ï¼Ÿè¦ä¸çœ‹ä¸€ä¸‹ç§€æ“ä½œè§†é¢‘==&gt;é“¾æ¥ å‡ºè£…è§£æ å‚€å„¡å†·å´è¾ƒé•¿ï¼Œæ¨èå†·å´é‹ ä¸»è¦é’ˆå¯¹è„†çš®ï¼Œæ¨èæš—å½±æˆ˜æ–§ ç§’æ€å‹åˆºå®¢ï¼Œè‡ªå¸¦æ–©æ€æ•ˆæœï¼Œé…åˆç ´å†›ï¼Œèƒ½ç§’æ€ä»»ä½•è„†çš® ä¼¤å®³è¶³å¤Ÿç§’æ€æ•Œæ–¹ï¼Œä¸ºäº†ç¡®ä¿ä¸‡æ— ä¸€å¤±ï¼Œæ¨èååˆ€ å¯¹é¢æˆ˜å¦å‡ºäº†è‚‰è£…ï¼Œå¯èƒ½ä¹Ÿæœ‰è„†çš®å‡ºäº†è‚‰è£…ï¼Œè¿™æ—¶æ¨èç¢æ˜Ÿé”¤ ä¼¤å®³å®Œå…¨è¶³å¤Ÿï¼Œä½†å…ƒæ­ŒåŸºç¡€è¡€é‡å¤ªä½ï¼Œä¸ºäº†ä¸è¢«ç§’æ€ä»¥åŠç»­èˆªèƒ½åŠ›ï¼Œæ¨èéœ¸è€…é‡è£… å¬å”¤å¸ˆæŠ€èƒ½ï¼Œæ¨èé—ªç° å£è¯€è§£æä¸å¾—ä¸è¯´â€œ1433223â€æ˜¯ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œæˆ‘æ¥è§£æä¸€ä¸‹æ¯ä¸€æ­¥éƒ½åšäº†ä»€ä¹ˆå§ï¼ 1æŠ€èƒ½é‡Šæ”¾å‚€å„¡ï¼Œå‚€å„¡å¼¹å‡ºçš„è·¯å¾„ä¸Šä¼šå¯¹è‹±é›„é€ æˆä¼¤å®³å’ŒåŠç§’å‡»é£(çœ©æ™•)ã€‚ å‚€å„¡äº¤æ¢çš„è·¯å¾„ä¸Šéƒ½æœ‰ä¼¤å®³å’Œä¸åˆ°åŠç§’çš„å‡»é£ã€‚ 4æŠ€èƒ½æ‹‰åŠ¨å‚€å„¡å’ŒçœŸèº«ä¸€æ®µè·ç¦»ã€‚å½“ç„¶ï¼Œè·¯å¾„ä¸Šä¹Ÿæœ‰ä¼¤å®³ï¼Œä½†æ²¡æ§åˆ¶ã€‚ 3æŠ€èƒ½è¿™ä¸¤ä¸ªä¸‰æŠ€èƒ½æ˜¯å…³é”®ï¼Œè¦å°†å‚€å„¡é è¿‘æ•Œæ–¹è‹±é›„ï¼Œä¸‰æŠ€èƒ½ç¬¬ä¸€æ®µæ˜¯å‡å°‘æ•Œæ–¹è‹±é›„500ç§»é€ŸæŒç»­ä¸¤ç§’ï¼Œ3ç§’å†…å¯é‡Šæ”¾ç¬¬äºŒæ®µã€‚å¤šä¹ˆå¼ºå¤§çš„å‡é€Ÿï¼ ç¬¬äºŒä¸ª3æŠ€èƒ½ä¸‰æŠ€èƒ½ç¬¬äºŒæ®µæ˜¯å…ƒæ­Œçš„æ ¸å¿ƒæ§åˆ¶æŠ€èƒ½ã€‚ç”¨å‚€å„¡è´´è¿‘æ•Œæ–¹è‹±é›„å¹¶é‡Šæ”¾ä¸‰æŠ€èƒ½ç¬¬äºŒæ®µï¼Œæ§åˆ¶æ•Œæ–¹çº¦2.5sã€‚å¤šä¹ˆå¼ºå¤§çš„æ§åˆ¶ï¼ 2æŠ€èƒ½åœ¨ä¸‰æŠ€èƒ½äºŒæ®µé‡Šæ”¾åçš„ä¸‰ç§’å†…é‡Šæ”¾äºŒæŠ€èƒ½ï¼Œå°†çœŸèº«æ‹‰åˆ°æ•Œäººèº«è¾¹ã€‚ ç¬¬äºŒä¸ª2æŠ€èƒ½ã€å³çœŸèº«çš„2æŠ€èƒ½ã€‘æ­¤æ—¶æ•Œäººè¿˜åœ¨è¢«æ§åˆ¶çš„çŠ¶æ€ï¼Œå¦‚åŒå¾…å®°çš„ç¾”ç¾Šï¼Œè¿™æ—¶é‡Šæ”¾ç¬¬äºŒä¸ªäºŒæŠ€èƒ½ï¼Œç›´æ¥ä½¿æ•Œæ–¹è‹±é›„æ®‹è¡€ã€‚ 3æŠ€èƒ½ã€å³çœŸèº«çš„3æŠ€èƒ½ã€‘æ­¤æ—¶æ•Œäººè¿˜åœ¨è¢«æ§åˆ¶çš„çŠ¶æ€ï¼Œå¦‚åŒå¾…å®°çš„ç¾”ç¾Šï¼Œä¸‰æŠ€èƒ½æ˜¯ç”±ä¸¤æ¡çº¿ç»„æˆï¼Œæ¯æ¡çº¿éƒ½æœ‰ä¼¤å®³ï¼Œäº¤å‰ç‚¹å‘½ä¸­è‹±é›„ä¼šå¯¹å…¶é€ æˆå·²æŸç”Ÿå‘½16%çš„é¢å¤–ä¼¤å®³ï¼Œå’ŒæŒç»­ä¸¤ç§’çš„50%å‡é€Ÿå¹¶å®‰å…¨é€€å‡ºæˆ˜åœºä¸€æ®µè·ç¦»ï¼ŒåŒæ—¶å¾—åˆ°ä¸€ä¸ªæŠ¤ç›¾å’Œç¬é—´åŠ é€Ÿæ•ˆæœã€‚å¦‚æœæ˜¯è„†çš®æˆ–è€…æ®‹è¡€å°±ç›´æ¥å¸¦èµ°äº†ï¼Œå¦‚æœç»æµé«˜å‡º1000ï¼Œè¿™ä¸€å¥—æ“ä½œä¸‹æ¥å°†ç§’æ€ä¸€åˆ‡ï¼ çœŸèº«4æŠ€èƒ½è‡ªå·±æŒ‰ä¸€ä¸‹å°±çŸ¥é“ï¼Œè§£æ§+ä½ç§»+å…ä¼¤ å¦‚ä½•ç»ƒä¹ æ ¹æ®ä¸Šé¢çš„å£è¯€è§£æï¼Œæˆ‘ä»¬å·²ç»å¤§è‡´çŸ¥é“å…ƒæ­Œçš„ç©æ³•ï¼Œä¸‹é¢å°±æ˜¯å…¥é—¨çš„ç»ƒä¹ æ–¹æ³•ã€‚ ä¸€å¼€å§‹è¿˜ä¸ç†Ÿæ‚‰ï¼Œè‚¯å®šä¼šå¾ˆæ‡µï¼Œæ‰€ä»¥ï¼Œå…ˆå°½å¯èƒ½æ…¢åœ°æŒ‰å‡º1-3-3-2-2-3ï¼Œè¶Šæ…¢è¶Šå¥½ï¼Œåƒä¸‡ä¸è¦æ€¥äºæ±‚æˆï¼Œä½“ä¼šæ¯ä¸€æ­¥åšäº†ä»€ä¹ˆï¼Œç„¶åå†é€æ¸åŠ å¿«é€Ÿåº¦ã€‚ ç©ä¸Šå‡ æŠŠåå°±ä¼šå¯¹1-3-3-2-2-3è¿™å¥—æ“ä½œæç„¶å¤§æ‚Ÿï¼Œè™½ç„¶å¯èƒ½æŒ‰ä¸ç†Ÿæ‚‰ï¼Œä½†ä¹Ÿç†è§£äº†æ¯ä¸€æ­¥çš„ä½œç”¨ã€‚ ä¹‹åï¼Œæˆ‘ä»¬åŠ å…¥4æŠ€èƒ½ï¼Œ1-4-3-3-2-2-3ï¼Œå°±ä¼šå‘ç°4æŠ€èƒ½çš„ç›®çš„å°±æ˜¯è®©å‚€å„¡æ¥è¿‘æ•Œæ–¹è‹±é›„ï¼Œæ¥æé«˜æ§ä½æ•Œæ–¹è‹±é›„çš„æ¦‚ç‡ã€‚ å†ç©ä¸Šå‡ æŠŠæˆ–åå‡ æŠŠåï¼Œä½ å°±ä¼šå¯¹4æŠ€èƒ½çš„è·ç¦»æœ‰ä¸€ä¸ªåˆæ­¥çš„æŠŠæ¡ï¼Œå£è¯€æ˜¯æ­»çš„ï¼Œäººæ˜¯æ´»çš„ï¼Œèªæ˜çš„ä½ è¿˜ä¼šå‘ç°ï¼Œå¯¹æ–¹åœ¨ä½ é¦–æ¬¡æŒ‰3æŠ€èƒ½å‡é€Ÿçš„æ—¶å€™ä½ç§»äº†æ€ä¹ˆåŠï¼Œè¿™æ—¶ï¼Œ4æŠ€èƒ½çš„ä½œç”¨å°±å¤§äº†ï¼Œä½ ä¼šæƒ³åˆ°1-3-4-3-2-2-3ï¼Œå³å…ˆå‡é€Ÿï¼Œå¯¹æ–¹ä½ç§»ï¼Œ4æŠ€èƒ½è®©å‚€å„¡è´´è¿‘ï¼Œ3æŠ€èƒ½æ§åˆ¶ï¼Œ2æŠ€èƒ½å°†çœŸèº«æ‹‰è¿‡æ¥ï¼Œ2æŠ€èƒ½æ‰“ä¼¤å®³ï¼Œ3æŠ€èƒ½æ”¶å‰²+é€€å‡ºæˆ˜åœºã€‚ æ€ä¹ˆæ ·ï¼Œè¡Œäº‘æµæ°´çš„æ“ä½œï¼è¿™æ—¶ä½ å°±ä¼šä½“ä¼šåˆ°ï¼Œç†è§£æ¯ä¸ªæ­¥éª¤çš„ä½œç”¨çš„é‡è¦æ€§ã€‚ç›¸ä¿¡åé¢ä½ è‚¯å®šä¼šæ›´åŠ çµæ´»ï¼ ä½ ä¼šå‘ç°ï¼Œå‚€å„¡é™¤äº†å¯ä»¥æ¢è§†é‡ï¼Œå“å”¬äººï¼Œéª—æŠ€èƒ½ä¹‹å¤–ï¼Œè¿˜å¯ä»¥è¹²åœ¨è‰ä¸›é‡Œå®ˆæ ªå¾…å…”ï¼Œæ•Œæ–¹è‹±é›„è¿›è‰ä¸›åï¼Œä¸€å¥—3-3-2-2-3ï¼Œå°±èƒ½ç›´æ¥å¸¦èµ°ï¼Œæ˜¯ä¸æ˜¯åƒæäº†å°å¦²å·±ï¼ å†å¤šæ‰“äº›åœºæ¬¡ï¼Œä½ å°±ä¼šå‘ç°ï¼Œæ‰‹é€Ÿä¸å¤Ÿå‘€ï¼åˆ«ç°å¿ƒï¼Œå¤šæ‰“å°±èƒ½æ‰¾åˆ°æ„Ÿè§‰ï¼Œä¿æŒç´§å¼ ï¼Œæ‰‹é€Ÿè‡ªç„¶ä¹Ÿæä¸Šæ¥äº†ï¼ ä¹…ç»å³¡è°·ï¼Œå› ä¸ºä½ æ·±åˆ»ç†è§£äº†å…ƒæ­Œçš„æ¯ä¸€æ­¥æ“ä½œï¼Œæ‰€ä»¥ä½ è‡ªç„¶æœ‰10000ç§æ–¹å¼é€ƒè·‘ï¼Œæ¯”å¦‚4-1-4-2-1-3-2ï¼Œæ¯”å¦‚1-2-1ï¼Œå†æ¯”å¦‚3-4-1-2-1ç­‰ç­‰â€¦æ²¡æœ‰äººèƒ½è½»æ˜“ææ­»ä½ ï¼Œæƒ³æ­»éƒ½éš¾â€¦ è¿›é˜¶æ­å–œä½ å…¥é—¨äº†ï¼æƒ³å¿…ä¹Ÿå‘ç°äº†å…ƒæ­Œè¿™ä¸ªè‹±é›„çš„å¼ºå¤§äº†ã€‚ä¸‹é¢æˆ‘è¿˜è¦ä»‹ç»ä¸€äº›ç‹¬é—¨ç§˜ç±ï¼ 15åº¦è§’Surpriseäº²è‡ªæˆªå›¾ç»™ä½ èŒçœ‹ï¼ å…ƒæ­Œçš„ä¸‰æŠ€èƒ½æ˜¯äº¤å‰çš„ï¼Œäº¤å‰ç‚¹å¾ˆè¿‘ï¼Œä½†çº¿çš„æ”»å‡»èŒƒå›´å¾ˆè¿œ å¦‚æœæƒ³æ‰“æ­£å‰æ–¹çš„æ•Œäººï¼Œåˆ™æ‰‹åŠ¨æ–½æ³•åƒåç¦»15åº¦è§’çš„æ–¹å‘é‡Šæ”¾3æŠ€èƒ½ æ•Œæ–¹æ®‹è¡€ä¼šè¢«3æŠ€èƒ½çš„çº¿åˆ®åˆ°ï¼Œè™½ç„¶ä¸å¦‚äº¤å‰ç‚¹ç–¼ï¼Œä½†åæœŸä¹Ÿæœ‰è‡³å°‘1000+çš„ä¼¤å®³ï¼Œè¶³ä»¥æ”¶å‰² è¿™ä¸ªæŠ€å·§è¾ƒéš¾ï¼Œéœ€è¦å¤šå»æ„Ÿå—å’Œè§‚å¯Ÿ è¶Šå¡”Surpriseæ³¨æ„ï¼šæ‰‹é€Ÿä¸å¤Ÿå¿«è¯·å¿½ç•¥æ­¤æ¡ï¼ ä¼—æ‰€å‘¨çŸ¥ï¼Œå…ƒæ­Œå‚€å„¡èƒ½æŠ—2-3æ¬¡é˜²å¾¡å¡”æ”»å‡»ã€‚ 2æ¬¡é˜²å¾¡å¡”æ”»å‡»å¤§çº¦2ç§’ã€‚ 2ç§’å†…è¦åšä»€ä¹ˆï¼š1-4-3-3-1(é‡Šæ”¾å‚€å„¡ï¼Œé è¿‘æ•Œæ–¹ï¼Œå‡é€Ÿç¬é—´æ§ä½æ•Œæ–¹ï¼Œå‚€å„¡è¦æ²¡è¡€äº†èµ¶å¿«æ”¶å›å‚€å„¡) å› ä¸ºæ”¶å›å‚€å„¡ä¹Ÿæœ‰å‡»é£0.5ç§’ï¼Œ0.5ç§’ä¹‹å†…åšä»€ä¹ˆï¼š3-2æˆ–2-3 æ­å–œå®Œæˆäº†è¶Šå¡”ç§’æ€ æ®‹è¡€æ”¶å‰²æœºçœ‹åˆ°æ®‹è¡€ï¼Œåˆ©ç”¨å‚€å„¡çš„å†²æ’å’Œæ”¶å›ç›´æ¥æ”¶å‰²ã€‚å…·ä½“æ“ä½œï¼š1-4-3-1(-2-4)æ‹¬å·é‡Œæ˜¯å¯é€‰æ“ä½œã€‚ 1æŠ€èƒ½é‡Šæ”¾å‚€å„¡ï¼Œ4æŠ€èƒ½å†²æ’å¯¹æ–¹é€ æˆå°‘é‡ä¼¤å®³ï¼Œå¯¹æ–¹å¯èƒ½è¿˜æ²¡æ­» 3æŠ€èƒ½ç¬¬ä¸€æ®µä¼¤å®³å¾ˆé«˜ï¼Œç›´æ¥æ”¶å‰² 1æŠ€èƒ½æ”¶å›å‚€å„¡ å¦‚æœæ®‹è¡€æ²¡æ­»å¯é€‰è¡¥ä¸ª2æŠ€èƒ½ï¼Œå¦‚æœæœ‰è¿½å…µï¼Œå¯é€‰4æŠ€èƒ½é€ƒè·‘ æŒç»­è¾“å‡ºå…¶å®è¿™ä¸ç®—å•¥æŠ€å·§ï¼Œå°±æ˜¯ä¸€å®šè¦å¤šå‚å›¢ã€‚å…ƒæ­ŒåæœŸ2ï¼Œ3æŠ€èƒ½è·ç¦»é•¿ï¼Œå†·å´çŸ­ï¼Œè·Ÿåœ¨å›¢åé¢æŒç»­æ¶ˆè€—ï¼Œæ”¶å‰²ï¼Œé…åˆ15åº¦è§’ï¼Œè°ä¹Ÿé¡¶ä¸ä½ï¼è¾“å‡ºæœ€é«˜éä½ è«å±ï¼ Endæ„Ÿè°¢ä½ çœ‹å®Œæˆ‘çš„å…ƒæ­Œæ”»ç•¥åˆ†äº«ï¼Œæœ‰ä»€ä¹ˆå¿ƒå¾—ä½“ä¼šï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€ï¼Bç«™å…ƒæ­Œè§†é¢‘ï¼šé“¾æ¥åŠ æ²¹ï¼Œä¸‹ä¸€ä¸ªå›½æœå…ƒæ­Œï¼Œå°±æ˜¯ä½ ï¼","categories":[{"name":"å…¶ä»–","slug":"å…¶ä»–","permalink":"https://shmily-qjj.top/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"ç‹è€…è£è€€","slug":"ç‹è€…è£è€€","permalink":"https://shmily-qjj.top/tags/%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80/"}],"keywords":[{"name":"å…¶ä»–","slug":"å…¶ä»–","permalink":"https://shmily-qjj.top/categories/%E5%85%B6%E4%BB%96/"}]},{"title":"Sqoopå­¦ä¹ ç¬”è®°","slug":"Sqoopå­¦ä¹ ç¬”è®°","date":"2019-11-03T05:15:27.000Z","updated":"2022-12-11T05:35:07.912Z","comments":true,"path":"26078/","link":"","permalink":"https://shmily-qjj.top/26078/","excerpt":"","text":"ä»€ä¹ˆæ˜¯SqoopSqoopæ˜¯ä¸€æ¬¾å¼€æºå·¥å…·ï¼Œç”¨äºHadoop(Hive)ä¸mysqlç­‰ä¼ ç»Ÿæ•°æ®åº“é—´è¿›è¡Œæ•°æ®ä¼ é€’ï¼Œå¯ä»¥å°†å…³ç³»å‹æ•°æ®åº“mysql,Oracleç­‰ä¸­çš„æ•°æ®å¯¼å…¥HDFSä¸­ï¼Œä¹Ÿå¯ä»¥æŠŠHDFSä¸­çš„æ•°æ®å¯¼å…¥åˆ°å…³ç³»å‹æ•°æ®åº“ä¸­ã€‚Sqoop2ä¸Sqoop1å®Œå…¨ä¸å…¼å®¹ï¼Œä¸€èˆ¬ç”Ÿäº§ç¯å¢ƒä½¿ç”¨Sqoop1ï¼Œè¿™é‡Œä¸»è¦è¯´Sqoop1 SqoopåŸç†SqoopåŸç†å¾ˆç®€å•ï¼Œå°±æ˜¯å°†å¯¼å…¥å¯¼å‡ºçš„å‘½ä»¤ç¿»è¯‘æˆMapReduceç¨‹åºï¼ŒSqoopçš„æ“ä½œä¸»è¦ç›®çš„ï¼ˆå·¥ä½œï¼‰æ˜¯å¯¹MRç¨‹åºçš„inputformatå’Œoutputformatè¿›è¡Œå®šåˆ¶.ä¸‹å›¾æ˜¯SqoopåŸç†æ¶æ„å›¾å›¾ä¸Šæ„æ€å¾ˆæ˜ç¡®ï¼Œè¿™é‡Œä¸å¤šèµ˜è¿°ã€‚æˆ³**å®˜æ–¹æ–‡æ¡£**äº†è§£æ›´å¤š Sqoopå®‰è£…éƒ¨ç½²å»å®˜ç½‘ä¸‹è½½Sqoopçš„äºŒè¿›åˆ¶åŒ… tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt/module/ cd /opt/module/ mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop vim /etc/profile export SQOOP_HOME=/opt/module/sqoop export PATH=$PATH:$SQOOP_HOME/bin source /etc/profile cd sqoop/conf cp sqoop-env-template.sh sqoop-env.sh vim sqoop-env.sh æ–‡ä»¶æœ«å°¾åŠ å…¥å¦‚ä¸‹é…ç½®ï¼š #Set the path for where zookeper config dir is #export ZOOCFGDIR= export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2 export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2 export HIVE_HOME=/opt/module/hive export HBASE_HOME=/opt/module/hbase export ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.13 export ZOOCFGDIR=/opt/module/zookeeper-3.4.13/conf # SQOOPå†™å“ªä¸ªé›†ç¾¤ï¼Œç”¨å“ªä¸ªYarnï¼Œç”¨ä»¥ä¸‹Hiveã€Hadoopå®¢æˆ·ç«¯é…ç½®æŒ‡å®šåˆ°sqoop-env.sh export HADOOP_CONF_DIR=/etc/hadoop/cluster_client_conf/hadoop-conf/ export HIVE_CONF_DIR=/etc/hadoop/cluster_client_conf/hive-conf/ æ‹·è´mysqlé©±åŠ¨åˆ°Sqoopçš„libç›®å½•ä¸‹ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/ è¿è¡Œsqoop helpå‘½ä»¤ éƒ¨ç½²å®Œæˆï¼Œæµ‹è¯•ï¼š æŸ¥çœ‹Mysqlè¡¨ sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password 000000 å¦‚æœä¸€åˆ‡æ­£å¸¸ - åˆ™å®‰è£…æ²¡é—®é¢˜äº† Sqoopæ“ä½œSqoopçš„å¯¼å…¥å’Œå¯¼å‡ºå¯¼å…¥ï¼šæ•°æ®ä»RDBMSåˆ°HDFSçš„è¿‡ç¨‹ï¼Œæ•°æ®æºæ˜¯RDBMSï¼Œç›®æ ‡æ˜¯HDFSå¯¼å‡ºï¼šæ•°æ®ä»HDFSåˆ°RDBMSçš„è¿‡ç¨‹ï¼Œæ•°æ®æºæ˜¯HDFSï¼Œç›®æ ‡æ˜¯RDBMS å¯¼æ•°æ®åˆ°HDFSå‡†å¤‡æ•°æ®ï¼š åˆ›å»ºæ•°æ®åº“å’Œè¡¨ create database test; create table sqoop_test(id int primary key not null auto_increment,name varchar(255),sex varchar(255)); insert into test.sqoop_test(name,sex) values(&#39;qjj&#39;,&#39;male&#39;); insert into test.sqoop_test(name,sex) values(&#39;abc&#39;,&#39;female&#39;); use test; select * from sqoop_test; å¯¼å…¥HDFSæ–¹å¼åˆ†ä¸ºå…¨éƒ¨å¯¼å…¥/æŸ¥è¯¢å¯¼å…¥/å¯¼å…¥æŒ‡å®šåˆ—/ç­›é€‰å¯¼å…¥/å¢é‡æ›´æ–° å…¨éƒ¨å¯¼å…¥ â€“num-mappers 1 è®¾ç½®ä¸€ä¸ªmapï¼Œè¾“å‡ºæ–‡ä»¶ä¸ªæ•°ä¹Ÿä¸º1â€“null-string æŒ‡å®šå­—æ®µä¸ºç©ºæ—¶ç”¨ä»€ä¹ˆä»£æ›¿ sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --null-string &quot;-&quot; \\ --target-dir /user/sqoop/out \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; æŸ¥è¯¢å¯¼å…¥ä¸é€šè¿‡**- -tableæ¥æŒ‡å®šï¼Œè€Œæ˜¯é€šè¿‡å†™- -query**æ¥æŒ‡å®š$CONDITIONSæ˜¯å¿…é¡»åŠ çš„ï¼Œä¸ºäº†åœ¨å¤šä¸ªMapçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä¼ é€’å‚æ•°ï¼Œä»¥ä¿è¯å¯¼å‡ºæ•°æ®çš„é¡ºåºä¸å˜ã€‚ __where $CONDITIONS__å¿…å¤‡``` shellsqoop import \\ â€“connect jdbc:mysql://localhost:3306/test â€“username root â€“password 000000 â€“target-dir /user/sqoop/out â€“num-mappers 1 â€“fields-terminated-by â€œ\\tâ€ â€“query â€˜select name,sex from sqoop_test where id &lt;= 1 and $CONDITIONS;â€™ (è¿™é‡Œå¦‚æœç”¨åŒå¼•å·ï¼Œåˆ™$CONDITIONSéœ€è¦è½¬ä¹‰) 3. å¯¼å…¥æŒ‡å®šåˆ— --delete-target-dir -&gt; å¦‚æœHDFSç›®å½•å·²ç»å­˜åœ¨åˆ™åˆ é™¤ --columnsæŒ‡å®šå¤šä¸ªåˆ— ``` shell sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --columns id,sex \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; ç­›é€‰å¯¼å…¥å…³é”®å­—ç­›é€‰/å­—æ®µç­›é€‰ â€“where â€œæ¡ä»¶â€è€Œä¸”â€“whereä¸â€“columnså¯ä»¥åŒæ—¶ä½¿ç”¨ï¼Œä½†ä¸èƒ½ä¸â€“queryåŒæ—¶ä½¿ç”¨ sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --where &quot;id=1&quot; \\ --target-dir /user/sqoop/out \\ --delete-target-dir \\ --num-mappers 1 \\ --fields-terminated-by &quot;\\t&quot; å¢é‡æ›´æ–°è¡¨æ›´æ–°æ—¶é‡æ–°å¯¼å…¥æµªè´¹æ—¶é—´å’Œèµ„æºå¢é‡æ›´æ–°ä¸‰ä¸ªé‡è¦å‚æ•° â€“incremental append æŒ‡å®šå¢é‡å¯¼å…¥â€“check-column col_name ä»¥ä¸€ä¸ªåˆ—ä½œä¸ºå¢é‡å¯¼å…¥çš„æ ‡å‡†ï¼Œè¿™ä¸ªåˆ—å˜åŒ–æ‰ä¼šè§¦å‘å¢é‡å¯¼å…¥â€“last-value æŒ‡å®šä¸Šæ¬¡å¯¼å…¥çš„å‚è€ƒåˆ—çš„æœ€åä¸€ä¸ªå€¼ï¼ˆæ¯”å¦‚check-columnä¸ºidï¼Œä¸Šæ¬¡å¯¼å…¥çš„idå€¼ä¸º4ï¼Œåˆ™å¢é‡å¯¼å…¥è¦æŒ‡å®šlast-valueä¸º4ï¼‰ Sqoopå®˜æ–¹ç”¨æˆ·æ–‡æ¡£: Sqoop User Guide å¯¼æ•°æ®åˆ°Hivemysqlæ•°æ®å¯¼å…¥Hiveè¿‡ç¨‹åˆ†ä¸¤æ­¥ï¼š Mysqlå…ˆå¯¼å…¥åˆ°HDFS å·²è¢«å¯¼å‡ºåˆ°HDFSçš„æ•°æ®ç§»åŠ¨åˆ°hiveä»“åº“ hive-importæŒ‡å®štargetä¸ºhivetarget hiveè¡¨ä¼šè‡ªåŠ¨åˆ›å»º sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --hive-import \\ --fields-terminated-by &quot;\\t&quot; \\ --hive-overwrite \\ --hive-table sqoop_hive Sqoopå®˜æ–¹å‚è€ƒ: Importing Data Into Hive å¯¼æ•°æ®åˆ°HBaseâ€“columnsæŒ‡å®šsourceè¡¨ä¸­å“ªå‡ åˆ—â€“column-familyæŒ‡å®šåˆ—æ—åç§°â€“split-byæŒ‰æŒ‡å®šåˆ—åå­—æ®µåšåˆ‡åˆ†æ³¨æ„ï¼šéœ€è¦æ‰‹åŠ¨åˆ›å»ºHBaseç›®æ ‡è¡¨ï¼ˆä»¥å‰1.0è€ç‰ˆæœ¬HBaseè‡ªåŠ¨åˆ›å»ºï¼‰ sqoop import \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --columns &quot;id,name,sex&quot; \\ --column-family &quot;info&quot; \\ --hbase-create-table \\ --hbase-row-key &quot;id&quot; \\ --hbase-table &quot;hbase_company&quot; \\ --split-by id Sqoopå®˜æ–¹å‚è€ƒ: Importing Data Into HBase Sqoopæ•°æ®å¯¼å‡ºHive/HDFSå¯¼å‡ºæ•°æ®åˆ°RDBMSè¿‡ç¨‹æ˜¯å°†è¡¨æ•°æ®æ¯è¡Œéƒ½ç¼–ç¨‹å­—ç¬¦ä¸²ï¼Œç„¶åæ’å…¥mysqlï¼Œæ‰€ä»¥å¿…é¡»æŒ‡å®šâ€“input-fields-terminated-byæ¥æŠŠå­—æ®µåˆ‡å‰²å¼€æ³¨æ„ï¼ŒRDBMSä½œä¸ºtargetè¡¨ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºtargetè¡¨â€“export-diræŒ‡å®šäº†æ•°æ®ä»“åº“ä¸­è¡¨æ•°æ®ä½ç½® sqoop export \\ --connect jdbc:mysql://localhost:3306/test \\ --username root \\ --password 000000 \\ --table sqoop_test \\ --num-mappers 1 \\ --export-dir /user/hive/warehouse/sqoop_hive \\ --input-fields-terminated-by &quot;\\t&quot; è„šæœ¬æ“ä½œSqoopå…¬å¸ä¸€èˆ¬ä¼šä½¿ç”¨è°ƒåº¦å·¥å…·å®šæœŸæ‰§è¡Œè„šæœ¬ï¼Œæ¯”å¦‚è·å–å‰ä¸€å¤©çš„æ•°æ®è¦åœ¨å‡Œæ™¨ä¸€ä¸¤ç‚¹è¿›è¡ŒæŠ½å–æ•°æ®ï¼Œè¿™å°±éœ€è¦å®šæ—¶ä»»åŠ¡ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿å®šæ—¶ä»»åŠ¡ï¼ŒSqoopå‚æ•°ä¹Ÿè¦å†™åœ¨è„šæœ¬é‡Œï¼Œç±»ä¼¼äºhive -f hql_file touch hdfs_to_mysql_job vim hdfs_to_mysql_job å†…å®¹å¦‚ä¸‹: export --connect jdbc:mysql://localhost:3306/test --username root --password 000000 --table sqoop_test --num-mappers 1 --export-dir /user/hive/warehouse/sqoop_hive --input-fields-terminated-by &quot;\\t&quot; æ‰§è¡Œè¯¥ä»»åŠ¡: sqoop --options-file hdfs_to_mysql_job Sqoopå‚æ•°ä¸­æ–‡å‚è€ƒæ–‡æ¡£Sqoopå‚æ•°ä¸­æ–‡æ–‡æ¡£ï¼Œé‡Œé¢è¿˜åŒ…æ‹¬äº†å‚æ•°çš„å®ç°ç±»ç±»åï¼Œä¾›å‚è€ƒå’Œæ·±å…¥å­¦ä¹ ï¼Œç‚¹å‡»é“¾æ¥ä¸‹è½½:Sqoopå‚æ•°-PDFç‰ˆ æ€»ç»“ å¼ºè¡Œç»“æŸMRä»»åŠ¡åï¼Œä¸æ€¥ç€å†å¯MRä»»åŠ¡ï¼ŒMRAppMasterä»»åŠ¡éœ€è¦killæ‰å†è¿è¡Œæ–°ä»»åŠ¡ å¤šçœ‹å®˜æ–¹æ–‡æ¡£ï¼Œé‡Œé¢å¾ˆè¯¦ç»† è¦æ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯å­¦ä¹ å®˜æ–¹æ–‡æ¡£ä¸­é‡è¦çš„å¸¸ç”¨çš„éƒ¨åˆ† Sqoopæ¯•ç«Ÿæ˜¯åŸºäºMapReduceçš„ï¼Œè€ŒMRçš„è¿ç®—é€Ÿåº¦å·²ç»ä¸èƒ½æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œæ‰€ä»¥å¯¼æ•°æ®å’ŒæŠ½å–æ•°æ®çš„æµç¨‹å®Œå…¨å¯ä»¥ç”¨Sparkæ¥ä»£æ›¿Sqoopï¼ŒSpark2.4ç‰ˆæœ¬åç¨³å®šæ€§å’Œæ•ˆç‡éƒ½æœ‰æå‡ï¼Œä¸”èƒ½å…¼å®¹å¤šç§æ•°æ®æºï¼Œèƒ½å®Œæˆ99%çš„Sqoopä»»åŠ¡ï¼Œå½“ç„¶æœ‰ä¸€äº›è¿½æ±‚ç¨³å®šè€Œéé€Ÿåº¦çš„æŠ½å–æ•°æ®çš„ä»»åŠ¡ä»ç„¶å¯ä»¥ä½¿ç”¨Sqoop","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Sqoopå·¥å…·","slug":"Sqoopå·¥å…·","permalink":"https://shmily-qjj.top/tags/Sqoop%E5%B7%A5%E5%85%B7/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"CentOS7å®‰è£…CDH6å®‰è£…ä¸æ’å‘","slug":"CentOS7å®‰è£…CDH6å…¨ç¨‹è®°å½•","date":"2019-10-29T02:50:40.000Z","updated":"2022-12-11T05:35:07.903Z","comments":true,"path":"38328/","link":"","permalink":"https://shmily-qjj.top/38328/","excerpt":"","text":"å‰è¨€ä¸€å¼€å§‹æ­é›†ç¾¤æ—¶ï¼Œéƒ½æ˜¯è£…ApacheåŸç”Ÿçš„Hadoopï¼ŒSparkåŒ…ï¼Œä¸€ä¸ªä¸€ä¸ªè£…ï¼Œä¸€ä¸ªä¸€ä¸ªé…ï¼Œå¥½éº»çƒ¦ï¼Œè€Œä¸”é€šè¿‡å‘½ä»¤æˆ–RESTç›‘æ§è¿˜å¾ˆä¸ç›´è§‚ï¼Œç›´åˆ°æˆ‘é‡åˆ°äº†Cloudera Managerï¼Œè¿™ä¸œè¥¿ç®€ç›´å°±æ˜¯ç¥å™¨ã€‚Cloudera Managerï¼ˆç®€ç§°CMï¼‰ï¼Œæ˜¯Clouderaå¼€å‘çš„ä¸€æ¬¾å¤§æ•°æ®é›†ç¾¤éƒ¨ç½²ç¥å™¨ï¼Œè€Œä¸”å®ƒå…·æœ‰é›†ç¾¤è‡ªåŠ¨åŒ–å®‰è£…ã€ä¸­å¿ƒåŒ–ç®¡ç†ã€é›†ç¾¤ç›‘æ§ã€æŠ¥è­¦ç­‰åŠŸèƒ½ï¼Œé€šè¿‡å®ƒï¼Œå¯ä»¥è½»æ¾ä¸€é”®éƒ¨ç½²ï¼Œå¤§å¤§æ–¹ä¾¿äº†è¿ç»´ï¼Œä¹Ÿæå¤§çš„æé«˜é›†ç¾¤ç®¡ç†çš„æ•ˆç‡ã€‚ä¸€å¼€å§‹å› ä¸ºCentOS8å‡ºæ¥äº†ï¼Œæƒ³å°é²œï¼Œå‘ç°ClouderaManageræ²¡æœ‰el8çš„ç‰ˆæœ¬ï¼Œæ‰€ä»¥æš‚æ—¶è¿˜ä¸èƒ½ç”¨CentOS8æ¥å®‰è£…CMï¼Œè¯·åƒä¸‡ä¸è¦å°è¯•ä½¿ç”¨CentOS8ã€‚ CMçš„ä¸»è¦åŠŸèƒ½ï¼š ç®¡ç†ï¼šå¯¹é›†ç¾¤è¿›è¡Œç®¡ç†ï¼Œå¦‚æ·»åŠ ã€åˆ é™¤èŠ‚ç‚¹ç­‰æ“ä½œ ç›‘æ§ï¼šç›‘æ§é›†ç¾¤çš„å¥åº·æƒ…å†µï¼Œå¯¹è®¾ç½®çš„å„ç§æŒ‡æ ‡å’Œç³»ç»Ÿè¿è¡Œæƒ…å†µè¿›è¡Œå…¨é¢ç›‘æ§ è¯Šæ–­ï¼šå¯¹é›†ç¾¤å‡ºç°çš„é—®é¢˜è¿›è¡Œè¯Šæ–­ï¼Œä¼šé’ˆå¯¹é›†ç¾¤é—®é¢˜ç»™å‡ºå»ºè®®çš„æ–¹æ¡ˆ é›†æˆï¼šå¯¹hadoopç”Ÿæ€çš„å¤šç§ç»„ä»¶å’Œæ¡†æ¶è¿›è¡Œæ•´åˆï¼Œå‡å°‘éƒ¨ç½²æ—¶é—´å’Œå·¥ä½œé‡ å…¼å®¹ï¼šä¸å„ä¸ªç”Ÿæ€åœˆçš„å…¼å®¹æ€§å¼º æ€»ç»“ä¸€ä¸‹å°±æ˜¯ï¼šæ–¹ä¾¿æ­å»ºå’Œè¿ç»´ï¼Œæä¾›å…¨é¢ç›‘æ§ CDHæ¶æ„CDHçš„ç»„ä»¶ï¼š Agentï¼šåœ¨æ¯å°æœºå™¨ä¸Šå®‰è£…ï¼Œè¯¥ä»£ç†ç¨‹åºè´Ÿè´£å¯åŠ¨å’Œåœæ­¢æœåŠ¡å’Œè§’è‰²çš„è¿‡ç¨‹ï¼Œæ‹†åŒ…é…ç½®ï¼Œè§¦å‘è£…ç½®å’Œç›‘æ§ä¸»æœºã€‚ Management Serviceï¼šè´Ÿè´£æ‰§è¡Œå„ç§ç›‘æ§ï¼Œè­¦æŠ¥å’ŒæŠ¥å‘ŠåŠŸèƒ½è§’è‰²ç­‰æœåŠ¡ã€‚ Databaseï¼šå­˜å‚¨é…ç½®å’Œç›‘è§†ä¿¡æ¯ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå¤šä¸ªé€»è¾‘æ•°æ®åº“åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®åº“æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚ä¾‹å¦‚ï¼ŒClouderaçš„ç®¡ç†æœåŠ¡å™¨å’Œç›‘æ§è§’è‰²ä½¿ç”¨ä¸åŒçš„é€»è¾‘æ•°æ®åº“ã€‚ Cloudera Repositoryï¼šè½¯ä»¶ç”±Clouderaç®¡ç†åˆ†å¸ƒå­˜å‚¨åº“ã€‚ Clientsï¼šæ˜¯ç”¨äºä¸æœåŠ¡å™¨è¿›è¡Œäº¤äº’çš„æ¥å£ CDHä¸­éƒ½æœ‰å“ªäº›æœåŠ¡? ç»„ä»¶åç§° ç”¨é€” Zookeeper Apache ZooKeeper æ˜¯ç”¨äºç»´æŠ¤å’ŒåŒæ­¥é…ç½®æ•°æ®çš„é›†ä¸­æœåŠ¡ã€‚ HDFS HDFSæ˜¯ Hadoop åº”ç”¨ç¨‹åºä½¿ç”¨çš„ä¸»è¦å­˜å‚¨ç³»ç»Ÿã€‚ yarn Apache Hadoop MapReduce 2.0 (MRv2) æˆ– YARN æ˜¯æ”¯æŒ MapReduce åº”ç”¨ç¨‹åºçš„æ•°æ®è®¡ç®—æ¡†æ¶ã€‚ä¾èµ–HDFSæœåŠ¡ã€‚ HBase æ”¯æŒéšæœºè¯»/å†™è®¿é—®çš„Hadoopæ•°æ®åº“(HBaseæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ã€é¢å‘åˆ—çš„å¼€æºæ•°æ®åº“ï¼Œ) Hive åœ¨å¤§æ•°æ®é›†åˆä¸Šçš„ç±»SQLæŸ¥è¯¢å’Œè¡¨ã€‚Hiveæ˜¯åŸºäºHadoopçš„ä¸€ä¸ªæ•°æ®ä»“åº“å·¥å…·ï¼Œå¯ä»¥å°†ç»“æ„åŒ–çš„æ•°æ®æ–‡ä»¶æ˜ å°„ä¸ºä¸€å¼ æ•°æ®åº“è¡¨ï¼Œå¹¶æä¾›ç®€å•çš„sqlæŸ¥è¯¢åŠŸèƒ½ï¼Œå¯ä»¥å°†sqlè¯­å¥è½¬æ¢ä¸ºMapReduceä»»åŠ¡è¿›è¡Œè¿è¡Œã€‚ impala Impalaæ˜¯ä¸€ä¸ªæ–°å‹æŸ¥è¯¢ç³»ç»Ÿï¼Œå®ƒæä¾›SQLè¯­ä¹‰ï¼Œèƒ½æŸ¥è¯¢å­˜å‚¨åœ¨Hadoopçš„HDFSå’ŒHBaseä¸­çš„PBçº§å¤§æ•°æ®ã€‚ solr Solræ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æœåŠ¡ï¼Œç”¨äºç¼–åˆ¶å­˜å‚¨åœ¨ HDFS ä¸­çš„æ•°æ®çš„ç´¢å¼•å¹¶æœç´¢è¿™äº›æ•°æ®ã€‚ spark Sparkæ˜¯å¼ºå¤§çš„å¼€æºå¹¶è¡Œè®¡ç®—å¼•æ“ï¼ŒåŸºäºå†…å­˜è®¡ç®—ï¼Œé€Ÿåº¦æ›´å¿«ï¼›æ¥å£ä¸°å¯Œï¼Œæ˜“äºå¼€å‘ï¼›é›†æˆSQLã€Streamingã€GraphXã€MLlibï¼Œæä¾›ä¸€æ ˆå¼è§£å†³æ–¹æ¡ˆã€‚ flume é«˜å¯é ã€å¯é…ç½®çš„æ•°æ®æµé›†åˆã€‚ storm Stormæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ã€å®¹é”™çš„å®æ—¶è®¡ç®—ç³»ç»Ÿã€‚ kafka Kafkaæ˜¯ä¸€ç§é«˜ååé‡çš„åˆ†å¸ƒå¼å‘å¸ƒè®¢é˜…æ¶ˆæ¯ç³»ç»Ÿã€‚ Hue å¯è§†åŒ–Hadoopåº”ç”¨çš„ç”¨æˆ·æ¥å£æ¡†æ¶å’ŒSDKã€‚ã€‚ Sqoop ä»¥é«˜åº¦å¯æ‰©å±•çš„æ–¹å¼è·¨å…³ç³»æ•°æ®åº“å’ŒHDFSç§»åŠ¨æ•°æ® oozie Oozieæ˜¯ä¸€ç§æ¡†æ¶ï¼Œæ˜¯ç”¨äºhadoopå¹³å°çš„ä½œä¸šè°ƒåº¦æœåŠ¡ã€‚ Avro æ•°æ®åºåˆ—åŒ–ï¼šä¸°å¯Œçš„æ•°æ®ç»“æ„ï¼Œå¿«é€Ÿ/ç´§å‡‘çš„äºŒè¿›åˆ¶æ ¼å¼å’ŒRPCã€‚ Crunch Javaåº“ï¼Œå¯ä»¥æ›´è½»æ¾åœ°ç¼–å†™ï¼Œæµ‹è¯•å’Œè¿è¡ŒMRç®¡é“ã€‚ DataFu ç”¨äºè¿›è¡Œå¤§è§„æ¨¡åˆ†æçš„æœ‰ç”¨ç»Ÿè®¡UDFåº“ã€‚ Mahout ç”¨äºç¾¤é›†ï¼Œåˆ†ç±»å’Œåä½œè¿‡æ»¤çš„åº“ã€‚ Parquet åœ¨Hadoopä¸­æä¾›å‹ç¼©ï¼Œé«˜æ•ˆçš„åˆ—å¼æ•°æ®è¡¨ç¤ºã€‚ Pig æä¾›ä½¿ç”¨é«˜çº§è¯­è¨€æ‰¹é‡åˆ†æå¤§å‹æ•°æ®é›†çš„æ¡†æ¶ã€‚ MapReduce å¼ºå¤§çš„å¹¶è¡Œæ•°æ®å¤„ç†æ¡†æ¶ã€‚ Pig æ•°æ®æµè¯­è¨€å’Œç¼–è¯‘å™¨ Sqoop åˆ©ç”¨é›†æˆåˆ°Hadoopçš„æ•°æ®åº“å’Œæ•°æ®ä»“åº“ Sentry ä¸ºHadoopç”¨æˆ·æä¾›ç²¾ç»†æ”¯æŒï¼ŒåŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ã€‚ Kudu å®ŒæˆHadoopçš„å­˜å‚¨å±‚ï¼Œä»¥å®ç°å¯¹å¿«é€Ÿæ•°æ®çš„å¿«é€Ÿåˆ†æã€‚ å®‰è£…éƒ¨ç½²åœ¨è™šæ‹Ÿæœºç¯å¢ƒä¸Šéƒ¨ç½²Cloudera Managerï¼Œå¯èƒ½è¾¾ä¸åˆ°é¢„æœŸçš„æ•ˆæœï¼Œä½†æ˜¯åŸºæœ¬çš„åŠŸèƒ½å¯ä»¥å®ç°ã€‚æˆ‘çš„ç”µè„‘å†…å­˜16GBå‹‰å¼ºå¯ä»¥ä½¿ç”¨ï¼Œå¦‚æœç”µè„‘16GBä»¥ä¸Šçš„å¯ä»¥è€ƒè™‘æŠ˜è…¾CDHï¼Œ16GBä»¥ä¸‹çš„æƒ³éƒ½ä¸è¦æƒ³â€¦ ç¯å¢ƒç‰©ç†æœºi7-6700hq 16GBå†…å­˜ 1T HDDè™šæ‹Ÿæœºå››å° 8ä¸ªé€»è¾‘æ ¸å¿ƒ å†…å­˜åˆ†é…åˆ†åˆ«æ˜¯ 5GB 3GB 2GB 2GB ï¼ˆå¯ä»¥è¯´æ˜¯æ¦¨å¹²äº†ç‰©ç†æœºæ€§èƒ½ï¼‰å»ºè®®å¦‚æœæ²¡æœ‰i7-8thåŠä»¥ä¸ŠCPUæˆ–æ²¡æœ‰32G+çš„ç”µè„‘ï¼Œå°±ä¸è¦å°è¯•äº†ã€‚è¿˜æ˜¯ç›´æ¥è£…Apacheç‰ˆçš„å¥½äº›ã€‚VMWare 15SecureCRT 8.1.4FileZilla 3.40.0CentOS 7ä»¥ä¸Šæ˜¯æ—§é…ç½®ï¼Œåé¢å†æœ‰æ›´æ–°å‡ä½¿ç”¨æ–°çš„é…ç½®ï¼šç‰©ç†æœºi7-9750h 64GBå†…å­˜ 2T HDD+2T SSDè™šæ‹Ÿæœºå››å° 12ä¸ªé€»è¾‘æ ¸å¿ƒ å†…å­˜åˆ†é…åˆ†åˆ«æ˜¯ 20GB 14GB 14GB 10GBHyper-Vè™šæ‹ŸæœºSecureCRT 8.5.3FileZilla 3.40.0CentOS 7èƒ½å¤ŸåŒæ—¶è¿è¡Œæ‰€æœ‰æœåŠ¡ã€‚ ä¸€.åŸºç¡€é…ç½®ä¸‹è½½CentOS7:CentOS7 Minimalä¸‹è½½ è™šæ‹Ÿæœºé…ç½® ä½¿ç”¨VMWareé‡‡ç”¨NATæ ¼å¼ç½‘å¡,æŒ‰å¦‚ä¸‹é…ç½®è™šæ‹Ÿç½‘å¡è®¾ç½®ï¼ˆç¼–è¾‘-è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨ï¼‰ç‚¹å‡»NATè®¾ç½®:ç‚¹å‡»DHCPè®¾ç½®:ä»¥åæˆ‘ä»¬çš„è™šæ‹Ÿæœºéƒ½ä½¿ç”¨NATç½‘å¡å®‰è£…CentOS7æ–‡ä»¶-&gt;æ–°å»ºè™šæ‹Ÿæœº-&gt;é€‰æ‹©è‡ªå®šä¹‰(é«˜çº§)-&gt;ä¸‹ä¸€æ­¥-&gt;ä¸‹ä¸€æ­¥-&gt;ç¨åå®‰è£…æ“ä½œç³»ç»Ÿ-&gt;é€‰æ‹©Linux/CentOS7 64ä½-&gt;ä¸‹ä¸€æ­¥-&gt;è™šæ‹Ÿæœºåç§°CDH066-&gt;ä¸‹ä¸€æ­¥-&gt;æ ¹æ®è‡ªå·±ç”µè„‘è®¾ç½®æ ¸å¿ƒæ•°-&gt;ä¸‹ä¸€æ­¥-&gt;è™šæ‹Ÿæœºå†…å­˜5120MB-&gt;ç½‘ç»œç±»å‹é€‰NAT-&gt;ä¸‹ä¸€æ­¥â€¦-&gt;ç£ç›˜åˆ†é…80GB-&gt;ä¸‹ä¸€æ­¥-&gt;ä¸‹ä¸€æ­¥-&gt;è‡ªå®šä¹‰ç¡¬ä»¶-&gt;é€‰æ‹©CentOS7çš„å®‰è£…é•œåƒ,å¦‚å›¾:å…³é—­-&gt;å®Œæˆ-&gt;å¼€å¯æ­¤è™šæ‹Ÿæœºå¼€å§‹å®‰è£…å®‰è£…Minimalç‰ˆçš„CentOSï¼Œæ„Ÿè§‰å¾ˆæ¸…çˆ½ï¼ä½†æ˜¯åç»­éœ€è¦è‡ªå·±æ‰‹åŠ¨è£…ä¸€äº›ä¾èµ–åŒ…ï¼Œä¸è¿‡è¿™æ ·ä¹Ÿå¥½ï¼Œå¯ä»¥é¿å…å®‰è£…è¿‡å¤šæ— ç”¨çš„ä¾èµ–ã€‚æ—¶åŒºé€‰æ‹©ShangHaiã€‚ æ­¤æ­¥éª¤æ—¶æŒ‡å®šrootå¯†ç 123456å®‰è£…æ—¶æŒ‡å®šä¸€ä¸ªç®¡ç†å‘˜ç”¨æˆ·shmily å¯†ç 123456 ä½¿ç”¨Hyper-Vï¼ˆæ¨èï¼‰åœ¨Hyper-Vç®¡ç†å™¨ä¸­çš„è™šæ‹Ÿäº¤æ¢æœºç®¡ç†å™¨æ–°å»ºå†…éƒ¨ç½‘ç»œï¼Œç„¶åå¦‚æœè¦æŒ‡å®šIPï¼Œéœ€è¦å»ç”µè„‘çš„ç½‘ç»œè®¾ç½®IPV4ï¼Œç„¶åè®¾ç½®æŠŠWifiç½‘ç»œå…±äº«ç»™è¿™ä¸ªç½‘å¡ã€‚IPV4ï¼š192.168.x.1 (xå‡æ›¿æ¢ä¸ºä½ å–œæ¬¢çš„å€¼ 1-254)ç½‘å…³255.255.255.255.0ç„¶åé…ç½®è™šæ‹Ÿæœºifcfg-eth0æ—¶IPADDR=192.168.x.101NETMASK=255.255.255.0DNS1=192.168.x.1DNS2=192.168.x.2â€¦â€¦..æˆ‘çš„è®¾ç½®å¦‚å›¾ï¼š å®‰è£…å®ŒæˆåRebootï¼ŒæŒ‰æ­¥éª¤è¿›è¡Œå¦‚ä¸‹é…ç½® rm -rf * vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=cdh066 vi /etc/sysconfig/network-scripts/ifcfg-ens33 ä¿®æ”¹ä»¥ä¸‹å‡ é¡¹çš„å€¼ BOOTPROTO=static ONBOOT=yes NM_CONTROLLED=yes IPADDR=192.168.1.66 GATEWAY=192.168.1.2 DNS1=192.168.1.2 vi /etc/sudoers æ·»åŠ ä»¥ä¸‹ï¼Œå¿…è¦çš„è¯å¯ä»¥åŠ å…¶ä»–ç”¨æˆ·æƒé™æ§åˆ¶ç­–ç•¥ï¼Œè¿™é‡Œæˆ‘å¯¹rootå’Œshmilyä¸¤ä¸ªç”¨æˆ·èµ‹æƒ root ALL=(ALL) ALL ä¸‹é¢æ·»åŠ ï¼š shmily ALL=(ALL) ALL systemctl start NetworkManger systemctl enable NetworkManger service NetworkManager status systemctl status firewalld.service # æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€ systemctl stop firewalld.service # å…³é—­é˜²ç«å¢™ systemctl disable firewalld.service # å…³é—­é˜²ç«å¢™å¼€æœºå¯åŠ¨ systemctl is-enabled firewalld.service # æŸ¥çœ‹é˜²ç«å¢™æ˜¯å¦å¼€æœºå¯åŠ¨ # å…³é—­selinux vi /etc/selinux/config é…ç½®æ–‡ä»¶ä¸­çš„ SELINUX=disabled # å¼€å¯SSHæœåŠ¡ï¼Œç”¨äºä½¿ç”¨SecureCRTè¿æ¥ # æ£€æŸ¥sshæœåŠ¡æ˜¯å¦å¼€å¯ï¼ˆCentOS7é»˜è®¤å¼€å¯ï¼‰ ps -e | grep sshd # ä¿®æ”¹Hostname vi /etc/hostname localhost.localdomainæ”¹ä¸ºcdh066 sudo hostnamectl set-hostname CDH067 vi /etc/hosts # æ·»åŠ å¦‚ä¸‹è®°å½• 192.168.1.66 cdh066 192.168.1.67 cdh067 192.168.1.68 cdh068 192.168.1.69 cdh069 reboot # é‡å¯æœºå™¨CDH066 # æ£€æŸ¥22ç«¯å£æ˜¯å¦å¼€å¯ ï¼ˆCentOS87é»˜è®¤å¼€å¯ï¼‰ yum install net-tools netstat -an | grep 22 SecureCRTè¿æ¥æµ‹è¯•SSHSecureCRTåˆ›å»ºNew Session -&gt; SSH2 -&gt; Hostnameæ˜¯CDH066 usernameæ˜¯rootå‘ç°è¿˜æ˜¯ä¼šæç¤ºHostname lookup failed: host not foundéœ€è¦ä¿®æ”¹Windowsçš„C:\\Windows\\System32\\drivers\\etc\\hostsæ·»åŠ å¦‚ä¸‹å¹¶ä¿å­˜192.168.1.66 cdh066192.168.1.67 cdh067192.168.1.68 cdh068192.168.1.69 cdh069 é‡æ–°ç”¨SecureCRTè¿æ¥å‡ºç°å¦‚ä¸‹å›¾:Accept &amp; Save,è¾“å…¥å¯†ç å¹¶å‹¾é€‰Save passwordå®ŒæˆFileZillaä¹Ÿèƒ½è¿æ¥äº†: æ£€æŸ¥ä¸€ä¸‹ç½‘ç»œ:ping 8.8.8.8èƒ½pingé€šå³å¯è¿›è¡Œä¸‹ä¸€æ­¥ï¼Œå¦‚æœpingä¸é€šï¼Œéœ€è¦ä»”ç»†æ£€æŸ¥ç½‘ç»œé…ç½®æ–‡ä»¶: å®‰è£…python:CentOS7 Minimalé»˜è®¤å¸¦Python2.7.5ç‰ˆæœ¬ï¼Œå·²ç»æ»¡è¶³éœ€æ±‚ï¼Œä¸ºäº†å¼€å‘æ–¹ä¾¿ï¼Œè¿˜æ˜¯å®‰è£…ä¸ªipythonå§ yum -y install epel-release yum install python-pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests # å®‰è£…å¿…è¦çš„åº“å¯ä»¥æŒ‡å®šæº ä»¥å®‰è£…requestsåº“ä¸ºä¾‹ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ipython # å®‰è£…ipython å®Œæˆåï¼Œå‘½ä»¤è¡Œæ‰§è¡Œpythonå³å¯è¿è¡Œpython2.7.5ï¼Œå‘½ä»¤è¡Œæ‰§è¡Œipythonå³å¯ä½¿ç”¨ipython å®‰è£…ä¸€äº›å¿…è¦çš„å¸¸ç”¨å‘½ä»¤[å¿…è¦]yum install bind-utilsyum -q install /usr/bin/iostatyum install vim wget iotop lsofyum install -y gityum install dstat (å…¨é¢çš„ç³»ç»Ÿç›‘æ§å·¥å…·-æ¨è)yum install nload å®‰è£…ä¸€äº›CDHæ‰€éœ€çš„å¿…è¦ä¾èµ–[å¿…è¦] yum -y install chkconfig bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs redhat-lsb httpd httpd-tools unzip ntp systemctl start httpd.service # å¯åŠ¨httpdæœåŠ¡ systemctl enable httpd.service # è®¾ç½®httpdå¼€æœºå¯åŠ¨ yum -y install httpd createrepo # createrepoæ˜¯å®‰è£…CDH6é›†ç¾¤å¿…å¤‡ vim /etc/rc.local æ·»åŠ  echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled chmod +x /etc/rc.d/rc.local å®‰è£…JDK1.8[åƒä¸‡ä¸è¦è‡ªè¡Œæ›´æ¢ç‰ˆæœ¬]å»Oracleå®˜ç½‘ä¸‹è½½1.8ç‰ˆæœ¬8u181çš„å®‰è£…åŒ…JDK 1.8å†å²ç‰ˆæœ¬ä¸‹è½½å¦‚æœå®‰è£…æœ€æ–°ç‰ˆæœ¬ï¼Œåç»­CDHå®‰è£…æœåŠ¡ä¼šæ— æ³•å¯åŠ¨ï¼Œé‡åˆ°å„ç§é—®é¢˜ã€‚è¦æ˜ç¡®CDH6.3æ”¯æŒçš„JDKç‰ˆæœ¬ï¼š è¿™é‡Œæ³¨æ„ï¼ŒJDKç›®å½•ä¸€å®šæ˜¯/usr/java/jdk_1.8.x_xxï¼Œè¿™æ ·CMæœåŠ¡æ‰èƒ½æ£€æµ‹åˆ°JDKï¼Œå¦åˆ™æœåŠ¡æ— æ³•å¯åŠ¨ mkdir /opt/software # é€šè¿‡FileZillaä¸Šä¼ åˆ°CDH066èŠ‚ç‚¹çš„ &lt;u&gt;/opt/software&lt;/u&gt;ç›®å½•ä¸‹ cd /opt/software mkdir /usr/java/ tar -zxvf jdk-8u181-linux-x64.tar.gz -C /usr/java/ cd .. vim /etc/profile æ·»åŠ  #JAVA_HOME export JAVA_HOME=/usr/java/jdk1.8.0_181 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar source /etc/profile java -version ä¼˜åŒ–æœåŠ¡å™¨é…ç½® # swappiness echo 10 &gt; /proc/sys/vm/swappiness # å…³é—­é€æ˜å¤§é¡µé¢å‹ç¼© echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled echo &quot;vm.swappiness=0&quot; &gt;&gt; /etc/sysctl.conf è¿˜æœ‰ä¸€äº›å…¶ä»–çš„ç›‘æ§å‘½ä»¤Linuxç›‘æ§å‘½ä»¤æ±‡æ€» Mysqlå®‰è£…(CDHå¿…å¤‡)é¦–å…ˆæŸ¥çœ‹Cloudera Managerå®˜ç½‘è¦æ±‚çš„Mysqlç‰ˆæœ¬ï¼šDatabase Requirementså‚è€ƒCDH6.xå…¼å®¹çš„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬é€‰æ‹©Mysql5.7ç‰ˆæœ¬æ³¨æ„:mysql-serverä¾èµ–mysql-clientmysql-clientä¾èµ–mysql-community-libsmysql-community-libsä¾èµ–mysql-community-commonæ‰€ä»¥å®‰è£…Serverä¼šé»˜è®¤å®‰è£…å…¶å…¨éƒ¨ä¾èµ–åœ¨çº¿å®‰è£…ï¼š rpm -qa|grep mariadb rpm -e --nodeps mariadb-libs-5.5.64-1.el7.x86_64 # å¸è½½MariaDB è™½ç„¶CDH6æ”¯æŒäº†MariaDBï¼Œä½†è¿˜æ˜¯æ¨èMysql wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum -y install mysql-community-server ç¦»çº¿å®‰è£…ï¼šå»Mysql-Archives ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„rpmåŒ… # å®‰è£…ä¾èµ–åŒ… rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm # è‹¥å®‰è£…å¤±è´¥ åˆ é™¤mariadbåŒ… rpm -qa | grep mariadb rpm -ve mariadb # å®‰è£…libs rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm # å®‰è£…å®¢æˆ·ç«¯ rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm # å®‰è£…æœåŠ¡ç«¯ rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm # å¯åŠ¨ systemctl status mysqld # å¼€æœºè‡ªå¯ systemctl enable mysqld # æŸ¥æ‰¾ä¸´æ—¶å¯†ç  grep &#39;temporary password&#39; /var/log/mysqld.log # è®¾ç½®å¯†ç å¼ºåº¦ set global validate_password_policy=LOW; set global validate_password_length=6; å¦‚å›¾å®‰è£…å®Œæˆï¼Œæ¥ç€æˆ‘ä»¬å¯¹å…¶è¿›è¡Œä¸€äº›é…ç½® systemctl start mysqld.service systemctl enable mysqld.service # è®¾ç½®å¼€æœºå¯åŠ¨ systemctl status mysqld.service # æŸ¥çœ‹mysqlè¿è¡ŒçŠ¶æ€ grep &#39;temporary password&#39; /var/log/mysqld.log # æ‰¾åˆ°rootåˆå§‹å¯†ç ï¼Œæˆ‘çš„æ˜¯cWgrI9:14%=_ mysql -uroot -p # ç™»é™†mysql # æç¤ºEnter Password cWgrI9:14%=_ set global validate_password_policy=LOW; # æ²¡æœ‰è¿™é¡¹ä¼šæç¤ºYour password does not satisfy the current policy requirements å¦‚æœä¸æ˜¯ç”Ÿäº§ç¯å¢ƒéœ€è¦ä¿®æ”¹å¯†ç å®‰å…¨ç­–ç•¥ç­‰çº§ä¸ºLOW set global validate_password_length=6; # æœ€ä½å¯†ç é•¿åº¦ï¼Œå› ä¸ºæµ‹è¯•æ‰€ä»¥è®¾ä¸ºäº†6 ç”Ÿäº§ç¯å¢ƒåˆ™ä¸éœ€è¦ä¿®æ”¹ ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; # ä¿®æ”¹æ•°æ®åº“å¯†ç ä¸º123456 CREATE USER &#39;mysql&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # rootç™»é™†ç„¶ååˆ›å»ºç”¨æˆ·åŠå…¶å¯†ç ï¼ˆç”¨æˆ·åmysqlä¸ºä¾‹ï¼‰ GRANT ALL ON mysql.* TO &#39;mysql&#39;@&#39;%&#39;; # èµ‹äºˆmysqlç”¨æˆ·æ‰€æœ‰æƒé™ flush privileges; # åˆ·æ–°é…ç½® status; # é€šè¿‡è¿™ä¸ªå‘½ä»¤å‘ç°Mysqlç›®å‰ä¸æ˜¯UTF-8å­—ç¬¦é›† é…ç½®utf-8å­—ç¬¦é›†vim /etc/my.cnf æ·»åŠ å¦‚ä¸‹é…ç½®æ³¨æ„é¡ºåºï¼Œclientä¸€å®šåœ¨mysqldå±æ€§çš„ä¸Šæ–¹ [client] default-character-set=utf8 [mysqld] init_connect=&#39;SET collation_connection = utf8_unicode_ci&#39; init_connect=&#39;SET NAMES utf8&#39; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake æ ¹æ®CDHå®˜æ–¹æ¨èçš„Mysqlå‚æ•°é…ç½®,ç»§ç»­æ·»åŠ å¦‚ä¸‹å‚æ•°:å¦‚æœç”Ÿäº§ç¯å¢ƒï¼Œéœ€è¦æ ¹æ®é›†ç¾¤é…ç½®çš„å®é™…æƒ…å†µæ¥è®¾å®š [mysqld] transaction-isolation = READ-COMMITTED symbolic-links = 0 key_buffer_size = 32M max_allowed_packet = 32M thread_stack = 256K thread_cache_size = 64 query_cache_limit = 8M query_cache_size = 64M query_cache_type = 1 max_connections = 550 expire_logs_days = 10 max_binlog_size = 100M log_bin=/var/lib/mysql/mysql_binary_log server_id=1 binlog_format = mixed read_buffer_size = 2M read_rnd_buffer_size = 16M sort_buffer_size = 8M join_buffer_size = 8M # InnoDB settings innodb_file_per_table = 1 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 64M innodb_buffer_pool_size = 128M innodb_thread_concurrency = 8 innodb_flush_method = O_DIRECT innodb_log_file_size = 512M sql_mode=STRICT_ALL_TABLES é‡å¯MysqlæœåŠ¡systemctl restart mysqld.service ç™»å½•Mysqlå¹¶æŸ¥çœ‹æ˜¯å¦ä¿®æ”¹æˆåŠŸmysql -hlocalhost -P3306 -uroot -p123456show variables like â€œ%character%â€;show variables like â€œ%collation%â€;å¦‚å›¾å³ä¸ºé…ç½®æˆåŠŸ åˆ›å»ºCMçš„æ•°æ®åº“å¹¶å¢åŠ æ•°æ®åº“æ‰€å±ç”¨æˆ·çš„è¿œç¨‹ç™»é™†æƒé™ï¼š CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;rman&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;sentry&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;nav&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;navms&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON scm.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON amon.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON rman.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hue.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON hive.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON sentry.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON nav.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON navms.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; GRANT ALL ON oozie.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; set global validate_password_policy=LOW; set global validate_password_length=6; GRANT ALL ON root.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; # è®©rootç”¨æˆ·å¯ä»¥åœ¨cdh066èŠ‚ç‚¹ä¸Šç™»å½• FLUSH PRIVILEGES; å…³äºå¦‚ä½•æŸ¥çœ‹å’Œä¿®æ”¹ç”¨æˆ·çš„è¿œç¨‹ç™»å½•æƒé™ï¼šselect user,host from mysql.user;hostå­—æ®µä¸º%çš„åˆ™æ˜¯å…è®¸è¿œç¨‹ç™»å½•çš„ç”¨æˆ·ï¼Œæ˜¯localhostçš„åªèƒ½æœ¬åœ°ç™»å½•æ‰€ä»¥æƒ³ç»™è¿œç¨‹æŸå°æœºå™¨å¼€é€šè¿œç¨‹è®¿é—®æŸä¸ªç”¨æˆ·çš„æƒé™ï¼š update mysql.user set host=â€™CDH066â€™ where user=â€™rootâ€™;æˆ–è€…æƒ³ç»™æŸä¸ªç”¨æˆ·æ‰€æœ‰å±€åŸŸç½‘å†…æœºå™¨çš„è®¿é—®æƒé™ï¼š update mysql.user set host=â€™%â€™ where user=â€™rootâ€™;ç„¶åé‡å¯æœåŠ¡æˆ–è€…åˆ·æ–°é…ç½®å°±å¯ä»¥é€šè¿‡mysql -hCDH066 -uroot -p123456æ¥ç™»å½•äº†è¿œç¨‹å…¶ä»–èŠ‚ç‚¹å¯ä»¥é€šè¿‡åˆ¶å®š-hæ¥è®¿é—®érootç”¨æˆ·çš„mysql Mysql JDBCåº“é…ç½®ï¼šå³é”® é“¾æ¥å¦å­˜ä¸º è¿›è¡Œä¸‹è½½**ä¸‹è½½mysql-connector-java-5.1.47-bin.jar**ï¼Œå°†mysql-connector-java-5.1.47-bin.jaræ–‡ä»¶ä¸Šä¼ åˆ°CDH066èŠ‚ç‚¹ä¸Šçš„/usr/share/java/ç›®å½•ä¸‹å¹¶é‡å‘½åä¸ºmysql-connector-java.jarï¼ˆå¦‚æœ/usr/share/java/ç›®å½•ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼‰ ç³»ç»Ÿæ–‡ä»¶æè¿°ç¬¦é™åˆ¶ä¿®æ”¹vi /etc/security/limits.conf * soft nofile 32728 * hard nofile 1029345 * soft nproc 65536 * hard nproc unlimited * soft memlock unlimited * hard memlock unlimited å¥½ç©çš„screenfetch(å¯é€‰ï¼Œç”¨æ¥å¨±ä¹â€¦) cd /usr/local/src git clone https://github.com/KittyKatt/screenFetch.git cp screenFetch/screenfetch-dev /usr/local/bin/screenfetch chmod 777 /usr/local/bin/screenfetch æ›´å¤šå®‰å…¨ä¸é˜²ç«å¢™é…ç½®å‚è€ƒå®‰å…¨ä¸é˜²ç«å¢™é…ç½®æœ‰å…³linuxç”¨æˆ·å’Œç»„çš„è¯¦ç»†æ–‡ç« :Linuxç”¨æˆ·å’Œç»„ å¼€å¯ntpdæ—¶é—´åŒæ­¥ï¼šå‚è€ƒï¼šntpæœ¬åœ°æœåŠ¡å™¨æ­å»º1.åˆ›å»ºæœ¬åœ°NTPæ—¶é—´æœåŠ¡å™¨ vim /etc/ntp.conf æ³¨é‡Šæ‰ï¼š #restrict default nomodify notrap nopeer noquery #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst æ·»åŠ ï¼š restrict default nomodify restrict 192.168.1.0 mask 255.255.255.0 nomodify æ˜¾å¼çš„æŒ‡å‡ºæ—¶é—´æœåŠ¡å™¨æ‰€æ¶‰åŠçš„ipèŒƒå›´ server 127.127.1.0 fudge 127.127.1.0 stratum 10 2.é…ç½®NTPå®¢æˆ·ç«¯ï¼ˆå…¶ä»–èŠ‚ç‚¹ï¼‰ æ³¨é‡Šæ‰ï¼š #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst æ·»åŠ ï¼š server cdh101 æŒ‡æ˜æœ¬åœ°ntpæœåŠ¡å™¨åœ°å€ 3.åˆ†åˆ«å¯åŠ¨ntpdæœåŠ¡systemctl status ntpdsystemctl start ntpdsystemctl enable ntpdntpdate -u cdh101 æ‰‹åŠ¨åŒæ­¥ä¸€æ¬¡ntpq â€“p æŸ¥çœ‹ntpdæœåŠ¡çŠ¶æ€ äºŒ.å…‹éš†è™šæ‹Ÿæœºå…‹éš†CDHæ‰€éœ€çš„å¦å¤–ä¸‰å°è™šæ‹Ÿæœºå³é”®CDH066è¿™å°å·²å…³é—­çš„è™šæ‹Ÿæœºï¼Œå³é”®-&gt;ç®¡ç†-&gt;å…‹éš†é€‰æ‹©è™šæ‹Ÿæœºä¸­å½“å‰çŠ¶æ€ ä¸‹ä¸€æ­¥é€‰æ‹©åˆ›å»ºå®Œæ•´å…‹éš† ä¸‹ä¸€æ­¥è™šæ‹Ÿæœºåç§° CDH067 å®ŒæˆåŒæ ·æ–¹æ³•å…‹éš† CDH068 CDH069å…‹éš†å®Œæˆå¯¹æœºå™¨è¿›è¡Œè®¾ç½®:CDH067 3GBå†…å­˜CDH068 2GBå†…å­˜CDH069 2GBå†…å­˜ å¼€å¯CDH067æœºå™¨ç¡®ä¿ /ect/hostsé‡Œå·²ç»æ·»åŠ äº†å…¶ä»–æœºå™¨çš„ipå’Œhostvim /etc/sysconfig/network-scripts/ifcfg-ens33åˆ é™¤UUIDå’ŒHWADDRIPADDRé‡æ–°åˆ†é…ä¸º192.168.1.67ä¿®æ”¹åå¦‚å›¾ vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=CDH067 vi /etc/hostnameCDH066æ”¹ä¸ºCDH067sudo hostnamectl set-hostname CDH067 é‡å¯ reboot åŒæ ·æ–¹å¼ä¿®æ”¹CDH068,CDH069 æ£€æŸ¥ï¼š ping8.8.8.8èƒ½é€š ä½¿ç”¨SecureCRTå¯ä»¥æ­£å¸¸è¿æ¥åˆ°æœºå™¨ ifconfig æˆ– ip addrå‘½ä»¤æŸ¥çœ‹ipåœ°å€æˆåŠŸæ”¹è¿‡æ¥äº†åˆ™é…ç½®æˆåŠŸ ä¸‰.é…ç½®å…å¯†ç™»å½•[å¯é€‰ éå¿…é¡»]åœ¨å››å°æœºå™¨åˆ†åˆ«æ“ä½œï¼šssh-keygen å¹¶è¿ç»­æ•²ä¸‰ä¸‹å›è½¦ åœ¨66æœºå™¨ä¸Šssh-copy-id cdh066 å»ºç«‹ cdh066è‡ªèº«å…å¯†ssh-copy-id cdh067 å»ºç«‹ cdh066 -&gt; cdh067å•å‘å…å¯†ssh-copy-id cdh068 å»ºç«‹ cdh066 -&gt; cdh068å•å‘å…å¯†ssh-copy-id cdh069 å»ºç«‹ cdh066 -&gt; cdh069å•å‘å…å¯† åœ¨67æœºå™¨ä¸Šssh-copy-id cdh066 å»ºç«‹ cdh067 -&gt; cdh066å•å‘å…å¯†ssh-copy-id cdh067 å»ºç«‹ cdh066è‡ªèº«å…å¯†ssh-copy-id cdh068 å»ºç«‹ cdh067 -&gt; cdh068å•å‘å…å¯†ssh-copy-id cdh069 å»ºç«‹ cdh067 -&gt; cdh069å•å‘å…å¯† åœ¨68æœºå™¨ä¸Šssh-copy-id cdh066 å»ºç«‹ cdh068 -&gt; cdh066å•å‘å…å¯†ssh-copy-id cdh067 å»ºç«‹ cdh068 -&gt; cdh067å•å‘å…å¯†ssh-copy-id cdh068 å»ºç«‹ cdh068è‡ªèº«å…å¯†ssh-copy-id cdh069 å»ºç«‹ cdh068 -&gt; cdh069å•å‘å…å¯† åœ¨69æœºå™¨ä¸Šssh-copy-id cdh066 å»ºç«‹ cdh069 -&gt; cdh066å•å‘å…å¯†ssh-copy-id cdh067 å»ºç«‹ cdh069 -&gt; cdh067å•å‘å…å¯†ssh-copy-id cdh068 å»ºç«‹ cdh069 -&gt; cdh068å•å‘å…å¯†ssh-copy-id cdh069 å»ºç«‹ cdh069è‡ªèº«å…å¯† æµ‹è¯•éƒ½èƒ½å…å¯†ç™»å½•:è‡³æ­¤å…å¯†ç™»å½•é…ç½®å®Œæˆ å››.CDH6å®‰è£…ä¸‹é¢çš„æ˜¯ä¸‹è½½åœ°å€ï¼Œå› ä¸ºæˆ‘ä¹‹å‰æ‰‹åŠ¨å®‰è£…äº†JDK1.8ï¼Œæ‰€ä»¥å¯ä»¥ä¸ä¸‹è½½oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpmè¿™ä¸ªåŒ…ï¼Œå…¶ä½™çš„åŒ…å…¨éƒ¨ä¸‹è½½ä¸‹æ¥CDH6.3.1ä¸‹è½½åœ°å€è¿˜éœ€è¦ä¸€ä¸ªascæ–‡ä»¶ï¼Œä¸‹è½½åœ°å€ï¼šallkeys.asc,å³é”®å¦å­˜ä¸ºå³å¯åœ¨cdh066èŠ‚ç‚¹ä¸Šè¿›è¡Œæ“ä½œmkdir /opt/software/cloudera-reposå°†ä¸‹è½½çš„æ‰€æœ‰æ–‡ä»¶é€šè¿‡FileZillaä¸Šä¼ åˆ°/opt/software/cloudera-reposç›®å½•ï¼Œç›®å½•ç»“æ„å¦‚ä¸‹:â”œâ”€â”€ allkeys.ascâ”œâ”€â”€ cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpmâ”œâ”€â”€ cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpmâ”œâ”€â”€ cloudera-manager-server-db-2-6.3.1-1466458.el7.x86_64.rpmâ”œâ”€â”€ enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpmâ””â”€â”€ cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm CDH066èŠ‚ç‚¹æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ ç›®çš„æ˜¯å»ºç«‹æœ¬åœ°å­˜å‚¨åº“ æ­å»ºæœ¬åœ°æº ä¸ºäº†èŠ‚çœç©ºé—´ï¼Œä¹Ÿå¯ä»¥åªåœ¨ä¸€å°æœºå™¨ä¸Šæ­å»ºæº cd /opt/software/cloudera-repos createrepo . # å°†cloudera-reposç›®å½•ç§»åŠ¨åˆ°httpdçš„htmlç›®å½•ä¸‹ åˆ¶ä½œæœ¬åœ°æº cd .. mv cloudera-repos /var/www/html/ cd /etc/yum.repos.d touch cloudera-manager.repo vim cloudera-manager.repo æ·»åŠ å¦‚ä¸‹ baseurlåœ°å€å¯¹åº”è‡ªå·±çš„ä¸»æœºhost å¦‚ cdh067èŠ‚ç‚¹:http://cdh067/cloudera-repos/ [cloudera-manager] name=Cloudera Manager 6.3.1 baseurl=http://cdh066/cloudera-repos/ gpgcheck=0 enabled=1 autorefresh=0 type=rpm-md yum clean all yum makecache åˆ¶ä½œæœ¬åœ°æºåhttp://cdh066/cloudera-repos/è¿™ä¸ªé“¾æ¥å¯ä»¥è®¿é—®åˆ°æºçš„æ–‡ä»¶æˆ‘ä»¬æ­å»ºçš„æœ¬åœ°æºï¼Œåé¢ä¼šç”¨åˆ° å®‰è£…Cloudera Managerç»„ä»¶: # åœ¨CDH066èŠ‚ç‚¹è¿è¡Œ yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server --skip-broken --nogpgcheck ä¸‹è½½parcelåŒ…ï¼Œï¼šIndex of cdh6/6.3.1/parcels/è¯¥é“¾æ¥ç›®å‰éœ€è¦æœ‰Licenseçš„Clouderaå¸å·æ‰å¯ä»¥ä¸‹è½½äº†ã€‚ cd /opt/cloudera/parcel-repo sha1sum CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel | awk &#39;&#123; print $1 &#125;&#39; &gt; CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel.sha chown -R cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo/* åˆå§‹åŒ–æ•°æ®åº“è¯¥æ­¥éª¤å¾ˆé‡è¦ï¼Œå¯ä»¥åœ¨ç¬¬ä¸€æ¬¡å¯åŠ¨ClouderaManagerå‰æ£€æµ‹æ•°æ®åº“è¿æ¥æ˜¯å¦æœ‰é—®é¢˜ï¼Œæ˜¯å¦ä¼šå½±å“åˆ°CMServeråˆå§‹åŒ–ã€‚é€šè¿‡è¯¥è„šæœ¬è¾“å‡ºçš„æ—¥å¿—å¯ä»¥å®šä½åˆ°é”™è¯¯åŸå› ï¼Œå¹¶ä¿®æ”¹mysqlä¸­ä¸åˆç†çš„é…ç½®æ–‡ä»¶ï¼Œä¿®æ”¹ç³»ç»Ÿç¯å¢ƒé…ç½®é”™è¯¯çš„åœ°æ–¹ã€‚ # ï¼ˆscm_prepare_database.sh åº“ç±»å‹ scmåº“åç§° scmåº“è¿æ¥çš„ç”¨æˆ·å å¯†ç  -hæœåŠ¡ç«¯æ‰€åœ¨åœ°å€ï¼‰ /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm 123456 -hHOST systemctl start cloudera-scm-server.service # å¯åŠ¨CMæœåŠ¡systemctl status cloudera-scm-server.service # æŸ¥çœ‹å¯åŠ¨çŠ¶æ€ ç­‰å¾…å‡ åˆ†é’Ÿåè®¿é—®http://cdh066:7180ï¼Œé»˜è®¤å¸å·å¯†ç éƒ½æ˜¯admin è¿™é‡Œé€‰æ‹©å…è´¹ç‰ˆæœ¬ ä¸‹é¢å°±æ˜¯ç¾¤é›†å®‰è£…çš„æ­¥éª¤ï¼šä¸»æœºåç§°å¡«å†™cdh066,cdh067,cdh068,cdh069ï¼Œç„¶åç‚¹å‡»æœç´¢æœç´¢è¿™é‡Œæœåˆ°äº†67ï¼Œ68ï¼Œ69èŠ‚ç‚¹ï¼Œä½†æ˜¯66èŠ‚ç‚¹æ˜¯ç°è‰²çš„ï¼Œå®‰è£…æ—¶ï¼Œ66èŠ‚ç‚¹ä¸ä¼šè¢«å®‰è£…Agentï¼Œæ„å‘³ç€åç»­å®‰è£…çš„ç»„ä»¶åªèƒ½éƒ¨ç½²åœ¨67ï¼Œ68ï¼Œ69èŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œä¸è¿‡æ²¡æœ‰å…³ç³»ï¼Œå¯ä»¥åœ¨æ·»åŠ ç»„ä»¶çš„æ­¥éª¤ä¹‹å‰æ–°å¼€ä¸ªé¡µé¢å°†cdh066ä¹ŸåŠ è¿›å»ã€‚ è¿™æ­¥ä½¿ç”¨æˆ‘ä»¬æ­å»ºçš„æœ¬åœ°æº http://cdh066/cloudera-repos/ å¦‚ä¸‹è®¾ç½®è‹¥ç»§ç»­æŒ‰é’®ä»ä¸ºç°è‰²ï¼Œå¯ä»¥ç‚¹å‡»æ›´å¤šé€‰é¡¹ï¼Œå°†æ‰€æœ‰å¤–éƒ¨æºçš„é“¾æ¥å…¨éƒ¨åˆ æ‰ï¼Œå¢åŠ æœ¬åœ°parcelæºåœ°å€ï¼Œä¿å­˜æ›´æ”¹ã€‚ è¿™æ­¥ä¸è¦å‹¾é€‰ å¡«å…¥rootç”¨æˆ·çš„å¯†ç  è¿™æ­¥è€å¿ƒç­‰å¾…ï¼Œä¸è¦æ‰‹åŠ¨åˆ·æ–° è¿™æ­¥å‹¾é€‰æœ€åä¸€é¡¹ä¼ä¸šå®‰è£…æ—¶ï¼Œæœ€å¥½å…ˆInspect Hostsï¼ŒéªŒè¯èŠ‚ç‚¹æ˜¯å¦æœ‰é…ç½®ä¸å½“çš„åœ°æ–¹ï¼Œé¿å…å½±å“ç¨³å®šæ€§ã€‚é’ˆå¯¹Inspect Hostsçš„ç»“æœï¼Œå¯ä»¥ä¸€æ¡ä¸€æ¡ä¼˜åŒ–é›†ç¾¤é…ç½®ã€‚ å¼€å§‹å®‰è£…æœåŠ¡ å¦‚å›¾ï¼Œé€‰è‡ªå®šä¹‰æœåŠ¡æ ¹æ®é›†ç¾¤ç¯å¢ƒå’Œéœ€æ±‚é€‰æ‹©åˆé€‚çš„æœåŠ¡å’Œæ­é…ã€‚ å¡«ä¸Šä¹‹å‰å»ºçš„æ•°æ®åº“ï¼Œé€‰çš„æœåŠ¡ä¸åŒè¦æ±‚ä¹Ÿä¸åŒ æœ€åéƒ¨ç½²æˆåŠŸï¼Œå¯åŠ¨æœåŠ¡ï¼šå› ä¸ºæˆ‘è™šæ‹Ÿæœºæ­å»ºï¼Œç‰©ç†æœºæœ¬èº«é…ç½®å°±å¾ˆå·®ï¼Œæœ‰å†…å­˜ä¸è¶³å’Œè¯·æ±‚å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œæ‰€ä»¥ï¼Œè™½ç„¶æœåŠ¡éƒ½èƒ½æ­£å¸¸æ‰“å¼€ï¼Œè·‘ä¸€ä¸¤ä¸ªå°çš„è®¡ç®—ä»»åŠ¡ä¹Ÿè¿˜èƒ½å‹‰å¼ºæ‰¿å—ï¼Œä½†CDHéƒ½ä¼šæŠ¥è­¦å‘Šï¼Œå¤§å¤šéƒ½æ˜¯æç¤ºåˆ†é…å†…å­˜ä½äº†ï¼Œè¯·æ±‚å»¶è¿Ÿé«˜äº†ï¼Œå†…å­˜ä¸è¶³ç­‰ä¿¡æ¯ã€‚åç»­æ–‡ç« æ›´æ–°å†…å®¹é‡‡ç”¨æ–°ç”µè„‘64GBå†…å­˜i7-9750hç‰©ç†æœºç¯å¢ƒï¼Œèƒ½æ­£å¸¸è¿è¡ŒCDHæœåŠ¡ï¼š ClockOffsetçš„æŠ¥è­¦ï¼šé›†ç¾¤å…¨çº¢ï¼Œæç¤ºClockOffset æœªæ£€æµ‹åˆ°ntpdæœåŠ¡ã€‚è¿™ä¸ªæ—¶å€™å°±éœ€è¦é…ç½®NTPæ—¶é—´åŒæ­¥æœåŠ¡ï¼Œé‡æ–°å‚è€ƒä¹‹å‰NTPæ—¶é’ŸåŒæ­¥çš„é…ç½®ã€‚ ä¼ä¸šçº§éƒ¨ç½²æ—¶ï¼Œéœ€è¦åœ¨å®‰è£…å¥½å„è§’è‰²åï¼Œä¾æ¬¡è®¾ç½®é‡Œé¢æ¯ä¸ªè§’è‰²(åŒ…æ‹¬CMServer)çš„dirã€pathç­‰è·¯å¾„ï¼Œå°†ä¸€äº›æ—¥å¿—ã€dumpè·¯å¾„ç­‰æ”¾åœ¨æ•°æ®ç›˜ã€‚ äº”.Flinké›†æˆé›†æˆå®˜æ–¹Flink-1.9.0åˆ°CDHç®¡ç†ä¸‹è½½ç›¸åº”çš„csdæ–‡ä»¶å’Œparcelsæ–‡ä»¶åˆ°æœ¬åœ°ï¼šcsdä¸‹è½½åœ°å€parcelsä¸‹è½½åœ°å€ä¸‹è½½åå¾—åˆ°å¦‚ä¸‹ï¼š FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jar FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.sha FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel manifest.json å°†FLINK-1.9.0-csa1.0.0.0-cdh6.3.0.jaræ”¾å…¥/opt/cloudera/csdä¸­å°†FLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcelå’ŒFLINK-1.9.0-csa1.0.0.0-cdh6.3.0-el7.parcel.shaæ”¾å…¥/opt/cloudera/parcel-repoä¸­ç„¶åé‡å¯Cloudera Manager ServeræœåŠ¡ï¼šsudo systemctl restart cloudera-scm-serveré‡å¯å®Œæˆåè¿›å…¥é¡µé¢ï¼Œä¸»æœº-&gt;Parcel-&gt;æ£€æŸ¥æ–°Parcel-&gt;æ‰¾åˆ°Flink-&gt;åˆ†é…å®Œæˆåˆ†é…åå¼€å§‹æ·»åŠ æœåŠ¡ï¼š Flink1.9.0ç‰ˆæœ¬æ¯”è¾ƒè€ï¼Œå¯¹Hiveçš„å…¼å®¹ä¸æ˜¯å¾ˆå‹å¥½ï¼Œå¯ä»¥å‚è€ƒï¼šhttps://blog.csdn.net/qq_31454379/article/details/110440037 å®‰è£…Flinkå®˜æ–¹1.12ç‰ˆæœ¬ å…­.åŠŸèƒ½æ‰©å±•è‡ªå®šä¹‰å‘Šè­¦è„šæœ¬ ä¸ƒ.å‘ç‚¹æ€»ç»“ å¦‚æœé‡åˆ°HDFSæ— æ³•å¯åŠ¨çš„é—®é¢˜ï¼Œå¯èƒ½æ˜¯å› ä¸º**/dfs/nn/,/dfs/dn/,/dfs/snn/è¿™äº›ç›®å½•å’Œé‡Œé¢çš„æ–‡ä»¶æƒé™ä¸å¤Ÿï¼Œè¯·æ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹çš„è¿™å‡ ä¸ªç›®å½•ï¼Œä¿è¯nn,dn,snnæ–‡ä»¶å¤¹æƒé™ä¸ºdrwxâ€”â€” 3 hdfs hadoopï¼Œå³hdfsç”¨æˆ·hadoopç»„ï¼Œé‡Œé¢çš„currentæ–‡ä»¶å¤¹çš„æƒé™ä¸ºdrwxr-xr-x 3 hdfs hdfs**ã€‚ æç¤ºError: JAVA_HOME is not set and Java could not be found å…ˆç¡®ä¿JDKå®‰è£…è·¯å¾„åœ¨/usr/java/jdkxxxxxï¼Œå†ç¡®å®šJAVAç‰ˆæœ¬æ˜¯å½“å‰CDHæ”¯æŒçš„JAVAç‰ˆæœ¬ï¼Œè¿‡é«˜è¿‡ä½éƒ½ä¸ä¼šå…¼å®¹ï¼Œå°±æŠ¥è¿™ä¸ªé”™è¯¯ã€‚ The number of live datanodes 2 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. ä¸å¤šè¯´ï¼Œå…³é—­å®‰å…¨æ¨¡å¼ hdfs dfsadmin -safemode leave æ³¨æ„ï¼Œéœ€è¦sudoåˆ°hdfsç”¨æˆ·æ“ä½œ å¦‚æœsudoåˆ°hdfså¤±è´¥ï¼Œå°±vim /etc/passwd å°†hdfsç”¨æˆ·å¯¹åº”çš„/sbin/nologinæ”¹æˆ/bin/bash å³å¯sudoåˆ°hdfs å¯åŠ¨ClouderaManagerServeræŠ¥é”™ INFO main:com.cloudera.server.cmf.bootstrap.EntityManagerFactoryBean: MYSQL database engine and table mapping: &#123;InnoDB=[AUDITS, COMMANDS, CONFIGS, SCHEMA_VERSION]&#125; 2020-03-25 16:27:22,021 WARN main:com.cloudera.server.cmf.bootstrap.EntityManagerFactoryBean: Failed to determine prior version. This is expected if you are starting Cloudera Manager for the first time. Please also ignore any error messages about missing tables. Moving ahead assuming no upgrade: org.hibernate.exception.SQLGrammarException: could not extract ResultSet 2020-03-25 16:27:22,031 INFO main:com.cloudera.enterprise.dbutil.DbUtil: Schema version table already exists. 2020-03-25 16:27:22,032 INFO main:com.cloudera.enterprise.dbutil.DbUtil: DB Schema version 1. 2020-03-25 16:27:22,039 WARN main:org.springframework.context.support.GenericApplicationContext: Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;com.cloudera.server.cmf.TrialState&#39;: Cannot resolve reference to bean &#39;entityManagerFactoryBean&#39; while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;entityManagerFactoryBean&#39;: FactoryBean threw exception on object creation; nested exception is java.lang.RuntimeException: Unable to obtain CM release version. 2020-03-25 16:27:22,040 ERROR main:com.cloudera.server.cmf.Main: Server failed. ä½¿ç”¨/opt/cloudera/cm/schema/scm_prepare_database.shå·¥å…·åˆå§‹åŒ–scmæ•°æ®åº“æ—¶æŠ¥é”™ï¼šwhen @@GLOBAL.ENFORCE_GTID_CONSISTENCY = 1åŸå› æ˜¯æ•°æ®åº“å¯ç”¨äº†gtid_modeï¼Œmy.cnfæœ‰å¦‚ä¸‹é…ç½® gtid_mode = on enforce_gtid_consistency = 1 binlog_gtid_simple_recovery = 1 è§£å†³ï¼šâ‘ æ³¨é‡Šæ‰å¦‚ä¸Šé…ç½®ï¼Œé‡å¯mysql serverï¼Œâ‘¡åˆ æ‰scmåº“å¹¶é‡å»ºï¼Œâ‘¢é‡å¯cloudera-scm-server å³å¯ å‘CMæ·»åŠ hostæ—¶ï¼Œåªæœ‰æŸå‡ å°ä¸»æœºå¯ä»¥è¢«è¯†åˆ«ï¼Œå…¶ä»–ä¸»æœºï¼Œèƒ½æ˜¾ç¤ºå‡ºæ¥ä½†æ˜¯ç°è‰²çš„ï¼Œæç¤ºæ— æ³•è§£æä¸»æœºåç§°è§£å†³ï¼šæ£€æŸ¥/etc/hosts é…ç½®æ˜¯å¦æ­£ç¡® HDFSé«˜å¯ç”¨è¿›å…¥HDFSè§’è‰²ï¼Œæ“ä½œ-&gt;å¯ç”¨High Availability-&gt;é€‰æ‹©ä¸¤ä¸ªNNã€ä¸‰ä¸ªJNèŠ‚ç‚¹ï¼Œä¸‹ä¸€æ­¥ï¼ˆæ³¨æ„éœ€è¦æ ¼å¼åŒ–NNï¼Œé‡å¯HDFSç›¸å…³æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä¾èµ–æœåŠ¡ï¼Œé‡æ–°éƒ¨ç½²å®¢æˆ·ç«¯é…ç½®ï¼‰-&gt;å®Œæˆè¿›å…¥HUE å°†â€œWebç•Œé¢è§’è‰²â€æ”¹ä¸ºhttpfså…³é—­Hiveï¼Œå¤‡ä»½HiveMetastoreæ•°æ®åº“çš„æ•°æ®ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰ï¼Œæ“ä½œ-&gt;æ›´æ–°Hive Metastore NameNodes-&gt;é‡å¯HiveæœåŠ¡ å‡çº§Pythonç‰ˆæœ¬ä¸º3.8cd /opt/software wget https://www.python.org/ftp/python/3.8.5/Python-3.8.5.tgz tar -zxvf Python-3.8.5.tgz xsyncæˆ–scp -r Python-3.8.5æ‹·è´åˆ°å…¶ä»–èŠ‚ç‚¹ï¼Œå¹¶å¯¹æ‰€æœ‰èŠ‚ç‚¹å¦‚ä¸‹æ“ä½œ cd Python-3.8.5 yum update -y yum groupinstall -y &#39;Development Tools&#39; yum install -y gcc openssl-devel bzip2-devel libffi-devel ./configure prefix=/usr/local/python3 make &amp;&amp; make install ls -la /usr/bin/python* vim /usr/bin/yum #!/usr/bin/python æ”¹ä¸º #!/usr/bin/python2 vim /usr/libexec/urlgrabber-ext-down #!/usr/bin/python æ”¹ä¸º #!/usr/bin/python2 mv /usr/bin/python /usr/bin/python_bak ln -s /usr/local/python3/bin/python3.8 /usr/bin/python ln -s /usr/local/python3/bin/python3.8 /usr/bin/python3 mv /usr/bin/pip /usr/bin/pip_bak ln -s /usr/local/python3/bin/pip3.8 /usr/bin/pip3 python -V &amp;&amp; pip3 -V ---------------åç»­æ“ä½œ---------------- /usr/local/python3/bin/python3.8 -m pip install --upgrade pip -i https://pypi.doubanio.com/simple pip3 install pyspark -i https://pypi.doubanio.com/simple spark-envå¢åŠ export PYSPARK_PYTHON=/usr/local/python3/bin/python3.8 pip3 install koalas -i https://pypi.doubanio.com/simple HiveMetastoreServerå¼‚å¸¸è§£å†³å»HMSæœºå™¨æ‰¾/var/log/hiveçœ‹åˆ°å¦‚ä¸‹Erroræ—¥å¿—ï¼šFailed to sync requested HMS notifications up to the event ID xxxxxæŸ¥çœ‹sentry å¼‚å¸¸CounterWaitæºç å‘ç°ä¼ é€’çš„idæ¯” currentid å¤§å¯¼è‡´ä¸€ç›´ç­‰å¾…è¶…æ—¶ï¼Œè¶…æ—¶æ—¶é—´ä¸º200sã€‚CDHå¯ä»¥å¯ç”¨SentryåŒæ­¥ACLæƒé™ï¼Œå¯åŠ¨åHDFSã€Sentryã€HMSä¸‰è€…é—´æƒé™åŒæ­¥çš„æ¶ˆæ¯å¤„ç†ï¼Œçªç„¶å¤§æ‰¹é‡çš„ç›®å½•æƒé™æ¶ˆæ¯éœ€è¦å¤„ç†ï¼Œåå°çº¿ç¨‹å¤„ç†ä¸è¿‡æ¥ï¼Œæ¶ˆæ¯ç§¯å‹å°±ä¼šæŠ¥Failed to sync requested HMS notifications up to the event ID: xxxxxï¼Œè¯¥é”™è¯¯ä¸ä¼šå¯¼è‡´HMSä¸å¯ç”¨ä½†ä¼šå¯¼è‡´å“åº”é€Ÿåº¦å¾ˆæ…¢ã€‚è§£å†³ï¼š sentry_hms_notification_idè¡¨æ’å…¥æœ€å¤§çš„IDï¼Œé‡å¯Sentryå¿½ç•¥æ‰ä¹‹å‰ç§¯å‹çš„æ¶ˆæ¯ è®¾ç½®Sentryå‚æ•°sentry.notification.sync.timeout.msï¼ˆé»˜è®¤200sï¼‰å‚æ•°è°ƒå°è¶…æ—¶æ—¶é—´ï¼Œå‡å°ç­‰å¾…æ—¶é—´ï¼Œç§¯å‹ä¸å¤šçš„è¯å¯ä»¥è®©å®ƒè‡ªè¡Œæ¶ˆè´¹å¤„ç†æ‰ CDHæ·»åŠ å¤–éƒ¨HDFSé›†ç¾¤çš„nameserviceç°åœ¨æ·»åŠ å¯¹å¤–éƒ¨HDFSé›†ç¾¤nameservice-testçš„æ”¯æŒã€‚åœ¨é…ç½®é¡¹hdfs-site.xml çš„ HDFS å®¢æˆ·ç«¯é«˜çº§é…ç½®ä»£ç æ®µï¼ˆå®‰å…¨é˜€ï¼‰ä¸­æ·»åŠ é…ç½® &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;nameservice-dev,nameservice-test&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.nameservice-test&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameservice-test.nn1&lt;/name&gt; &lt;value&gt;test1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameservice-test.nn2&lt;/name&gt; &lt;value&gt;test2:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.nameservice-test&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; impala-shellè„šæœ¬æŠ¥é”™å‡çº§Pythonç‰ˆæœ¬åï¼ŒImpala-shellæŠ¥è¯­æ³•é”™è¯¯åŸpythonå‘½ä»¤æ˜¯python2.7ï¼Œç°pythonå‘½ä»¤æ˜¯python3.8ï¼Œè€Œ/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/bin/impala-shellè„šæœ¬æ²¡è€ƒè™‘ç¯å¢ƒå˜é‡ï¼Œç›´æ¥ç”¨pythonå‘½ä»¤æ‰§è¡Œï¼Œæ•…å‡ºç°è¯­æ³•å…¼å®¹æ€§é—®é¢˜ã€‚è§£å†³ï¼švim /opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/bin/impala-shellå°†æœ€åä¸€è¡Œexec pythonâ€¦æ›¿æ¢ä¸ºexec python2.7 PYTHONPATH=&quot;$&#123;EGG_PATH&#125;$&#123;SHELL_HOME&#125;/gen-py:$&#123;SHELL_HOME&#125;/lib:$&#123;PYTHONPATH&#125;&quot; \\ exec python2.7 $&#123;SHELL_HOME&#125;/impala_shell.py &quot;$@&quot; beelineè¿æ¥Impala1.æ‹·è´ImpalaJDBC41.jaråˆ°/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/hive/auxlib2.beeline -d â€œcom.cloudera.impala.jdbc41.Driverâ€ -u â€œjdbc:impala://one_impalad_ip:21050â€3.å¦‚æœæŠ¥warnï¼šError: [Simba]JDBC Unsupported transaction isolation level: 4. (state=HY000,code=11975) åˆ™åŠ beelineå‚æ•°â€“isolation=default CDHå‡çº§JDKç‰ˆæœ¬ä¸»æœº-&gt; é€‰æ‹©ä¸€å° -&gt; é…ç½® -&gt; Java ä¸»ç›®å½• -&gt; å¡«ä¸Šæ–°çš„jdkè·¯å¾„æ¯å°èŠ‚ç‚¹è®¾ç½®/etc/profileä¸­JAVA_HOMEå’ŒPATHä¸ºæ–°jdkè·¯å¾„ï¼ˆæˆ–è€…ä¿®æ”¹cloudera-scm-server\\cloudera-scm-agentå¯åŠ¨è„šæœ¬ä¸Šæ·»åŠ export JAVA_HOME=xxxï¼‰åœæ­¢æœåŠ¡é‡å¯CMServeré‡å¯å„èŠ‚ç‚¹CM Agentåœ¨é¡µé¢é‡å¯CMServiceå¯åŠ¨æœåŠ¡ éƒ¨ç½²ä¸CDH6ç»„ä»¶ç‰ˆæœ¬å…¼å®¹çš„å¼€æºç‰ˆSpark 2.xCDHä¸æ˜¯è‡ªå¸¦Sparkå—ï¼Œä¸ºä»€ä¹ˆè¦ä½¿ç”¨å¼€æºç‰ˆï¼Ÿå› ä¸ºCDH Sparké˜‰å‰²äº†å¾ˆå¤šSparkçš„åŠŸèƒ½ï¼Œä¸æ”¯æŒspark-sqlå…¥å£å’Œspark-thriftserverå…¥å£ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–åŠŸèƒ½ï¼Œå¯ä»¥å‚è€ƒ:ä½¿ç”¨Spark2.3.xï¼šä¸‹è½½æˆ–ç¼–è¯‘è·å¾—spark2.3.2-bin-hadoop2.6å®˜æ–¹tarballå¯ä»¥ç›´æ¥å…¼å®¹å½“å‰CDHä¸ªç‰ˆæœ¬ã€‚å…¶æºç ä¸­hiveä¾èµ–ç‰ˆæœ¬ä¸º1.2.1.spark2ï¼Œhadoopç‰ˆæœ¬ä¸º2.6.5ï¼Œä½œä¸ºå…¼å®¹hiveã€hadoopçš„å®¢æˆ·ç«¯ã€‚ä½¿ç”¨Spark2.4.x: ä¸‹è½½æˆ–ç¼–è¯‘è·å¾—spark2.4.8-bin-hadoop2.7å®˜æ–¹tarballï¼Œä¸å¯ç›´æ¥å…¼å®¹ï¼Œéœ€è¦æ›¿æ¢jaråŒ…ï¼š echo &quot;export SPARK_HOME=/opt/spark/spark-2.4.8-bin-hadoop2.7&quot; &gt;&gt; /etc/profile;source /etc/profile rm -f $SPARK_HOME:/jars/hadoop-yarn* cp /opt/cloudera/parcels/CDH/jars/hadoop-yarn* $SPARK_HOME:/jars/ cp /opt/cloudera/parcels/CDH/jars/hive-shims-scheduler-2.1.1-cdh6.3.2.jar $SPARK_HOME:/jars/ spark-env.sh JAVA_HOME=/usr/local/jdk1.8.0_181/ HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_DIST_CLASSPATH=$(/opt/cloudera/parcels/CDH/bin/hadoop classpath) export SPARK_LOCAL_DIRS=/data/spark_tmp_data spark-defaults.conf spark.kryoserializer.buffer.max=512m spark.serializer=org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled=false spark.eventLog.dir=file:///data/spark_tmp_data spark.driver.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.executor.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.yarn.am.extraLibraryPath=/hadoop/cloudera/parcels/CDH/lib/hadoop/lib/native spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_181/ spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_181/ spark.local.dir=/data/spark_tmp_data ln -s /etc/hadoop/conf/core-site.xml $SPARK_HOME:/conf/core-site.xmlln -s /etc/hadoop/conf/hdfs-site.xml $SPARK_HOME:/conf/hdfs-site.xmlln -s /etc/hadoop/conf/mapred-site.xml $SPARK_HOME:/conf/mapred-site.xmlln -s /etc/hadoop/conf/yarn-site.xml $SPARK_HOME:/conf/yarn-site.xmlln -s /etc/hive/conf/hive-site.xml $SPARK_HOME:/conf/hive-site.xmlå¦‚æœåœ¨Kerberosé›†ç¾¤å¯ç”¨ThriftServerï¼Œåœ¨hive-site.xmlæ·»åŠ æˆ–ä¿®æ”¹å¦‚ä¸‹å‚æ•° &lt;property&gt; &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- ä»¥ä¸‹ä¸‰é¡¹å¡«å†™HMSæ‰€åœ¨æœåŠ¡å™¨çš„åœ°å€å’ŒHiveServer2æœåŠ¡çš„keytabï¼ŒHS2æœåŠ¡çš„keytabæ–‡ä»¶åˆ°HS2èŠ‚ç‚¹æ‰¾æœ€æ–°/var/run/cloudera-scm-agent/process/ é‡Œé¢çš„hive.keytab --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/cdh02@SMYOA.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/cdh02@SMYOA.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/hadoop/bigdata/kerberos/keytab/hiveserver2_cdh02.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;NONE&lt;/value&gt; &lt;/property&gt; æäº¤ä»»åŠ¡ï¼š cd $SPARK_HOME bin/spark-sql --master yarn sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10002 --queue thrift --master yarn --executor-memory 8g --executor-cores 5 --num-executors 20 [å¯ç”¨Kerberos] sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10002 --queue thrift --master yarn --executor-memory 8g --executor-cores 5 --num-executors 20 --hiveconf hive.server2.authentication.kerberos.keytab /hadoop/bigdata/kerberos/keytab/hiveserver2_cdh02.keytab CDHé›†ç¾¤ä¿®æ”¹IPå°†åŸæ¥çš„192.168.1.xç½‘æ®µä¿®æ”¹ä¸º10.2.5.xç½‘æ®µåœ¨Hyper-Våˆ›å»ºæ–°çš„è™šæ‹Ÿç½‘å¡ æ¨¡æ‹Ÿç½‘å¡åœ°å€å˜æ›´æ‰“å¼€æ§åˆ¶é¢æ¿-&gt;æŸ¥çœ‹ç½‘ç»œè¿æ¥-&gt;æ‰¾åˆ°Cluster è®¾ç½®IPå’ŒDNSè®¾ç½®å¤–ç½‘å…±äº« è®©è™šæ‹Ÿæœºå¯ä»¥è¿æ¥å¤–éƒ¨ç½‘ç»œé‡æ–°å›åˆ°ä¸Šä¸€æ­¥è®¾ç½®IPå’ŒDNSä¿®æ”¹è™šæ‹Ÿæœºè®¾ç½®ç½‘ç»œè®¾ç½®åœæ­¢æ‰€æœ‰clouderaæœåŠ¡ systemctl stop cloudera-scm-server;systemctl stop cloudera-scm-agent;systemctl stop supervisord ä¿®æ”¹å„èŠ‚ç‚¹ipå’Œhostsï¼ˆä»¥CDH101ä¸ºä¾‹ï¼‰ # ä¿®æ”¹ç½‘å¡é…ç½® vim /etc/sysconfig/network-scripts/ifcfg-eth0 å†…å®¹å¦‚ä¸‹ TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no # IPV6INIT=yes # IPV6_AUTOCONF=yes # IPV6_DEFROUTE=yes # IPV6_FAILURE_FATAL=no # IPV6_ADDR_GEN_MODE=stable-privacy NAME=eth0 UUID=b6de3f15-8829-44bc-8743-d3cab3cfb62f DEVICE=eth0 ONBOOT=yes # IPV6_PRIVACY=no NM_CONTROLLED=yes IPADDR=10.2.5.101 NETMASK=255.0.0.0 GATEWAY=10.2.5.2 # DNS1=10.2.5.1 DNS2=10.2.5.2 # ä¿®æ”¹hosts vim /etc/hosts å†…å®¹å¦‚ä¸‹ 10.2.5.101 cdh101 10.2.5.102 cdh102 10.2.5.103 cdh103 10.2.5.104 cdh104 ä¿®æ”¹CMåº“çš„hostsè¡¨ä¸­IP_ADDRESSå­—æ®µä¸ºæ–°IP update scm.HOSTS set IP_ADDRESS = &#39;10.2.5.101&#39; where IP_ADDRESS = &#39;192.168.1.101&#39;; update scm.HOSTS set IP_ADDRESS = &#39;10.2.5.102&#39; where IP_ADDRESS = &#39;192.168.1.102&#39;; update scm.HOSTS set IP_ADDRESS = &#39;10.2.5.103&#39; where IP_ADDRESS = &#39;192.168.1.103&#39;; update scm.HOSTS set IP_ADDRESS = &#39;10.2.5.104&#39; where IP_ADDRESS = &#39;192.168.1.104&#39;; ä¿®æ”¹/etc/hostsä¸‹å¯¹åº”çš„ipæ˜ å°„ä¿®æ”¹Agenté…ç½®æ–‡ä»¶ä¸­server_hostå­—æ®µvim /etc/cloudera-scm-agent/config.ini å€¼ä¸ºcmserverçš„hostnameç„¶åpoweroffå…³æœºä¿®æ”¹è™šæ‹Ÿæœºç½‘å¡ï¼Œæ›¿æ¢ä¸ºDNSä¸º10.2.5.2çš„æ–°çš„è™šæ‹Ÿç½‘å¡å¯åŠ¨èŠ‚ç‚¹ï¼Œåœ¨é¡µé¢é‡å¯ClouderaManagerServiceã€éƒ¨ç½²å„ä¸ªç»„ä»¶çš„å®¢æˆ·ç«¯é…ç½®å³å¯ å‡çº§ä¸CDH6ç»„ä»¶ç‰ˆæœ¬å…¼å®¹çš„å¼€æºç‰ˆSpark 3.xç¼–è¯‘https://blog.csdn.net/Young2018/article/details/108856622éƒ¨ç½²https://www.pianshen.com/article/46531976066/ Cloudera Managerä½¿ç”¨ CDHå¸è½½https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247484118&amp;idx=1&amp;sn=e7109978013a286fe1f172f99459c45a&amp;chksm=ec2ad2dfdb5d5bc96fcaeb6ce2ed42a025e1d95ab251372fb4769c29434f0d84fb66bbfec1d2&amp;scene=21#wechat_redirect","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"https://shmily-qjj.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"CDH6+CentOS7","slug":"CDH6-CentOS7","permalink":"https://shmily-qjj.top/tags/CDH6-CentOS7/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"è®°ä¸€æ¬¡å‚åŠ QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼š","slug":"è®°ä¸€æ¬¡å‚åŠ QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼š","date":"2019-10-21T11:59:50.000Z","updated":"2022-12-11T05:35:07.925Z","comments":true,"path":"39595/","link":"","permalink":"https://shmily-qjj.top/39595/","excerpt":"","text":"QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼šå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼šæ˜¯ä¸ºå›¢é˜Ÿé¢†å¯¼è€…ã€æ¶æ„å¸ˆã€é¡¹ç›®ç»ç†å’Œé«˜çº§è½¯ä»¶å¼€å‘äººå‘˜é‡èº«æ‰“é€ çš„ä¼ä¸šè½¯ä»¶å¼€å‘å¤§ä¼šï¼Œå…¶æ‰€è¦†ç›–çš„ä¸»é¢˜å†…å®¹ä¸InfoQç½‘ç«™ç›¸åŒï¼Œå…³æ³¨æ¶æ„ä¸è®¾è®¡ã€çœŸå®æ¡ˆä¾‹åˆ†æç­‰ç­‰ã€‚ç§‰æ‰¿â€ä¿ƒè¿›è½¯ä»¶å¼€å‘é¢†åŸŸçŸ¥è¯†ä¸åˆ›æ–°çš„ä¼ æ’­â€åŸåˆ™ï¼ŒQConå„é¡¹è®®é¢˜ä¸“ä¸ºä¸­é«˜ç«¯æŠ€æœ¯äººå‘˜è®¾è®¡ï¼Œå†…å®¹æºäºå®è·µå¹¶é¢å‘ç¤¾åŒºã€‚æ¼”è®²å˜‰å®¾ä¾æ®å„é‡ç‚¹å’Œçƒ­ç‚¹è¯é¢˜ï¼Œåˆ†äº«æŠ€æœ¯è¶‹åŠ¿å’Œæœ€ä½³å®è·µã€‚**äº†è§£æ›´å¤šè¯·æˆ³ğŸ‘‰ QConå®˜ç½‘ ** çŠ¹è±«å»ä¸å»QConæ˜¯ä¸ºå›¢é˜Ÿé¢†å¯¼è€…ã€æ¶æ„å¸ˆã€é¡¹ç›®ç»ç†å’Œé«˜çº§è½¯ä»¶å¼€å‘äººå‘˜é‡èº«æ‰“é€ çš„ä¼ä¸šè½¯ä»¶å¼€å‘å¤§ä¼šï¼Œå¦‚æœä½ ä»¬å…¬å¸ä¹°äº†ç¥¨ç»™æœºä¼šå‚åŠ ï¼Œé‚£å°±è¦çæƒœå‘€ï¼çœ‹çœ‹è¿˜æ˜¯å¾ˆæœ‰å¥½å¤„çš„ï¼Œé€‰è‡ªå·±æ„Ÿå…´è¶£çš„æ–¹å‘åœºæ¬¡å‚åŠ ï¼Œå¯ä»¥æ‰©å®½æ€è·¯ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜å…¶å®è¿˜æœ‰æ›´å¤šè§£å†³æ–¹æ¡ˆï¼å“ªæ€•æœ‰å¬ä¸æ‡‚çš„åœ°æ–¹ï¼Œè®°ä¸‹æ¥å›å»æŸ¥éƒ½å¾ˆå—ç›Šï¼ åˆ†äº«æˆ‘æ‰€å¬å› ä¸ºæˆ‘åªå‚åŠ äº†18å·ä¸‹åˆåœºçš„QConï¼Œæ‰€ä»¥ä¹Ÿåªå¬äº†ä¸åˆ°å››ä¸ªåˆ†äº«ä¼š,ä½†æˆ‘ä¼šæŠŠæˆ‘è§‰å¾—å¾ˆæœ‰ç”¨çš„æŠ€æœ¯æˆ–è€…æ€è·¯åˆ†äº«å‡ºæ¥ï¼ Splash Shuffle Manager å…³äºSparkçš„ShuffleShuffleç®€è€Œè¨€ä¹‹:ä¸‹ä¸€ä¸ªStageå‘ä¸Šä¸€ä¸ªStageè¦æ•°æ®è¿™ä¸ªè¿‡ç¨‹ï¼Œå°±ç§°ä¹‹ä¸º Shuffle.å­¦è¿‡Sparkçš„ç«¥é‹éƒ½çŸ¥é“å¤§å¤šæ•°Sparkä½œä¸šçš„è¿è¡Œæ—¶é—´ä¸»è¦æµªè´¹åœ¨Shuflleè¿‡ç¨‹ä¸­,å› ä¸ºè¯¥è¿‡ç¨‹åŒ…å«äº†å¤§é‡çš„æœ¬åœ°ç£ç›˜IO,ç½‘ç»œIOå’Œåºåˆ—åŒ–è¿‡ç¨‹.è€Œçœ‹è¿‡Sparkæºç çš„ç«¥é‹åº”è¯¥éƒ½çŸ¥é“Sparkçš„ShuffleManager,è™½ç„¶Spark2.xå·²ç»æ‘’å¼ƒäº†HashShuffleManager,ä½†æ˜¯å¦‚æœè¿‡å¤§çš„è¡¨é‡åˆ°â€å»é‡â€,â€èšåˆâ€,â€æ’åºâ€,â€é‡åˆ†åŒºâ€æˆ–â€é›†åˆâ€æ“ä½œç­‰shuffleç®—å­æ—¶è¿˜æ˜¯ä¼šæœ‰å¤§é‡æ–‡ä»¶è½ç›˜,è€Œæœ¬åœ°ç£ç›˜çš„æ€§èƒ½ä¼šä¸¥é‡æ‹–æ…¢Sparkè®¡ç®—çš„æ•´ä½“é€Ÿåº¦. è€Œä¸”Shuffleå‘ç”Ÿçš„æœºå™¨å¦‚æœå‘ç”Ÿæ•…éšœè¿˜ä¼šå¯¼è‡´Stageé‡ç®—,æ€§èƒ½å’Œç¨³å®šæ€§éƒ½å¤§å¤§é™ä½æœ‰äº›å¤§è§„æ¨¡çš„è®¡ç®—æ˜¯Shuffleè°ƒä¼˜ä¸èƒ½è§£å†³çš„ Splashä»‹ç»å…³äºä»¥ä¸Šé—®é¢˜,æˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹Shuffle Managerçš„æºç æ¥å®ç°è‡ªå®šä¹‰Shuffleçš„æº¢å†™æ–‡ä»¶å­˜å‚¨ä½ç½®,ä½†æ˜¯,æ”¹æºç è¾£ä¹ˆéš¾â€¦â€¦å’‹åŠâ€¦â€¦Splash-æ”¯æŒæŒ‡å®šShuffleè¿‡ç¨‹æº¢å†™æ–‡ä»¶çš„å­˜å‚¨ä½ç½® å¯ä»¥æŒ‡å®šShuffleæ–‡ä»¶å­˜å‚¨åˆ°é«˜å¯é çš„åˆ†å¸ƒå¼å­˜å‚¨ä¸­ ShuffleFileæ¥å£ä»£æ›¿æœ¬åœ°æ–‡ä»¶è®¿é—® å¯ä»¥ä½¿ç”¨ä¸åŒçš„ç½‘ç»œä¼ è¾“å’Œåç«¯å­˜å‚¨åè®®æ¥å®ç°éšæœºè¯»å–å’Œå†™å…¥ Splash Shuffle Managerä¼˜ç‚¹ ä½¿Executorå˜ä¸ºæ— çŠ¶æ€ ä½¿æ·»åŠ åˆ é™¤èŠ‚ç‚¹æ›´æœ‰çµæ´»æ€§,å®•æœºæ— éœ€é‡å¤è®¡ç®—æ•´ä¸ªshuffleæ–‡ä»¶ Shuffleæ–‡ä»¶æäº¤ç¬¦åˆåŸå­æ€§,æœªæäº¤çš„æ–‡ä»¶å¯ä»¥è½»æ¾æ¸…ç† éšæœºå­˜å‚¨å’Œè®¡ç®—çš„åˆ†ç¦»,æä¾›Shuffleå­˜å‚¨ä»‹è´¨çš„æ›´å¤šé€‰æ‹© Splash Shuffle Managerä½äºExecutorä¸Š,é™ä½éƒ¨ç½²éš¾åº¦ Shuffle Performance Toolå¯ä»¥æ£€éªŒå­˜å‚¨ä»‹è´¨æ€§èƒ½ Splashç»“æ„å’ŒåŸç†å¦‚å›¾è“è‰²æ¡†ä¸ºSplashå®ç°ç±»,æ©™è‰²æ¡†æ˜¯Sparkå®šä¹‰çš„æ¥å£,ç»¿è‰²æ¡†æ˜¯åŸºæœ¬æ•°æ®ç»“æ„ä½¿ç”¨Splashåçš„Shuffleè¿‡ç¨‹:ShuffleManageræ˜¯å…¥å£,ShuffleWriteråœ¨map stageå†™shuffleæ•°æ®,ç”¨SplashSorteræˆ–SplashUnsafeSorterå°†æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­,å†…å­˜ä¸è¶³æ—¶åˆ™ä¼šå°†æ•°æ®æº¢å†™åˆ°TmpShuffleFile,ç­‰æ‰€æœ‰æ•°æ®è®¡ç®—å®Œæˆ,SplashSorteræˆ–SplashUnsafeSorter,åˆå¹¶å†…å­˜æ–‡ä»¶å’Œæº¢å†™æ–‡ä»¶,SplashAggregatorè´Ÿè´£æ•°æ®èšåˆ,ä½¿ç”¨SplashAppendOnlyMapæ•°æ®ç»“æ„,å†…å­˜ä¸å¤Ÿæ—¶æŒä¹…åŒ–åˆ°shuffleæ•°æ®å­˜å‚¨ç³»ç»Ÿ;ShuffleReaderä»shuffleæ•°æ®å­˜å‚¨ç³»ç»Ÿæ”¶é›†reduce stageéœ€è¦çš„æ•°æ®,SplashShuffleBlockResolverç”¨æ¥æŸ¥æ‰¾éšå³æ•°æ®,æ˜¯æ— çŠ¶æ€çš„. æˆ‘è®¤ä¸ºæˆ‘è®¤ä¸ºSplashçš„è®¾è®¡æ¯”è¾ƒç¬¦åˆSparkè®¡ç®—ä¸å­˜å‚¨åˆ†ç¦»çš„ç†å¿µ,æ‰€ä»¥Splashçš„æ€è·¯æ˜¯å¥½çš„,ä½†å±•æœ›Spark3.0çš„æ–°ç‰¹æ€§,ä¹Ÿå‘ç°äº†å…¶å®Spark3.0ä¹Ÿæœ‰ä¸€ä¸ªæ–°ç‰¹æ€§å«â€Remote Shuffle Serviceâ€,Remote Shuffle Serviceçš„åŸºæœ¬æƒ³æ³•æ˜¯ï¼Œå¦‚æœMap Taskèƒ½å°†Shuffleæ•°æ®å†™åˆ°ç‹¬ç«‹çš„ShuffleæœåŠ¡ï¼Œç„¶åReduce Taskä»è¿™ä¸ªShuffleæœåŠ¡è¯»Shuffleæ•°æ®ï¼Œè¿™æ ·è®¡ç®—èŠ‚ç‚¹å°±ä¸å†éœ€è¦ä¸ºShuffleä»»åŠ¡ä¿ç•™æœ¬åœ°ç£ç›˜ç©ºé—´äº†ã€‚è¿™ä¸ªç†å¿µä¸Splashè¿™ä¸ªé¡¹ç›®çš„ç†å¿µå¾ˆç›¸è¿‘,æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°è¯•å’Œéƒ¨ç½²Splashä¹Ÿå¯ä»¥æœŸå¾…Spark3.0çš„æ–°ç‰¹æ€§! æœ€åé™„ä¸Š:Splashé¡¹ç›®çš„Gitåœ°å€ è‹±ç‰¹å°”æŒä¹…å†…å­˜Intel Optane DC Persistent Memory,ä¸“ä¸ºæ•°æ®ä¸­å¿ƒä½¿ç”¨è€Œè®¾è®¡çš„æ–°çš„å†…å­˜å’Œå­˜å‚¨æŠ€æœ¯ç‰¹æ€§å°±æ˜¯æœ‰åª²ç¾DRAMçš„æ€§èƒ½(è¾ƒDRAMç•¥å·®)å’Œæœ‰SSDä¸€èˆ¬çš„å®¹é‡å¤§å°(ç›®å‰å•æ¡512GB),ä¸€å®šç¨‹åº¦æ¶ˆé™¤ååé‡ç“¶é¢ˆå¯¹å¤§æ•°æ®è®¡ç®—è¿˜æ˜¯æœ‰ä¸€å®šåŠ æˆçš„ï¼Œè¿™é‡Œæˆ‘å°±ä¸åšå¹¿å‘Šå•¦ï¼Œæ¯•ç«Ÿæ²¡æœ‰å¹¿å‘Šè´¹å“ˆå“ˆï¼ å…¶ä»– ä»€ä¹ˆDBDK?è¿™æ˜¯æˆ‘å¬åˆ°çš„åè¯,æœäº†ä¸€ä¸‹è§‰å¾—è¿˜æŒºå‰å®³â€¦ä¸èƒ½æ€ªæˆ‘å­¤é™‹å¯¡é—».DBDKå‡ºçš„æ—¶å€™æˆ‘è¿HelloWorldéƒ½è¿˜ä¸ä¼šâ€¦æ•°æ®å¹³é¢å¼€å‘å¥—ä»¶,å¯ä»¥æå¤§æé«˜æ•°æ®å¤„ç†æ€§èƒ½å’Œååé‡ï¼Œæé«˜æ•°æ®å¹³é¢åº”ç”¨ç¨‹åºçš„å·¥ä½œæ•ˆç‡ã€‚DPDKä½¿ç”¨äº†è½®è¯¢(polling)è€Œä¸æ˜¯ä¸­æ–­æ¥å¤„ç†æ•°æ®åŒ…ã€‚åœ¨æ”¶åˆ°æ•°æ®åŒ…æ—¶ï¼Œç»DPDKé‡è½½çš„ç½‘å¡é©±åŠ¨ä¸ä¼šé€šè¿‡ä¸­æ–­é€šçŸ¥CPUï¼Œè€Œæ˜¯ç›´æ¥å°†æ•°æ®åŒ…å­˜å…¥å†…å­˜ï¼Œäº¤ä»˜åº”ç”¨å±‚è½¯ä»¶é€šè¿‡DPDKæä¾›çš„æ¥å£æ¥ç›´æ¥å¤„ç†ï¼Œè¿™æ ·èŠ‚çœäº†å¤§é‡çš„CPUä¸­æ–­æ—¶é—´å’Œå†…å­˜æ‹·è´æ—¶é—´ã€‚ ä»€ä¹ˆæ¨èä¸­å°?æ¨èä¸­å°æ˜¯ä¸ªå¾ˆæ–°çš„åè¯å§â€¦æˆ‘æ²¡æ€ä¹ˆæ¥è§¦æ¨èè¿™å—,å¬çš„è¦ç¡ç€äº†â€¦å—¯,è¿™åœºåˆ†äº«å°±æ˜¯çˆ±å¥‡è‰ºæ¨èä¸­å°â€¦ä¸Šé“¾æ¥å§:çˆ±å¥‡è‰ºæ¨èä¸­å°ï¼šä»æ­å»ºåˆ°ä¸Šçº¿ä»…10å¤©ï¼Œæ•ˆç‡æå‡è¶…30% ä»€ä¹ˆæ•°æ®ä¸­å°?åˆ†ä¸‰éƒ¨åˆ†:æ•°æ®ä»“åº“,å¤§æ•°æ®ä¸­é—´ä»¶,æ•°æ®èµ„äº§ç®¡ç†ä¸»è¦å…ƒç´ :ç´¯è®¡æ¥å…¥åº”ç”¨æ•°,æœåŠ¡è°ƒç”¨,æ•°ä»“æ ¸å¿ƒè¡¨â€¦ä¸»è¦ä½œç”¨:è§£å†³æ•°æ®ç®¡æ§é—®é¢˜,çŸ¥é“è°ç”¨.ç”¨åœ¨å“ªæ•°æ®ä¸­å°èƒ½åŠ›: æ•°æ®èµ„äº§ç®¡ç†,æ•°æ®è´¨é‡ç®¡ç†,æ•°æ®æ¨¡å‹ç®¡ç†,æ„å»ºæ ‡ç­¾ä½“ç³»,æ•°æ®åº”ç”¨è§„åˆ’åŠå®ç°ä¸Šé“¾æ¥å§:ä»€ä¹ˆæ˜¯æ•°æ®ä¸­å°,å…³äºæ•°æ®ä¸­å°æœ€å¥½çš„è§£è¯» å†™åœ¨æœ€åQConè®©æˆ‘å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿,æ‰©å±•äº†æ€è·¯,é¼“åŠ±æˆ‘åœ¨å­¦ä¹ æ–°æŠ€æœ¯çš„é“è·¯ä¸Šå¥‹å‹‡å‘å‰!æ„Ÿè§‰å¯¹æ–°æŠ€æœ¯æ›´åŠ æ„Ÿå…´è¶£äº†,å¦‚æœä¸‹æ¬¡è¿˜æœ‰æœºä¼šå‚åŠ é‚£è¯¥å¤šå¥½å‘€!è¯è¯´æ˜¯ä¸æ˜¯æˆ‘ä¸‹æ¬¡å†å»å°±èƒ½å¬æ‡‚é‚£äº›å¤§ä½¬è¯´çš„ä¸œè¥¿äº†å§â€¦åšä¸ªç®€å•çš„æ€»ç»“å§ å¯¹æ–°æŠ€æœ¯è¦ä»”ç»†è°ƒç ”ï¼Œç¢ç£¨å­˜åœ¨çš„é—®é¢˜ å¯¹äºé™Œç”Ÿçš„æŠ€æœ¯è¦å…ˆæ˜ç™½å®ƒæ˜¯åšä»€ä¹ˆçš„ï¼Œå¯¹æ‰©å®½æ€è·¯æœ‰å¾ˆå¤§å¸®åŠ© ç¼–ç¨‹ä¸ä»…æ˜¯å†™ä»£ç ï¼Œæ›´è¦è€ƒè™‘æ€§èƒ½ï¼Œå®‰å…¨æ€§å’Œç¨³å®šæ€§ ä¸€ä¸ªæ–°æ¶æ„çš„å‡ºç°å¿…ç„¶æœ‰å…¶ä¼˜åŠ¿ï¼Œä»”ç»†æ€è€ƒå®ƒå¸¦æ¥çš„å½±å“ æˆ‘é¡ºä¾¿åœ¨ä¼šåœºå‘¨è¾¹ç©äº†ä¸€åœˆ,è´´å‡ å¼ è‡ªè®¤ä¸ºä¸æ˜¯ç›´ç”·æ‹çš„ç…§ç‰‡:","categories":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼š","slug":"QConå…¨çƒè½¯ä»¶å¼€å‘å¤§ä¼š","permalink":"https://shmily-qjj.top/tags/QCon%E5%85%A8%E7%90%83%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E5%A4%A7%E4%BC%9A/"}],"keywords":[{"name":"æŠ€æœ¯","slug":"æŠ€æœ¯","permalink":"https://shmily-qjj.top/categories/%E6%8A%80%E6%9C%AF/"}]},{"title":"éšæƒ³ç±»åšå®¢-å¾…æ›´æ–°","slug":"éšæƒ³ç±»æ¨¡æ¿","date":"2019-09-21T14:16:00.000Z","updated":"2022-12-11T05:35:07.926Z","comments":true,"path":"14419/","link":"","permalink":"https://shmily-qjj.top/14419/","excerpt":"","text":"ä»Šå¤©çš„è¯é¢˜æ˜¯â€¦. å†…å®¹â€¦â€¦..æ³¨æ„ç»“å°¾ä¸¤ä¸ªç©ºæ ¼ ä¸­æ ‡é¢˜0ä¸­æ ‡é¢˜1ä¸­æ ‡é¢˜2å­—é¢œè‰²å¤§å°This is some text!This is some text!This is some text! æ›´å¤šå†…å®¹: Writing æˆ‘è®¤ä¸ºï¼ˆä¸­æ ‡é¢˜ï¼‰xxxxx å°æ ‡é¢˜å°æ ‡é¢˜0 å­—ä½“ æ–œä½“æ–‡æœ¬*æ–œä½“æ–‡æœ¬ *ç²—ä½“æ–‡æœ¬**ç²—ä½“æ–‡æœ¬ *ç²—æ–œä½“æ–‡æœ¬**ç²—æ–œä½“æ–‡æœ¬å¸¦ä¸‹åˆ’çº¿æ–‡æœ¬ è„šæ³¨ åˆ—è¡¨æ— åºåˆ—è¡¨ç”¨* + -ä¸‰ç§ç¬¦å·è¡¨ç¤º åˆ—è¡¨åµŒå¥— æœ‰åºåˆ—è¡¨ç¬¬ä¸€é¡¹ï¼š ç¬¬ä¸€é¡¹åµŒå¥—çš„ç¬¬ä¸€ä¸ªå…ƒç´  ç¬¬ä¸€é¡¹åµŒå¥—çš„ç¬¬äºŒä¸ªå…ƒç´  æœ‰åºåˆ—è¡¨ç¬¬äºŒé¡¹ï¼š ç¬¬äºŒé¡¹åµŒå¥—çš„ç¬¬ä¸€ä¸ªå…ƒç´  ç¬¬äºŒé¡¹åµŒå¥—çš„ç¬¬äºŒä¸ªå…ƒç´  æœ€å¤šç¬¬ä¸‰å±‚åµŒå¥— æœ€å¤šç¬¬ä¸‰å±‚åµŒå¥— æœ€å¤šç¬¬ä¸‰å±‚åµŒå¥— æ›´å¤šå†…å®¹: Generating æ€»ç»“","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://shmily-qjj.top/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"æ ‡ç­¾1","slug":"æ ‡ç­¾1","permalink":"https://shmily-qjj.top/tags/%E6%A0%87%E7%AD%BE1/"},{"name":"æ ‡ç­¾2","slug":"æ ‡ç­¾2","permalink":"https://shmily-qjj.top/tags/%E6%A0%87%E7%AD%BE2/"}],"keywords":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://shmily-qjj.top/categories/%E9%9A%8F%E6%83%B3/"}]}]}